{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: The Winter is here\n",
    "##### This works best with epic battle music. No spoilers present.\n",
    "<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tywin Lannister was right when he said: \"The great war is between death and life, ice and fire. If we loose, the night will never end\"<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It has been six months since the white walkers' army marched into the north, led by the night king himself on a dead dragon. It has been a battle like never before: never before have men faced such an enemy in battle, never before have men fought so bravely against a united threat, and never before have they been so gravely defeated.<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; While Cersei is in King's landing, brave men have died fighting the great war. Among others, Tyrion is dead, Arya is dead and Jon Snow is dead, again. In a desperate battle, Daenerys leads all her forces in a final stand-off with the dead just south of Winterfell. <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Her army defeated, she is now on the run on her dragon in an air battle, being chased by two of her own dragons, the Night king and a dead Jon Snow. Suddenly, the Night king's spear hits Danny's dragon, who, raining blood and fire, falls into ice, taking the lost queen, with him. <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Daenerys opens her eyes in a strange place, a place which does not follow the rules of space and time, where the dead souls killed by the dead men are trapped, forever. But who woke her up? There stands near her, Tyrion, with Jorah, Davos, Jon Snow, and everybody else. They all indulge in a heartfelt reunion when someone yells- \"But how do we get out?<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Varys sees a talking crystal close by, who asks them of completing a task, which on completion would allow them to go back to the land of the living, with the ultimate tool to defeat the white-walkers and kills the night king, the Dragon-axe. They have summoned you for help, as the task is out of their expertise, to apply a modified CNN to solve the object detection problem on the PASCAL VOC dataset. Varys, the master of whisperers, has used his talents to import the following for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# You can ask Varys to get you more if you desire\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "import PIL.Image\n",
    "import PIL.ImageChops\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import skimage.measure\n",
    "import skimage.morphology\n",
    "from random import randint\n",
    "\n",
    "resnet_input = 224#size of resnet18 input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cersei chose violence, you choose your hyper-parameters wisely using validation data!\n",
    "batch_size = 100\n",
    "num_epochs = 5\n",
    "learning_rate =  0.01\n",
    "hyp_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "The hound who was in charge for getting the data, brought you the following links:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>He also told you that the dataset(datascrolls :P) consists of images from of 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, ie. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the given 20 classes(aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, TV). For parsing the xml file, you can ask Varys to import xml.etree.ElementTree for you. <br/>\n",
    "<br/> You can then ask Bronn and Jamie to organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be 21. This is important for applying the sliding window method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "           'cow', 'diningtable', 'dog', 'horse',\n",
    "           'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taken form pyimagesearch for calculating intersection over union\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    " \n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = (xB - xA + 1) * (yB - yA + 1)\n",
    " \n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    " \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + np.finfo(float).eps)\n",
    " \n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taken from stackoverflow for drawing random bounding boxes\n",
    "def random_bbox(bbox):\n",
    "    v = [randint(0, v) for v in bbox]\n",
    "    left = min(v[0], v[2])\n",
    "    upper = min(v[1], v[3])\n",
    "    right = max(v[0], v[2])\n",
    "    lower = max(v[1], v[3])\n",
    "    return [left, upper, right, lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jamie_bronn_build_dataset(dir,img_path,train):            \n",
    "    image = PIL.Image.open(img_path)\n",
    "    img_name = img_path.split(\"/\")[-1].split(\".\")[-2]\n",
    "    print(img_name)\n",
    "    xml_path = dir+'/Annotations/'+img_name+'.xml'\n",
    "    xml_tree = ET.parse(xml_path)\n",
    "    xml_root = xml_tree.getroot()\n",
    "\n",
    "    temp_img = PIL.Image.new('RGB',image.size,0)\n",
    "#     image.show()\n",
    "    \n",
    "    location = []\n",
    "    labels = []\n",
    "    object_img = []\n",
    "    img_whole = []\n",
    "    for object in xml_root.findall('object'):\n",
    "        name = object.find('name').text\n",
    "        position = [int(object.find('bndbox').find('xmin').text), int(object.find('bndbox').find('ymin').text),\n",
    "                    int(object.find('bndbox').find('xmax').text), int(object.find('bndbox').find('ymax').text)]\n",
    "        location.append(position)\n",
    "        crop_img = image.crop(position).convert('RGB')\n",
    "        object_img.append(crop_img)\n",
    "        labels.append(classes.index(name))\n",
    "        if not train:\n",
    "            img_whole.append(image)\n",
    "        \n",
    "#         temp_img.paste(crop_img,position)\n",
    "\n",
    "#     temp_img = PIL.ImageChops.subtract(image,temp_img)\n",
    "#     l = skimage.morphology.label(np.array(image.convert('L')))\n",
    "#     regions = skimage.measure.regionprops(l)\n",
    "#     max_area = 0\n",
    "#     for region in regions:\n",
    "#         if region.area >= max_area:\n",
    "#             position = region.bbox\n",
    "#             max_area = region.area\n",
    "            \n",
    "#     location.append(position)        \n",
    "#     crop_img = image.crop(position).convert('RGB')\n",
    "# #     crop_img.show()\n",
    "#     object_img.append(crop_img)\n",
    "#     labels.append(classes.index('__background__'))\n",
    "\n",
    "    iou_threshold = 0.3\n",
    "    num = 0\n",
    "    while(num < 1):\n",
    "        bbox = image.getbbox()\n",
    "        boxA = random_bbox(bbox)\n",
    "        mscore = 0\n",
    "        for boxB in location:\n",
    "                score = (bb_intersection_over_union(boxA, boxB))\n",
    "                if (score > mscore):\n",
    "                    mscore = score\n",
    "        if (mscore < iou_threshold):\n",
    "            object_img.append(image.crop(boxA).convert('RGB'))\n",
    "            labels.append(classes.index('__background__'))\n",
    "            if not train:\n",
    "                img_whole.append(image)\n",
    "            num = num + 1\n",
    "        \n",
    "    return object_img, labels, img_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hound_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        print(train)\n",
    "        if(train):\n",
    "            dir = root_dir + '/train/VOCdevkit/VOC2007'\n",
    "        else :\n",
    "            dir = root_dir + '/test/VOCdevkit/VOC2007'\n",
    "        self.transform = transform\n",
    "        self.img = [];\n",
    "        self.label = [];\n",
    "        self.img_whole = []\n",
    "        i = 0\n",
    "        for img_path in glob.glob(dir+'/JPEGImages/*.jpg'):\n",
    "            object_img, name, image = jamie_bronn_build_dataset(dir,img_path,train)\n",
    "            self.img.extend(object_img)\n",
    "            self.label.extend(name)\n",
    "            self.img_whole.extend(image)\n",
    "#             i = i+1\n",
    "#             if i == 7:\n",
    "#                 break\n",
    "                       \n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is None:\n",
    "            if not train:\n",
    "                return (self.img[idx],self.label[idx],self.img_whole[idx])\n",
    "            else:\n",
    "                return (self.img[idx],self.label[idx],-1)\n",
    "        else:\n",
    "            img_transformed = self.transform(self.img[idx])\n",
    "            if not train:\n",
    "                return (img_transformed,self.label[idx],self.img_whole[idx])\n",
    "            else:\n",
    "                return (img_transformed,self.label[idx],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_dataset = hound_dataset(root_dir='.', train=False, transform=None) # Supply proper root_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the netwok\n",
    "<br/>You can ask Arya to train the network on the created dataset. This will yield a classification network on the 21 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "008310\n",
      "001492\n",
      "004514\n",
      "004611\n",
      "003239\n",
      "004685\n",
      "000806\n",
      "008390\n",
      "002933\n",
      "003699\n",
      "001137\n",
      "002520\n",
      "007803\n",
      "007330\n",
      "002056\n",
      "005173\n",
      "004512\n",
      "006346\n",
      "008100\n",
      "007311\n",
      "002036\n",
      "003382\n",
      "007997\n",
      "005058\n",
      "009764\n",
      "006209\n",
      "003790\n",
      "003280\n",
      "001486\n",
      "003618\n",
      "003646\n",
      "002253\n",
      "008213\n",
      "006357\n",
      "007141\n",
      "008771\n",
      "000977\n",
      "007970\n",
      "008076\n",
      "009745\n",
      "009905\n",
      "004111\n",
      "002272\n",
      "004186\n",
      "000524\n",
      "006398\n",
      "000352\n",
      "006424\n",
      "002180\n",
      "004228\n",
      "003783\n",
      "005217\n",
      "000164\n",
      "006468\n",
      "008360\n",
      "004019\n",
      "003337\n",
      "008982\n",
      "008529\n",
      "007897\n",
      "005845\n",
      "009532\n",
      "003299\n",
      "007292\n",
      "001755\n",
      "000270\n",
      "002899\n",
      "005851\n",
      "004140\n",
      "004607\n",
      "004926\n",
      "003135\n",
      "009299\n",
      "004423\n",
      "006725\n",
      "006852\n",
      "007544\n",
      "001442\n",
      "001504\n",
      "000477\n",
      "006687\n",
      "005186\n",
      "003039\n",
      "003824\n",
      "009834\n",
      "007208\n",
      "007132\n",
      "001522\n",
      "002345\n",
      "005487\n",
      "000435\n",
      "005086\n",
      "006355\n",
      "006179\n",
      "002140\n",
      "002858\n",
      "005930\n",
      "008426\n",
      "007138\n",
      "004033\n",
      "003821\n",
      "008318\n",
      "005358\n",
      "000867\n",
      "008069\n",
      "000215\n",
      "000608\n",
      "003654\n",
      "005094\n",
      "001677\n",
      "002969\n",
      "007745\n",
      "007370\n",
      "001523\n",
      "004649\n",
      "006035\n",
      "003094\n",
      "006133\n",
      "006304\n",
      "005517\n",
      "002405\n",
      "000308\n",
      "007474\n",
      "006275\n",
      "007398\n",
      "000726\n",
      "000851\n",
      "001068\n",
      "003017\n",
      "001012\n",
      "006919\n",
      "009285\n",
      "009945\n",
      "008253\n",
      "003260\n",
      "008268\n",
      "005897\n",
      "006314\n",
      "005169\n",
      "001239\n",
      "004999\n",
      "001290\n",
      "005668\n",
      "002699\n",
      "005343\n",
      "001164\n",
      "000949\n",
      "002803\n",
      "006269\n",
      "003554\n",
      "008926\n",
      "004814\n",
      "004815\n",
      "000305\n",
      "003042\n",
      "002129\n",
      "006375\n",
      "000156\n",
      "005790\n",
      "001453\n",
      "003284\n",
      "006699\n",
      "001554\n",
      "006588\n",
      "005838\n",
      "007768\n",
      "005028\n",
      "008026\n",
      "003349\n",
      "002916\n",
      "001475\n",
      "007200\n",
      "004023\n",
      "007971\n",
      "009411\n",
      "007503\n",
      "001950\n",
      "001102\n",
      "000224\n",
      "001614\n",
      "002343\n",
      "002112\n",
      "006291\n",
      "009676\n",
      "002257\n",
      "002589\n",
      "005318\n",
      "005803\n",
      "002896\n",
      "006474\n",
      "005210\n",
      "007258\n",
      "009703\n",
      "001717\n",
      "003376\n",
      "004152\n",
      "002135\n",
      "001860\n",
      "002459\n",
      "003214\n",
      "002287\n",
      "000699\n",
      "003961\n",
      "000849\n",
      "009205\n",
      "000219\n",
      "004974\n",
      "006866\n",
      "009144\n",
      "005457\n",
      "002410\n",
      "000518\n",
      "008342\n",
      "007767\n",
      "006260\n",
      "001617\n",
      "003002\n",
      "005029\n",
      "002460\n",
      "008960\n",
      "009194\n",
      "001858\n",
      "002722\n",
      "002006\n",
      "005220\n",
      "008364\n",
      "008951\n",
      "001203\n",
      "005145\n",
      "004885\n",
      "009018\n",
      "003262\n",
      "005741\n",
      "001185\n",
      "006038\n",
      "008607\n",
      "005828\n",
      "007275\n",
      "004135\n",
      "004984\n",
      "003991\n",
      "006803\n",
      "005483\n",
      "007919\n",
      "003356\n",
      "004950\n",
      "005719\n",
      "005027\n",
      "000312\n",
      "006702\n",
      "009617\n",
      "009155\n",
      "009312\n",
      "009686\n",
      "008985\n",
      "007721\n",
      "000958\n",
      "008012\n",
      "003885\n",
      "001119\n",
      "009496\n",
      "008191\n",
      "000823\n",
      "003912\n",
      "000454\n",
      "004956\n",
      "001166\n",
      "008061\n",
      "002276\n",
      "001082\n",
      "005368\n",
      "008831\n",
      "007519\n",
      "004391\n",
      "005085\n",
      "007877\n",
      "006243\n",
      "005885\n",
      "004632\n",
      "003638\n",
      "007038\n",
      "007381\n",
      "007084\n",
      "000374\n",
      "009860\n",
      "002563\n",
      "005878\n",
      "001933\n",
      "002116\n",
      "006309\n",
      "004566\n",
      "000245\n",
      "002493\n",
      "007884\n",
      "007046\n",
      "008810\n",
      "003300\n",
      "007683\n",
      "001559\n",
      "007663\n",
      "004271\n",
      "004705\n",
      "007048\n",
      "006023\n",
      "007236\n",
      "007883\n",
      "003923\n",
      "009215\n",
      "003147\n",
      "000519\n",
      "001607\n",
      "006587\n",
      "004364\n",
      "007389\n",
      "001902\n",
      "008756\n",
      "004562\n",
      "005185\n",
      "000282\n",
      "001444\n",
      "003754\n",
      "001260\n",
      "003090\n",
      "000680\n",
      "000967\n",
      "006289\n",
      "000929\n",
      "001472\n",
      "001151\n",
      "009289\n",
      "007900\n",
      "000859\n",
      "003732\n",
      "006550\n",
      "008859\n",
      "004671\n",
      "004722\n",
      "009512\n",
      "007482\n",
      "008720\n",
      "000482\n",
      "006674\n",
      "004189\n",
      "003311\n",
      "003150\n",
      "007172\n",
      "005061\n",
      "005360\n",
      "003803\n",
      "005023\n",
      "006466\n",
      "007772\n",
      "000469\n",
      "004093\n",
      "002826\n",
      "002192\n",
      "005985\n",
      "002300\n",
      "003461\n",
      "003497\n",
      "002372\n",
      "007974\n",
      "003054\n",
      "009958\n",
      "003110\n",
      "008958\n",
      "006262\n",
      "000888\n",
      "007535\n",
      "007928\n",
      "003072\n",
      "006026\n",
      "005179\n",
      "009523\n",
      "004991\n",
      "005789\n",
      "009085\n",
      "001160\n",
      "009368\n",
      "006159\n",
      "009859\n",
      "005908\n",
      "009531\n",
      "005471\n",
      "007425\n",
      "006534\n",
      "000829\n",
      "009528\n",
      "004457\n",
      "003868\n",
      "004495\n",
      "008236\n",
      "003407\n",
      "004459\n",
      "003667\n",
      "007421\n",
      "004209\n",
      "006153\n",
      "009698\n",
      "005884\n",
      "006536\n",
      "005812\n",
      "008316\n",
      "007351\n",
      "003781\n",
      "000760\n",
      "007847\n",
      "008760\n",
      "007068\n",
      "008766\n",
      "005424\n",
      "008275\n",
      "004841\n",
      "008883\n",
      "001582\n",
      "008036\n",
      "003511\n",
      "005685\n",
      "007093\n",
      "003675\n",
      "006858\n",
      "003395\n",
      "007263\n",
      "008168\n",
      "005586\n",
      "004274\n",
      "001995\n",
      "007795\n",
      "009197\n",
      "003688\n",
      "006261\n",
      "008876\n",
      "001777\n",
      "002034\n",
      "008482\n",
      "008209\n",
      "007586\n",
      "009614\n",
      "009255\n",
      "002420\n",
      "002366\n",
      "009481\n",
      "000494\n",
      "005956\n",
      "008553\n",
      "005702\n",
      "004876\n",
      "008049\n",
      "000552\n",
      "004539\n",
      "007840\n",
      "007285\n",
      "009007\n",
      "000545\n",
      "009959\n",
      "009587\n",
      "008307\n",
      "006350\n",
      "004117\n",
      "009250\n",
      "002645\n",
      "009600\n",
      "002704\n",
      "000816\n",
      "009778\n",
      "001186\n",
      "008122\n",
      "008186\n",
      "000225\n",
      "005177\n",
      "000908\n",
      "004535\n",
      "004857\n",
      "006124\n",
      "000707\n",
      "007427\n",
      "005430\n",
      "001036\n",
      "008442\n",
      "006878\n",
      "003213\n",
      "007668\n",
      "002807\n",
      "002508\n",
      "003011\n",
      "005566\n",
      "004295\n",
      "000033\n",
      "009537\n",
      "003694\n",
      "008968\n",
      "008422\n",
      "002847\n",
      "002153\n",
      "002367\n",
      "006396\n",
      "000802\n",
      "003714\n",
      "004339\n",
      "006381\n",
      "008032\n",
      "002404\n",
      "001298\n",
      "000555\n",
      "008410\n",
      "002867\n",
      "009721\n",
      "002194\n",
      "001498\n",
      "005736\n",
      "003510\n",
      "008048\n",
      "001501\n",
      "002594\n",
      "005146\n",
      "003259\n",
      "004185\n",
      "001420\n",
      "006065\n",
      "004346\n",
      "009618\n",
      "002096\n",
      "002480\n",
      "001612\n",
      "006666\n",
      "000129\n",
      "006229\n",
      "000198\n",
      "008920\n",
      "008503\n",
      "006753\n",
      "008416\n",
      "001590\n",
      "005478\n",
      "000017\n",
      "002767\n",
      "006067\n",
      "006909\n",
      "000036\n",
      "002472\n",
      "002995\n",
      "009865\n",
      "004370\n",
      "005541\n",
      "002174\n",
      "007072\n",
      "005363\n",
      "002512\n",
      "009303\n",
      "008595\n",
      "004496\n",
      "006777\n",
      "001215\n",
      "008718\n",
      "002693\n",
      "002947\n",
      "002667\n",
      "003320\n",
      "000332\n",
      "002749\n",
      "004963\n",
      "006456\n",
      "004776\n",
      "005764\n",
      "000686\n",
      "003421\n",
      "004968\n",
      "002855\n",
      "004108\n",
      "006175\n",
      "002263\n",
      "007073\n",
      "000879\n",
      "000831\n",
      "001011\n",
      "008293\n",
      "007731\n",
      "001936\n",
      "002658\n",
      "002845\n",
      "008225\n",
      "005875\n",
      "007729\n",
      "006794\n",
      "009641\n",
      "001875\n",
      "008635\n",
      "005229\n",
      "002348\n",
      "002795\n",
      "002766\n",
      "003827\n",
      "009180\n",
      "009151\n",
      "000486\n",
      "007601\n",
      "008060\n",
      "003551\n",
      "005258\n",
      "001765\n",
      "001002\n",
      "009961\n",
      "004834\n",
      "007953\n",
      "009749\n",
      "005732\n",
      "009634\n",
      "003589\n",
      "003926\n",
      "007959\n",
      "001680\n",
      "000896\n",
      "004565\n",
      "004015\n",
      "005370\n",
      "000654\n",
      "002864\n",
      "001901\n",
      "007565\n",
      "000165\n",
      "003359\n",
      "000132\n",
      "003137\n",
      "002690\n",
      "005417\n",
      "007809\n",
      "000431\n",
      "005305\n",
      "009371\n",
      "007222\n",
      "004174\n",
      "005906\n",
      "004517\n",
      "002288\n",
      "004105\n",
      "007064\n",
      "009650\n",
      "000528\n",
      "006947\n",
      "001043\n",
      "002869\n",
      "008884\n",
      "000450\n",
      "008180\n",
      "004910\n",
      "009681\n",
      "006671\n",
      "008987\n",
      "006497\n",
      "000142\n",
      "008336\n",
      "008592\n",
      "003129\n",
      "008723\n",
      "005743\n",
      "005079\n",
      "000154\n",
      "005274\n",
      "005600\n",
      "005761\n",
      "000648\n",
      "008085\n",
      "005369\n",
      "000349\n",
      "006631\n",
      "003294\n",
      "007414\n",
      "002213\n",
      "007095\n",
      "001952\n",
      "005606\n",
      "007009\n",
      "008980\n",
      "003743\n",
      "005467\n",
      "003763\n",
      "007980\n",
      "002091\n",
      "003008\n",
      "004777\n",
      "001174\n",
      "000880\n",
      "006363\n",
      "006270\n",
      "006233\n",
      "003390\n",
      "007130\n",
      "003671\n",
      "006632\n",
      "009491\n",
      "000906\n",
      "004997\n",
      "001468\n",
      "007915\n",
      "003983\n",
      "008483\n",
      "002094\n",
      "007261\n",
      "009053\n",
      "008272\n",
      "001649\n",
      "001594\n",
      "006529\n",
      "009020\n",
      "006259\n",
      "009227\n",
      "007692\n",
      "006404\n",
      "004528\n",
      "007517\n",
      "002786\n",
      "000476\n",
      "009926\n",
      "000805\n",
      "005585\n",
      "007533\n",
      "009136\n",
      "004789\n",
      "009323\n",
      "002374\n",
      "004392\n",
      "001311\n",
      "002812\n",
      "005407\n",
      "002559\n",
      "008468\n",
      "002879\n",
      "006459\n",
      "006827\n",
      "007029\n",
      "008854\n",
      "003374\n",
      "003308\n",
      "000989\n",
      "002958\n",
      "008874\n",
      "006735\n",
      "009878\n",
      "001499\n",
      "009315\n",
      "005413\n",
      "007078\n",
      "008914\n",
      "002976\n",
      "007605\n",
      "003416\n",
      "009776\n",
      "000218\n",
      "005373\n",
      "001676\n",
      "006903\n",
      "000146\n",
      "004325\n",
      "002931\n",
      "008582\n",
      "002369\n",
      "008886\n",
      "009126\n",
      "005841\n",
      "002741\n",
      "001981\n",
      "009605\n",
      "009159\n",
      "008731\n",
      "005273\n",
      "007484\n",
      "000073\n",
      "008871\n",
      "007649\n",
      "005960\n",
      "006111\n",
      "003103\n",
      "008610\n",
      "000755\n",
      "002546\n",
      "009295\n",
      "005620\n",
      "006883\n",
      "009497\n",
      "003433\n",
      "004201\n",
      "008796\n",
      "006860\n",
      "007624\n",
      "007300\n",
      "009867\n",
      "000637\n",
      "001741\n",
      "000520\n",
      "000256\n",
      "007079\n",
      "008692\n",
      "004675\n",
      "005979\n",
      "003430\n",
      "003093\n",
      "008449\n",
      "006509\n",
      "007656\n",
      "004279\n",
      "002166\n",
      "008840\n",
      "002268\n",
      "005269\n",
      "006628\n",
      "000249\n",
      "006530\n",
      "007318\n",
      "006924\n",
      "001233\n",
      "000931\n",
      "003546\n",
      "001778\n",
      "001627\n",
      "005064\n",
      "007022\n",
      "004869\n",
      "002566\n",
      "000408\n",
      "002584\n",
      "002988\n",
      "008391\n",
      "000684\n",
      "009938\n",
      "002193\n",
      "001107\n",
      "003798\n",
      "004244\n",
      "008940\n",
      "002267\n",
      "002593\n",
      "001739\n",
      "004120\n",
      "003398\n",
      "003229\n",
      "008199\n",
      "006216\n",
      "005756\n",
      "005020\n",
      "008445\n",
      "004673\n",
      "006617\n",
      "000298\n",
      "003133\n",
      "009286\n",
      "002423\n",
      "002668\n",
      "005591\n",
      "002844\n",
      "001771\n",
      "007762\n",
      "001464\n",
      "005877\n",
      "000325\n",
      "008280\n",
      "004938\n",
      "003403\n",
      "009809\n",
      "008817\n",
      "006018\n",
      "009691\n",
      "003706\n",
      "003614\n",
      "001008\n",
      "008042\n",
      "002834\n",
      "000605\n",
      "005130\n",
      "003698\n",
      "006718\n",
      "003496\n",
      "001548\n",
      "009684\n",
      "009089\n",
      "009398\n",
      "005419\n",
      "004676\n",
      "002595\n",
      "006611\n",
      "004409\n",
      "002215\n",
      "004636\n",
      "002759\n",
      "002518\n",
      "005550\n",
      "002172\n",
      "008927\n",
      "006939\n",
      "003450\n",
      "003155\n",
      "008132\n",
      "006395\n",
      "006146\n",
      "005697\n",
      "001689\n",
      "006224\n",
      "005469\n",
      "009066\n",
      "003524\n",
      "008900\n",
      "006329\n",
      "008169\n",
      "009049\n",
      "005351\n",
      "004405\n",
      "005441\n",
      "003806\n",
      "006836\n",
      "005984\n",
      "003722\n",
      "007445\n",
      "006264\n",
      "009325\n",
      "008973\n",
      "008235\n",
      "002901\n",
      "005429\n",
      "006727\n",
      "008967\n",
      "006353\n",
      "009667\n",
      "009034\n",
      "008345\n",
      "002184\n",
      "002684\n",
      "005614\n",
      "005899\n",
      "001360\n",
      "005873\n",
      "005510\n",
      "009350\n",
      "000438\n",
      "004850\n",
      "009424\n",
      "003847\n",
      "006625\n",
      "002734\n",
      "002942\n",
      "009939\n",
      "000021\n",
      "009796\n",
      "006300\n",
      "003642\n",
      "004379\n",
      "009611\n",
      "007376\n",
      "008398\n",
      "004727\n",
      "004656\n",
      "008248\n",
      "005755\n",
      "009359\n",
      "004710\n",
      "003100\n",
      "001806\n",
      "007162\n",
      "004256\n",
      "000834\n",
      "006421\n",
      "006761\n",
      "004300\n",
      "008005\n",
      "003704\n",
      "006225\n",
      "008302\n",
      "000591\n",
      "008456\n",
      "002723\n",
      "007390\n",
      "006867\n",
      "009596\n",
      "009283\n",
      "001248\n",
      "007194\n",
      "009064\n",
      "003860\n",
      "007479\n",
      "005006\n",
      "007086\n",
      "000104\n",
      "002099\n",
      "000459\n",
      "004581\n",
      "009746\n",
      "004537\n",
      "001932\n",
      "007798\n",
      "002171\n",
      "008008\n",
      "008738\n",
      "007491\n",
      "002291\n",
      "005951\n",
      "003935\n",
      "003887\n",
      "000685\n",
      "008655\n",
      "004746\n",
      "009273\n",
      "003396\n",
      "007606\n",
      "000347\n",
      "005111\n",
      "002848\n",
      "001490\n",
      "003820\n",
      "009477\n",
      "008430\n",
      "000515\n",
      "004935\n",
      "009882\n",
      "008299\n",
      "006251\n",
      "004494\n",
      "000814\n",
      "005374\n",
      "002468\n",
      "008443\n",
      "008174\n",
      "001934\n",
      "007976\n",
      "005549\n",
      "006704\n",
      "008409\n",
      "004463\n",
      "006864\n",
      "001734\n",
      "002445\n",
      "006922\n",
      "003244\n",
      "003760\n",
      "004404\n",
      "002197\n",
      "007276\n",
      "008075\n",
      "003604\n",
      "000034\n",
      "004438\n",
      "000439\n",
      "008320\n",
      "002134\n",
      "000777\n",
      "005134\n",
      "006697\n",
      "005055\n",
      "008216\n",
      "004647\n",
      "009884\n",
      "008814\n",
      "001515\n",
      "001918\n",
      "003877\n",
      "003674\n",
      "000919\n",
      "005393\n",
      "007023\n",
      "009036\n",
      "000012\n",
      "002689\n",
      "009902\n",
      "003362\n",
      "000885\n",
      "005923\n",
      "004719\n",
      "002247\n",
      "002342\n",
      "003588\n",
      "006800\n",
      "007410\n",
      "005981\n",
      "003066\n",
      "002413\n",
      "004773\n",
      "006855\n",
      "009576\n",
      "001334\n",
      "009743\n",
      "002732\n",
      "008514\n",
      "009609\n",
      "002695\n",
      "009419\n",
      "006475\n",
      "007631\n",
      "000531\n",
      "003161\n",
      "006661\n",
      "001579\n",
      "001144\n",
      "000130\n",
      "006437\n",
      "002579\n",
      "009461\n",
      "001509\n",
      "009247\n",
      "001904\n",
      "000395\n",
      "002385\n",
      "005669\n",
      "005181\n",
      "005116\n",
      "003898\n",
      "005526\n",
      "007763\n",
      "001726\n",
      "000192\n",
      "008478\n",
      "006859\n",
      "001154\n",
      "003487\n",
      "004479\n",
      "000754\n",
      "007004\n",
      "006238\n",
      "006025\n",
      "007059\n",
      "009872\n",
      "001718\n",
      "003947\n",
      "000266\n",
      "002170\n",
      "001350\n",
      "000770\n",
      "005799\n",
      "003869\n",
      "003913\n",
      "008279\n",
      "005605\n",
      "006679\n",
      "004122\n",
      "006499\n",
      "002501\n",
      "009117\n",
      "005928\n",
      "005531\n",
      "006484\n",
      "009458\n",
      "000318\n",
      "009508\n",
      "008654\n",
      "001750\n",
      "004692\n",
      "005416\n",
      "006654\n",
      "007139\n",
      "008847\n",
      "000972\n",
      "004069\n",
      "000461\n",
      "000320\n",
      "002504\n",
      "002561\n",
      "001050\n",
      "004257\n",
      "005236\n",
      "003061\n",
      "002470\n",
      "004500\n",
      "007100\n",
      "000381\n",
      "007619\n",
      "001845\n",
      "005486\n",
      "004433\n",
      "001254\n",
      "001042\n",
      "002307\n",
      "000982\n",
      "003441\n",
      "001643\n",
      "003124\n",
      "001441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "006208\n",
      "000892\n",
      "003767\n",
      "004145\n",
      "006128\n",
      "001014\n",
      "001259\n",
      "006802\n",
      "004195\n",
      "006949\n",
      "002145\n",
      "000464\n",
      "002218\n",
      "008093\n",
      "009836\n",
      "007052\n",
      "003857\n",
      "004307\n",
      "000275\n",
      "002437\n",
      "003159\n",
      "007097\n",
      "009108\n",
      "001555\n",
      "009209\n",
      "007890\n",
      "000516\n",
      "004644\n",
      "009131\n",
      "000794\n",
      "004396\n",
      "000748\n",
      "009039\n",
      "003466\n",
      "002477\n",
      "001877\n",
      "004169\n",
      "004576\n",
      "000592\n",
      "000081\n",
      "009271\n",
      "007621\n",
      "007446\n",
      "005352\n",
      "005856\n",
      "005144\n",
      "007470\n",
      "002954\n",
      "008843\n",
      "005278\n",
      "005796\n",
      "002944\n",
      "001121\n",
      "005404\n",
      "002324\n",
      "006417\n",
      "001024\n",
      "000946\n",
      "004121\n",
      "005542\n",
      "006098\n",
      "001205\n",
      "001978\n",
      "008542\n",
      "000480\n",
      "008425\n",
      "007080\n",
      "007077\n",
      "007727\n",
      "007350\n",
      "003425\n",
      "000934\n",
      "005652\n",
      "000782\n",
      "002967\n",
      "003462\n",
      "006610\n",
      "000047\n",
      "008346\n",
      "000973\n",
      "007753\n",
      "009189\n",
      "009463\n",
      "004911\n",
      "006874\n",
      "004468\n",
      "007667\n",
      "007831\n",
      "006706\n",
      "008653\n",
      "004674\n",
      "001409\n",
      "003021\n",
      "006538\n",
      "008190\n",
      "003218\n",
      "000964\n",
      "004077\n",
      "004253\n",
      "006335\n",
      "003023\n",
      "005786\n",
      "004552\n",
      "004687\n",
      "006840\n",
      "003895\n",
      "002912\n",
      "008106\n",
      "006011\n",
      "009586\n",
      "003997\n",
      "003117\n",
      "003567\n",
      "009908\n",
      "008663\n",
      "008801\n",
      "008115\n",
      "001084\n",
      "009807\n",
      "008068\n",
      "004708\n",
      "007876\n",
      "004524\n",
      "005388\n",
      "007475\n",
      "006965\n",
      "000259\n",
      "008125\n",
      "005700\n",
      "000965\n",
      "009585\n",
      "000899\n",
      "006681\n",
      "007125\n",
      "006562\n",
      "002891\n",
      "009278\n",
      "000276\n",
      "006055\n",
      "000514\n",
      "007558\n",
      "003828\n",
      "003636\n",
      "005191\n",
      "000403\n",
      "002401\n",
      "000372\n",
      "004341\n",
      "006677\n",
      "000996\n",
      "007506\n",
      "000810\n",
      "004788\n",
      "009141\n",
      "005903\n",
      "001413\n",
      "004321\n",
      "008037\n",
      "009543\n",
      "006108\n",
      "008454\n",
      "005328\n",
      "007987\n",
      "001556\n",
      "003127\n",
      "000122\n",
      "002987\n",
      "003628\n",
      "009500\n",
      "007885\n",
      "006427\n",
      "002572\n",
      "006482\n",
      "008688\n",
      "001200\n",
      "009735\n",
      "004502\n",
      "001470\n",
      "005203\n",
      "006198\n",
      "000682\n",
      "006658\n",
      "002881\n",
      "002279\n",
      "003576\n",
      "002452\n",
      "005662\n",
      "008644\n",
      "008523\n",
      "000257\n",
      "001182\n",
      "004073\n",
      "000125\n",
      "001693\n",
      "001314\n",
      "004672\n",
      "004437\n",
      "005636\n",
      "002917\n",
      "000971\n",
      "000993\n",
      "004082\n",
      "008757\n",
      "002880\n",
      "003965\n",
      "006323\n",
      "003140\n",
      "008306\n",
      "005788\n",
      "002913\n",
      "005489\n",
      "008063\n",
      "009502\n",
      "009407\n",
      "002815\n",
      "008727\n",
      "007956\n",
      "001493\n",
      "005970\n",
      "007359\n",
      "008138\n",
      "000728\n",
      "002873\n",
      "001104\n",
      "007551\n",
      "007468\n",
      "007280\n",
      "004411\n",
      "000943\n",
      "002678\n",
      "006400\n",
      "008526\n",
      "003621\n",
      "002281\n",
      "000815\n",
      "000921\n",
      "001628\n",
      "000500\n",
      "007372\n",
      "003963\n",
      "005378\n",
      "002156\n",
      "001524\n",
      "009954\n",
      "004034\n",
      "008300\n",
      "001229\n",
      "003379\n",
      "006932\n",
      "004470\n",
      "008031\n",
      "002054\n",
      "002284\n",
      "006618\n",
      "001268\n",
      "000808\n",
      "007219\n",
      "008177\n",
      "004542\n",
      "009213\n",
      "009196\n",
      "006203\n",
      "002190\n",
      "001231\n",
      "006320\n",
      "001842\n",
      "003837\n",
      "007260\n",
      "002064\n",
      "006409\n",
      "005716\n",
      "002109\n",
      "004191\n",
      "004017\n",
      "001483\n",
      "002234\n",
      "003468\n",
      "003247\n",
      "001989\n",
      "001266\n",
      "006162\n",
      "001841\n",
      "009389\n",
      "005414\n",
      "008518\n",
      "007950\n",
      "003586\n",
      "004430\n",
      "004897\n",
      "003932\n",
      "001861\n",
      "009015\n",
      "005253\n",
      "001828\n",
      "008822\n",
      "001110\n",
      "003793\n",
      "004133\n",
      "002260\n",
      "003121\n",
      "005161\n",
      "003369\n",
      "001531\n",
      "001393\n",
      "008494\n",
      "003145\n",
      "006772\n",
      "006337\n",
      "007109\n",
      "007575\n",
      "008269\n",
      "002859\n",
      "007432\n",
      "001352\n",
      "002641\n",
      "007559\n",
      "009738\n",
      "004131\n",
      "000169\n",
      "009712\n",
      "005905\n",
      "008965\n",
      "008905\n",
      "003419\n",
      "006814\n",
      "000826\n",
      "006595\n",
      "006215\n",
      "009565\n",
      "004859\n",
      "000311\n",
      "008647\n",
      "001250\n",
      "007901\n",
      "003525\n",
      "004882\n",
      "000903\n",
      "006089\n",
      "004571\n",
      "006828\n",
      "007815\n",
      "003051\n",
      "006189\n",
      "002514\n",
      "001247\n",
      "005590\n",
      "002120\n",
      "004585\n",
      "001341\n",
      "008645\n",
      "003360\n",
      "002256\n",
      "007150\n",
      "009306\n",
      "002854\n",
      "006258\n",
      "007279\n",
      "008728\n",
      "004527\n",
      "001938\n",
      "007327\n",
      "000862\n",
      "002063\n",
      "002337\n",
      "009659\n",
      "008558\n",
      "000904\n",
      "007908\n",
      "005434\n",
      "005202\n",
      "001387\n",
      "009037\n",
      "008872\n",
      "004701\n",
      "004986\n",
      "005138\n",
      "002727\n",
      "000845\n",
      "006352\n",
      "001494\n",
      "006043\n",
      "003112\n",
      "003655\n",
      "006622\n",
      "006593\n",
      "003537\n",
      "008470\n",
      "005350\n",
      "004361\n",
      "006130\n",
      "002333\n",
      "007274\n",
      "006281\n",
      "005701\n",
      "004258\n",
      "007223\n",
      "002226\n",
      "004953\n",
      "002633\n",
      "005077\n",
      "000589\n",
      "000269\n",
      "003986\n",
      "002744\n",
      "006916\n",
      "002657\n",
      "009291\n",
      "003408\n",
      "009896\n",
      "005379\n",
      "003509\n",
      "008108\n",
      "007117\n",
      "007212\n",
      "004356\n",
      "006387\n",
      "005839\n",
      "000954\n",
      "005420\n",
      "004368\n",
      "001332\n",
      "003316\n",
      "000026\n",
      "009655\n",
      "003971\n",
      "007451\n",
      "003274\n",
      "009468\n",
      "003727\n",
      "001651\n",
      "000379\n",
      "000705\n",
      "003834\n",
      "004597\n",
      "003874\n",
      "002042\n",
      "000793\n",
      "001638\n",
      "005475\n",
      "004946\n",
      "003969\n",
      "006285\n",
      "007146\n",
      "008638\n",
      "009420\n",
      "005779\n",
      "009094\n",
      "001299\n",
      "001143\n",
      "009365\n",
      "002915\n",
      "003350\n",
      "002340\n",
      "006824\n",
      "008995\n",
      "008742\n",
      "005686\n",
      "005292\n",
      "000554\n",
      "005307\n",
      "004011\n",
      "007751\n",
      "007843\n",
      "009437\n",
      "007932\n",
      "003013\n",
      "005980\n",
      "001170\n",
      "008911\n",
      "000433\n",
      "009935\n",
      "004421\n",
      "000950\n",
      "003907\n",
      "003866\n",
      "004905\n",
      "000304\n",
      "009244\n",
      "005840\n",
      "003470\n",
      "007259\n",
      "009869\n",
      "002966\n",
      "004973\n",
      "001633\n",
      "004992\n",
      "003844\n",
      "007016\n",
      "008312\n",
      "002906\n",
      "001273\n",
      "001708\n",
      "008823\n",
      "001837\n",
      "003105\n",
      "009913\n",
      "001125\n",
      "008710\n",
      "008202\n",
      "001892\n",
      "003634\n",
      "004548\n",
      "002680\n",
      "003518\n",
      "006935\n",
      "003204\n",
      "008909\n",
      "005244\n",
      "000203\n",
      "009479\n",
      "007864\n",
      "009567\n",
      "008856\n",
      "008975\n",
      "001281\n",
      "001688\n",
      "005625\n",
      "000370\n",
      "009434\n",
      "003774\n",
      "004376\n",
      "003960\n",
      "007718\n",
      "000134\n",
      "009794\n",
      "008667\n",
      "002529\n",
      "007325\n",
      "006463\n",
      "008043\n",
      "002321\n",
      "001287\n",
      "004742\n",
      "000463\n",
      "008220\n",
      "000296\n",
      "006841\n",
      "005346\n",
      "003994\n",
      "001272\n",
      "008628\n",
      "006657\n",
      "009343\n",
      "004102\n",
      "008879\n",
      "009432\n",
      "002465\n",
      "001727\n",
      "000131\n",
      "009177\n",
      "004630\n",
      "009647\n",
      "006963\n",
      "005653\n",
      "005397\n",
      "008241\n",
      "001526\n",
      "005735\n",
      "004389\n",
      "008833\n",
      "008722\n",
      "009636\n",
      "009123\n",
      "000690\n",
      "002794\n",
      "001418\n",
      "007859\n",
      "004269\n",
      "008067\n",
      "004612\n",
      "001768\n",
      "009710\n",
      "000141\n",
      "006765\n",
      "005639\n",
      "005440\n",
      "005867\n",
      "001018\n",
      "001899\n",
      "008294\n",
      "003177\n",
      "004587\n",
      "006339\n",
      "007191\n",
      "006575\n",
      "009413\n",
      "001485\n",
      "001878\n",
      "005576\n",
      "007424\n",
      "008633\n",
      "006321\n",
      "002603\n",
      "005544\n",
      "004682\n",
      "001289\n",
      "005176\n",
      "001787\n",
      "007579\n",
      "006027\n",
      "008465\n",
      "008978\n",
      "000448\n",
      "000742\n",
      "006091\n",
      "008535\n",
      "000121\n",
      "005418\n",
      "004359\n",
      "006030\n",
      "005507\n",
      "004662\n",
      "002938\n",
      "007213\n",
      "000303\n",
      "006602\n",
      "009950\n",
      "007639\n",
      "003047\n",
      "000661\n",
      "009518\n",
      "000337\n",
      "006647\n",
      "003970\n",
      "000620\n",
      "003987\n",
      "009733\n",
      "003271\n",
      "007855\n",
      "001443\n",
      "004365\n",
      "001870\n",
      "007148\n",
      "007709\n",
      "008819\n",
      "003918\n",
      "000474\n",
      "008970\n",
      "003032\n",
      "002179\n",
      "006249\n",
      "002104\n",
      "007863\n",
      "000912\n",
      "004436\n",
      "006829\n",
      "007438\n",
      "003791\n",
      "000363\n",
      "003363\n",
      "000579\n",
      "006825\n",
      "003077\n",
      "002011\n",
      "003865\n",
      "003826\n",
      "007227\n",
      "001310\n",
      "009562\n",
      "002021\n",
      "003088\n",
      "007297\n",
      "007576\n",
      "008437\n",
      "002273\n",
      "001711\n",
      "008695\n",
      "006450\n",
      "000328\n",
      "003703\n",
      "002015\n",
      "005248\n",
      "009469\n",
      "008784\n",
      "000354\n",
      "005831\n",
      "001091\n",
      "005648\n",
      "005695\n",
      "004149\n",
      "000236\n",
      "009465\n",
      "004896\n",
      "004193\n",
      "009756\n",
      "009080\n",
      "009418\n",
      "004831\n",
      "007538\n",
      "001881\n",
      "005613\n",
      "001112\n",
      "008961\n",
      "009098\n",
      "000740\n",
      "009557\n",
      "004520\n",
      "002362\n",
      "004345\n",
      "005909\n",
      "005522\n",
      "006272\n",
      "007008\n",
      "008917\n",
      "000078\n",
      "008944\n",
      "001145\n",
      "006782\n",
      "002717\n",
      "006690\n",
      "002776\n",
      "004171\n",
      "007036\n",
      "002158\n",
      "001816\n",
      "000210\n",
      "009619\n",
      "005270\n",
      "004976\n",
      "001759\n",
      "004142\n",
      "004754\n",
      "008332\n",
      "009894\n",
      "009230\n",
      "009133\n",
      "002757\n",
      "009726\n",
      "007612\n",
      "000380\n",
      "004526\n",
      "008116\n",
      "003429\n",
      "007898\n",
      "005514\n",
      "003911\n",
      "005968\n",
      "005230\n",
      "006344\n",
      "000329\n",
      "003808\n",
      "004903\n",
      "004129\n",
      "009900\n",
      "009480\n",
      "004929\n",
      "007308\n",
      "006306\n",
      "007905\n",
      "006070\n",
      "009281\n",
      "001678\n",
      "008292\n",
      "007820\n",
      "005143\n",
      "006472\n",
      "008208\n",
      "005383\n",
      "007812\n",
      "002558\n",
      "008327\n",
      "009456\n",
      "004604\n",
      "007568\n",
      "003521\n",
      "008933\n",
      "008586\n",
      "007996\n",
      "004718\n",
      "005729\n",
      "008415\n",
      "008999\n",
      "005952\n",
      "009238\n",
      "002302\n",
      "008189\n",
      "000695\n",
      "009819\n",
      "001214\n",
      "008536\n",
      "002058\n",
      "000761\n",
      "003876\n",
      "002241\n",
      "004009\n",
      "005057\n",
      "005715\n",
      "009499\n",
      "009520\n",
      "000107\n",
      "005508\n",
      "002393\n",
      "006319\n",
      "007585\n",
      "000771\n",
      "002683\n",
      "001384\n",
      "009072\n",
      "002540\n",
      "002165\n",
      "009670\n",
      "008232\n",
      "006910\n",
      "003343\n",
      "007821\n",
      "009767\n",
      "003464\n",
      "007998\n",
      "009079\n",
      "007105\n",
      "002625\n",
      "000359\n",
      "005045\n",
      "008709\n",
      "006001\n",
      "002043\n",
      "004634\n",
      "004484\n",
      "000667\n",
      "003871\n",
      "008556\n",
      "008137\n",
      "007810\n",
      "000246\n",
      "007724\n",
      "009354\n",
      "000489\n",
      "008317\n",
      "008355\n",
      "004555\n",
      "000462\n",
      "003107\n",
      "007791\n",
      "001292\n",
      "009331\n",
      "004434\n",
      "006066\n",
      "004916\n",
      "000228\n",
      "002715\n",
      "004913\n",
      "000812\n",
      "009785\n",
      "002270\n",
      "006520\n",
      "001732\n",
      "001426\n",
      "002454\n",
      "002224\n",
      "001854\n",
      "003219\n",
      "009270\n",
      "008381\n",
      "005438\n",
      "002039\n",
      "008639\n",
      "008223\n",
      "003539\n",
      "003979\n",
      "006458\n",
      "002774\n",
      "002368\n",
      "004242\n",
      "003620\n",
      "002308\n",
      "006553\n",
      "000382\n",
      "004349\n",
      "008296\n",
      "003780\n",
      "005815\n",
      "001849\n",
      "006210\n",
      "005806\n",
      "002935\n",
      "009679\n",
      "000787\n",
      "005893\n",
      "004931\n",
      "006449\n",
      "009494\n",
      "000019\n",
      "009378\n",
      "007670\n",
      "006455\n",
      "003915\n",
      "003921\n",
      "009825\n",
      "002649\n",
      "002990\n",
      "006911\n",
      "008775\n",
      "007302\n",
      "007671\n",
      "000400\n",
      "008244\n",
      "007244\n",
      "003098\n",
      "006987\n",
      "005297\n",
      "001106\n",
      "008313\n",
      "000338\n",
      "000110\n",
      "007481\n",
      "000799\n",
      "007523\n",
      "001467\n",
      "007243\n",
      "000714\n",
      "003176\n",
      "002653\n",
      "008572\n",
      "004192\n",
      "005658\n",
      "000898\n",
      "009620\n",
      "005783\n",
      "009032\n",
      "007054\n",
      "002191\n",
      "000509\n",
      "001097\n",
      "003608\n",
      "003713\n",
      "007454\n",
      "000420\n",
      "009597\n",
      "005631\n",
      "004808\n",
      "005246\n",
      "001113\n",
      "008101\n",
      "000847\n",
      "003700\n",
      "005859\n",
      "003256\n",
      "000577\n",
      "004591\n",
      "005052\n",
      "004735\n",
      "000878\n",
      "008620\n",
      "008372\n",
      "008698\n",
      "006945\n",
      "009801\n",
      "001738\n",
      "006221\n",
      "008621\n",
      "006020\n",
      "006276\n",
      "005340\n",
      "002555\n",
      "003528\n",
      "002047\n",
      "008376\n",
      "000947\n",
      "005242\n",
      "009394\n",
      "002952\n",
      "007101\n",
      "006445\n",
      "002934\n",
      "002718\n",
      "001465\n",
      "001345\n",
      "007600\n",
      "001920\n",
      "008929\n",
      "000109\n",
      "002277\n",
      "002775\n",
      "007536\n",
      "008141\n",
      "000061\n",
      "002738\n",
      "006097\n",
      "003946\n",
      "000093\n",
      "004737\n",
      "009105\n",
      "001066\n",
      "007566\n",
      "004369\n",
      "001140\n",
      "000613\n",
      "006150\n",
      "006989\n",
      "000763\n",
      "007104\n",
      "004962\n",
      "004168\n",
      "008001\n",
      "006900\n",
      "009542\n",
      "001286\n",
      "002578\n",
      "005988\n",
      "008103\n",
      "009405\n",
      "006136\n",
      "002599\n",
      "009709\n",
      "002259\n",
      "007314\n",
      "001390\n",
      "001397\n",
      "004286\n",
      "006799\n",
      "001434\n",
      "003007\n",
      "005259\n",
      "008794\n",
      "007163\n",
      "000660\n",
      "008521\n",
      "002800\n",
      "007271\n",
      "000171\n",
      "000563\n",
      "006325\n",
      "001927\n",
      "001430\n",
      "002186\n",
      "006442\n",
      "000501\n",
      "009524\n",
      "006914\n",
      "006944\n",
      "003580\n",
      "005072\n",
      "009422\n",
      "001691\n",
      "005001\n",
      "007214\n",
      "000051\n",
      "008495\n",
      "000767\n",
      "001970\n",
      "000066\n",
      "005584\n",
      "005007\n",
      "000180\n",
      "006880\n",
      "005421\n",
      "006507\n",
      "004380\n",
      "007480\n",
      "002953\n",
      "009347\n",
      "002155\n",
      "002347\n",
      "007435\n",
      "006201\n",
      "009805\n",
      "008171\n",
      "007694\n",
      "001976\n",
      "001662\n",
      "007088\n",
      "007699\n",
      "003162\n",
      "006751\n",
      "007419\n",
      "007940\n",
      "002804\n",
      "006123\n",
      "006220\n",
      "003163\n",
      "003663\n",
      "009185\n",
      "008127\n",
      "001056\n",
      "002946\n",
      "002634\n",
      "002497\n",
      "004158\n",
      "001327\n",
      "007954\n",
      "007149\n",
      "005160\n",
      "006148\n",
      "002069\n",
      "004432\n",
      "002310\n",
      "002427\n",
      "006959\n",
      "003848\n",
      "005454\n",
      "009068\n",
      "004654\n",
      "003444\n",
      "009157\n",
      "008466\n",
      "000089\n",
      "003587\n",
      "009059\n",
      "007224\n",
      "004987\n",
      "003184\n",
      "004995\n",
      "001236\n",
      "002290\n",
      "002567\n",
      "006769\n",
      "002086\n",
      "007910\n",
      "000756\n",
      "007886\n",
      "004481\n",
      "001192\n",
      "004936\n",
      "005405\n",
      "002355\n",
      "007947\n",
      "004498\n",
      "001385\n",
      "007294\n",
      "001463\n",
      "008744\n",
      "001642\n",
      "009078\n",
      "008319\n",
      "002479\n",
      "003355\n",
      "005723\n",
      "004707\n",
      "009718\n",
      "006480\n",
      "008759\n",
      "009577\n",
      "003992\n",
      "003695\n",
      "004037\n",
      "001234\n",
      "005110\n",
      "006069\n",
      "003835\n",
      "009161\n",
      "001636\n",
      "003644\n",
      "002178\n",
      "000635\n",
      "005208\n",
      "003585\n",
      "002785\n",
      "009272\n",
      "007289\n",
      "007284\n",
      "008461\n",
      "005410\n",
      "000582\n",
      "008564\n",
      "004706\n",
      "007528\n",
      "006703\n",
      "000730\n",
      "006301\n",
      "004284\n",
      "009168\n",
      "004371\n",
      "004047\n",
      "001172\n",
      "005964\n",
      "001898\n",
      "005283\n",
      "009308\n",
      "006950\n",
      "006305\n",
      "009290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009476\n",
      "007678\n",
      "002481\n",
      "003338\n",
      "006682\n",
      "009868\n",
      "003335\n",
      "001544\n",
      "005911\n",
      "006519\n",
      "001593\n",
      "006849\n",
      "002126\n",
      "004452\n",
      "003721\n",
      "008988\n",
      "004579\n",
      "002632\n",
      "001176\n",
      "005825\n",
      "002311\n",
      "004441\n",
      "001045\n",
      "008096\n",
      "006523\n",
      "000060\n",
      "004627\n",
      "001127\n",
      "002067\n",
      "003038\n",
      "002932\n",
      "004474\n",
      "002228\n",
      "004110\n",
      "001684\n",
      "004281\n",
      "007672\n",
      "000407\n",
      "002139\n",
      "007040\n",
      "006933\n",
      "003849\n",
      "001603\n",
      "001827\n",
      "008098\n",
      "006214\n",
      "004025\n",
      "008250\n",
      "000321\n",
      "008596\n",
      "004998\n",
      "000417\n",
      "001948\n",
      "007521\n",
      "004544\n",
      "000625\n",
      "008044\n",
      "007211\n",
      "009613\n",
      "004439\n",
      "003949\n",
      "000367\n",
      "003435\n",
      "009832\n",
      "007611\n",
      "002611\n",
      "006046\n",
      "005408\n",
      "006696\n",
      "008576\n",
      "008942\n",
      "004239\n",
      "004170\n",
      "000997\n",
      "002571\n",
      "004013\n",
      "004584\n",
      "001136\n",
      "009792\n",
      "005655\n",
      "003057\n",
      "001982\n",
      "002682\n",
      "006570\n",
      "009823\n",
      "007411\n",
      "006512\n",
      "007498\n",
      "002836\n",
      "006171\n",
      "006436\n",
      "007230\n",
      "003603\n",
      "003202\n",
      "008699\n",
      "006103\n",
      "000882\n",
      "005910\n",
      "008755\n",
      "007493\n",
      "004595\n",
      "004588\n",
      "007007\n",
      "002212\n",
      "008509\n",
      "009810\n",
      "009163\n",
      "005398\n",
      "006369\n",
      "005519\n",
      "003548\n",
      "005918\n",
      "004384\n",
      "003412\n",
      "006074\n",
      "000233\n",
      "003045\n",
      "006548\n",
      "009940\n",
      "002884\n",
      "003108\n",
      "007058\n",
      "004912\n",
      "009221\n",
      "008258\n",
      "009091\n",
      "002637\n",
      "007197\n",
      "004872\n",
      "007712\n",
      "009016\n",
      "002810\n",
      "001406\n",
      "008433\n",
      "005536\n",
      "001537\n",
      "009406\n",
      "008040\n",
      "003491\n",
      "006134\n",
      "000800\n",
      "005784\n",
      "005097\n",
      "007033\n",
      "005854\n",
      "008783\n",
      "009193\n",
      "000522\n",
      "001069\n",
      "006839\n",
      "002417\n",
      "001274\n",
      "008263\n",
      "008979\n",
      "009309\n",
      "002745\n",
      "001378\n",
      "004825\n",
      "004743\n",
      "006028\n",
      "006698\n",
      "003083\n",
      "002492\n",
      "009330\n",
      "006411\n",
      "001553\n",
      "005592\n",
      "006564\n",
      "001980\n",
      "005618\n",
      "006061\n",
      "002352\n",
      "000610\n",
      "004699\n",
      "009421\n",
      "001793\n",
      "009282\n",
      "003392\n",
      "004842\n",
      "004315\n",
      "007921\n",
      "004003\n",
      "000140\n",
      "009316\n",
      "003556\n",
      "006419\n",
      "004683\n",
      "009439\n",
      "004089\n",
      "001801\n",
      "001445\n",
      "005682\n",
      "006948\n",
      "006418\n",
      "002838\n",
      "005465\n",
      "009148\n",
      "005042\n",
      "006236\n",
      "009337\n",
      "007071\n",
      "000221\n",
      "003717\n",
      "004310\n",
      "001294\n",
      "007011\n",
      "000622\n",
      "008550\n",
      "002755\n",
      "006678\n",
      "006125\n",
      "002387\n",
      "009507\n",
      "009027\n",
      "000675\n",
      "007140\n",
      "000549\n",
      "005637\n",
      "009019\n",
      "002816\n",
      "005281\n",
      "006549\n",
      "003627\n",
      "003027\n",
      "008562\n",
      "009644\n",
      "009181\n",
      "009150\n",
      "004631\n",
      "007968\n",
      "007713\n",
      "005868\n",
      "007642\n",
      "006385\n",
      "008453\n",
      "006995\n",
      "004146\n",
      "009490\n",
      "004958\n",
      "003708\n",
      "008139\n",
      "003331\n",
      "006652\n",
      "005990\n",
      "002537\n",
      "006670\n",
      "004190\n",
      "002842\n",
      "006739\n",
      "002747\n",
      "005577\n",
      "003629\n",
      "006202\n",
      "009571\n",
      "000689\n",
      "009224\n",
      "003797\n",
      "002266\n",
      "006129\n",
      "003856\n",
      "003354\n",
      "006968\n",
      "003307\n",
      "003465\n",
      "001521\n",
      "005189\n",
      "009362\n",
      "001414\n",
      "001532\n",
      "006943\n",
      "001316\n",
      "000598\n",
      "001001\n",
      "005039\n",
      "008329\n",
      "004966\n",
      "007147\n",
      "006884\n",
      "003261\n",
      "009191\n",
      "004553\n",
      "003449\n",
      "009621\n",
      "005948\n",
      "005107\n",
      "002209\n",
      "002024\n",
      "004272\n",
      "001480\n",
      "005582\n",
      "005971\n",
      "001789\n",
      "004628\n",
      "001622\n",
      "002478\n",
      "000717\n",
      "001945\n",
      "009816\n",
      "000470\n",
      "008506\n",
      "009045\n",
      "007121\n",
      "003064\n",
      "004826\n",
      "000937\n",
      "006377\n",
      "004570\n",
      "001010\n",
      "008502\n",
      "001168\n",
      "006896\n",
      "006172\n",
      "005222\n",
      "009433\n",
      "001201\n",
      "004715\n",
      "006440\n",
      "003272\n",
      "004429\n",
      "004237\n",
      "001529\n",
      "005975\n",
      "005254\n",
      "003984\n",
      "006391\n",
      "005647\n",
      "009828\n",
      "006506\n",
      "001843\n",
      "002318\n",
      "003004\n",
      "008498\n",
      "006560\n",
      "000804\n",
      "009324\n",
      "002542\n",
      "000143\n",
      "003622\n",
      "005062\n",
      "006181\n",
      "005056\n",
      "005568\n",
      "009706\n",
      "005728\n",
      "005264\n",
      "003658\n",
      "003773\n",
      "005081\n",
      "006931\n",
      "002505\n",
      "006643\n",
      "002095\n",
      "004008\n",
      "001977\n",
      "002419\n",
      "002564\n",
      "009516\n",
      "000187\n",
      "007958\n",
      "008891\n",
      "009246\n",
      "002083\n",
      "007050\n",
      "003391\n",
      "009214\n",
      "000677\n",
      "001348\n",
      "007394\n",
      "005992\n",
      "002012\n",
      "004786\n",
      "005641\n",
      "002354\n",
      "005805\n",
      "002992\n",
      "009904\n",
      "005829\n",
      "001408\n",
      "004229\n",
      "003905\n",
      "005285\n",
      "008865\n",
      "009656\n",
      "004466\n",
      "005563\n",
      "006438\n",
      "008082\n",
      "008533\n",
      "007793\n",
      "006041\n",
      "007746\n",
      "006348\n",
      "005395\n",
      "007704\n",
      "007814\n",
      "002101\n",
      "009917\n",
      "008285\n",
      "005263\n",
      "006781\n",
      "006893\n",
      "002125\n",
      "007056\n",
      "009685\n",
      "005214\n",
      "001061\n",
      "005036\n",
      "009772\n",
      "006719\n",
      "000005\n",
      "004805\n",
      "002265\n",
      "005713\n",
      "004051\n",
      "008427\n",
      "000336\n",
      "002697\n",
      "005014\n",
      "001654\n",
      "006599\n",
      "008083\n",
      "008203\n",
      "008615\n",
      "001595\n",
      "003681\n",
      "009160\n",
      "008793\n",
      "001073\n",
      "000158\n",
      "002136\n",
      "005245\n",
      "003044\n",
      "009944\n",
      "003536\n",
      "009445\n",
      "003566\n",
      "001576\n",
      "005325\n",
      "001079\n",
      "008261\n",
      "002835\n",
      "008151\n",
      "004651\n",
      "006105\n",
      "003684\n",
      "003157\n",
      "001064\n",
      "004057\n",
      "008284\n",
      "006667\n",
      "004255\n",
      "007413\n",
      "007457\n",
      "006551\n",
      "008534\n",
      "007133\n",
      "007039\n",
      "008197\n",
      "009717\n",
      "001539\n",
      "001074\n",
      "007416\n",
      "007715\n",
      "005090\n",
      "003063\n",
      "008130\n",
      "003651\n",
      "005288\n",
      "008200\n",
      "003122\n",
      "002779\n",
      "002627\n",
      "002886\n",
      "003258\n",
      "001721\n",
      "007074\n",
      "000583\n",
      "002117\n",
      "004592\n",
      "005963\n",
      "002893\n",
      "007760\n",
      "003186\n",
      "000232\n",
      "004052\n",
      "002647\n",
      "005680\n",
      "004549\n",
      "002943\n",
      "000344\n",
      "003861\n",
      "006095\n",
      "009401\n",
      "007555\n",
      "002648\n",
      "007590\n",
      "006183\n",
      "006983\n",
      "004220\n",
      "004902\n",
      "008585\n",
      "007189\n",
      "002233\n",
      "005104\n",
      "006664\n",
      "009808\n",
      "001142\n",
      "006250\n",
      "004092\n",
      "007615\n",
      "001199\n",
      "002721\n",
      "005290\n",
      "005026\n",
      "001270\n",
      "005345\n",
      "001597\n",
      "007216\n",
      "009880\n",
      "007167\n",
      "003814\n",
      "009732\n",
      "002285\n",
      "006892\n",
      "002248\n",
      "000491\n",
      "003625\n",
      "001928\n",
      "006139\n",
      "007344\n",
      "007834\n",
      "009851\n",
      "002391\n",
      "001062\n",
      "009446\n",
      "002817\n",
      "000523\n",
      "008923\n",
      "002523\n",
      "009517\n",
      "009638\n",
      "006166\n",
      "001258\n",
      "004204\n",
      "008450\n",
      "001543\n",
      "000936\n",
      "002545\n",
      "008769\n",
      "005781\n",
      "009457\n",
      "000209\n",
      "004714\n",
      "003000\n",
      "008676\n",
      "000387\n",
      "004066\n",
      "007765\n",
      "009747\n",
      "009327\n",
      "009005\n",
      "006726\n",
      "002361\n",
      "004318\n",
      "003086\n",
      "003846\n",
      "000483\n",
      "005615\n",
      "009949\n",
      "009288\n",
      "001598\n",
      "006747\n",
      "008813\n",
      "006731\n",
      "000343\n",
      "008229\n",
      "004087\n",
      "004750\n",
      "003415\n",
      "009242\n",
      "008297\n",
      "007460\n",
      "001221\n",
      "008444\n",
      "000543\n",
      "002524\n",
      "005371\n",
      "006473\n",
      "005881\n",
      "000832\n",
      "007682\n",
      "001404\n",
      "003919\n",
      "007735\n",
      "005068\n",
      "004867\n",
      "009128\n",
      "000177\n",
      "008023\n",
      "009208\n",
      "006784\n",
      "000242\n",
      "005704\n",
      "006470\n",
      "009147\n",
      "004907\n",
      "005699\n",
      "007732\n",
      "007035\n",
      "004794\n",
      "002994\n",
      "009236\n",
      "002710\n",
      "003255\n",
      "002219\n",
      "002662\n",
      "001277\n",
      "001466\n",
      "001204\n",
      "002691\n",
      "004312\n",
      "009392\n",
      "001004\n",
      "006079\n",
      "008429\n",
      "007680\n",
      "000170\n",
      "001237\n",
      "000700\n",
      "005338\n",
      "001864\n",
      "005863\n",
      "008029\n",
      "008017\n",
      "008072\n",
      "002476\n",
      "008492\n",
      "001109\n",
      "005159\n",
      "008773\n",
      "008584\n",
      "006000\n",
      "003451\n",
      "006576\n",
      "001647\n",
      "001563\n",
      "002350\n",
      "008752\n",
      "004265\n",
      "007025\n",
      "009541\n",
      "003417\n",
      "009349\n",
      "009212\n",
      "008665\n",
      "003228\n",
      "003370\n",
      "005674\n",
      "001206\n",
      "008601\n",
      "005212\n",
      "006694\n",
      "008323\n",
      "000083\n",
      "004010\n",
      "004551\n",
      "006374\n",
      "000307\n",
      "007180\n",
      "003367\n",
      "007408\n",
      "003164\n",
      "007924\n",
      "004143\n",
      "006338\n",
      "002586\n",
      "001799\n",
      "009373\n",
      "000428\n",
      "007688\n",
      "002778\n",
      "002669\n",
      "003786\n",
      "005894\n",
      "008581\n",
      "005752\n",
      "006196\n",
      "002820\n",
      "003956\n",
      "002305\n",
      "004508\n",
      "003211\n",
      "002957\n",
      "002962\n",
      "009920\n",
      "007939\n",
      "001187\n",
      "005150\n",
      "009112\n",
      "009666\n",
      "000263\n",
      "000776\n",
      "001561\n",
      "001640\n",
      "008064\n",
      "007546\n",
      "009946\n",
      "000860\n",
      "004076\n",
      "003200\n",
      "009830\n",
      "009174\n",
      "000241\n",
      "009443\n",
      "007089\n",
      "007396\n",
      "008702\n",
      "009591\n",
      "006797\n",
      "002984\n",
      "007356\n",
      "004606\n",
      "005813\n",
      "006722\n",
      "001630\n",
      "002798\n",
      "003146\n",
      "007790\n",
      "002114\n",
      "002377\n",
      "002187\n",
      "004753\n",
      "007865\n",
      "008524\n",
      "006621\n",
      "001818\n",
      "008618\n",
      "002533\n",
      "000612\n",
      "008051\n",
      "004796\n",
      "001528\n",
      "007854\n",
      "005601\n",
      "007049\n",
      "008888\n",
      "005705\n",
      "009138\n",
      "007152\n",
      "003194\n",
      "003489\n",
      "002496\n",
      "005629\n",
      "004232\n",
      "002329\n",
      "001427\n",
      "001362\n",
      "004490\n",
      "003313\n",
      "003530\n",
      "004832\n",
      "009693\n",
      "003242\n",
      "006252\n",
      "005710\n",
      "002176\n",
      "002870\n",
      "000503\n",
      "005306\n",
      "007696\n",
      "004085\n",
      "005912\n",
      "009410\n",
      "006042\n",
      "006503\n",
      "006185\n",
      "003664\n",
      "001887\n",
      "001009\n",
      "008475\n",
      "007603\n",
      "001383\n",
      "003292\n",
      "002415\n",
      "008175\n",
      "001333\n",
      "002202\n",
      "003254\n",
      "003015\n",
      "001175\n",
      "009831\n",
      "009060\n",
      "004609\n",
      "008282\n",
      "000923\n",
      "008953\n",
      "004223\n",
      "009342\n",
      "002220\n",
      "000918\n",
      "003175\n",
      "007363\n",
      "009627\n",
      "005782\n",
      "003879\n",
      "008519\n",
      "004270\n",
      "003679\n",
      "001324\n",
      "005861\n",
      "002675\n",
      "000626\n",
      "001604\n",
      "004304\n",
      "004360\n",
      "008836\n",
      "007666\n",
      "008749\n",
      "009862\n",
      "000709\n",
      "003645\n",
      "009048\n",
      "005451\n",
      "003231\n",
      "003889\n",
      "009488\n",
      "007193\n",
      "000874\n",
      "004830\n",
      "006835\n",
      "001402\n",
      "007594\n",
      "003796\n",
      "001586\n",
      "007909\n",
      "004100\n",
      "003102\n",
      "004839\n",
      "007383\n",
      "000508\n",
      "007923\n",
      "004351\n",
      "007369\n",
      "005129\n",
      "008943\n",
      "009780\n",
      "009114\n",
      "001240\n",
      "004200\n",
      "001872\n",
      "004563\n",
      "007159\n",
      "008636\n",
      "003993\n",
      "003899\n",
      "009713\n",
      "005583\n",
      "002539\n",
      "004625\n",
      "002441\n",
      "003657\n",
      "009762\n",
      "002569\n",
      "006029\n",
      "004329\n",
      "005102\n",
      "003120\n",
      "006290\n",
      "001230\n",
      "001749\n",
      "000391\n",
      "000601\n",
      "000653\n",
      "001940\n",
      "004878\n",
      "008368\n",
      "007633\n",
      "008142\n",
      "005499\n",
      "004761\n",
      "003336\n",
      "002924\n",
      "001785\n",
      "009748\n",
      "004178\n",
      "004067\n",
      "003838\n",
      "003606\n",
      "007449\n",
      "000091\n",
      "000750\n",
      "004138\n",
      "008983\n",
      "003632\n",
      "005406\n",
      "003056\n",
      "001747\n",
      "007417\n",
      "009558\n",
      "009296\n",
      "000095\n",
      "007511\n",
      "003413\n",
      "005940\n",
      "000046\n",
      "002919\n",
      "009386\n",
      "000935\n",
      "004866\n",
      "003250\n",
      "002737\n",
      "000220\n",
      "004246\n",
      "002025\n",
      "001903\n",
      "000738\n",
      "006648\n",
      "005819\n",
      "008262\n",
      "009842\n",
      "000999\n",
      "009326\n",
      "006838\n",
      "007685\n",
      "000822\n",
      "009897\n",
      "004879\n",
      "006773\n",
      "008144\n",
      "000235\n",
      "009438\n",
      "006786\n",
      "003420\n",
      "006762\n",
      "008776\n",
      "008931\n",
      "000306\n",
      "007540\n",
      "005344\n",
      "001588\n",
      "006868\n",
      "007204\n",
      "005814\n",
      "000323\n",
      "004856\n",
      "007373\n",
      "002534\n",
      "000712\n",
      "004106\n",
      "004863\n",
      "008966\n",
      "001212\n",
      "009455\n",
      "002238\n",
      "000023\n",
      "009734\n",
      "000731\n",
      "003290\n",
      "009918\n",
      "008365\n",
      "009692\n",
      "006556\n",
      "009408\n",
      "001375\n",
      "001833\n",
      "008326\n",
      "008499\n",
      "000050\n",
      "003779\n",
      "005215\n",
      "000200\n",
      "001754\n",
      "004298\n",
      "002435\n",
      "001072\n",
      "008424\n",
      "007305\n",
      "000190\n",
      "004816\n",
      "002914\n",
      "004852\n",
      "003690\n",
      "007856\n",
      "003830\n",
      "008725\n",
      "009898\n",
      "000865\n",
      "007295\n",
      "006821\n",
      "000032\n",
      "005122\n",
      "008811\n",
      "003154\n",
      "000720\n",
      "000289\n",
      "007436\n",
      "005365\n",
      "002208\n",
      "008838\n",
      "005355\n",
      "007526\n",
      "009459\n",
      "003678\n",
      "004768\n",
      "001365\n",
      "008701\n",
      "005315\n",
      "006958\n",
      "002371\n",
      "009377\n",
      "000902\n",
      "005718\n",
      "002713\n",
      "007329\n",
      "005348\n",
      "001682\n",
      "002037\n",
      "000780\n",
      "003236\n",
      "008880\n",
      "002433\n",
      "009333\n",
      "005573\n",
      "002940\n",
      "000895\n",
      "003937\n",
      "008173\n",
      "003611\n",
      "004247\n",
      "008799\n",
      "001782\n",
      "000159\n",
      "003188\n",
      "009175\n",
      "003750\n",
      "009877\n",
      "009202\n",
      "008559\n",
      "009186\n",
      "000711\n",
      "005331\n",
      "000676\n",
      "002730\n",
      "006660\n",
      "003639\n",
      "005874\n",
      "003327\n",
      "001931\n",
      "007824\n",
      "006886\n",
      "003216\n",
      "007675\n",
      "003469\n",
      "001386\n",
      "009287\n",
      "005183\n",
      "009629\n",
      "001309\n",
      "002956\n",
      "002466\n",
      "008691\n",
      "004060\n",
      "008385\n",
      "005769\n",
      "005693\n",
      "009022\n",
      "004748\n",
      "009358\n",
      "004499\n",
      "005983\n",
      "000331\n",
      "003564\n",
      "000174\n",
      "004322\n",
      "003074\n",
      "004141\n",
      "001028\n",
      "003811\n",
      "001053\n",
      "000920\n",
      "009822\n",
      "001853\n",
      "005257\n",
      "009879\n",
      "007999\n",
      "005742\n",
      "000966\n",
      "006668\n",
      "004224\n",
      "007154\n",
      "002549\n",
      "007563\n",
      "003365\n",
      "002875\n",
      "007298\n",
      "005821\n",
      "006161\n",
      "004890\n",
      "001147\n",
      "006254\n",
      "005640\n",
      "005731\n",
      "003936\n",
      "007042\n",
      "000991\n",
      "007543\n",
      "003240\n",
      "001713\n",
      "006934\n",
      "001685\n",
      "004779\n",
      "005171\n",
      "003024\n",
      "006637\n",
      "000498\n",
      "000926\n",
      "003452\n",
      "000250\n",
      "006295\n",
      "009858\n",
      "007168\n",
      "009845\n",
      "006429\n",
      "004091\n",
      "002585\n",
      "000609\n",
      "008713\n",
      "001060\n",
      "009042\n",
      "005455\n",
      "007926\n",
      "006349\n",
      "001436\n",
      "005914\n",
      "008892\n",
      "003998\n",
      "001092\n",
      "003685\n",
      "007773\n",
      "009678\n",
      "009184\n",
      "005190\n",
      "000987\n",
      "006962\n",
      "000863\n",
      "000268\n",
      "006299\n",
      "006819\n",
      "006918\n",
      "009388\n",
      "006184\n",
      "006177\n",
      "007075\n",
      "001608\n",
      "009374\n",
      "004723\n",
      "007465\n",
      "008549\n",
      "008252\n",
      "003493\n",
      "005114\n",
      "002696\n",
      "003529\n",
      "000443\n",
      "003078\n",
      "003859\n",
      "000768\n",
      "005124\n",
      "008222\n",
      "009024\n",
      "005108\n",
      "005917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000672\n",
      "005268\n",
      "000035\n",
      "000102\n",
      "001057\n",
      "000827\n",
      "006206\n",
      "001882\n",
      "004148\n",
      "001772\n",
      "003477\n",
      "004948\n",
      "000688\n",
      "006734\n",
      "003996\n",
      "006605\n",
      "006483\n",
      "006158\n",
      "006425\n",
      "009551\n",
      "007385\n",
      "005336\n",
      "004783\n",
      "001577\n",
      "004836\n",
      "004095\n",
      "004655\n",
      "004273\n",
      "000713\n",
      "003740\n",
      "007916\n",
      "009580\n",
      "007626\n",
      "004898\n",
      "000786\n",
      "005773\n",
      "009707\n",
      "000430\n",
      "007841\n",
      "003377\n",
      "002334\n",
      "003499\n",
      "006241\n",
      "000214\n",
      "005033\n",
      "004793\n",
      "009755\n",
      "002461\n",
      "002605\n",
      "003138\n",
      "007486\n",
      "001862\n",
      "000537\n",
      "007374\n",
      "003605\n",
      "006330\n",
      "006170\n",
      "007250\n",
      "006071\n",
      "000042\n",
      "006488\n",
      "003886\n",
      "009568\n",
      "002102\n",
      "003339\n",
      "003555\n",
      "003034\n",
      "003205\n",
      "000460\n",
      "004785\n",
      "001288\n",
      "004075\n",
      "000189\n",
      "009761\n",
      "002221\n",
      "001263\n",
      "007422\n",
      "001330\n",
      "008624\n",
      "007210\n",
      "002315\n",
      "003784\n",
      "007182\n",
      "006004\n",
      "002494\n",
      "005136\n",
      "006626\n",
      "009448\n",
      "009351\n",
      "005239\n",
      "006833\n",
      "004164\n",
      "008815\n",
      "009412\n",
      "008790\n",
      "002004\n",
      "007819\n",
      "008862\n",
      "002827\n",
      "004303\n",
      "007458\n",
      "001999\n",
      "005461\n",
      "003609\n",
      "007723\n",
      "006462\n",
      "004812\n",
      "003009\n",
      "006740\n",
      "007184\n",
      "001780\n",
      "005037\n",
      "002201\n",
      "001027\n",
      "002364\n",
      "006842\n",
      "009781\n",
      "003455\n",
      "001451\n",
      "007489\n",
      "001972\n",
      "001557\n",
      "009800\n",
      "001699\n",
      "004574\n",
      "007868\n",
      "002646\n",
      "004275\n",
      "007477\n",
      "002375\n",
      "005579\n",
      "009687\n",
      "008477\n",
      "007513\n",
      "006033\n",
      "008930\n",
      "008387\n",
      "008753\n",
      "005588\n",
      "008140\n",
      "006971\n",
      "000194\n",
      "001312\n",
      "008522\n",
      "000559\n",
      "003058\n",
      "007889\n",
      "006176\n",
      "009533\n",
      "008858\n",
      "002609\n",
      "000647\n",
      "003788\n",
      "006689\n",
      "004386\n",
      "008159\n",
      "009440\n",
      "003941\n",
      "000404\n",
      "004215\n",
      "007177\n",
      "000948\n",
      "002323\n",
      "003458\n",
      "000753\n",
      "007483\n",
      "004016\n",
      "008848\n",
      "005552\n",
      "005509\n",
      "007144\n",
      "004679\n",
      "007963\n",
      "008484\n",
      "002977\n",
      "001017\n",
      "006492\n",
      "007749\n",
      "005879\n",
      "003092\n",
      "009460\n",
      "001484\n",
      "007920\n",
      "003623\n",
      "007640\n",
      "005860\n",
      "000063\n",
      "006341\n",
      "007045\n",
      "000117\n",
      "008587\n",
      "005396\n",
      "003492\n",
      "001937\n",
      "008837\n",
      "009773\n",
      "002359\n",
      "009955\n",
      "000599\n",
      "006392\n",
      "006808\n",
      "007524\n",
      "002196\n",
      "009178\n",
      "006708\n",
      "009848\n",
      "008687\n",
      "000525\n",
      "000427\n",
      "003966\n",
      "002444\n",
      "005853\n",
      "006766\n",
      "003089\n",
      "008604\n",
      "009429\n",
      "000468\n",
      "008002\n",
      "006585\n",
      "001962\n",
      "001725\n",
      "007836\n",
      "002783\n",
      "000288\n",
      "002784\n",
      "007964\n",
      "000917\n",
      "002124\n",
      "009279\n",
      "005599\n",
      "003748\n",
      "006096\n",
      "007673\n",
      "001810\n",
      "009200\n",
      "004263\n",
      "009546\n",
      "003751\n",
      "000161\n",
      "007691\n",
      "004507\n",
      "001855\n",
      "004231\n",
      "009526\n",
      "001015\n",
      "006447\n",
      "001265\n",
      "002963\n",
      "006908\n",
      "000016\n",
      "009863\n",
      "007991\n",
      "002618\n",
      "009699\n",
      "003386\n",
      "006768\n",
      "004790\n",
      "006953\n",
      "001650\n",
      "000843\n",
      "006045\n",
      "005527\n",
      "000302\n",
      "008770\n",
      "001457\n",
      "001052\n",
      "009654\n",
      "004331\n",
      "004347\n",
      "007448\n",
      "001497\n",
      "003519\n",
      "005063\n",
      "007736\n",
      "002001\n",
      "005128\n",
      "005453\n",
      "001101\n",
      "009029\n",
      "004292\n",
      "003673\n",
      "004455\n",
      "007779\n",
      "008359\n",
      "002237\n",
      "002251\n",
      "003565\n",
      "003282\n",
      "005389\n",
      "000530\n",
      "006597\n",
      "001836\n",
      "007547\n",
      "003106\n",
      "008254\n",
      "002249\n",
      "007123\n",
      "004259\n",
      "003436\n",
      "002791\n",
      "007245\n",
      "001421\n",
      "008370\n",
      "002000\n",
      "000820\n",
      "008608\n",
      "005762\n",
      "008086\n",
      "007622\n",
      "002937\n",
      "004352\n",
      "007527\n",
      "009268\n",
      "005547\n",
      "006736\n",
      "003181\n",
      "002163\n",
      "009121\n",
      "003404\n",
      "000848\n",
      "005199\n",
      "007655\n",
      "002450\n",
      "009887\n",
      "002939\n",
      "004643\n",
      "002659\n",
      "006247\n",
      "009664\n",
      "002547\n",
      "005337\n",
      "008087\n",
      "001225\n",
      "008513\n",
      "002714\n",
      "003948\n",
      "005073\n",
      "008730\n",
      "008374\n",
      "004039\n",
      "003223\n",
      "000222\n",
      "007647\n",
      "003669\n",
      "004241\n",
      "003134\n",
      "006234\n",
      "009339\n",
      "001211\n",
      "001510\n",
      "006612\n",
      "004770\n",
      "000632\n",
      "007946\n",
      "003422\n",
      "007497\n",
      "002293\n",
      "008835\n",
      "002519\n",
      "008164\n",
      "009790\n",
      "007439\n",
      "005608\n",
      "007664\n",
      "005168\n",
      "002456\n",
      "008932\n",
      "002108\n",
      "004983\n",
      "005071\n",
      "009623\n",
      "002030\n",
      "000590\n",
      "006240\n",
      "009729\n",
      "004230\n",
      "007571\n",
      "007431\n",
      "007114\n",
      "006844\n",
      "007758\n",
      "008690\n",
      "001723\n",
      "007979\n",
      "006351\n",
      "000355\n",
      "006806\n",
      "004797\n",
      "002255\n",
      "003053\n",
      "000007\n",
      "008224\n",
      "007592\n",
      "002565\n",
      "001450\n",
      "007187\n",
      "009774\n",
      "007283\n",
      "003082\n",
      "008733\n",
      "000499\n",
      "004487\n",
      "008913\n",
      "001752\n",
      "005672\n",
      "009409\n",
      "006286\n",
      "007578\n",
      "006501\n",
      "002782\n",
      "006619\n",
      "003845\n",
      "005327\n",
      "004540\n",
      "008403\n",
      "009566\n",
      "001707\n",
      "008351\n",
      "003550\n",
      "005889\n",
      "006899\n",
      "008260\n",
      "002772\n",
      "009560\n",
      "007813\n",
      "005696\n",
      "008004\n",
      "006212\n",
      "002214\n",
      "000772\n",
      "006188\n",
      "000484\n",
      "002049\n",
      "003696\n",
      "002088\n",
      "005676\n",
      "009789\n",
      "008878\n",
      "003344\n",
      "001571\n",
      "004849\n",
      "009573\n",
      "001958\n",
      "001565\n",
      "001512\n",
      "005047\n",
      "007323\n",
      "004518\n",
      "007530\n",
      "006366\n",
      "007437\n",
      "002335\n",
      "001733\n",
      "004264\n",
      "000039\n",
      "005836\n",
      "005554\n",
      "008281\n",
      "009504\n",
      "009852\n",
      "001315\n",
      "006433\n",
      "008084\n",
      "002382\n",
      "008472\n",
      "006917\n",
      "007740\n",
      "008204\n",
      "002068\n",
      "009737\n",
      "002002\n",
      "001807\n",
      "005067\n",
      "003691\n",
      "009668\n",
      "006887\n",
      "003269\n",
      "006180\n",
      "002986\n",
      "000446\n",
      "002796\n",
      "009269\n",
      "002027\n",
      "006296\n",
      "001714\n",
      "005231\n",
      "002677\n",
      "000951\n",
      "009911\n",
      "000419\n",
      "007217\n",
      "003351\n",
      "006100\n",
      "002702\n",
      "007334\n",
      "005657\n",
      "002169\n",
      "009417\n",
      "006813\n",
      "000163\n",
      "003549\n",
      "008867\n",
      "002090\n",
      "004397\n",
      "004205\n",
      "006218\n",
      "000153\n",
      "008019\n",
      "005991\n",
      "001941\n",
      "001632\n",
      "008335\n",
      "003990\n",
      "009466\n",
      "002442\n",
      "007020\n",
      "005660\n",
      "002615\n",
      "004326\n",
      "005524\n",
      "004702\n",
      "006865\n",
      "006371\n",
      "008747\n",
      "003662\n",
      "009540\n",
      "000900\n",
      "008388\n",
      "000147\n",
      "005791\n",
      "005747\n",
      "003656\n",
      "009874\n",
      "009254\n",
      "008434\n",
      "008750\n",
      "007935\n",
      "006542\n",
      "006223\n",
      "006428\n",
      "004113\n",
      "001888\n",
      "007748\n",
      "004990\n",
      "000868\n",
      "003178\n",
      "000334\n",
      "005024\n",
      "009550\n",
      "005539\n",
      "004691\n",
      "007677\n",
      "005654\n",
      "000564\n",
      "000565\n",
      "006930\n",
      "008166\n",
      "009923\n",
      "005757\n",
      "000828\n",
      "002513\n",
      "003735\n",
      "009763\n",
      "004031\n",
      "002670\n",
      "000962\n",
      "000540\n",
      "008841\n",
      "008105\n",
      "009192\n",
      "004622\n",
      "002960\n",
      "002244\n",
      "001343\n",
      "009724\n",
      "006609\n",
      "007899\n",
      "003575\n",
      "000645\n",
      "005431\n",
      "004196\n",
      "006486\n",
      "000496\n",
      "003118\n",
      "009393\n",
      "003807\n",
      "002132\n",
      "009454\n",
      "004626\n",
      "001041\n",
      "008849\n",
      "006382\n",
      "001825\n",
      "007697\n",
      "001944\n",
      "002500\n",
      "006972\n",
      "005830\n",
      "004840\n",
      "002022\n",
      "005314\n",
      "000024\n",
      "005223\n",
      "002181\n",
      "002965\n",
      "003709\n",
      "009162\n",
      "007247\n",
      "004291\n",
      "007838\n",
      "001729\n",
      "007826\n",
      "005989\n",
      "000854\n",
      "004928\n",
      "001455\n",
      "002471\n",
      "002061\n",
      "005018\n",
      "007654\n",
      "001439\n",
      "001847\n",
      "000619\n",
      "003210\n",
      "007679\n",
      "000889\n",
      "006532\n",
      "004868\n",
      "005535\n",
      "002306\n",
      "004558\n",
      "007618\n",
      "003758\n",
      "009527\n",
      "001686\n",
      "007375\n",
      "004955\n",
      "000048\n",
      "000657\n",
      "006465\n",
      "000411\n",
      "008706\n",
      "004848\n",
      "003705\n",
      "008541\n",
      "006636\n",
      "000251\n",
      "009549\n",
      "006783\n",
      "007205\n",
      "002760\n",
      "003508\n",
      "002600\n",
      "009603\n",
      "007857\n",
      "006009\n",
      "008557\n",
      "003945\n",
      "008467\n",
      "000113\n",
      "000588\n",
      "009086\n",
      "004446\n",
      "004653\n",
      "001400\n",
      "008948\n",
      "008452\n",
      "005016\n",
      "000211\n",
      "007299\n",
      "005919\n",
      "001152\n",
      "001149\n",
      "001224\n",
      "006583\n",
      "003243\n",
      "001610\n",
      "006748\n",
      "001156\n",
      "003809\n",
      "006845\n",
      "007003\n",
      "005998\n",
      "002502\n",
      "005391\n",
      "009058\n",
      "003279\n",
      "004046\n",
      "001517\n",
      "009099\n",
      "007777\n",
      "007129\n",
      "005780\n",
      "000052\n",
      "003165\n",
      "003233\n",
      "006282\n",
      "005032\n",
      "006131\n",
      "000120\n",
      "001481\n",
      "000193\n",
      "004943\n",
      "003597\n",
      "000746\n",
      "005450\n",
      "000656\n",
      "006584\n",
      "005530\n",
      "007361\n",
      "002019\n",
      "006673\n",
      "003207\n",
      "004150\n",
      "000138\n",
      "005433\n",
      "005175\n",
      "006279\n",
      "003522\n",
      "006876\n",
      "001758\n",
      "000317\n",
      "001364\n",
      "005852\n",
      "000133\n",
      "001675\n",
      "006606\n",
      "003031\n",
      "001209\n",
      "005481\n",
      "008311\n",
      "009252\n",
      "007466\n",
      "008962\n",
      "008091\n",
      "006284\n",
      "008748\n",
      "008423\n",
      "005101\n",
      "007637\n",
      "007018\n",
      "006104\n",
      "004600\n",
      "006120\n",
      "000663\n",
      "006684\n",
      "007365\n",
      "006078\n",
      "005768\n",
      "007853\n",
      "002709\n",
      "005448\n",
      "000797\n",
      "001894\n",
      "000628\n",
      "007490\n",
      "001346\n",
      "002320\n",
      "006848\n",
      "003142\n",
      "007122\n",
      "009813\n",
      "000065\n",
      "001171\n",
      "005385\n",
      "002490\n",
      "003863\n",
      "003410\n",
      "005310\n",
      "008188\n",
      "004873\n",
      "008462\n",
      "005895\n",
      "001371\n",
      "005818\n",
      "005888\n",
      "007153\n",
      "004450\n",
      "008921\n",
      "005843\n",
      "006084\n",
      "003953\n",
      "004799\n",
      "003660\n",
      "005384\n",
      "008936\n",
      "001906\n",
      "008338\n",
      "005995\n",
      "008485\n",
      "006912\n",
      "005518\n",
      "004136\n",
      "000872\n",
      "005920\n",
      "007943\n",
      "002666\n",
      "005574\n",
      "003599\n",
      "003003\n",
      "007914\n",
      "004137\n",
      "007984\n",
      "005521\n",
      "008341\n",
      "006940\n",
      "008617\n",
      "008112\n",
      "003500\n",
      "006444\n",
      "004652\n",
      "008741\n",
      "005436\n",
      "007388\n",
      "000041\n",
      "004618\n",
      "007070\n",
      "001071\n",
      "006443\n",
      "002801\n",
      "004293\n",
      "004747\n",
      "001284\n",
      "002941\n",
      "000099\n",
      "009886\n",
      "002613\n",
      "005811\n",
      "004287\n",
      "005311\n",
      "005938\n",
      "001325\n",
      "003116\n",
      "002606\n",
      "003890\n",
      "005749\n",
      "000876\n",
      "000550\n",
      "003817\n",
      "007166\n",
      "009702\n",
      "001683\n",
      "004846\n",
      "009637\n",
      "009063\n",
      "000729\n",
      "002598\n",
      "002384\n",
      "003792\n",
      "009035\n",
      "004694\n",
      "000915\n",
      "006638\n",
      "000285\n",
      "005004\n",
      "005131\n",
      "001337\n",
      "006760\n",
      "005485\n",
      "001964\n",
      "000278\n",
      "006572\n",
      "007781\n",
      "001830\n",
      "001226\n",
      "006635\n",
      "003406\n",
      "006515\n",
      "004354\n",
      "009472\n",
      "009002\n",
      "003516\n",
      "004323\n",
      "007687\n",
      "006850\n",
      "005445\n",
      "001241\n",
      "004961\n",
      "007833\n",
      "003170\n",
      "008955\n",
      "003648\n",
      "000150\n",
      "004648\n",
      "003169\n",
      "007754\n",
      "007241\n",
      "009881\n",
      "006789\n",
      "009855\n",
      "004605\n",
      "005901\n",
      "007872\n",
      "005996\n",
      "005624\n",
      "001129\n",
      "009239\n",
      "009870\n",
      "000818\n",
      "000791\n",
      "003855\n",
      "000633\n",
      "004035\n",
      "006627\n",
      "007925\n",
      "008873\n",
      "009173\n",
      "000322\n",
      "006862\n",
      "001580\n",
      "004338\n",
      "005954\n",
      "001293\n",
      "001907\n",
      "006430\n",
      "000541\n",
      "008386\n",
      "009758\n",
      "002261\n",
      "005687\n",
      "008976\n",
      "004221\n",
      "009719\n",
      "004387\n",
      "004028\n",
      "002151\n",
      "001130\n",
      "005664\n",
      "008349\n",
      "002636\n",
      "001784\n",
      "000911\n",
      "005084\n",
      "002544\n",
      "007343\n",
      "000752\n",
      "007537\n",
      "005093\n",
      "008612\n",
      "001611\n",
      "004424\n",
      "009000\n",
      "002439\n",
      "004327\n",
      "004972\n",
      "007786\n",
      "000857\n",
      "009004\n",
      "005367\n",
      "001545\n",
      "008079\n",
      "006117\n",
      "004014\n",
      "007002\n",
      "004792\n",
      "007657\n",
      "005439\n",
      "004994\n",
      "005224\n",
      "001161\n",
      "004886\n",
      "009615\n",
      "001960\n",
      "008057\n",
      "007911\n",
      "007702\n",
      "002436\n",
      "000774\n",
      "003443\n",
      "009658\n",
      "004488\n",
      "001124\n",
      "001915\n",
      "005559\n",
      "002664\n",
      "005645\n",
      "003301\n",
      "003818\n",
      "007165\n",
      "001911\n",
      "005065\n",
      "007461\n",
      "005262\n",
      "006135\n",
      "001323\n",
      "003772\n",
      "008163\n",
      "000123\n",
      "001191\n",
      "006012\n",
      "005817\n",
      "005195\n",
      "002841\n",
      "004985\n",
      "002142\n",
      "000396\n",
      "004280\n",
      "003635\n",
      "007234\n",
      "006822\n",
      "008512\n",
      "000294\n",
      "002182\n",
      "007570\n",
      "007845\n",
      "006318\n",
      "005260\n",
      "005611\n",
      "002910\n",
      "009187\n",
      "007629\n",
      "006547\n",
      "003270\n",
      "001279\n",
      "008315\n",
      "005961\n",
      "006367\n",
      "006981\n",
      "006151\n",
      "004163\n",
      "000173\n",
      "004212\n",
      "008602\n",
      "009515\n",
      "006434\n",
      "008670\n",
      "005423\n",
      "000118\n",
      "008969\n",
      "000733\n",
      "001361\n",
      "000112\n",
      "003330\n",
      "004689\n",
      "004951\n",
      "003273\n",
      "009106\n",
      "008764\n",
      "003711\n",
      "007433\n",
      "006543\n",
      "003484\n",
      "003640\n",
      "006565\n",
      "005312\n",
      "002443\n",
      "001730\n",
      "005603\n",
      "000694\n",
      "003005\n",
      "000855\n",
      "001618\n",
      "009116\n",
      "000072\n",
      "002373\n",
      "005293\n",
      "002554\n",
      "009336\n",
      "009545\n",
      "004058\n",
      "002199\n",
      "007467\n",
      "005320\n",
      "006073\n",
      "005765\n",
      "001661\n",
      "001724\n",
      "008397\n",
      "005644\n",
      "001795\n",
      "009251\n",
      "004818\n",
      "009932\n",
      "009839\n",
      "005511\n",
      "008251\n",
      "008530\n",
      "002425\n",
      "006190\n",
      "002735\n",
      "006495\n",
      "005121\n",
      "001673\n",
      "003729\n",
      "002411\n",
      "005515\n",
      "003065\n",
      "006219\n",
      "006141\n",
      "002483\n",
      "006988\n",
      "002868\n",
      "009006\n",
      "009754\n",
      "008095\n",
      "009249\n",
      "000340\n",
      "000207\n",
      "009129\n",
      "007705\n",
      "000262\n",
      "000871\n",
      "003752\n",
      "008806\n",
      "000739\n",
      "003749\n",
      "000830\n",
      "002280\n",
      "006759\n",
      "009797\n",
      "002763\n",
      "001488\n",
      "001460\n",
      "004801\n",
      "001194\n",
      "007090\n",
      "004493\n",
      "003424\n",
      "008160\n",
      "000671\n",
      "009100\n",
      "007869\n",
      "005714\n",
      "000101\n",
      "005497\n",
      "008739\n",
      "007031\n",
      "002635\n",
      "001985\n",
      "007873\n",
      "007113\n",
      "000044\n",
      "006956\n",
      "001800\n",
      "008717\n",
      "009414\n",
      "009598\n",
      "007902\n",
      "006976\n",
      "007743\n",
      "009166\n",
      "006362\n",
      "008890\n",
      "001514\n",
      "000492\n",
      "004954\n",
      "001541\n",
      "002045\n",
      "007775\n",
      "002975\n",
      "003973\n",
      "007270\n",
      "004982\n",
      "007174\n",
      "008384\n",
      "007653\n",
      "007346\n",
      "004686\n",
      "001653\n",
      "003954\n",
      "000009\n",
      "000513\n",
      "002765\n",
      "002407\n",
      "001207\n",
      "000184\n",
      "006578\n",
      "006235\n",
      "003028\n",
      "001766\n",
      "009382\n",
      "008919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "007525\n",
      "003253\n",
      "002023\n",
      "005593\n",
      "008716\n",
      "003759\n",
      "000764\n",
      "001479\n",
      "004012\n",
      "000710\n",
      "001432\n",
      "009584\n",
      "003401\n",
      "007108\n",
      "006755\n",
      "006140\n",
      "005387\n",
      "000406\n",
      "005826\n",
      "001304\n",
      "004732\n",
      "002458\n",
      "000424\n",
      "006805\n",
      "000544\n",
      "007021\n",
      "002590\n",
      "008939\n",
      "002376\n",
      "006990\n",
      "002328\n",
      "001746\n",
      "006994\n",
      "001184\n",
      "003183\n",
      "009195\n",
      "000208\n",
      "006156\n",
      "000597\n",
      "006730\n",
      "009179\n",
      "004020\n",
      "001093\n",
      "003924\n",
      "004519\n",
      "004372\n",
      "005760\n",
      "003593\n",
      "008971\n",
      "001954\n",
      "004333\n",
      "008062\n",
      "005609\n",
      "003126\n",
      "002866\n",
      "001148\n",
      "003380\n",
      "006222\n",
      "004693\n",
      "008438\n",
      "005630\n",
      "006524\n",
      "009087\n",
      "006709\n",
      "003957\n",
      "001761\n",
      "003594\n",
      "006058\n",
      "005738\n",
      "006448\n",
      "006230\n",
      "009259\n",
      "008322\n",
      "009073\n",
      "003988\n",
      "008009\n",
      "006107\n",
      "002448\n",
      "001821\n",
      "009588\n",
      "003596\n",
      "003149\n",
      "006088\n",
      "009519\n",
      "005349\n",
      "005326\n",
      "008568\n",
      "004782\n",
      "005209\n",
      "000077\n",
      "005003\n",
      "004660\n",
      "008517\n",
      "007065\n",
      "004823\n",
      "008150\n",
      "007336\n",
      "009700\n",
      "007933\n",
      "000796\n",
      "008413\n",
      "001388\n",
      "007296\n",
      "002392\n",
      "007572\n",
      "007799\n",
      "002989\n",
      "002146\n",
      "006569\n",
      "006738\n",
      "002491\n",
      "008606\n",
      "007185\n",
      "009470\n",
      "006847\n",
      "003303\n",
      "005304\n",
      "001756\n",
      "001536\n",
      "006163\n",
      "009671\n",
      "002330\n",
      "005470\n",
      "007720\n",
      "005496\n",
      "005303\n",
      "008053\n",
      "005824\n",
      "000416\n",
      "007662\n",
      "000850\n",
      "003325\n",
      "004939\n",
      "005740\n",
      "009245\n",
      "003397\n",
      "009334\n",
      "004837\n",
      "001395\n",
      "009218\n",
      "005679\n",
      "002098\n",
      "006810\n",
      "000526\n",
      "005078\n",
      "004194\n",
      "005319\n",
      "000702\n",
      "003939\n",
      "008218\n",
      "007742\n",
      "006695\n",
      "003577\n",
      "003439\n",
      "009464\n",
      "002020\n",
      "004283\n",
      "001077\n",
      "004895\n",
      "001775\n",
      "005054\n",
      "003753\n",
      "002751\n",
      "004509\n",
      "005135\n",
      "003293\n",
      "006005\n",
      "008680\n",
      "004601\n",
      "008107\n",
      "004960\n",
      "009841\n",
      "002462\n",
      "008826\n",
      "001809\n",
      "009579\n",
      "001834\n",
      "002055\n",
      "009113\n",
      "005219\n",
      "001405\n",
      "003189\n",
      "009051\n",
      "008772\n",
      "002706\n",
      "002183\n",
      "009695\n",
      "004471\n",
      "004977\n",
      "005794\n",
      "003872\n",
      "007322\n",
      "002978\n",
      "005850\n",
      "008588\n",
      "000162\n",
      "008117\n",
      "006714\n",
      "007614\n",
      "009833\n",
      "004005\n",
      "007128\n",
      "004390\n",
      "002833\n",
      "004532\n",
      "009375\n",
      "000020\n",
      "000980\n",
      "007006\n",
      "009649\n",
      "004203\n",
      "004760\n",
      "001690\n",
      "002889\n",
      "004530\n",
      "008211\n",
      "009318\n",
      "002332\n",
      "008768\n",
      "009484\n",
      "002082\n",
      "003974\n",
      "001326\n",
      "008301\n",
      "009942\n",
      "003453\n",
      "000394\n",
      "006952\n",
      "008732\n",
      "001158\n",
      "007931\n",
      "003199\n",
      "003296\n",
      "001963\n",
      "002152\n",
      "005864\n",
      "009307\n",
      "001832\n",
      "005730\n",
      "002643\n",
      "005947\n",
      "007256\n",
      "007092\n",
      "002525\n",
      "006174\n",
      "006062\n",
      "004623\n",
      "008683\n",
      "007776\n",
      "004828\n",
      "002403\n",
      "007249\n",
      "003185\n",
      "002278\n",
      "008121\n",
      "002070\n",
      "001930\n",
      "008989\n",
      "003195\n",
      "008805\n",
      "006277\n",
      "007215\n",
      "006645\n",
      "008809\n",
      "001269\n",
      "003373\n",
      "001083\n",
      "002762\n",
      "001669\n",
      "001922\n",
      "001797\n",
      "009711\n",
      "005267\n",
      "004464\n",
      "000244\n",
      "004967\n",
      "000030\n",
      "003285\n",
      "009348\n",
      "007443\n",
      "006966\n",
      "006869\n",
      "008033\n",
      "006476\n",
      "007062\n",
      "000842\n",
      "009947\n",
      "002051\n",
      "007878\n",
      "004510\n",
      "001840\n",
      "006603\n",
      "005153\n",
      "006267\n",
      "008885\n",
      "002378\n",
      "000064\n",
      "000887\n",
      "003506\n",
      "004296\n",
      "001896\n",
      "001971\n",
      "000535\n",
      "005380\n",
      "008226\n",
      "004367\n",
      "000581\n",
      "008024\n",
      "003891\n",
      "005298\n",
      "008997\n",
      "007650\n",
      "008573\n",
      "000229\n",
      "007266\n",
      "001078\n",
      "005156\n",
      "006707\n",
      "009153\n",
      "000373\n",
      "008295\n",
      "003085\n",
      "006187\n",
      "002621\n",
      "False\n",
      "008546\n",
      "001585\n",
      "005832\n",
      "009849\n",
      "001635\n",
      "003278\n",
      "002671\n",
      "001222\n",
      "009253\n",
      "003547\n",
      "003347\n",
      "006360\n",
      "003123\n",
      "005479\n",
      "009069\n",
      "001232\n",
      "003264\n",
      "006127\n",
      "004343\n",
      "006086\n",
      "000076\n",
      "002580\n",
      "008407\n",
      "001295\n",
      "002451\n",
      "007695\n",
      "009701\n",
      "005386\n",
      "008623\n",
      "004827\n",
      "001822\n",
      "005426\n",
      "005381\n",
      "005722\n",
      "005512\n",
      "004800\n",
      "005498\n",
      "004819\n",
      "002227\n",
      "001975\n",
      "007178\n",
      "000505\n",
      "001422\n",
      "007681\n",
      "008567\n",
      "007906\n",
      "005862\n",
      "002616\n",
      "007024\n",
      "002363\n",
      "006733\n",
      "000283\n",
      "007067\n",
      "002981\n",
      "005532\n",
      "000314\n",
      "009075\n",
      "009505\n",
      "007719\n",
      "009431\n",
      "006056\n",
      "005287\n",
      "008898\n",
      "008073\n",
      "002084\n",
      "007288\n",
      "001659\n",
      "004690\n",
      "006750\n",
      "002274\n",
      "003097\n",
      "008754\n",
      "008877\n",
      "006361\n",
      "006287\n",
      "006558\n",
      "005935\n",
      "008986\n",
      "002770\n",
      "006623\n",
      "007393\n",
      "002032\n",
      "008401\n",
      "000397\n",
      "007623\n",
      "008952\n",
      "001412\n",
      "002206\n",
      "001377\n",
      "008249\n",
      "002185\n",
      "001535\n",
      "003884\n",
      "003612\n",
      "008984\n",
      "006663\n",
      "004621\n",
      "008389\n",
      "009752\n",
      "008870\n",
      "004550\n",
      "003746\n",
      "001652\n",
      "002317\n",
      "002731\n",
      "006746\n",
      "007463\n",
      "008671\n",
      "003168\n",
      "002797\n",
      "006149\n",
      "007711\n",
      "002536\n",
      "007207\n",
      "005848\n",
      "009067\n",
      "005562\n",
      "006168\n",
      "002062\n",
      "000247\n",
      "002639\n",
      "006513\n",
      "003934\n",
      "002111\n",
      "000437\n",
      "003917\n",
      "004781\n",
      "009581\n",
      "007196\n",
      "003813\n",
      "002370\n",
      "008034\n",
      "006514\n",
      "000386\n",
      "009811\n",
      "009538\n",
      "006511\n",
      "008234\n",
      "006268\n",
      "006685\n",
      "006544\n",
      "004207\n",
      "006566\n",
      "009674\n",
      "000212\n",
      "005880\n",
      "005925\n",
      "002541\n",
      "000272\n",
      "003381\n",
      "004482\n",
      "008657\n",
      "009928\n",
      "008938\n",
      "006426\n",
      "000953\n",
      "001120\n",
      "007669\n",
      "005953\n",
      "008363\n",
      "000636\n",
      "000057\n",
      "006496\n",
      "005833\n",
      "005564\n",
      "000062\n",
      "005733\n",
      "008460\n",
      "005043\n",
      "002298\n",
      "003841\n",
      "001692\n",
      "009901\n",
      "004569\n",
      "003087\n",
      "002264\n",
      "004803\n",
      "006376\n",
      "001947\n",
      "007957\n",
      "000638\n",
      "005587\n",
      "004340\n",
      "008994\n",
      "005993\n",
      "005876\n",
      "003590\n",
      "002923\n",
      "000405\n",
      "008543\n",
      "009513\n",
      "007357\n",
      "002961\n",
      "000548\n",
      "005409\n",
      "001744\n",
      "003295\n",
      "001770\n",
      "000932\n",
      "008153\n",
      "003769\n",
      "008622\n",
      "000238\n",
      "009297\n",
      "006160\n",
      "002080\n",
      "009142\n",
      "008041\n",
      "003822\n",
      "000223\n",
      "001913\n",
      "005580\n",
      "008613\n",
      "007441\n",
      "000432\n",
      "002188\n",
      "006936\n",
      "001322\n",
      "009092\n",
      "005974\n",
      "007173\n",
      "001354\n",
      "007360\n",
      "006403\n",
      "008257\n",
      "000681\n",
      "006479\n",
      "007707\n",
      "002574\n",
      "007630\n",
      "007291\n",
      "002885\n",
      "000038\n",
      "000928\n",
      "003001\n",
      "001059\n",
      "005921\n",
      "009001\n",
      "001790\n",
      "000188\n",
      "007203\n",
      "008527\n",
      "005929\n",
      "009475\n",
      "007061\n",
      "003804\n",
      "009256\n",
      "008400\n",
      "005238\n",
      "002509\n",
      "005095\n",
      "008147\n",
      "006737\n",
      "005651\n",
      "000944\n",
      "007170\n",
      "008027\n",
      "000339\n",
      "005361\n",
      "002301\n",
      "004313\n",
      "000487\n",
      "005133\n",
      "009369\n",
      "007487\n",
      "009803\n",
      "006752\n",
      "002773\n",
      "005005\n",
      "005557\n",
      "004641\n",
      "008028\n",
      "003394\n",
      "005724\n",
      "007747\n",
      "006692\n",
      "006579\n",
      "009435\n",
      "004933\n",
      "009043\n",
      "001663\n",
      "000664\n",
      "002065\n",
      "008194\n",
      "004454\n",
      "002017\n",
      "006048\n",
      "007554\n",
      "004894\n",
      "001513\n",
      "005671\n",
      "005488\n",
      "001736\n",
      "004572\n",
      "007584\n",
      "006205\n",
      "008693\n",
      "008288\n",
      "009065\n",
      "005503\n",
      "008291\n",
      "004981\n",
      "000521\n",
      "004182\n",
      "009294\n",
      "005060\n",
      "003880\n",
      "000981\n",
      "000351\n",
      "009008\n",
      "009660\n",
      "005706\n",
      "004765\n",
      "006998\n",
      "000261\n",
      "001228\n",
      "001826\n",
      "008046\n",
      "009570\n",
      "000485\n",
      "001300\n",
      "003473\n",
      "001606\n",
      "001449\n",
      "009642\n",
      "007044\n",
      "005934\n",
      "008039\n",
      "006951\n",
      "009777\n",
      "000297\n",
      "001946\n",
      "006778\n",
      "004749\n",
      "001331\n",
      "000864\n",
      "004048\n",
      "001587\n",
      "007340\n",
      "001257\n",
      "001804\n",
      "004425\n",
      "008493\n",
      "009176\n",
      "009535\n",
      "000576\n",
      "008264\n",
      "007476\n",
      "006327\n",
      "004395\n",
      "006973\n",
      "007326\n",
      "006200\n",
      "008580\n",
      "009340\n",
      "002018\n",
      "001560\n",
      "008104\n",
      "000616\n",
      "004124\n",
      "003226\n",
      "008566\n",
      "002150\n",
      "002729\n",
      "007658\n",
      "003182\n",
      "008131\n",
      "008941\n",
      "004557\n",
      "005187\n",
      "009387\n",
      "003981\n",
      "002568\n",
      "001367\n",
      "004720\n",
      "005341\n",
      "000254\n",
      "006812\n",
      "002235\n",
      "001477\n",
      "001979\n",
      "008515\n",
      "006169\n",
      "007501\n",
      "006052\n",
      "005495\n",
      "001219\n",
      "002289\n",
      "007242\n",
      "005002\n",
      "006793\n",
      "008214\n",
      "008394\n",
      "004583\n",
      "000890\n",
      "006157\n",
      "001578\n",
      "003851\n",
      "000873\n",
      "002446\n",
      "002792\n",
      "009057\n",
      "003692\n",
      "007107\n",
      "003929\n",
      "003148\n",
      "008684\n",
      "005390\n",
      "005551\n",
      "009010\n",
      "004759\n",
      "000453\n",
      "005188\n",
      "004940\n",
      "009893\n",
      "002467\n",
      "005955\n",
      "002008\n",
      "001956\n",
      "007175\n",
      "000015\n",
      "003747\n",
      "000451\n",
      "006582\n",
      "001380\n",
      "007499\n",
      "001547\n",
      "003217\n",
      "006889\n",
      "001029\n",
      "005758\n",
      "008020\n",
      "008412\n",
      "005105\n",
      "008846\n",
      "002861\n",
      "003494\n",
      "000136\n",
      "009522\n",
      "000429\n",
      "003304\n",
      "003964\n",
      "001196\n",
      "002484\n",
      "007620\n",
      "000910\n",
      "007561\n",
      "008050\n",
      "002438\n",
      "003076\n",
      "000090\n",
      "004094\n",
      "009385\n",
      "007949\n",
      "003944\n",
      "007195\n",
      "009320\n",
      "005255\n",
      "000202\n",
      "005506\n",
      "000291\n",
      "006390\n",
      "006975\n",
      "007783\n",
      "006481\n",
      "004445\n",
      "000360\n",
      "002397\n",
      "007352\n",
      "001303\n",
      "006927\n",
      "006414\n",
      "001243\n",
      "004278\n",
      "004924\n",
      "008802\n",
      "002148\n",
      "002908\n",
      "009033\n",
      "003570\n",
      "009395\n",
      "000960\n",
      "009199\n",
      "000604\n",
      "003631\n",
      "003870\n",
      "003446\n",
      "001710\n",
      "004729\n",
      "006164\n",
      "007316\n",
      "003765\n",
      "003591\n",
      "003297\n",
      "009787\n",
      "002041\n",
      "007353\n",
      "003610\n",
      "008246\n",
      "007807\n",
      "002733\n",
      "004582\n",
      "000512\n",
      "001448\n",
      "001100\n",
      "006049\n",
      "005100\n",
      "006885\n",
      "009233\n",
      "007198\n",
      "002358\n",
      "001031\n",
      "007409\n",
      "008795\n",
      "006517\n",
      "009891\n",
      "000022\n",
      "003318\n",
      "006710\n",
      "002296\n",
      "004090\n",
      "001407\n",
      "004835\n",
      "002839\n",
      "009396\n",
      "005323\n",
      "001574\n",
      "000088\n",
      "009118\n",
      "005046\n",
      "000392\n",
      "000775\n",
      "003481\n",
      "009400\n",
      "007541\n",
      "005334\n",
      "005727\n",
      "008591\n",
      "002856\n",
      "008619\n",
      "009791\n",
      "000284\n",
      "005250\n",
      "007257\n",
      "002344\n",
      "000126\n",
      "005207\n",
      "008196\n",
      "009300\n",
      "004337\n",
      "007471\n",
      "005030\n",
      "005941\n",
      "004147\n",
      "004176\n",
      "006047\n",
      "006901\n",
      "006106\n",
      "000327\n",
      "006937\n",
      "004112\n",
      "009925\n",
      "009962\n",
      "000587\n",
      "008899\n",
      "009402\n",
      "008129\n",
      "001735\n",
      "005301\n",
      "007087\n",
      "003718\n",
      "001016\n",
      "002177\n",
      "006559\n",
      "003908\n",
      "003672\n",
      "003557\n",
      "003985\n",
      "002736\n",
      "003019\n",
      "002830\n",
      "005973\n",
      "001716\n",
      "009261\n",
      "005890\n",
      "000998\n",
      "002122\n",
      "005048\n",
      "009783\n",
      "003862\n",
      "009563\n",
      "002222\n",
      "003502\n",
      "000071\n",
      "002365\n",
      "008152\n",
      "000213\n",
      "008545\n",
      "006137\n",
      "006015\n",
      "005827\n",
      "007502\n",
      "009452\n",
      "003842\n",
      "006313\n",
      "004554\n",
      "000018\n",
      "000687\n",
      "007936\n",
      "000978\n",
      "009552\n",
      "008629\n",
      "000861\n",
      "003101\n",
      "000449\n",
      "000924\n",
      "000415\n",
      "008343\n",
      "008816\n",
      "004179\n",
      "005976\n",
      "006853\n",
      "002837\n",
      "004511\n",
      "003878\n",
      "001165\n",
      "009694\n",
      "005265\n",
      "004114\n",
      "008161\n",
      "002353\n",
      "001823\n",
      "000452\n",
      "004173\n",
      "002394\n",
      "001993\n",
      "001706\n",
      "007778\n",
      "004613\n",
      "004726\n",
      "002119\n",
      "008277\n",
      "006246\n",
      "007750\n",
      "005666\n",
      "004154\n",
      "005294\n",
      "002769\n",
      "002121\n",
      "009829\n",
      "008743\n",
      "002303\n",
      "004418\n",
      "007978\n",
      "004362\n",
      "008321\n",
      "005356\n",
      "006614\n",
      "006232\n",
      "006875\n",
      "009814\n",
      "004320\n",
      "004251\n",
      "001339\n",
      "005688\n",
      "002888\n",
      "004688\n",
      "008649\n",
      "000568\n",
      "009293\n",
      "009788\n",
      "004267\n",
      "007127\n",
      "002951\n",
      "001179\n",
      "004211\n",
      "003291\n",
      "003552\n",
      "008787\n",
      "003485\n",
      "009120\n",
      "008563\n",
      "003850\n",
      "003541\n",
      "005359\n",
      "003174\n",
      "000510\n",
      "009070\n",
      "004428\n",
      "004078\n",
      "001188\n",
      "001335\n",
      "007478\n",
      "009915\n",
      "000986\n",
      "009572\n",
      "000659\n",
      "006964\n",
      "009364\n",
      "000881\n",
      "000821\n",
      "007548\n",
      "002877\n",
      "000409\n",
      "007651\n",
      "007102\n",
      "008605\n",
      "007609\n",
      "009047\n",
      "009602\n",
      "006081\n",
      "007823\n",
      "004740\n",
      "007583\n",
      "009052\n",
      "000584\n",
      "004453\n",
      "004416\n",
      "005834\n",
      "006680\n",
      "000319\n",
      "002679\n",
      "004724\n",
      "007722\n",
      "003115\n",
      "004666\n",
      "009784\n",
      "006358\n",
      "008255\n",
      "008035\n",
      "003483\n",
      "006002\n",
      "001609\n",
      "005180\n",
      "003602\n",
      "001829\n",
      "006616\n",
      "006317\n",
      "003070\n",
      "000027\n",
      "008165\n",
      "004319\n",
      "008712\n",
      "004567\n",
      "004063\n",
      "001302\n",
      "008414\n",
      "007017\n",
      "007698\n",
      "005205\n",
      "007580\n",
      "005837\n",
      "008123\n",
      "000310\n",
      "003224\n",
      "008375\n",
      "005969\n",
      "006322\n",
      "001883\n",
      "007349\n",
      "000611\n",
      "004442\n",
      "002949\n",
      "005141\n",
      "008266\n",
      "007975\n",
      "003172\n",
      "008751\n",
      "007171\n",
      "004297\n",
      "002053\n",
      "009751\n",
      "004941\n",
      "002897\n",
      "009871\n",
      "004711\n",
      "008418\n",
      "004603\n",
      "003252\n",
      "009806\n",
      "009383\n",
      "000010\n",
      "000607\n",
      "004820\n",
      "005482\n",
      "004858\n",
      "003428\n",
      "000425\n",
      "002506\n",
      "003633\n",
      "006386\n",
      "006204\n",
      "000206\n",
      "005725\n",
      "001076\n",
      "003952\n",
      "005785\n",
      "008834\n",
      "003035\n",
      "006410\n",
      "002604\n",
      "001000\n",
      "003075\n",
      "005944\n",
      "004458\n",
      "005096\n",
      "000172\n",
      "007161\n",
      "008827\n",
      "000803\n",
      "004700\n",
      "001007\n",
      "005197\n",
      "000178\n",
      "005437\n",
      "003079\n",
      "008824\n",
      "008631\n",
      "006792\n",
      "005931\n",
      "001657\n",
      "009344\n",
      "000105\n",
      "009023\n",
      "002079\n",
      "009012\n",
      "008803\n",
      "003976\n",
      "003060\n",
      "000558\n",
      "006347\n",
      "008431\n",
      "003558\n",
      "001984\n",
      "007342\n",
      "005798\n",
      "002432\n",
      "005900\n",
      "008143\n",
      "002430\n",
      "002304\n",
      "008193\n",
      "001013\n",
      "002066\n",
      "008678\n",
      "009876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009264\n",
      "008447\n",
      "004640\n",
      "003927\n",
      "004049\n",
      "009575\n",
      "005347\n",
      "006796\n",
      "001162\n",
      "002650\n",
      "008598\n",
      "009014\n",
      "004248\n",
      "001786\n",
      "009017\n",
      "008724\n",
      "002521\n",
      "004930\n",
      "000442\n",
      "002052\n",
      "001462\n",
      "000658\n",
      "002076\n",
      "004030\n",
      "008851\n",
      "001951\n",
      "008734\n",
      "003533\n",
      "005596\n",
      "009257\n",
      "003454\n",
      "008609\n",
      "004072\n",
      "007176\n",
      "003999\n",
      "007553\n",
      "002832\n",
      "000067\n",
      "004889\n",
      "004861\n",
      "000053\n",
      "000856\n",
      "006928\n",
      "001458\n",
      "009356\n",
      "005759\n",
      "006843\n",
      "000927\n",
      "008183\n",
      "006489\n",
      "000723\n",
      "000573\n",
      "008178\n",
      "002622\n",
      "003764\n",
      "007267\n",
      "004311\n",
      "005528\n",
      "004234\n",
      "005213\n",
      "007367\n",
      "006552\n",
      "006555\n",
      "003728\n",
      "002203\n",
      "000693\n",
      "007560\n",
      "007832\n",
      "003245\n",
      "009941\n",
      "000168\n",
      "003766\n",
      "001838\n",
      "004103\n",
      "000623\n",
      "003777\n",
      "006676\n",
      "001890\n",
      "001957\n",
      "003897\n",
      "003906\n",
      "006083\n",
      "006359\n",
      "004262\n",
      "007989\n",
      "009960\n",
      "005196\n",
      "000634\n",
      "000533\n",
      "003368\n",
      "005178\n",
      "006906\n",
      "001505\n",
      "007613\n",
      "008700\n",
      "009653\n",
      "009225\n",
      "000553\n",
      "003277\n",
      "008393\n",
      "005279\n",
      "006144\n",
      "007944\n",
      "000715\n",
      "008918\n",
      "004316\n",
      "003864\n",
      "000265\n",
      "005958\n",
      "001641\n",
      "009622\n",
      "008915\n",
      "009134\n",
      "000286\n",
      "005691\n",
      "002614\n",
      "000473\n",
      "000094\n",
      "005139\n",
      "001381\n",
      "003512\n",
      "003220\n",
      "007136\n",
      "000002\n",
      "007304\n",
      "006539\n",
      "004206\n",
      "005261\n",
      "008839\n",
      "007192\n",
      "006364\n",
      "002127\n",
      "006672\n",
      "009263\n",
      "004276\n",
      "002138\n",
      "009824\n",
      "002793\n",
      "000139\n",
      "001850\n",
      "007112\n",
      "004927\n",
      "008352\n",
      "006817\n",
      "002077\n",
      "001319\n",
      "000201\n",
      "001909\n",
      "006431\n",
      "001216\n",
      "001055\n",
      "000621\n",
      "003693\n",
      "005291\n",
      "007788\n",
      "004755\n",
      "005887\n",
      "005402\n",
      "003503\n",
      "001146\n",
      "008145\n",
      "007770\n",
      "006265\n",
      "007887\n",
      "005112\n",
      "003364\n",
      "007333\n",
      "002511\n",
      "002144\n",
      "001252\n",
      "008490\n",
      "001394\n",
      "000082\n",
      "004299\n",
      "005525\n",
      "001356\n",
      "001566\n",
      "006099\n",
      "007860\n",
      "000547\n",
      "000773\n",
      "003882\n",
      "001329\n",
      "009821\n",
      "005537\n",
      "006980\n",
      "004501\n",
      "005942\n",
      "000669\n",
      "008488\n",
      "009172\n",
      "002455\n",
      "002526\n",
      "005598\n",
      "001814\n",
      "005750\n",
      "001338\n",
      "009363\n",
      "006790\n",
      "000757\n",
      "000006\n",
      "005902\n",
      "003166\n",
      "008897\n",
      "006521\n",
      "000237\n",
      "005399\n",
      "008479\n",
      "004469\n",
      "000665\n",
      "009003\n",
      "005080\n",
      "001745\n",
      "006383\n",
      "006186\n",
      "008089\n",
      "004874\n",
      "008812\n",
      "003476\n",
      "003287\n",
      "007895\n",
      "005411\n",
      "005698\n",
      "009352\n",
      "004462\n",
      "008439\n",
      "002538\n",
      "000106\n",
      "003167\n",
      "004525\n",
      "006415\n",
      "002998\n",
      "008508\n",
      "007392\n",
      "003317\n",
      "001108\n",
      "006467\n",
      "008704\n",
      "004515\n",
      "007310\n",
      "008420\n",
      "008674\n",
      "003592\n",
      "000930\n",
      "006010\n",
      "005898\n",
      "000479\n",
      "009616\n",
      "007358\n",
      "001815\n",
      "002105\n",
      "002475\n",
      "001428\n",
      "007825\n",
      "008219\n",
      "009061\n",
      "007792\n",
      "001089\n",
      "007041\n",
      "001035\n",
      "000040\n",
      "004165\n",
      "002232\n",
      "005751\n",
      "001679\n",
      "008646\n",
      "001033\n",
      "001550\n",
      "003812\n",
      "003014\n",
      "000393\n",
      "008707\n",
      "001997\n",
      "002223\n",
      "007254\n",
      "007348\n",
      "005324\n",
      "003733\n",
      "005770\n",
      "004745\n",
      "001988\n",
      "004709\n",
      "006729\n",
      "002719\n",
      "004665\n",
      "003903\n",
      "001267\n",
      "006326\n",
      "008000\n",
      "005546\n",
      "004851\n",
      "009119\n",
      "008661\n",
      "004061\n",
      "009511\n",
      "002236\n",
      "006898\n",
      "004081\n",
      "006054\n",
      "005607\n",
      "000369\n",
      "009115\n",
      "001942\n",
      "008993\n",
      "004713\n",
      "007508\n",
      "003578\n",
      "005477\n",
      "009555\n",
      "001974\n",
      "007780\n",
      "009384\n",
      "002985\n",
      "002217\n",
      "008118\n",
      "007816\n",
      "007701\n",
      "000350\n",
      "002406\n",
      "007625\n",
      "004216\n",
      "006101\n",
      "000532\n",
      "009482\n",
      "005460\n",
      "005523\n",
      "009280\n",
      "009639\n",
      "009360\n",
      "006653\n",
      "002814\n",
      "008078\n",
      "004900\n",
      "006871\n",
      "006929\n",
      "003904\n",
      "007368\n",
      "004472\n",
      "003431\n",
      "002010\n",
      "005797\n",
      "008206\n",
      "002700\n",
      "008789\n",
      "004249\n",
      "007028\n",
      "009167\n",
      "000976\n",
      "009736\n",
      "007797\n",
      "005243\n",
      "007118\n",
      "003951\n",
      "006758\n",
      "005703\n",
      "004752\n",
      "009569\n",
      "000617\n",
      "006152\n",
      "000817\n",
      "006138\n",
      "006785\n",
      "007738\n",
      "007051\n",
      "005775\n",
      "003776\n",
      "000813\n",
      "008662\n",
      "006795\n",
      "003909\n",
      "005400\n",
      "002414\n",
      "009739\n",
      "003283\n",
      "005170\n",
      "007164\n",
      "004763\n",
      "004377\n",
      "009651\n",
      "002035\n",
      "000378\n",
      "002085\n",
      "009156\n",
      "009607\n",
      "002431\n",
      "002216\n",
      "008600\n",
      "001103\n",
      "004414\n",
      "002672\n",
      "002005\n",
      "002787\n",
      "001696\n",
      "009782\n",
      "003723\n",
      "000356\n",
      "002072\n",
      "000721\n",
      "009963\n",
      "007995\n",
      "005321\n",
      "007406\n",
      "002473\n",
      "009683\n",
      "000570\n",
      "001291\n",
      "005500\n",
      "008058\n",
      "001802\n",
      "009906\n",
      "000905\n",
      "006608\n",
      "004157\n",
      "009514\n",
      "002591\n",
      "008111\n",
      "005621\n",
      "002993\n",
      "000413\n",
      "009041\n",
      "006102\n",
      "001368\n",
      "007744\n",
      "007545\n",
      "008894\n",
      "006505\n",
      "008991\n",
      "004492\n",
      "001271\n",
      "000414\n",
      "001213\n",
      "003055\n",
      "002200\n",
      "007983\n",
      "001572\n",
      "009426\n",
      "009226\n",
      "009223\n",
      "000271\n",
      "008239\n",
      "009077\n",
      "002874\n",
      "003191\n",
      "000835\n",
      "005939\n",
      "000013\n",
      "006087\n",
      "000175\n",
      "001884\n",
      "000536\n",
      "000837\n",
      "003928\n",
      "004324\n",
      "007362\n",
      "007335\n",
      "006921\n",
      "002973\n",
      "005008\n",
      "003745\n",
      "003230\n",
      "004575\n",
      "004448\n",
      "000458\n",
      "002898\n",
      "004744\n",
      "008935\n",
      "005774\n",
      "005792\n",
      "002422\n",
      "005501\n",
      "007599\n",
      "005982\n",
      "008829\n",
      "003650\n",
      "009054\n",
      "000441\n",
      "002118\n",
      "003615\n",
      "008593\n",
      "008672\n",
      "001626\n",
      "002601\n",
      "007332\n",
      "006749\n",
      "003931\n",
      "009827\n",
      "004541\n",
      "002210\n",
      "004444\n",
      "000179\n",
      "002205\n",
      "008185\n",
      "007539\n",
      "007013\n",
      "004979\n",
      "004252\n",
      "008304\n",
      "000376\n",
      "006487\n",
      "008807\n",
      "005675\n",
      "008548\n",
      "006897\n",
      "003432\n",
      "000940\n",
      "008071\n",
      "005412\n",
      "006590\n",
      "004088\n",
      "009124\n",
      "004980\n",
      "004833\n",
      "001929\n",
      "002805\n",
      "007320\n",
      "002748\n",
      "009547\n",
      "009750\n",
      "007674\n",
      "008047\n",
      "000759\n",
      "001698\n",
      "004891\n",
      "003345\n",
      "007094\n",
      "001910\n",
      "005289\n",
      "003171\n",
      "001155\n",
      "000955\n",
      "002865\n",
      "006191\n",
      "004909\n",
      "006644\n",
      "009044\n",
      "001709\n",
      "005965\n",
      "008788\n",
      "009011\n",
      "008697\n",
      "008497\n",
      "004615\n",
      "000025\n",
      "006013\n",
      "004126\n",
      "006540\n",
      "009723\n",
      "002638\n",
      "001737\n",
      "006116\n",
      "005192\n",
      "005271\n",
      "002424\n",
      "008314\n",
      "004486\n",
      "009648\n",
      "006518\n",
      "005795\n",
      "002904\n",
      "000145\n",
      "003251\n",
      "002752\n",
      "006435\n",
      "001306\n",
      "000975\n",
      "000603\n",
      "008791\n",
      "009140\n",
      "004160\n",
      "006974\n",
      "006774\n",
      "004559\n",
      "000618\n",
      "009509\n",
      "008474\n",
      "002474\n",
      "004388\n",
      "001575\n",
      "002048\n",
      "001190\n",
      "004153\n",
      "000778\n",
      "009665\n",
      "003829\n",
      "006253\n",
      "007628\n",
      "004738\n",
      "000697\n",
      "003540\n",
      "001886\n",
      "007993\n",
      "000049\n",
      "009391\n",
      "007026\n",
      "009771\n",
      "004845\n",
      "009744\n",
      "006594\n",
      "001702\n",
      "002808\n",
      "001307\n",
      "003215\n",
      "003139\n",
      "007595\n",
      "005276\n",
      "009559\n",
      "004127\n",
      "007922\n",
      "003682\n",
      "000891\n",
      "007771\n",
      "006888\n",
      "002823\n",
      "008441\n",
      "009093\n",
      "007083\n",
      "008551\n",
      "009608\n",
      "006894\n",
      "004188\n",
      "004871\n",
      "008377\n",
      "003448\n",
      "000069\n",
      "006469\n",
      "005962\n",
      "008022\n",
      "004922\n",
      "007239\n",
      "003902\n",
      "008949\n",
      "004342\n",
      "009544\n",
      "001051\n",
      "005810\n",
      "003517\n",
      "008102\n",
      "004373\n",
      "002489\n",
      "009201\n",
      "004335\n",
      "007644\n",
      "003442\n",
      "000819\n",
      "002927\n",
      "001039\n",
      "007844\n",
      "009222\n",
      "007206\n",
      "008242\n",
      "007638\n",
      "004915\n",
      "003825\n",
      "003995\n",
      "008560\n",
      "000096\n",
      "002694\n",
      "000968\n",
      "004000\n",
      "005720\n",
      "009625\n",
      "001656\n",
      "008922\n",
      "003050\n",
      "003227\n",
      "002894\n",
      "000961\n",
      "001965\n",
      "002849\n",
      "000792\n",
      "001474\n",
      "002588\n",
      "009534\n",
      "001520\n",
      "000551\n",
      "009319\n",
      "006941\n",
      "001308\n",
      "005957\n",
      "005070\n",
      "007610\n",
      "004664\n",
      "006651\n",
      "005317\n",
      "009501\n",
      "009229\n",
      "001564\n",
      "009899\n",
      "004101\n",
      "001994\n",
      "007929\n",
      "000290\n",
      "005172\n",
      "001923\n",
      "004151\n",
      "004971\n",
      "006717\n",
      "007453\n",
      "008658\n",
      "006913\n",
      "002892\n",
      "006656\n",
      "000719\n",
      "007893\n",
      "003982\n",
      "001081\n",
      "009009\n",
      "002339\n",
      "008399\n",
      "003687\n",
      "006008\n",
      "006145\n",
      "003910\n",
      "005847\n",
      "000941\n",
      "004669\n",
      "006586\n",
      "008664\n",
      "007272\n",
      "008719\n",
      "009804\n",
      "009135\n",
      "000366\n",
      "003241\n",
      "006255\n",
      "006715\n",
      "006545\n",
      "004993\n",
      "000807\n",
      "006508\n",
      "002530\n",
      "000029\n",
      "008486\n",
      "001762\n",
      "001139\n",
      "002250\n",
      "008347\n",
      "005216\n",
      "009055\n",
      "005612\n",
      "000722\n",
      "007098\n",
      "005999\n",
      "008451\n",
      "004254\n",
      "003943\n",
      "000785\n",
      "000358\n",
      "007495\n",
      "003526\n",
      "003701\n",
      "008094\n",
      "001123\n",
      "002936\n",
      "003977\n",
      "002517\n",
      "008471\n",
      "006165\n",
      "001893\n",
      "005846\n",
      "006846\n",
      "004945\n",
      "000097\n",
      "006811\n",
      "007774\n",
      "009021\n",
      "004128\n",
      "000572\n",
      "004736\n",
      "000144\n",
      "003544\n",
      "005474\n",
      "001094\n",
      "000299\n",
      "008238\n",
      "001914\n",
      "002929\n",
      "009592\n",
      "003799\n",
      "005802\n",
      "003916\n",
      "003607\n",
      "004068\n",
      "006823\n",
      "000385\n",
      "001856\n",
      "006311\n",
      "006112\n",
      "000824\n",
      "001461\n",
      "004969\n",
      "000342\n",
      "001569\n",
      "001022\n",
      "003630\n",
      "001320\n",
      "007851\n",
      "009487\n",
      "003069\n",
      "003978\n",
      "008860\n",
      "000315\n",
      "007377\n",
      "008632\n",
      "006686\n",
      "003782\n",
      "000969\n",
      "006890\n",
      "007549\n",
      "007238\n",
      "006490\n",
      "002926\n",
      "006691\n",
      "005353\n",
      "001003\n",
      "002905\n",
      "000839\n",
      "003900\n",
      "000716\n",
      "004503\n",
      "003286\n",
      "002607\n",
      "001411\n",
      "008500\n",
      "005225\n",
      "003309\n",
      "002562\n",
      "008099\n",
      "001552\n",
      "009415\n",
      "000655\n",
      "005635\n",
      "009937\n",
      "007962\n",
      "003136\n",
      "000909\n",
      "005642\n",
      "006076\n",
      "009234\n",
      "002242\n",
      "007397\n",
      "009525\n",
      "006471\n",
      "005943\n",
      "004741\n",
      "004731\n",
      "002239\n",
      "006407\n",
      "009766\n",
      "006961\n",
      "009486\n",
      "007209\n",
      "004002\n",
      "002543\n",
      "008696\n",
      "005916\n",
      "002587\n",
      "007917\n",
      "002346\n",
      "000844\n",
      "004406\n",
      "007019\n",
      "009101\n",
      "003246\n",
      "004725\n",
      "008065\n",
      "004923\n",
      "007961\n",
      "001645\n",
      "000295\n",
      "001500\n",
      "006368\n",
      "005787\n",
      "001623\n",
      "009606\n",
      "007676\n",
      "008554\n",
      "004393\n",
      "004677\n",
      "000701\n",
      "004798\n",
      "004213\n",
      "007591\n",
      "009714\n",
      "004344\n",
      "008109\n",
      "001694\n",
      "005778\n",
      "005737\n",
      "000056\n",
      "001087\n",
      "007226\n",
      "000629\n",
      "004162\n",
      "004795\n",
      "009837\n",
      "004483\n",
      "006831\n",
      "004680\n",
      "008154\n",
      "006372\n",
      "007010\n",
      "001695\n",
      "007896\n",
      "004456\n",
      "002429\n",
      "001025\n",
      "002396\n",
      "008357\n",
      "002014\n",
      "009554\n",
      "006613\n",
      "006589\n",
      "009708\n",
      "004529\n",
      "000641\n",
      "003322\n",
      "005540\n",
      "006388\n",
      "002516\n",
      "001496\n",
      "000840\n",
      "007428\n",
      "004261\n",
      "002286\n",
      "005322\n",
      "004440\n",
      "004728\n",
      "004381\n",
      "004055\n",
      "005462\n",
      "005163\n",
      "009690\n",
      "008574\n",
      "009731\n",
      "004667\n",
      "005376\n",
      "002655\n",
      "003643\n",
      "000988\n",
      "003384\n",
      "007604\n",
      "002552\n",
      "003340\n",
      "004767\n",
      "004222\n",
      "000507\n",
      "007183\n",
      "008480\n",
      "000287\n",
      "000886\n",
      "005162\n",
      "006263\n",
      "002167\n",
      "004289\n",
      "004538\n",
      "007103\n",
      "009530\n",
      "004817\n",
      "004305\n",
      "002685\n",
      "000412\n",
      "007574\n",
      "004756\n",
      "004118\n",
      "006960\n",
      "000455\n",
      "006439\n",
      "002871\n",
      "004762\n",
      "003314\n",
      "000539\n",
      "005357\n",
      "000945\n",
      "002629\n",
      "007399\n",
      "007516\n",
      "003475\n",
      "009274\n",
      "008659\n",
      "008383\n",
      "008808\n",
      "008133\n",
      "001080\n",
      "003854\n",
      "002781\n",
      "002159\n",
      "008379\n",
      "001925\n",
      "008210\n",
      "009680\n",
      "004620\n",
      "008845\n",
      "005553\n",
      "008830\n",
      "001596\n",
      "004771\n",
      "009284\n",
      "003892\n",
      "002195\n",
      "003482\n",
      "003893\n",
      "004838\n",
      "006620\n",
      "001602\n",
      "000933\n",
      "000114\n",
      "003128\n",
      "004403\n",
      "004589\n",
      "008538\n",
      "001624\n",
      "000301\n",
      "007842\n",
      "009380\n",
      "009345\n",
      "008380\n",
      "005721\n",
      "004306\n",
      "009028\n",
      "004007\n",
      "007309\n",
      "007005\n",
      "008172\n",
      "000157\n",
      "009779\n",
      "003275\n",
      "001351\n",
      "000706\n",
      "001183\n",
      "003026\n",
      "002790\n",
      "000280\n",
      "008708\n",
      "005946\n",
      "003768\n",
      "006592\n",
      "009026\n",
      "009204\n",
      "005120\n",
      "004412\n",
      "007686\n",
      "009826\n",
      "003472\n",
      "001246\n",
      "004668\n",
      "009715\n",
      "001157\n",
      "009740\n",
      "005038\n",
      "002115\n",
      "005896\n",
      "000068\n",
      "001851\n",
      "007512\n",
      "001032\n",
      "008954\n",
      "002137\n",
      "008243\n",
      "001959\n",
      "000858\n",
      "003084\n",
      "008455\n",
      "009675\n",
      "001005\n",
      "003321\n",
      "005463\n",
      "004505\n",
      "008761\n",
      "008781\n",
      "004865\n",
      "007986\n",
      "002862\n",
      "007708\n",
      "007371\n",
      "004236\n",
      "002911\n",
      "007769\n",
      "009451\n",
      "003457\n",
      "003800\n",
      "006307\n",
      "000426\n",
      "003099\n",
      "006604\n",
      "009843\n",
      "009084\n",
      "001416\n",
      "002379\n",
      "007918\n",
      "009430\n",
      "001227\n",
      "003478\n",
      "002258\n",
      "005082\n",
      "009311\n",
      "005193\n",
      "007588\n",
      "003755\n",
      "004378\n",
      "005286\n",
      "009725\n",
      "007739\n",
      "002903\n",
      "003805\n",
      "008361\n",
      "005684\n",
      "001438\n",
      "001132\n",
      "002711\n",
      "000011\n",
      "005392\n",
      "000567\n",
      "005010\n",
      "009688\n",
      "008324\n",
      "006085\n",
      "004546\n",
      "008579\n",
      "009071\n",
      "005132\n",
      "004887\n",
      "006324\n",
      "003179\n",
      "007927\n",
      "007034\n",
      "007858\n",
      "008544\n",
      "004353\n",
      "003626\n",
      "002154\n",
      "009219\n",
      "008673\n",
      "000267\n",
      "007387\n",
      "004074\n",
      "007710\n",
      "003959\n",
      "003095\n",
      "000595\n",
      "001763\n",
      "005211\n",
      "008850\n",
      "003572\n",
      "002416\n",
      "008308\n",
      "002716\n",
      "008868\n",
      "003922\n",
      "006370\n",
      "001020\n",
      "009663\n",
      "006394\n",
      "007492\n",
      "007262\n",
      "002640\n",
      "008656\n",
      "003073\n",
      "006863\n",
      "008681\n",
      "002013\n",
      "008074\n",
      "005021\n",
      "007892\n",
      "003456\n",
      "001131\n",
      "004774\n",
      "007643\n",
      "004348\n",
      "008903\n",
      "003375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "006970\n",
      "000779\n",
      "007218\n",
      "003762\n",
      "006877\n",
      "005626\n",
      "001173\n",
      "009317\n",
      "009038\n",
      "007913\n",
      "007290\n",
      "008464\n",
      "003757\n",
      "009561\n",
      "009216\n",
      "005767\n",
      "004079\n",
      "004156\n",
      "005804\n",
      "008367\n",
      "003520\n",
      "007734\n",
      "008505\n",
      "008804\n",
      "007030\n",
      "000281\n",
      "009031\n",
      "005466\n",
      "005712\n",
      "004290\n",
      "001774\n",
      "005627\n",
      "002950\n",
      "008866\n",
      "002026\n",
      "004944\n",
      "003409\n",
      "009626\n",
      "001403\n",
      "005610\n",
      "005059\n",
      "009444\n",
      "004892\n",
      "003113\n",
      "001503\n",
      "000836\n",
      "006646\n",
      "000368\n",
      "007659\n",
      "000111\n",
      "008006\n",
      "008957\n",
      "008476\n",
      "006826\n",
      "003332\n",
      "005468\n",
      "004383\n",
      "002297\n",
      "007870\n",
      "001038\n",
      "001664\n",
      "008245\n",
      "003479\n",
      "003319\n",
      "006528\n",
      "003043\n",
      "007733\n",
      "000884\n",
      "005808\n",
      "001824\n",
      "000560\n",
      "004235\n",
      "007456\n",
      "004919\n",
      "005157\n",
      "004199\n",
      "009548\n",
      "001006\n",
      "001704\n",
      "004536\n",
      "004564\n",
      "000004\n",
      "003582\n",
      "002326\n",
      "000087\n",
      "001090\n",
      "005745\n",
      "009211\n",
      "006451\n",
      "003883\n",
      "008844\n",
      "007982\n",
      "001355\n",
      "001943\n",
      "007402\n",
      "005137\n",
      "003676\n",
      "006143\n",
      "001297\n",
      "003201\n",
      "003574\n",
      "000248\n",
      "000155\n",
      "000399\n",
      "004187\n",
      "004336\n",
      "009097\n",
      "007281\n",
      "008179\n",
      "008110\n",
      "009936\n",
      "005066\n",
      "001876\n",
      "005240\n",
      "007246\n",
      "005904\n",
      "002507\n",
      "008149\n",
      "004449\n",
      "000199\n",
      "006192\n",
      "005069\n",
      "007142\n",
      "007596\n",
      "000466\n",
      "000736\n",
      "004775\n",
      "000985\n",
      "000243\n",
      "004394\n",
      "001700\n",
      "009206\n",
      "001067\n",
      "004780\n",
      "002620\n",
      "002948\n",
      "001808\n",
      "004132\n",
      "005959\n",
      "007811\n",
      "002271\n",
      "002097\n",
      "004130\n",
      "005339\n",
      "005206\n",
      "005891\n",
      "008428\n",
      "001392\n",
      "007660\n",
      "003411\n",
      "005649\n",
      "007032\n",
      "008240\n",
      "005967\n",
      "001905\n",
      "009441\n",
      "008625\n",
      "003659\n",
      "007690\n",
      "001671\n",
      "008396\n",
      "000529\n",
      "004698\n",
      "002074\n",
      "003329\n",
      "008289\n",
      "008616\n",
      "000758\n",
      "007014\n",
      "001122\n",
      "002851\n",
      "009556\n",
      "004730\n",
      "006297\n",
      "007085\n",
      "005809\n",
      "003371\n",
      "004447\n",
      "008270\n",
      "002656\n",
      "001996\n",
      "002674\n",
      "001748\n",
      "001968\n",
      "000696\n",
      "005118\n",
      "004001\n",
      "000907\n",
      "005800\n",
      "006036\n",
      "001530\n",
      "002275\n",
      "005362\n",
      "008469\n",
      "000388\n",
      "006537\n",
      "006525\n",
      "009645\n",
      "000562\n",
      "009485\n",
      "002402\n",
      "000037\n",
      "000901\n",
      "000119\n",
      "006274\n",
      "005534\n",
      "002093\n",
      "005933\n",
      "001873\n",
      "001939\n",
      "002495\n",
      "003400\n",
      "006598\n",
      "001425\n",
      "004175\n",
      "009889\n",
      "005661\n",
      "004208\n",
      "003545\n",
      "001431\n",
      "002100\n",
      "007488\n",
      "005458\n",
      "002113\n",
      "003091\n",
      "003819\n",
      "007155\n",
      "002900\n",
      "006573\n",
      "003504\n",
      "009104\n",
      "006982\n",
      "000014\n",
      "008637\n",
      "000465\n",
      "001275\n",
      "002527\n",
      "009888\n",
      "003263\n",
      "005316\n",
      "000527\n",
      "006770\n",
      "008276\n",
      "008594\n",
      "007534\n",
      "008362\n",
      "004070\n",
      "004733\n",
      "004516\n",
      "002485\n",
      "008552\n",
      "009850\n",
      "007741\n",
      "001817\n",
      "009241\n",
      "001419\n",
      "001819\n",
      "005820\n",
      "007384\n",
      "005907\n",
      "003440\n",
      "002204\n",
      "009471\n",
      "003490\n",
      "001731\n",
      "004210\n",
      "002688\n",
      "007515\n",
      "009322\n",
      "002720\n",
      "006271\n",
      "008745\n",
      "005364\n",
      "005927\n",
      "000585\n",
      "000939\n",
      "001085\n",
      "005504\n",
      "000227\n",
      "006640\n",
      "005375\n",
      "007115\n",
      "007001\n",
      "006293\n",
      "008578\n",
      "004661\n",
      "002597\n",
      "007808\n",
      "009427\n",
      "003823\n",
      "000240\n",
      "004824\n",
      "007635\n",
      "004822\n",
      "001296\n",
      "009912\n",
      "006194\n",
      "007324\n",
      "003741\n",
      "007278\n",
      "001023\n",
      "003289\n",
      "001391\n",
      "008782\n",
      "005882\n",
      "003787\n",
      "004426\n",
      "003196\n",
      "002149\n",
      "003334\n",
      "000176\n",
      "006568\n",
      "001549\n",
      "003940\n",
      "002930\n",
      "009277\n",
      "008348\n",
      "008016\n",
      "004417\n",
      "009046\n",
      "003383\n",
      "000108\n",
      "009154\n",
      "005594\n",
      "004400\n",
      "002531\n",
      "001728\n",
      "004038\n",
      "002314\n",
      "000239\n",
      "007157\n",
      "000128\n",
      "000963\n",
      "005174\n",
      "009090\n",
      "005000\n",
      "003581\n",
      "002821\n",
      "005106\n",
      "008855\n",
      "006624\n",
      "006454\n",
      "008113\n",
      "000103\n",
      "007801\n",
      "009536\n",
      "006040\n",
      "001218\n",
      "003725\n",
      "007955\n",
      "005690\n",
      "004970\n",
      "007684\n",
      "001482\n",
      "008532\n",
      "007069\n",
      "006464\n",
      "009165\n",
      "007552\n",
      "005491\n",
      "008366\n",
      "006904\n",
      "009107\n",
      "001644\n",
      "001701\n",
      "009957\n",
      "002254\n",
      "000698\n",
      "005844\n",
      "001357\n",
      "001895\n",
      "002225\n",
      "007307\n",
      "005659\n",
      "002612\n",
      "006851\n",
      "005892\n",
      "007407\n",
      "009795\n",
      "009953\n",
      "004614\n",
      "006705\n",
      "005051\n",
      "007179\n",
      "003573\n",
      "002383\n",
      "006502\n",
      "004543\n",
      "007315\n",
      "007587\n",
      "000186\n",
      "005031\n",
      "009125\n",
      "008895\n",
      "003361\n",
      "005886\n",
      "007500\n",
      "005589\n",
      "001949\n",
      "004413\n",
      "001540\n",
      "001967\n",
      "009266\n",
      "007942\n",
      "004218\n",
      "002705\n",
      "008531\n",
      "006060\n",
      "001276\n",
      "000100\n",
      "006650\n",
      "000160\n",
      "006999\n",
      "002349\n",
      "000423\n",
      "001908\n",
      "004056\n",
      "002619\n",
      "007264\n",
      "002057\n",
      "001262\n",
      "007403\n",
      "009493\n",
      "005098\n",
      "007319\n",
      "004758\n",
      "004083\n",
      "008457\n",
      "001114\n",
      "003474\n",
      "008233\n",
      "004238\n",
      "008555\n",
      "001197\n",
      "008198\n",
      "008378\n",
      "002768\n",
      "005754\n",
      "002386\n",
      "000028\n",
      "006561\n",
      "004804\n",
      "009183\n",
      "002133\n",
      "004875\n",
      "004766\n",
      "007861\n",
      "007096\n",
      "008767\n",
      "004184\n",
      "005091\n",
      "004751\n",
      "000196\n",
      "002739\n",
      "008114\n",
      "003372\n",
      "006182\n",
      "009817\n",
      "001592\n",
      "005520\n",
      "002756\n",
      "004053\n",
      "007444\n",
      "002887\n",
      "005707\n",
      "006563\n",
      "005053\n",
      "007725\n",
      "007301\n",
      "002418\n",
      "005777\n",
      "006857\n",
      "000893\n",
      "008705\n",
      "009170\n",
      "005442\n",
      "007313\n",
      "000732\n",
      "005739\n",
      "007464\n",
      "005149\n",
      "008945\n",
      "003237\n",
      "000596\n",
      "004854\n",
      "006422\n",
      "000341\n",
      "006675\n",
      "001429\n",
      "001839\n",
      "008689\n",
      "000670\n",
      "002229\n",
      "006478\n",
      "001280\n",
      "000389\n",
      "008528\n",
      "004624\n",
      "004844\n",
      "007338\n",
      "003265\n",
      "003152\n",
      "003312\n",
      "007848\n",
      "009574\n",
      "003486\n",
      "009856\n",
      "009582\n",
      "001812\n",
      "008537\n",
      "003153\n",
      "004952\n",
      "001666\n",
      "003950\n",
      "001366\n",
      "001178\n",
      "003221\n",
      "001019\n",
      "005049\n",
      "009231\n",
      "007782\n",
      "001897\n",
      "004250\n",
      "001776\n",
      "006178\n",
      "001518\n",
      "005125\n",
      "007990\n",
      "002876\n",
      "006754\n",
      "004989\n",
      "000644\n",
      "005443\n",
      "000769\n",
      "003894\n",
      "001058\n",
      "008402\n",
      "004843\n",
      "008540\n",
      "005987\n",
      "005556\n",
      "009793\n",
      "003595\n",
      "006003\n",
      "008778\n",
      "002089\n",
      "008187\n",
      "001138\n",
      "003569\n",
      "001133\n",
      "005284\n",
      "000137\n",
      "006393\n",
      "001495\n",
      "006328\n",
      "002883\n",
      "009207\n",
      "008371\n",
      "001086\n",
      "001489\n",
      "007220\n",
      "001202\n",
      "002175\n",
      "009243\n",
      "009321\n",
      "002141\n",
      "008901\n",
      "001935\n",
      "002608\n",
      "001358\n",
      "000493\n",
      "005167\n",
      "001128\n",
      "008686\n",
      "002665\n",
      "009704\n",
      "000678\n",
      "003942\n",
      "003062\n",
      "002398\n",
      "001217\n",
      "000410\n",
      "007717\n",
      "009742\n",
      "002843\n",
      "001282\n",
      "008181\n",
      "003437\n",
      "001150\n",
      "006837\n",
      "005109\n",
      "009232\n",
      "007459\n",
      "004465\n",
      "009095\n",
      "001760\n",
      "008950\n",
      "004864\n",
      "000990\n",
      "006881\n",
      "001629\n",
      "001844\n",
      "001719\n",
      "008907\n",
      "000897\n",
      "001599\n",
      "008018\n",
      "004932\n",
      "000646\n",
      "005569\n",
      "002408\n",
      "000330\n",
      "002164\n",
      "006121\n",
      "009145\n",
      "003109\n",
      "008937\n",
      "006688\n",
      "008648\n",
      "006336\n",
      "001791\n",
      "005869\n",
      "000557\n",
      "007992\n",
      "002890\n",
      "008597\n",
      "008124\n",
      "009820\n",
      "004116\n",
      "001446\n",
      "004225\n",
      "008634\n",
      "007472\n",
      "002602\n",
      "002750\n",
      "000444\n",
      "009103\n",
      "000274\n",
      "004813\n",
      "001506\n",
      "007960\n",
      "009302\n",
      "006485\n",
      "007273\n",
      "008237\n",
      "009074\n",
      "005711\n",
      "003619\n",
      "009467\n",
      "005667\n",
      "000098\n",
      "006461\n",
      "009775\n",
      "002357\n",
      "007462\n",
      "002294\n",
      "003111\n",
      "005432\n",
      "001264\n",
      "003399\n",
      "002292\n",
      "001321\n",
      "008511\n",
      "002161\n",
      "007796\n",
      "004533\n",
      "005673\n",
      "007800\n",
      "007317\n",
      "003568\n",
      "008184\n",
      "005075\n",
      "007233\n",
      "009473\n",
      "002160\n",
      "005200\n",
      "008504\n",
      "006245\n",
      "009885\n",
      "002038\n",
      "002029\n",
      "003534\n",
      "007930\n",
      "007265\n",
      "009267\n",
      "003852\n",
      "003535\n",
      "002592\n",
      "005763\n",
      "006872\n",
      "009610\n",
      "001452\n",
      "005277\n",
      "003697\n",
      "004560\n",
      "009873\n",
      "005516\n",
      "007941\n",
      "007529\n",
      "002610\n",
      "004619\n",
      "005464\n",
      "000683\n",
      "009376\n",
      "003040\n",
      "003387\n",
      "006294\n",
      "008176\n",
      "003542\n",
      "008440\n",
      "003707\n",
      "002173\n",
      "005870\n",
      "008539\n",
      "001646\n",
      "000749\n",
      "007531\n",
      "007967\n",
      "007607\n",
      "009332\n",
      "002421\n",
      "001998\n",
      "001135\n",
      "002724\n",
      "007785\n",
      "003858\n",
      "001471\n",
      "008869\n",
      "001433\n",
      "005427\n",
      "005490\n",
      "007689\n",
      "007379\n",
      "009919\n",
      "009632\n",
      "001672\n",
      "009143\n",
      "007027\n",
      "006630\n",
      "004573\n",
      "001581\n",
      "001047\n",
      "000079\n",
      "008119\n",
      "002381\n",
      "005570\n",
      "001456\n",
      "004022\n",
      "001096\n",
      "007507\n",
      "009304\n",
      "004227\n",
      "005377\n",
      "004806\n",
      "004918\n",
      "001195\n",
      "009768\n",
      "003388\n",
      "004475\n",
      "009612\n",
      "003405\n",
      "000195\n",
      "000402\n",
      "007573\n",
      "002630\n",
      "005372\n",
      "003310\n",
      "001705\n",
      "003815\n",
      "003414\n",
      "007972\n",
      "001180\n",
      "000984\n",
      "005201\n",
      "008651\n",
      "005683\n",
      "000578\n",
      "001105\n",
      "001794\n",
      "000230\n",
      "007759\n",
      "008446\n",
      "001210\n",
      "004769\n",
      "002106\n",
      "004594\n",
      "000957\n",
      "009930\n",
      "005708\n",
      "001803\n",
      "006310\n",
      "007382\n",
      "006997\n",
      "001473\n",
      "009416\n",
      "005567\n",
      "003968\n",
      "001141\n",
      "007496\n",
      "004045\n",
      "009122\n",
      "002553\n",
      "002806\n",
      "002426\n",
      "006915\n",
      "000264\n",
      "005035\n",
      "008972\n",
      "001867\n",
      "005753\n",
      "006764\n",
      "001040\n",
      "003840\n",
      "003665\n",
      "007235\n",
      "003266\n",
      "008215\n",
      "003560\n",
      "009840\n",
      "005332\n",
      "000673\n",
      "009730\n",
      "003785\n",
      "002703\n",
      "009381\n",
      "009786\n",
      "003353\n",
      "004139\n",
      "004477\n",
      "002813\n",
      "002503\n",
      "002211\n",
      "005822\n",
      "001987\n",
      "006132\n",
      "001037\n",
      "006720\n",
      "001370\n",
      "009812\n",
      "009705\n",
      "003012\n",
      "002003\n",
      "008912\n",
      "008694\n",
      "008974\n",
      "009769\n",
      "005548\n",
      "000580\n",
      "009190\n",
      "001342\n",
      "004064\n",
      "005147\n",
      "009601\n",
      "009110\n",
      "001687\n",
      "006109\n",
      "000326\n",
      "000151\n",
      "005480\n",
      "007321\n",
      "004663\n",
      "000639\n",
      "006406\n",
      "002486\n",
      "005164\n",
      "001126\n",
      "004888\n",
      "008013\n",
      "008611\n",
      "006278\n",
      "000974\n",
      "003234\n",
      "009083\n",
      "000703\n",
      "007229\n",
      "000789\n",
      "008411\n",
      "006423\n",
      "001900\n",
      "004024\n",
      "002959\n",
      "003514\n",
      "003933\n",
      "009799\n",
      "005638\n",
      "008146\n",
      "000472\n",
      "004899\n",
      "006946\n",
      "009631\n",
      "001519\n",
      "008011\n",
      "006420\n",
      "009276\n",
      "003006\n",
      "008583\n",
      "005308\n",
      "007827\n",
      "004107\n",
      "006815\n",
      "009922\n",
      "004467\n",
      "007822\n",
      "008247\n",
      "004578\n",
      "006804\n",
      "000313\n",
      "007287\n",
      "004134\n",
      "004561\n",
      "006978\n",
      "007891\n",
      "008287\n",
      "002499\n",
      "006533\n",
      "009088\n",
      "000488\n",
      "001034\n",
      "000384\n",
      "007652\n",
      "007994\n",
      "004198\n",
      "006713\n",
      "003538\n",
      "003151\n",
      "009164\n",
      "008487\n",
      "003710\n",
      "008081\n",
      "005074\n",
      "009474\n",
      "006303\n",
      "009483\n",
      "000542\n",
      "003975\n",
      "008501\n",
      "001796\n",
      "006700\n",
      "003719\n",
      "000662\n",
      "004317\n",
      "008825\n",
      "008162\n",
      "000074\n",
      "005218\n",
      "001966\n",
      "007420\n",
      "007015\n",
      "006207\n",
      "003206\n",
      "004521\n",
      "003989\n",
      "006477\n",
      "006343\n",
      "000724\n",
      "008088\n",
      "007145\n",
      "003802\n",
      "009492\n",
      "006118\n",
      "007253\n",
      "000747\n",
      "008496\n",
      "008170\n",
      "005226\n",
      "002336\n",
      "006405\n",
      "000383\n",
      "006155\n",
      "008135\n",
      "001613\n",
      "002660\n",
      "003896\n",
      "007966\n",
      "001621\n",
      "005152\n",
      "004616\n",
      "000422\n",
      "000086\n",
      "001811\n",
      "003488\n",
      "002673\n",
      "001278\n",
      "006879\n",
      "009657\n",
      "004099\n",
      "008703\n",
      "009425\n",
      "001533\n",
      "005865\n",
      "004829\n",
      "008627\n",
      "004712\n",
      "003532\n",
      "009372\n",
      "004358\n",
      "001063\n",
      "006546\n",
      "003033\n",
      "005771\n",
      "006193\n",
      "007830\n",
      "000506\n",
      "007135\n",
      "009595\n",
      "008136\n",
      "007331\n",
      "008510\n",
      "005113\n",
      "006723\n",
      "005493\n",
      "008652\n",
      "009741\n",
      "006006\n",
      "001313\n",
      "006014\n",
      "001099\n",
      "003730\n",
      "008010\n",
      "005127\n",
      "009152\n",
      "003980\n",
      "006923\n",
      "001648\n",
      "001359\n",
      "001098\n",
      "007965\n",
      "003680\n",
      "002412\n",
      "001953\n",
      "008273\n",
      "001668\n",
      "007948\n",
      "004374\n",
      "008334\n",
      "008666\n",
      "005634\n",
      "001075\n",
      "005247\n",
      "001990\n",
      "007255\n",
      "007000\n",
      "003583\n",
      "000127\n",
      "001242\n",
      "009948\n",
      "002573\n",
      "005565\n",
      "004427\n",
      "009310\n",
      "001424\n",
      "007401\n",
      "000745\n",
      "005936\n",
      "008589\n",
      "005849\n",
      "002872\n",
      "006092\n",
      "009237\n",
      "000401\n",
      "007156\n",
      "005581\n",
      "005842\n",
      "002198\n",
      "005616\n",
      "007752\n",
      "008981\n",
      "002852\n",
      "000922\n",
      "008330\n",
      "009217\n",
      "006662\n",
      "000651\n",
      "005597\n",
      "005425\n",
      "000666\n",
      "001813\n",
      "006334\n",
      "009370\n",
      "001591\n",
      "002400\n",
      "002282\n",
      "008134\n",
      "001491\n",
      "008569\n",
      "002824\n",
      "004772\n",
      "008070\n",
      "006412\n",
      "008227\n",
      "005866\n",
      "001743\n",
      "004513\n",
      "009313\n",
      "001757\n",
      "004906\n",
      "001538\n",
      "002016\n",
      "009931\n",
      "006119\n",
      "002809\n",
      "003068\n",
      "005415\n",
      "004657\n",
      "006365\n",
      "002692\n",
      "007366\n",
      "002107\n",
      "003081\n",
      "008992\n",
      "004703\n",
      "004041\n",
      "007328\n",
      "005746\n",
      "005300\n",
      "009866\n",
      "003562\n",
      "003326\n",
      "009539\n",
      "009924\n",
      "005476\n",
      "008217\n",
      "004219\n",
      "004473\n",
      "000650\n",
      "000788\n",
      "004599\n",
      "006237\n",
      "001570\n",
      "002522\n",
      "002789\n",
      "008565\n",
      "003553\n",
      "006801\n",
      "003141\n",
      "008228\n",
      "007879\n",
      "006016\n",
      "000870\n",
      "006034\n",
      "007405\n",
      "001344\n",
      "005256\n",
      "000979\n",
      "008575\n",
      "002389\n",
      "009728\n",
      "001879\n",
      "002818\n",
      "003208\n",
      "007806\n",
      "004809\n",
      "004883\n",
      "001044\n",
      "004681\n",
      "008045\n",
      "007593\n",
      "003513\n",
      "006227\n",
      "000260\n",
      "004123\n",
      "006639\n",
      "001660\n",
      "004155\n",
      "000045\n",
      "007634\n",
      "004245\n",
      "004098\n",
      "008059\n",
      "001363\n",
      "009883\n",
      "009753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001869\n",
      "000234\n",
      "002907\n",
      "006075\n",
      "006280\n",
      "004443\n",
      "004197\n",
      "001912\n",
      "003459\n",
      "005123\n",
      "009599\n",
      "009727\n",
      "007063\n",
      "004040\n",
      "003794\n",
      "006830\n",
      "003875\n",
      "003527\n",
      "009328\n",
      "001487\n",
      "000495\n",
      "004214\n",
      "008599\n",
      "000649\n",
      "008711\n",
      "001340\n",
      "007784\n",
      "004590\n",
      "007761\n",
      "006788\n",
      "004593\n",
      "008038\n",
      "004086\n",
      "001030\n",
      "006780\n",
      "002464\n",
      "001891\n",
      "005807\n",
      "007726\n",
      "004385\n",
      "008432\n",
      "008832\n",
      "003447\n",
      "001508\n",
      "003324\n",
      "001181\n",
      "006039\n",
      "000569\n",
      "005533\n",
      "001605\n",
      "009564\n",
      "007391\n",
      "002661\n",
      "004784\n",
      "002528\n",
      "001336\n",
      "007577\n",
      "004921\n",
      "008283\n",
      "003831\n",
      "004119\n",
      "009921\n",
      "006531\n",
      "005816\n",
      "008916\n",
      "003378\n",
      "000191\n",
      "009640\n",
      "000809\n",
      "003341\n",
      "003761\n",
      "002453\n",
      "009449\n",
      "000182\n",
      "001208\n",
      "006021\n",
      "002970\n",
      "001134\n",
      "007875\n",
      "006032\n",
      "001703\n",
      "008998\n",
      "006554\n",
      "004180\n",
      "009633\n",
      "007190\n",
      "001573\n",
      "004097\n",
      "003962\n",
      "002922\n",
      "005013\n",
      "003438\n",
      "007518\n",
      "004716\n",
      "000167\n",
      "005677\n",
      "009335\n",
      "000679\n",
      "006984\n",
      "003816\n",
      "007581\n",
      "008934\n",
      "008765\n",
      "004629\n",
      "006211\n",
      "005538\n",
      "004408\n",
      "000765\n",
      "001805\n",
      "002788\n",
      "004914\n",
      "001410\n",
      "000058\n",
      "004309\n",
      "002110\n",
      "006600\n",
      "007221\n",
      "007520\n",
      "002033\n",
      "001401\n",
      "003203\n",
      "002245\n",
      "007589\n",
      "008056\n",
      "006891\n",
      "002850\n",
      "009275\n",
      "006926\n",
      "005158\n",
      "008290\n",
      "005166\n",
      "008947\n",
      "003501\n",
      "008736\n",
      "006969\n",
      "000003\n",
      "007757\n",
      "008779\n",
      "001527\n",
      "001546\n",
      "006516\n",
      "005309\n",
      "008195\n",
      "007505\n",
      "006051\n",
      "002857\n",
      "003059\n",
      "005632\n",
      "000273\n",
      "002581\n",
      "008333\n",
      "007882\n",
      "008303\n",
      "005333\n",
      "000456\n",
      "001674\n",
      "006526\n",
      "002698\n",
      "004485\n",
      "000059\n",
      "000614\n",
      "008207\n",
      "009498\n",
      "007469\n",
      "007341\n",
      "002624\n",
      "007485\n",
      "004633\n",
      "009864\n",
      "009933\n",
      "004522\n",
      "009895\n",
      "000217\n",
      "009583\n",
      "006077\n",
      "001753\n",
      "002269\n",
      "008852\n",
      "008902\n",
      "008090\n",
      "001846\n",
      "005428\n",
      "002059\n",
      "005034\n",
      "006955\n",
      "007312\n",
      "009857\n",
      "009171\n",
      "006541\n",
      "001835\n",
      "009428\n",
      "002651\n",
      "008265\n",
      "007293\n",
      "000735\n",
      "002909\n",
      "006053\n",
      "003114\n",
      "006195\n",
      "003192\n",
      "001396\n",
      "003067\n",
      "005182\n",
      "007199\n",
      "004243\n",
      "002325\n",
      "003348\n",
      "006996\n",
      "006113\n",
      "008097\n",
      "003173\n",
      "007060\n",
      "003715\n",
      "000631\n",
      "007602\n",
      "000371\n",
      "005633\n",
      "000258\n",
      "001459\n",
      "007378\n",
      "002920\n",
      "001374\n",
      "009062\n",
      "001251\n",
      "000324\n",
      "005011\n",
      "004717\n",
      "004937\n",
      "003328\n",
      "007988\n",
      "004288\n",
      "006979\n",
      "004638\n",
      "007802\n",
      "004363\n",
      "005233\n",
      "009390\n",
      "002686\n",
      "000364\n",
      "007871\n",
      "007645\n",
      "000643\n",
      "001859\n",
      "006854\n",
      "006807\n",
      "007835\n",
      "001220\n",
      "006527\n",
      "001021\n",
      "002463\n",
      "008128\n",
      "004398\n",
      "005997\n",
      "008677\n",
      "005650\n",
      "002556\n",
      "008590\n",
      "006378\n",
      "006147\n",
      "005329\n",
      "006744\n",
      "002283\n",
      "000956\n",
      "003613\n",
      "003346\n",
      "004489\n",
      "001469\n",
      "005561\n",
      "004104\n",
      "001193\n",
      "002231\n",
      "005694\n",
      "003030\n",
      "008758\n",
      "003385\n",
      "005494\n",
      "001885\n",
      "009096\n",
      "008682\n",
      "003393\n",
      "002327\n",
      "007839\n",
      "000377\n",
      "000511\n",
      "004460\n",
      "007455\n",
      "000055\n",
      "005115\n",
      "001583\n",
      "006379\n",
      "002319\n",
      "002687\n",
      "002652\n",
      "008762\n",
      "001163\n",
      "007874\n",
      "009260\n",
      "003020\n",
      "001783\n",
      "008774\n",
      "007522\n",
      "004461\n",
      "001318\n",
      "008924\n",
      "006907\n",
      "002928\n",
      "009203\n",
      "007354\n",
      "008066\n",
      "004577\n",
      "007945\n",
      "008030\n",
      "006967\n",
      "008729\n",
      "007706\n",
      "009248\n",
      "001871\n",
      "003563\n",
      "009798\n",
      "001568\n",
      "000846\n",
      "003160\n",
      "003158\n",
      "003666\n",
      "000279\n",
      "007126\n",
      "006019\n",
      "009907\n",
      "006500\n",
      "004330\n",
      "006649\n",
      "001415\n",
      "006902\n",
      "002189\n",
      "000375\n",
      "008231\n",
      "000085\n",
      "000362\n",
      "003342\n",
      "005184\n",
      "009624\n",
      "005681\n",
      "007903\n",
      "005994\n",
      "006298\n",
      "005726\n",
      "003726\n",
      "001048\n",
      "000346\n",
      "001926\n",
      "004880\n",
      "007057\n",
      "002780\n",
      "000556\n",
      "009661\n",
      "009056\n",
      "009366\n",
      "005295\n",
      "007188\n",
      "008603\n",
      "005949\n",
      "004610\n",
      "001305\n",
      "006283\n",
      "003571\n",
      "004096\n",
      "003716\n",
      "001116\n",
      "006453\n",
      "006226\n",
      "009040\n",
      "005204\n",
      "003686\n",
      "005019\n",
      "008054\n",
      "008996\n",
      "007582\n",
      "006072\n",
      "004407\n",
      "000292\n",
      "009916\n",
      "002447\n",
      "004233\n",
      "002853\n",
      "007904\n",
      "006244\n",
      "004678\n",
      "001600\n",
      "006316\n",
      "007665\n",
      "007081\n",
      "000866\n",
      "008818\n",
      "007850\n",
      "000252\n",
      "006557\n",
      "007473\n",
      "007232\n",
      "002380\n",
      "007169\n",
      "008977\n",
      "003972\n",
      "006721\n",
      "007110\n",
      "009025\n",
      "005092\n",
      "000959\n",
      "007867\n",
      "008463\n",
      "008344\n",
      "007509\n",
      "003724\n",
      "005871\n",
      "003731\n",
      "008489\n",
      "005251\n",
      "004568\n",
      "007116\n",
      "008354\n",
      "001447\n",
      "009298\n",
      "001525\n",
      "009835\n",
      "003670\n",
      "003647\n",
      "008715\n",
      "009489\n",
      "009102\n",
      "004650\n",
      "002440\n",
      "009847\n",
      "008015\n",
      "006142\n",
      "009188\n",
      "008274\n",
      "006154\n",
      "005148\n",
      "000084\n",
      "002081\n",
      "002557\n",
      "006068\n",
      "007661\n",
      "003267\n",
      "004596\n",
      "002754\n",
      "001616\n",
      "004026\n",
      "006757\n",
      "005926\n",
      "001720\n",
      "000853\n",
      "009503\n",
      "006861\n",
      "001507\n",
      "007567\n",
      "006292\n",
      "003515\n",
      "009802\n",
      "005009\n",
      "004054\n",
      "000255\n",
      "002128\n",
      "000418\n",
      "007124\n",
      "007693\n",
      "006397\n",
      "001865\n",
      "008267\n",
      "007151\n",
      "009346\n",
      "003071\n",
      "007442\n",
      "003197\n",
      "003702\n",
      "003281\n",
      "007186\n",
      "008205\n",
      "007616\n",
      "009301\n",
      "008964\n",
      "006315\n",
      "007907\n",
      "003836\n",
      "006308\n",
      "003770\n",
      "008459\n",
      "005932\n",
      "000361\n",
      "004778\n",
      "008737\n",
      "000357\n",
      "002548\n",
      "000668\n",
      "007764\n",
      "005915\n",
      "003958\n",
      "007440\n",
      "001372\n",
      "007429\n",
      "001437\n",
      "007430\n",
      "003833\n",
      "008786\n",
      "008182\n",
      "001788\n",
      "003193\n",
      "001349\n",
      "006870\n",
      "004637\n",
      "005117\n",
      "004547\n",
      "002535\n",
      "003739\n",
      "002582\n",
      "003778\n",
      "004802\n",
      "008421\n",
      "008419\n",
      "002654\n",
      "004177\n",
      "002863\n",
      "009132\n",
      "007846\n",
      "003736\n",
      "002743\n",
      "006024\n",
      "005772\n",
      "009367\n",
      "000467\n",
      "000152\n",
      "001961\n",
      "009353\n",
      "004821\n",
      "000316\n",
      "009757\n",
      "003257\n",
      "001658\n",
      "007937\n",
      "001715\n",
      "008561\n",
      "001916\n",
      "003624\n",
      "008212\n",
      "002103\n",
      "006895\n",
      "002726\n",
      "002551\n",
      "008800\n",
      "006059\n",
      "007269\n",
      "008405\n",
      "006242\n",
      "007303\n",
      "009673\n",
      "004451\n",
      "008003\n",
      "006522\n",
      "008052\n",
      "002964\n",
      "006607\n",
      "000116\n",
      "000743\n",
      "009050\n",
      "001423\n",
      "000390\n",
      "004787\n",
      "008286\n",
      "006342\n",
      "004602\n",
      "004226\n",
      "000080\n",
      "009682\n",
      "006728\n",
      "007866\n",
      "009553\n",
      "006498\n",
      "002991\n",
      "007131\n",
      "003775\n",
      "009652\n",
      "004645\n",
      "008382\n",
      "000253\n",
      "002846\n",
      "007632\n",
      "001589\n",
      "005449\n",
      "000784\n",
      "003873\n",
      "003298\n",
      "009669\n",
      "006985\n",
      "007252\n",
      "008821\n",
      "009076\n",
      "001751\n",
      "005452\n",
      "002878\n",
      "000600\n",
      "006312\n",
      "008259\n",
      "006389\n",
      "004420\n",
      "000762\n",
      "006167\n",
      "000852\n",
      "002007\n",
      "008305\n",
      "005280\n",
      "002046\n",
      "000185\n",
      "008353\n",
      "005744\n",
      "005578\n",
      "002515\n",
      "003480\n",
      "008570\n",
      "003046\n",
      "001379\n",
      "006567\n",
      "001417\n",
      "001389\n",
      "006007\n",
      "005937\n",
      "002707\n",
      "001542\n",
      "007043\n",
      "001924\n",
      "007012\n",
      "006581\n",
      "007082\n",
      "005623\n",
      "003144\n",
      "008679\n",
      "001857\n",
      "005872\n",
      "008507\n",
      "007412\n",
      "008853\n",
      "007251\n",
      "004260\n",
      "004877\n",
      "005235\n",
      "005776\n",
      "002840\n",
      "009760\n",
      "008278\n",
      "000992\n",
      "009529\n",
      "007627\n",
      "007817\n",
      "009903\n",
      "007510\n",
      "001852\n",
      "000790\n",
      "008271\n",
      "006856\n",
      "007888\n",
      "008828\n",
      "001792\n",
      "004159\n",
      "008339\n",
      "001969\n",
      "005555\n",
      "000674\n",
      "000970\n",
      "007037\n",
      "008640\n",
      "000691\n",
      "007880\n",
      "003052\n",
      "007557\n",
      "004697\n",
      "006634\n",
      "003018\n",
      "002777\n",
      "009844\n",
      "001921\n",
      "007137\n",
      "005801\n",
      "002764\n",
      "005670\n",
      "004893\n",
      "007434\n",
      "008881\n",
      "002725\n",
      "007862\n",
      "009510\n",
      "001983\n",
      "002746\n",
      "008875\n",
      "005595\n",
      "004018\n",
      "008668\n",
      "009838\n",
      "009495\n",
      "006288\n",
      "009220\n",
      "000502\n",
      "008167\n",
      "006742\n",
      "009589\n",
      "003925\n",
      "003737\n",
      "002168\n",
      "000606\n",
      "006345\n",
      "003288\n",
      "008956\n",
      "008373\n",
      "007106\n",
      "007766\n",
      "000181\n",
      "002092\n",
      "001615\n",
      "003248\n",
      "005678\n",
      "001670\n",
      "002131\n",
      "003276\n",
      "004422\n",
      "004704\n",
      "007805\n",
      "004988\n",
      "008369\n",
      "001049\n",
      "006017\n",
      "000054\n",
      "007532\n",
      "005017\n",
      "004635\n",
      "009952\n",
      "000008\n",
      "002628\n",
      "001177\n",
      "006402\n",
      "006816\n",
      "006741\n",
      "002434\n",
      "003467\n",
      "004308\n",
      "002071\n",
      "001283\n",
      "008626\n",
      "005734\n",
      "007099\n",
      "005154\n",
      "002925\n",
      "009759\n",
      "006873\n",
      "003025\n",
      "007951\n",
      "002921\n",
      "002395\n",
      "000938\n",
      "006596\n",
      "005241\n",
      "006452\n",
      "008350\n",
      "009182\n",
      "000293\n",
      "002487\n",
      "005858\n",
      "006993\n",
      "002147\n",
      "007447\n",
      "002040\n",
      "004791\n",
      "007286\n",
      "007053\n",
      "005883\n",
      "008436\n",
      "006580\n",
      "004167\n",
      "007400\n",
      "009934\n",
      "005015\n",
      "001880\n",
      "006641\n",
      "004314\n",
      "004402\n",
      "008721\n",
      "009399\n",
      "005571\n",
      "006050\n",
      "003198\n",
      "009590\n",
      "005119\n",
      "004975\n",
      "007804\n",
      "005922\n",
      "002811\n",
      "003302\n",
      "004739\n",
      "007828\n",
      "002570\n",
      "001115\n",
      "009927\n",
      "006331\n",
      "006022\n",
      "000566\n",
      "009361\n",
      "007076\n",
      "002050\n",
      "003427\n",
      "004043\n",
      "006044\n",
      "006064\n",
      "009720\n",
      "003967\n",
      "003881\n",
      "000894\n",
      "005529\n",
      "000197\n",
      "005313\n",
      "001223\n",
      "008928\n",
      "005692\n",
      "001973\n",
      "007345\n",
      "009139\n",
      "008417\n",
      "003036\n",
      "008577\n",
      "001551\n",
      "001986\n",
      "002338\n",
      "006809\n",
      "004639\n",
      "000811\n",
      "000421\n",
      "005044\n",
      "000795\n",
      "008726\n",
      "007700\n",
      "004294\n",
      "004497\n",
      "008642\n",
      "001558\n",
      "006332\n",
      "009853\n",
      "005663\n",
      "002299\n",
      "003920\n",
      "009013\n",
      "003734\n",
      "007730\n",
      "007355\n",
      "005656\n",
      "007794\n",
      "003016\n",
      "003041\n",
      "002044\n",
      "006882\n",
      "000135\n",
      "001476\n",
      "000640\n",
      "000766\n",
      "004125\n",
      "003463\n",
      "006037\n",
      "005717\n",
      "000833\n",
      "005155\n",
      "004810\n",
      "007837\n",
      "000001\n",
      "008990\n",
      "000534\n",
      "002532\n",
      "008221\n",
      "005748\n",
      "009158\n",
      "003323\n",
      "006571\n",
      "004642\n",
      "007787\n",
      "009228\n",
      "005296\n",
      "008669\n",
      "005665\n",
      "002802\n",
      "003683\n",
      "001722\n",
      "001712\n",
      "002449\n",
      "003096\n",
      "001681\n",
      "004350\n",
      "003130\n",
      "004764\n",
      "001562\n",
      "008525\n",
      "006925\n",
      "006446\n",
      "009846\n",
      "009109\n",
      "007852\n",
      "006354\n",
      "000594\n",
      "002157\n",
      "002701\n",
      "009890\n",
      "009130\n",
      "007337\n",
      "000148\n",
      "006199\n",
      "005628\n",
      "007756\n",
      "009169\n",
      "008155\n",
      "005978\n",
      "006771\n",
      "002341\n",
      "003616\n",
      "007158\n",
      "001245\n",
      "002771\n",
      "005099\n",
      "005950\n",
      "005435\n",
      "007066\n",
      "005505\n",
      "009357\n",
      "001301\n",
      "002498\n",
      "007598\n",
      "009578\n",
      "002753\n",
      "004277\n",
      "009716\n",
      "003505\n",
      "006063\n",
      "005986\n",
      "001376\n",
      "002955\n",
      "008660\n",
      "006356\n",
      "004580\n",
      "009643\n",
      "008547\n",
      "008735\n",
      "006716\n",
      "002428\n",
      "004301\n",
      "004240\n",
      "005354\n",
      "008435\n",
      "004006\n",
      "005422\n",
      "004334\n",
      "005103\n",
      "006711\n",
      "000561\n",
      "009914\n",
      "002031\n",
      "004375\n",
      "002644\n",
      "005266\n",
      "007969\n",
      "000734\n",
      "009672\n",
      "000642\n",
      "003190\n",
      "009235\n",
      "004684\n",
      "002560\n",
      "006094\n",
      "002996\n",
      "005330\n",
      "007641\n",
      "001399\n",
      "003832\n",
      "008356\n",
      "001991\n",
      "008337\n",
      "007228\n",
      "005401\n",
      "004957\n",
      "002576\n",
      "005275\n",
      "001065\n",
      "001046\n",
      "003426\n",
      "002999\n",
      "005689\n",
      "003756\n",
      "006776\n",
      "008780\n",
      "007347\n",
      "001779\n",
      "008192\n",
      "003423\n",
      "000751\n",
      "004410\n",
      "009341\n",
      "009478\n",
      "001153\n",
      "000365\n",
      "003333\n",
      "005076\n",
      "003652\n",
      "004357\n",
      "004870\n",
      "008906\n",
      "002482\n",
      "008896\n",
      "005473\n",
      "008325\n",
      "005089\n",
      "003637\n",
      "002469\n",
      "009697\n",
      "006408\n",
      "006712\n",
      "001917\n",
      "003789\n",
      "003601\n",
      "000727\n",
      "004811\n",
      "002313\n",
      "004947\n",
      "009355\n",
      "001169\n",
      "002972\n",
      "003888\n",
      "004925\n",
      "000593\n",
      "006248\n",
      "004862\n",
      "007894\n",
      "009127\n",
      "000216\n",
      "004586\n",
      "003955\n",
      "009506\n",
      "000075\n",
      "005403\n",
      "001874\n",
      "004480\n",
      "001773\n",
      "002828\n",
      "004670\n",
      "007938\n",
      "007703\n",
      "004646\n",
      "002322\n",
      "001866\n",
      "000615\n",
      "004847\n",
      "002331\n",
      "001781\n",
      "007364\n",
      "002162\n",
      "006683\n",
      "003080\n",
      "002997\n",
      "004285\n",
      "007380\n",
      "008256\n",
      "003561\n",
      "008863\n",
      "003445\n",
      "007564\n",
      "007395\n",
      "004302\n",
      "003132\n",
      "007386\n",
      "009111\n",
      "005342\n",
      "005394\n",
      "007934\n",
      "008448\n",
      "001167\n",
      "003209\n",
      "003617\n",
      "001639\n",
      "003143\n",
      "006082\n",
      "007597\n",
      "006457\n",
      "002819\n",
      "006767\n",
      "000481\n",
      "003598\n",
      "002143\n",
      "008358\n",
      "006257\n",
      "004491\n",
      "003048\n",
      "003742\n",
      "008959\n",
      "004431\n",
      "008516\n",
      "004080\n",
      "006724\n",
      "009909\n",
      "007143\n",
      "005165\n",
      "001764\n",
      "004964\n",
      "002761\n",
      "007404\n",
      "008650\n",
      "000914\n",
      "006504\n",
      "001863\n",
      "000624\n",
      "004050\n",
      "002316\n",
      "006228\n",
      "002918\n",
      "008675\n",
      "002663\n",
      "009423\n",
      "000475\n",
      "004996\n",
      "000031\n",
      "004556\n",
      "006384\n",
      "000434\n",
      "009404\n",
      "003022\n",
      "007426\n",
      "007716\n",
      "008785\n",
      "004059\n",
      "003938\n",
      "005012\n",
      "001767\n",
      "003212\n",
      "000875\n",
      "002060\n",
      "003434\n",
      "005602\n",
      "001238\n",
      "005272\n",
      "004853\n",
      "008714\n",
      "008889\n",
      "009030\n",
      "000869\n",
      "009262\n",
      "008963\n",
      "008643\n",
      "004042\n",
      "001026\n",
      "004901\n",
      "008080\n",
      "006217\n",
      "004965\n",
      "002829\n",
      "000838\n",
      "005560\n",
      "004044\n",
      "008792\n",
      "002009\n",
      "004504\n",
      "005484\n",
      "003104\n",
      "002799\n",
      "001516\n",
      "004908\n",
      "002740\n",
      "003839\n",
      "005050\n",
      "007514\n",
      "007225\n",
      "009258\n",
      "003744\n",
      "003771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "002982\n",
      "006401\n",
      "002388\n",
      "007617\n",
      "008158\n",
      "005227\n",
      "002312\n",
      "003523\n",
      "009450\n",
      "009662\n",
      "005545\n",
      "002246\n",
      "006574\n",
      "005793\n",
      "005221\n",
      "000348\n",
      "001625\n",
      "005558\n",
      "003119\n",
      "005335\n",
      "003653\n",
      "008857\n",
      "006273\n",
      "000718\n",
      "009815\n",
      "003738\n",
      "005857\n",
      "009198\n",
      "000798\n",
      "004476\n",
      "008157\n",
      "006093\n",
      "000708\n",
      "000916\n",
      "002356\n",
      "001235\n",
      "004506\n",
      "007119\n",
      "006942\n",
      "003498\n",
      "000652\n",
      "002123\n",
      "003131\n",
      "004696\n",
      "006665\n",
      "000983\n",
      "004734\n",
      "002309\n",
      "009137\n",
      "005237\n",
      "005945\n",
      "004598\n",
      "000783\n",
      "000070\n",
      "009265\n",
      "006820\n",
      "002758\n",
      "009910\n",
      "003531\n",
      "006413\n",
      "001111\n",
      "004036\n",
      "008904\n",
      "000741\n",
      "002390\n",
      "000781\n",
      "005572\n",
      "001567\n",
      "003843\n",
      "009436\n",
      "007881\n",
      "005126\n",
      "005232\n",
      "000504\n",
      "006756\n",
      "006493\n",
      "004920\n",
      "001637\n",
      "000995\n",
      "002979\n",
      "005766\n",
      "006957\n",
      "005619\n",
      "001198\n",
      "005604\n",
      "002882\n",
      "002550\n",
      "006333\n",
      "001619\n",
      "008797\n",
      "009442\n",
      "004401\n",
      "002130\n",
      "007240\n",
      "007120\n",
      "005040\n",
      "009210\n",
      "003402\n",
      "006787\n",
      "004608\n",
      "000725\n",
      "002457\n",
      "003306\n",
      "009292\n",
      "005472\n",
      "001454\n",
      "007550\n",
      "000925\n",
      "009403\n",
      "002028\n",
      "003222\n",
      "008798\n",
      "006732\n",
      "007091\n",
      "006834\n",
      "003867\n",
      "003689\n",
      "004959\n",
      "005142\n",
      "002087\n",
      "002681\n",
      "001256\n",
      "000471\n",
      "002626\n",
      "000841\n",
      "005972\n",
      "001369\n",
      "005444\n",
      "000149\n",
      "008520\n",
      "006577\n",
      "003305\n",
      "004855\n",
      "003180\n",
      "009635\n",
      "002488\n",
      "006920\n",
      "006491\n",
      "006256\n",
      "004161\n",
      "008820\n",
      "008340\n",
      "008392\n",
      "007789\n",
      "000627\n",
      "002510\n",
      "008120\n",
      "002902\n",
      "002577\n",
      "009722\n",
      "005543\n",
      "006693\n",
      "004942\n",
      "006615\n",
      "004978\n",
      "008910\n",
      "001534\n",
      "001769\n",
      "000092\n",
      "006954\n",
      "008156\n",
      "005643\n",
      "003507\n",
      "009521\n",
      "008021\n",
      "002968\n",
      "003225\n",
      "004415\n",
      "005446\n",
      "006416\n",
      "006302\n",
      "003049\n",
      "005140\n",
      "006535\n",
      "001189\n",
      "003668\n",
      "006655\n",
      "002252\n",
      "009379\n",
      "003037\n",
      "007818\n",
      "001382\n",
      "007047\n",
      "000333\n",
      "008025\n",
      "008614\n",
      "003543\n",
      "005646\n",
      "004534\n",
      "005855\n",
      "006031\n",
      "009646\n",
      "003357\n",
      "000737\n",
      "002642\n",
      "001159\n",
      "000436\n",
      "008777\n",
      "005282\n",
      "008007\n",
      "003010\n",
      "004065\n",
      "003914\n",
      "005447\n",
      "006122\n",
      "006441\n",
      "003389\n",
      "005456\n",
      "004904\n",
      "004029\n",
      "001655\n",
      "007849\n",
      "002360\n",
      "009689\n",
      "000574\n",
      "001955\n",
      "008328\n",
      "009943\n",
      "008630\n",
      "001740\n",
      "000300\n",
      "001820\n",
      "008925\n",
      "006743\n",
      "006905\n",
      "004181\n",
      "001440\n",
      "006938\n",
      "002945\n",
      "006114\n",
      "004032\n",
      "009892\n",
      "003460\n",
      "003677\n",
      "009338\n",
      "003418\n",
      "007985\n",
      "006126\n",
      "001584\n",
      "002712\n",
      "000205\n",
      "003235\n",
      "001095\n",
      "005041\n",
      "008092\n",
      "007556\n",
      "006494\n",
      "005151\n",
      "003366\n",
      "008882\n",
      "009604\n",
      "009082\n",
      "001831\n",
      "004217\n",
      "000115\n",
      "008404\n",
      "005083\n",
      "004435\n",
      "004617\n",
      "003600\n",
      "002728\n",
      "003358\n",
      "006991\n",
      "006779\n",
      "007646\n",
      "007636\n",
      "004523\n",
      "003801\n",
      "005299\n",
      "002971\n",
      "008740\n",
      "002631\n",
      "003901\n",
      "001261\n",
      "003232\n",
      "008842\n",
      "007306\n",
      "006629\n",
      "009397\n",
      "001889\n",
      "006340\n",
      "009628\n",
      "005513\n",
      "008571\n",
      "006110\n",
      "008148\n",
      "008887\n",
      "003156\n",
      "007973\n",
      "006510\n",
      "003238\n",
      "003661\n",
      "002351\n",
      "001631\n",
      "004115\n",
      "009770\n",
      "008055\n",
      "009696\n",
      "006266\n",
      "000602\n",
      "006701\n",
      "008014\n",
      "006173\n",
      "009329\n",
      "004659\n",
      "007912\n",
      "001620\n",
      "005913\n",
      "005228\n",
      "008746\n",
      "000801\n",
      "006213\n",
      "005835\n",
      "006791\n",
      "006818\n",
      "005502\n",
      "006399\n",
      "005234\n",
      "004695\n",
      "000942\n",
      "008309\n",
      "008298\n",
      "008864\n",
      "009818\n",
      "003249\n",
      "004084\n",
      "002742\n",
      "006090\n",
      "000353\n",
      "009305\n",
      "000043\n",
      "000877\n",
      "007055\n",
      "005382\n",
      "001634\n",
      "005249\n",
      "003315\n",
      "004366\n",
      "001328\n",
      "007248\n",
      "008230\n",
      "008331\n",
      "000517\n",
      "003352\n",
      "000994\n",
      "001054\n",
      "005302\n",
      "002596\n",
      "006601\n",
      "006197\n",
      "000883\n",
      "000546\n",
      "007339\n",
      "003810\n",
      "006986\n",
      "001070\n",
      "000630\n",
      "005088\n",
      "004172\n",
      "004860\n",
      "009081\n",
      "008473\n",
      "005617\n",
      "001088\n",
      "009462\n",
      "001478\n",
      "004332\n",
      "003641\n",
      "009453\n",
      "004917\n",
      "005025\n",
      "002974\n",
      "002895\n",
      "006080\n",
      "003649\n",
      "001601\n",
      "002243\n",
      "004884\n",
      "000952\n",
      "007282\n",
      "002262\n",
      "003720\n",
      "009240\n",
      "000277\n",
      "003930\n",
      "008763\n",
      "000447\n",
      "001868\n",
      "004071\n",
      "004062\n",
      "008481\n",
      "002207\n",
      "001511\n",
      "009854\n",
      "009765\n",
      "007981\n",
      "002825\n",
      "002617\n",
      "007202\n",
      "000457\n",
      "004328\n",
      "004478\n",
      "008126\n",
      "000166\n",
      "009677\n",
      "004004\n",
      "007237\n",
      "004282\n",
      "001347\n",
      "000445\n",
      "005366\n",
      "003125\n",
      "007952\n",
      "005823\n",
      "007277\n",
      "000231\n",
      "006642\n",
      "000704\n",
      "006977\n",
      "001697\n",
      "003853\n",
      "001253\n",
      "007728\n",
      "008458\n",
      "008946\n",
      "002078\n",
      "006591\n",
      "006239\n",
      "000345\n",
      "002980\n",
      "007737\n",
      "001353\n",
      "004399\n",
      "008408\n",
      "009861\n",
      "006115\n",
      "004949\n",
      "006992\n",
      "007268\n",
      "002708\n",
      "005198\n",
      "009951\n",
      "003029\n",
      "004419\n",
      "000825\n",
      "005966\n",
      "009593\n",
      "000183\n",
      "007829\n",
      "006057\n",
      "001919\n",
      "004027\n",
      "001285\n",
      "004382\n",
      "004531\n",
      "002676\n",
      "003579\n",
      "007111\n",
      "007569\n",
      "007542\n",
      "009875\n",
      "008406\n",
      "001992\n",
      "002075\n",
      "005252\n",
      "005924\n",
      "002583\n",
      "006460\n",
      "001502\n",
      "003795\n",
      "001398\n",
      "003268\n",
      "005492\n",
      "008077\n",
      "002822\n",
      "000478\n",
      "000490\n",
      "001118\n",
      "001742\n",
      "000692\n",
      "002240\n",
      "004144\n",
      "006373\n",
      "004757\n",
      "002983\n",
      "003495\n",
      "006745\n",
      "008685\n",
      "004202\n",
      "009929\n",
      "001798\n",
      "001435\n",
      "004183\n",
      "000440\n",
      "004934\n",
      "007494\n",
      "006798\n",
      "003471\n",
      "000398\n",
      "004109\n",
      "007452\n",
      "004881\n",
      "001667\n",
      "007977\n",
      "007181\n",
      "000204\n",
      "008395\n",
      "001848\n",
      "000744\n",
      "006231\n",
      "007423\n",
      "005709\n",
      "002073\n",
      "007231\n",
      "008201\n",
      "006633\n",
      "007415\n",
      "007648\n",
      "005977\n",
      "009594\n",
      "008893\n",
      "002409\n",
      "008641\n",
      "004658\n",
      "007562\n",
      "000538\n",
      "005022\n",
      "007608\n",
      "004807\n",
      "001249\n",
      "002230\n",
      "007134\n",
      "001255\n",
      "006832\n",
      "003559\n",
      "002575\n",
      "001373\n",
      "002831\n",
      "000124\n",
      "006669\n",
      "007450\n",
      "009146\n",
      "005622\n",
      "008908\n",
      "000586\n",
      "006380\n",
      "007418\n",
      "004545\n",
      "007201\n",
      "001244\n",
      "005087\n",
      "009314\n",
      "006763\n",
      "000335\n",
      "003712\n",
      "007714\n",
      "006659\n",
      "009630\n",
      "000497\n",
      "003584\n",
      "000309\n",
      "006432\n",
      "008861\n",
      "009956\n",
      "003187\n",
      "005194\n",
      "002623\n",
      "000571\n",
      "005459\n",
      "002860\n",
      "002295\n",
      "008491\n",
      "000575\n",
      "007160\n",
      "005575\n",
      "004721\n",
      "009447\n",
      "004166\n",
      "006775\n",
      "007504\n",
      "009149\n",
      "001665\n",
      "001317\n",
      "002399\n",
      "001117\n",
      "004355\n",
      "000226\n",
      "004268\n",
      "000913\n",
      "004266\n",
      "004021\n",
      "007755\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((resnet_input,resnet_input)),\n",
    "                                         transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.ToTensor()])\n",
    "train_dataset = hound_dataset(root_dir='.', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = hound_dataset(root_dir='.', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4166666666666667, 0.4166666666666667, 0.16666666666666669, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "labels_list = [0,0,1,1,2,2,2,2,2]\n",
    "# for image,label in train_loader:\n",
    "#     labels_list.extend(label)\n",
    "weights = []\n",
    "for i in range(len(classes)):\n",
    "    if labels_list.count(i) == 0 :\n",
    "        weights.append(labels_list.count(i))\n",
    "    else:\n",
    "        weights.append(1/labels_list.count(i))\n",
    "weights = [x/sum(weights) for x in weights]\n",
    "print (weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Litlefinger has brought you a pre-trained network. Fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 21)\n",
    "\n",
    "# Add code for using CUDA here\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    resnet18.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-02 *\n",
      "  2.7845\n",
      "  0.1839\n",
      "  0.2323\n",
      "  0.3329\n",
      "  0.2212\n",
      "  0.3523\n",
      "  0.1511\n",
      "  0.9135\n",
      "  0.2162\n",
      "  0.7957\n",
      "  0.1978\n",
      "  0.1723\n",
      "  0.2990\n",
      "  0.2256\n",
      "  0.2167\n",
      "  3.0268\n",
      "  0.3473\n",
      "  0.1962\n",
      "  0.2362\n",
      "  0.1823\n",
      "  0.2039\n",
      "[torch.cuda.FloatTensor of size 21 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(torch.FloatTensor(weights).cuda())\n",
    "print(torch.FloatTensor(weights).cuda())\n",
    "# Update if any errors occur\n",
    "optimizer = torch.optim.SGD(resnet18.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arya_train():\n",
    "    # Begin\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels, _) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if(use_gpu):\n",
    "                images=images.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_arr.append(loss.data[0])\n",
    "            if (i+1) % batch_size == 0:\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2/10336], Loss: 2.6672\n",
      "Epoch [1/5], Step [4/10336], Loss: 2.5739\n",
      "Epoch [1/5], Step [6/10336], Loss: 1.8674\n",
      "Epoch [1/5], Step [8/10336], Loss: 1.3433\n",
      "Epoch [1/5], Step [10/10336], Loss: 1.2273\n",
      "Epoch [1/5], Step [12/10336], Loss: 4.5346\n",
      "Epoch [1/5], Step [14/10336], Loss: 1.1433\n",
      "Epoch [1/5], Step [16/10336], Loss: 4.7906\n",
      "Epoch [1/5], Step [18/10336], Loss: 3.1026\n",
      "Epoch [1/5], Step [20/10336], Loss: 1.0096\n",
      "Epoch [1/5], Step [22/10336], Loss: 3.0411\n",
      "Epoch [1/5], Step [24/10336], Loss: 1.7962\n",
      "Epoch [1/5], Step [26/10336], Loss: 1.5870\n",
      "Epoch [1/5], Step [28/10336], Loss: 1.6730\n",
      "Epoch [1/5], Step [30/10336], Loss: 0.7179\n",
      "Epoch [1/5], Step [32/10336], Loss: 0.8908\n",
      "Epoch [1/5], Step [34/10336], Loss: 5.0378\n",
      "Epoch [1/5], Step [36/10336], Loss: 1.1625\n",
      "Epoch [1/5], Step [38/10336], Loss: 0.9738\n",
      "Epoch [1/5], Step [40/10336], Loss: 4.1939\n",
      "Epoch [1/5], Step [42/10336], Loss: 1.7670\n",
      "Epoch [1/5], Step [44/10336], Loss: 2.2570\n",
      "Epoch [1/5], Step [46/10336], Loss: 4.3289\n",
      "Epoch [1/5], Step [48/10336], Loss: 1.9311\n",
      "Epoch [1/5], Step [50/10336], Loss: 0.8561\n",
      "Epoch [1/5], Step [52/10336], Loss: 1.8491\n",
      "Epoch [1/5], Step [54/10336], Loss: 5.1762\n",
      "Epoch [1/5], Step [56/10336], Loss: 0.3650\n",
      "Epoch [1/5], Step [58/10336], Loss: 1.3736\n",
      "Epoch [1/5], Step [60/10336], Loss: 0.7231\n",
      "Epoch [1/5], Step [62/10336], Loss: 3.5483\n",
      "Epoch [1/5], Step [64/10336], Loss: 0.8445\n",
      "Epoch [1/5], Step [66/10336], Loss: 1.5352\n",
      "Epoch [1/5], Step [68/10336], Loss: 0.5915\n",
      "Epoch [1/5], Step [70/10336], Loss: 1.1584\n",
      "Epoch [1/5], Step [72/10336], Loss: 1.1175\n",
      "Epoch [1/5], Step [74/10336], Loss: 0.1204\n",
      "Epoch [1/5], Step [76/10336], Loss: 1.2480\n",
      "Epoch [1/5], Step [78/10336], Loss: 0.8313\n",
      "Epoch [1/5], Step [80/10336], Loss: 0.9926\n",
      "Epoch [1/5], Step [82/10336], Loss: 0.6672\n",
      "Epoch [1/5], Step [84/10336], Loss: 2.6261\n",
      "Epoch [1/5], Step [86/10336], Loss: 0.0809\n",
      "Epoch [1/5], Step [88/10336], Loss: 0.4021\n",
      "Epoch [1/5], Step [90/10336], Loss: 0.6840\n",
      "Epoch [1/5], Step [92/10336], Loss: 2.9608\n",
      "Epoch [1/5], Step [94/10336], Loss: 5.9891\n",
      "Epoch [1/5], Step [96/10336], Loss: 0.3756\n",
      "Epoch [1/5], Step [98/10336], Loss: 1.3033\n",
      "Epoch [1/5], Step [100/10336], Loss: 1.0026\n",
      "Epoch [1/5], Step [102/10336], Loss: 1.2567\n",
      "Epoch [1/5], Step [104/10336], Loss: 0.5758\n",
      "Epoch [1/5], Step [106/10336], Loss: 1.0312\n",
      "Epoch [1/5], Step [108/10336], Loss: 2.6267\n",
      "Epoch [1/5], Step [110/10336], Loss: 0.1642\n",
      "Epoch [1/5], Step [112/10336], Loss: 4.9208\n",
      "Epoch [1/5], Step [114/10336], Loss: 0.0507\n",
      "Epoch [1/5], Step [116/10336], Loss: 5.3111\n",
      "Epoch [1/5], Step [118/10336], Loss: 3.8658\n",
      "Epoch [1/5], Step [120/10336], Loss: 0.2512\n",
      "Epoch [1/5], Step [122/10336], Loss: 6.4113\n",
      "Epoch [1/5], Step [124/10336], Loss: 0.1481\n",
      "Epoch [1/5], Step [126/10336], Loss: 1.1511\n",
      "Epoch [1/5], Step [128/10336], Loss: 4.8239\n",
      "Epoch [1/5], Step [130/10336], Loss: 1.9039\n",
      "Epoch [1/5], Step [132/10336], Loss: 1.9219\n",
      "Epoch [1/5], Step [134/10336], Loss: 0.9708\n",
      "Epoch [1/5], Step [136/10336], Loss: 2.1935\n",
      "Epoch [1/5], Step [138/10336], Loss: 4.6099\n",
      "Epoch [1/5], Step [140/10336], Loss: 2.1217\n",
      "Epoch [1/5], Step [142/10336], Loss: 0.4810\n",
      "Epoch [1/5], Step [144/10336], Loss: 0.7685\n",
      "Epoch [1/5], Step [146/10336], Loss: 1.1225\n",
      "Epoch [1/5], Step [148/10336], Loss: 0.2712\n",
      "Epoch [1/5], Step [150/10336], Loss: 2.3312\n",
      "Epoch [1/5], Step [152/10336], Loss: 5.0598\n",
      "Epoch [1/5], Step [154/10336], Loss: 4.3374\n",
      "Epoch [1/5], Step [156/10336], Loss: 1.9841\n",
      "Epoch [1/5], Step [158/10336], Loss: 0.3323\n",
      "Epoch [1/5], Step [160/10336], Loss: 3.4355\n",
      "Epoch [1/5], Step [162/10336], Loss: 3.0593\n",
      "Epoch [1/5], Step [164/10336], Loss: 0.4903\n",
      "Epoch [1/5], Step [166/10336], Loss: 1.0266\n",
      "Epoch [1/5], Step [168/10336], Loss: 1.2679\n",
      "Epoch [1/5], Step [170/10336], Loss: 1.7327\n",
      "Epoch [1/5], Step [172/10336], Loss: 7.9078\n",
      "Epoch [1/5], Step [174/10336], Loss: 6.5467\n",
      "Epoch [1/5], Step [176/10336], Loss: 0.4477\n",
      "Epoch [1/5], Step [178/10336], Loss: 4.4992\n",
      "Epoch [1/5], Step [180/10336], Loss: 3.8789\n",
      "Epoch [1/5], Step [182/10336], Loss: 0.8052\n",
      "Epoch [1/5], Step [184/10336], Loss: 2.6565\n",
      "Epoch [1/5], Step [186/10336], Loss: 3.8080\n",
      "Epoch [1/5], Step [188/10336], Loss: 1.8289\n",
      "Epoch [1/5], Step [190/10336], Loss: 1.8271\n",
      "Epoch [1/5], Step [192/10336], Loss: 0.3565\n",
      "Epoch [1/5], Step [194/10336], Loss: 2.4299\n",
      "Epoch [1/5], Step [196/10336], Loss: 8.9317\n",
      "Epoch [1/5], Step [198/10336], Loss: 4.6534\n",
      "Epoch [1/5], Step [200/10336], Loss: 1.6369\n",
      "Epoch [1/5], Step [202/10336], Loss: 2.5405\n",
      "Epoch [1/5], Step [204/10336], Loss: 3.4201\n",
      "Epoch [1/5], Step [206/10336], Loss: 5.3147\n",
      "Epoch [1/5], Step [208/10336], Loss: 0.9206\n",
      "Epoch [1/5], Step [210/10336], Loss: 5.2032\n",
      "Epoch [1/5], Step [212/10336], Loss: 1.2193\n",
      "Epoch [1/5], Step [214/10336], Loss: 6.7937\n",
      "Epoch [1/5], Step [216/10336], Loss: 3.3738\n",
      "Epoch [1/5], Step [218/10336], Loss: 4.6248\n",
      "Epoch [1/5], Step [220/10336], Loss: 1.8931\n",
      "Epoch [1/5], Step [222/10336], Loss: 3.0889\n",
      "Epoch [1/5], Step [224/10336], Loss: 0.2604\n",
      "Epoch [1/5], Step [226/10336], Loss: 4.5310\n",
      "Epoch [1/5], Step [228/10336], Loss: 0.5003\n",
      "Epoch [1/5], Step [230/10336], Loss: 3.0743\n",
      "Epoch [1/5], Step [232/10336], Loss: 1.7353\n",
      "Epoch [1/5], Step [234/10336], Loss: 0.4127\n",
      "Epoch [1/5], Step [236/10336], Loss: 4.7358\n",
      "Epoch [1/5], Step [238/10336], Loss: 0.7408\n",
      "Epoch [1/5], Step [240/10336], Loss: 0.8975\n",
      "Epoch [1/5], Step [242/10336], Loss: 0.4482\n",
      "Epoch [1/5], Step [244/10336], Loss: 5.4112\n",
      "Epoch [1/5], Step [246/10336], Loss: 0.7842\n",
      "Epoch [1/5], Step [248/10336], Loss: 3.3467\n",
      "Epoch [1/5], Step [250/10336], Loss: 2.6444\n",
      "Epoch [1/5], Step [252/10336], Loss: 0.6801\n",
      "Epoch [1/5], Step [254/10336], Loss: 2.5647\n",
      "Epoch [1/5], Step [256/10336], Loss: 3.6584\n",
      "Epoch [1/5], Step [258/10336], Loss: 0.5028\n",
      "Epoch [1/5], Step [260/10336], Loss: 5.8540\n",
      "Epoch [1/5], Step [262/10336], Loss: 1.5271\n",
      "Epoch [1/5], Step [264/10336], Loss: 3.2896\n",
      "Epoch [1/5], Step [266/10336], Loss: 0.2861\n",
      "Epoch [1/5], Step [268/10336], Loss: 1.1866\n",
      "Epoch [1/5], Step [270/10336], Loss: 4.4342\n",
      "Epoch [1/5], Step [272/10336], Loss: 0.3773\n",
      "Epoch [1/5], Step [274/10336], Loss: 0.9398\n",
      "Epoch [1/5], Step [276/10336], Loss: 2.5290\n",
      "Epoch [1/5], Step [278/10336], Loss: 0.6967\n",
      "Epoch [1/5], Step [280/10336], Loss: 1.0501\n",
      "Epoch [1/5], Step [282/10336], Loss: 6.7789\n",
      "Epoch [1/5], Step [284/10336], Loss: 0.4307\n",
      "Epoch [1/5], Step [286/10336], Loss: 0.7373\n",
      "Epoch [1/5], Step [288/10336], Loss: 1.1984\n",
      "Epoch [1/5], Step [290/10336], Loss: 6.2041\n",
      "Epoch [1/5], Step [292/10336], Loss: 0.4265\n",
      "Epoch [1/5], Step [294/10336], Loss: 1.0104\n",
      "Epoch [1/5], Step [296/10336], Loss: 5.9058\n",
      "Epoch [1/5], Step [298/10336], Loss: 3.3076\n",
      "Epoch [1/5], Step [300/10336], Loss: 0.3151\n",
      "Epoch [1/5], Step [302/10336], Loss: 6.2693\n",
      "Epoch [1/5], Step [304/10336], Loss: 1.2182\n",
      "Epoch [1/5], Step [306/10336], Loss: 0.7513\n",
      "Epoch [1/5], Step [308/10336], Loss: 4.1774\n",
      "Epoch [1/5], Step [310/10336], Loss: 4.8020\n",
      "Epoch [1/5], Step [312/10336], Loss: 2.3338\n",
      "Epoch [1/5], Step [314/10336], Loss: 2.4012\n",
      "Epoch [1/5], Step [316/10336], Loss: 1.1962\n",
      "Epoch [1/5], Step [318/10336], Loss: 0.6373\n",
      "Epoch [1/5], Step [320/10336], Loss: 0.4691\n",
      "Epoch [1/5], Step [322/10336], Loss: 0.7388\n",
      "Epoch [1/5], Step [324/10336], Loss: 4.6193\n",
      "Epoch [1/5], Step [326/10336], Loss: 0.5297\n",
      "Epoch [1/5], Step [328/10336], Loss: 0.2845\n",
      "Epoch [1/5], Step [330/10336], Loss: 5.2467\n",
      "Epoch [1/5], Step [332/10336], Loss: 4.5831\n",
      "Epoch [1/5], Step [334/10336], Loss: 0.1302\n",
      "Epoch [1/5], Step [336/10336], Loss: 1.0035\n",
      "Epoch [1/5], Step [338/10336], Loss: 4.6065\n",
      "Epoch [1/5], Step [340/10336], Loss: 0.2319\n",
      "Epoch [1/5], Step [342/10336], Loss: 0.1591\n",
      "Epoch [1/5], Step [344/10336], Loss: 2.1118\n",
      "Epoch [1/5], Step [346/10336], Loss: 4.3824\n",
      "Epoch [1/5], Step [348/10336], Loss: 1.5917\n",
      "Epoch [1/5], Step [350/10336], Loss: 3.2291\n",
      "Epoch [1/5], Step [352/10336], Loss: 0.7571\n",
      "Epoch [1/5], Step [354/10336], Loss: 0.4613\n",
      "Epoch [1/5], Step [356/10336], Loss: 2.4302\n",
      "Epoch [1/5], Step [358/10336], Loss: 1.5038\n",
      "Epoch [1/5], Step [360/10336], Loss: 3.1933\n",
      "Epoch [1/5], Step [362/10336], Loss: 3.7395\n",
      "Epoch [1/5], Step [364/10336], Loss: 2.5635\n",
      "Epoch [1/5], Step [366/10336], Loss: 2.7267\n",
      "Epoch [1/5], Step [368/10336], Loss: 0.6092\n",
      "Epoch [1/5], Step [370/10336], Loss: 3.1540\n",
      "Epoch [1/5], Step [372/10336], Loss: 1.2794\n",
      "Epoch [1/5], Step [374/10336], Loss: 2.0484\n",
      "Epoch [1/5], Step [376/10336], Loss: 1.5991\n",
      "Epoch [1/5], Step [378/10336], Loss: 5.3229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [380/10336], Loss: 1.6705\n",
      "Epoch [1/5], Step [382/10336], Loss: 2.2245\n",
      "Epoch [1/5], Step [384/10336], Loss: 0.0673\n",
      "Epoch [1/5], Step [386/10336], Loss: 0.4526\n",
      "Epoch [1/5], Step [388/10336], Loss: 6.4292\n",
      "Epoch [1/5], Step [390/10336], Loss: 3.4289\n",
      "Epoch [1/5], Step [392/10336], Loss: 1.8234\n",
      "Epoch [1/5], Step [394/10336], Loss: 1.4770\n",
      "Epoch [1/5], Step [396/10336], Loss: 0.3720\n",
      "Epoch [1/5], Step [398/10336], Loss: 0.1693\n",
      "Epoch [1/5], Step [400/10336], Loss: 1.4315\n",
      "Epoch [1/5], Step [402/10336], Loss: 4.0207\n",
      "Epoch [1/5], Step [404/10336], Loss: 5.5498\n",
      "Epoch [1/5], Step [406/10336], Loss: 4.0518\n",
      "Epoch [1/5], Step [408/10336], Loss: 1.3014\n",
      "Epoch [1/5], Step [410/10336], Loss: 1.2411\n",
      "Epoch [1/5], Step [412/10336], Loss: 2.1808\n",
      "Epoch [1/5], Step [414/10336], Loss: 0.8486\n",
      "Epoch [1/5], Step [416/10336], Loss: 4.5412\n",
      "Epoch [1/5], Step [418/10336], Loss: 4.9621\n",
      "Epoch [1/5], Step [420/10336], Loss: 5.8257\n",
      "Epoch [1/5], Step [422/10336], Loss: 1.7946\n",
      "Epoch [1/5], Step [424/10336], Loss: 3.0696\n",
      "Epoch [1/5], Step [426/10336], Loss: 3.3457\n",
      "Epoch [1/5], Step [428/10336], Loss: 4.7498\n",
      "Epoch [1/5], Step [430/10336], Loss: 2.6218\n",
      "Epoch [1/5], Step [432/10336], Loss: 2.0161\n",
      "Epoch [1/5], Step [434/10336], Loss: 3.9766\n",
      "Epoch [1/5], Step [436/10336], Loss: 6.7065\n",
      "Epoch [1/5], Step [438/10336], Loss: 6.8625\n",
      "Epoch [1/5], Step [440/10336], Loss: 0.1453\n",
      "Epoch [1/5], Step [442/10336], Loss: 1.5517\n",
      "Epoch [1/5], Step [444/10336], Loss: 0.6916\n",
      "Epoch [1/5], Step [446/10336], Loss: 1.5886\n",
      "Epoch [1/5], Step [448/10336], Loss: 1.7595\n",
      "Epoch [1/5], Step [450/10336], Loss: 5.8631\n",
      "Epoch [1/5], Step [452/10336], Loss: 1.9090\n",
      "Epoch [1/5], Step [454/10336], Loss: 6.1324\n",
      "Epoch [1/5], Step [456/10336], Loss: 1.2904\n",
      "Epoch [1/5], Step [458/10336], Loss: 0.7645\n",
      "Epoch [1/5], Step [460/10336], Loss: 0.8011\n",
      "Epoch [1/5], Step [462/10336], Loss: 0.7391\n",
      "Epoch [1/5], Step [464/10336], Loss: 0.6470\n",
      "Epoch [1/5], Step [466/10336], Loss: 0.9185\n",
      "Epoch [1/5], Step [468/10336], Loss: 7.6869\n",
      "Epoch [1/5], Step [470/10336], Loss: 5.9988\n",
      "Epoch [1/5], Step [472/10336], Loss: 0.9473\n",
      "Epoch [1/5], Step [474/10336], Loss: 3.8198\n",
      "Epoch [1/5], Step [476/10336], Loss: 3.6949\n",
      "Epoch [1/5], Step [478/10336], Loss: 0.3353\n",
      "Epoch [1/5], Step [480/10336], Loss: 2.0641\n",
      "Epoch [1/5], Step [482/10336], Loss: 3.8635\n",
      "Epoch [1/5], Step [484/10336], Loss: 2.1012\n",
      "Epoch [1/5], Step [486/10336], Loss: 0.3598\n",
      "Epoch [1/5], Step [488/10336], Loss: 3.2688\n",
      "Epoch [1/5], Step [490/10336], Loss: 2.5930\n",
      "Epoch [1/5], Step [492/10336], Loss: 2.3697\n",
      "Epoch [1/5], Step [494/10336], Loss: 0.3567\n",
      "Epoch [1/5], Step [496/10336], Loss: 4.6125\n",
      "Epoch [1/5], Step [498/10336], Loss: 0.6498\n",
      "Epoch [1/5], Step [500/10336], Loss: 0.6544\n",
      "Epoch [1/5], Step [502/10336], Loss: 1.2536\n",
      "Epoch [1/5], Step [504/10336], Loss: 4.3116\n",
      "Epoch [1/5], Step [506/10336], Loss: 2.8319\n",
      "Epoch [1/5], Step [508/10336], Loss: 1.1757\n",
      "Epoch [1/5], Step [510/10336], Loss: 1.5060\n",
      "Epoch [1/5], Step [512/10336], Loss: 3.2588\n",
      "Epoch [1/5], Step [514/10336], Loss: 1.9474\n",
      "Epoch [1/5], Step [516/10336], Loss: 1.4971\n",
      "Epoch [1/5], Step [518/10336], Loss: 5.7783\n",
      "Epoch [1/5], Step [520/10336], Loss: 5.4369\n",
      "Epoch [1/5], Step [522/10336], Loss: 0.6604\n",
      "Epoch [1/5], Step [524/10336], Loss: 1.3427\n",
      "Epoch [1/5], Step [526/10336], Loss: 3.2630\n",
      "Epoch [1/5], Step [528/10336], Loss: 0.6344\n",
      "Epoch [1/5], Step [530/10336], Loss: 1.2806\n",
      "Epoch [1/5], Step [532/10336], Loss: 0.8777\n",
      "Epoch [1/5], Step [534/10336], Loss: 2.2255\n",
      "Epoch [1/5], Step [536/10336], Loss: 1.5518\n",
      "Epoch [1/5], Step [538/10336], Loss: 0.8537\n",
      "Epoch [1/5], Step [540/10336], Loss: 0.6044\n",
      "Epoch [1/5], Step [542/10336], Loss: 0.5546\n",
      "Epoch [1/5], Step [544/10336], Loss: 1.0824\n",
      "Epoch [1/5], Step [546/10336], Loss: 4.0633\n",
      "Epoch [1/5], Step [548/10336], Loss: 0.5013\n",
      "Epoch [1/5], Step [550/10336], Loss: 2.5423\n",
      "Epoch [1/5], Step [552/10336], Loss: 1.6713\n",
      "Epoch [1/5], Step [554/10336], Loss: 1.5648\n",
      "Epoch [1/5], Step [556/10336], Loss: 1.0878\n",
      "Epoch [1/5], Step [558/10336], Loss: 1.6171\n",
      "Epoch [1/5], Step [560/10336], Loss: 0.9128\n",
      "Epoch [1/5], Step [562/10336], Loss: 0.8281\n",
      "Epoch [1/5], Step [564/10336], Loss: 3.3509\n",
      "Epoch [1/5], Step [566/10336], Loss: 0.1537\n",
      "Epoch [1/5], Step [568/10336], Loss: 2.1601\n",
      "Epoch [1/5], Step [570/10336], Loss: 0.9979\n",
      "Epoch [1/5], Step [572/10336], Loss: 1.4916\n",
      "Epoch [1/5], Step [574/10336], Loss: 2.5033\n",
      "Epoch [1/5], Step [576/10336], Loss: 3.3886\n",
      "Epoch [1/5], Step [578/10336], Loss: 4.4232\n",
      "Epoch [1/5], Step [580/10336], Loss: 2.2801\n",
      "Epoch [1/5], Step [582/10336], Loss: 1.2764\n",
      "Epoch [1/5], Step [584/10336], Loss: 2.1332\n",
      "Epoch [1/5], Step [586/10336], Loss: 2.4574\n",
      "Epoch [1/5], Step [588/10336], Loss: 5.0290\n",
      "Epoch [1/5], Step [590/10336], Loss: 0.6467\n",
      "Epoch [1/5], Step [592/10336], Loss: 0.6730\n",
      "Epoch [1/5], Step [594/10336], Loss: 1.0624\n",
      "Epoch [1/5], Step [596/10336], Loss: 5.8168\n",
      "Epoch [1/5], Step [598/10336], Loss: 0.5476\n",
      "Epoch [1/5], Step [600/10336], Loss: 0.4360\n",
      "Epoch [1/5], Step [602/10336], Loss: 0.4136\n",
      "Epoch [1/5], Step [604/10336], Loss: 9.2635\n",
      "Epoch [1/5], Step [606/10336], Loss: 6.5780\n",
      "Epoch [1/5], Step [608/10336], Loss: 0.8626\n",
      "Epoch [1/5], Step [610/10336], Loss: 0.1427\n",
      "Epoch [1/5], Step [612/10336], Loss: 4.8828\n",
      "Epoch [1/5], Step [614/10336], Loss: 0.9915\n",
      "Epoch [1/5], Step [616/10336], Loss: 4.5110\n",
      "Epoch [1/5], Step [618/10336], Loss: 2.3967\n",
      "Epoch [1/5], Step [620/10336], Loss: 1.9529\n",
      "Epoch [1/5], Step [622/10336], Loss: 1.0035\n",
      "Epoch [1/5], Step [624/10336], Loss: 0.5355\n",
      "Epoch [1/5], Step [626/10336], Loss: 0.7690\n",
      "Epoch [1/5], Step [628/10336], Loss: 1.4905\n",
      "Epoch [1/5], Step [630/10336], Loss: 2.7763\n",
      "Epoch [1/5], Step [632/10336], Loss: 0.8063\n",
      "Epoch [1/5], Step [634/10336], Loss: 4.3200\n",
      "Epoch [1/5], Step [636/10336], Loss: 2.7597\n",
      "Epoch [1/5], Step [638/10336], Loss: 0.6218\n",
      "Epoch [1/5], Step [640/10336], Loss: 1.0107\n",
      "Epoch [1/5], Step [642/10336], Loss: 3.5801\n",
      "Epoch [1/5], Step [644/10336], Loss: 0.9102\n",
      "Epoch [1/5], Step [646/10336], Loss: 1.2383\n",
      "Epoch [1/5], Step [648/10336], Loss: 0.5007\n",
      "Epoch [1/5], Step [650/10336], Loss: 4.5952\n",
      "Epoch [1/5], Step [652/10336], Loss: 4.9952\n",
      "Epoch [1/5], Step [654/10336], Loss: 0.9392\n",
      "Epoch [1/5], Step [656/10336], Loss: 6.0581\n",
      "Epoch [1/5], Step [658/10336], Loss: 1.9782\n",
      "Epoch [1/5], Step [660/10336], Loss: 0.2506\n",
      "Epoch [1/5], Step [662/10336], Loss: 0.3318\n",
      "Epoch [1/5], Step [664/10336], Loss: 1.4061\n",
      "Epoch [1/5], Step [666/10336], Loss: 6.0928\n",
      "Epoch [1/5], Step [668/10336], Loss: 0.0532\n",
      "Epoch [1/5], Step [670/10336], Loss: 6.2343\n",
      "Epoch [1/5], Step [672/10336], Loss: 0.7630\n",
      "Epoch [1/5], Step [674/10336], Loss: 0.4588\n",
      "Epoch [1/5], Step [676/10336], Loss: 0.6816\n",
      "Epoch [1/5], Step [678/10336], Loss: 1.9771\n",
      "Epoch [1/5], Step [680/10336], Loss: 4.4310\n",
      "Epoch [1/5], Step [682/10336], Loss: 5.3430\n",
      "Epoch [1/5], Step [684/10336], Loss: 2.6279\n",
      "Epoch [1/5], Step [686/10336], Loss: 1.3375\n",
      "Epoch [1/5], Step [688/10336], Loss: 1.8728\n",
      "Epoch [1/5], Step [690/10336], Loss: 0.3829\n",
      "Epoch [1/5], Step [692/10336], Loss: 5.1374\n",
      "Epoch [1/5], Step [694/10336], Loss: 2.5290\n",
      "Epoch [1/5], Step [696/10336], Loss: 5.1456\n",
      "Epoch [1/5], Step [698/10336], Loss: 0.5592\n",
      "Epoch [1/5], Step [700/10336], Loss: 5.4216\n",
      "Epoch [1/5], Step [702/10336], Loss: 5.8711\n",
      "Epoch [1/5], Step [704/10336], Loss: 5.2890\n",
      "Epoch [1/5], Step [706/10336], Loss: 1.5980\n",
      "Epoch [1/5], Step [708/10336], Loss: 4.2086\n",
      "Epoch [1/5], Step [710/10336], Loss: 1.5228\n",
      "Epoch [1/5], Step [712/10336], Loss: 0.6225\n",
      "Epoch [1/5], Step [714/10336], Loss: 2.4488\n",
      "Epoch [1/5], Step [716/10336], Loss: 1.5185\n",
      "Epoch [1/5], Step [718/10336], Loss: 2.9261\n",
      "Epoch [1/5], Step [720/10336], Loss: 0.8529\n",
      "Epoch [1/5], Step [722/10336], Loss: 5.5635\n",
      "Epoch [1/5], Step [724/10336], Loss: 0.7645\n",
      "Epoch [1/5], Step [726/10336], Loss: 1.1228\n",
      "Epoch [1/5], Step [728/10336], Loss: 1.7624\n",
      "Epoch [1/5], Step [730/10336], Loss: 0.8390\n",
      "Epoch [1/5], Step [732/10336], Loss: 0.7541\n",
      "Epoch [1/5], Step [734/10336], Loss: 0.4218\n",
      "Epoch [1/5], Step [736/10336], Loss: 0.6538\n",
      "Epoch [1/5], Step [738/10336], Loss: 3.7672\n",
      "Epoch [1/5], Step [740/10336], Loss: 0.4085\n",
      "Epoch [1/5], Step [742/10336], Loss: 4.6143\n",
      "Epoch [1/5], Step [744/10336], Loss: 0.9233\n",
      "Epoch [1/5], Step [746/10336], Loss: 2.5697\n",
      "Epoch [1/5], Step [748/10336], Loss: 5.6643\n",
      "Epoch [1/5], Step [750/10336], Loss: 3.1640\n",
      "Epoch [1/5], Step [752/10336], Loss: 2.1003\n",
      "Epoch [1/5], Step [754/10336], Loss: 2.1996\n",
      "Epoch [1/5], Step [756/10336], Loss: 2.6243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [758/10336], Loss: 1.7093\n",
      "Epoch [1/5], Step [760/10336], Loss: 0.9254\n",
      "Epoch [1/5], Step [762/10336], Loss: 3.3251\n",
      "Epoch [1/5], Step [764/10336], Loss: 0.9083\n",
      "Epoch [1/5], Step [766/10336], Loss: 2.4006\n",
      "Epoch [1/5], Step [768/10336], Loss: 0.8749\n",
      "Epoch [1/5], Step [770/10336], Loss: 0.3503\n",
      "Epoch [1/5], Step [772/10336], Loss: 0.6107\n",
      "Epoch [1/5], Step [774/10336], Loss: 7.3194\n",
      "Epoch [1/5], Step [776/10336], Loss: 0.2031\n",
      "Epoch [1/5], Step [778/10336], Loss: 0.6034\n",
      "Epoch [1/5], Step [780/10336], Loss: 0.0993\n",
      "Epoch [1/5], Step [782/10336], Loss: 1.1254\n",
      "Epoch [1/5], Step [784/10336], Loss: 0.6051\n",
      "Epoch [1/5], Step [786/10336], Loss: 0.8812\n",
      "Epoch [1/5], Step [788/10336], Loss: 1.5037\n",
      "Epoch [1/5], Step [790/10336], Loss: 0.5456\n",
      "Epoch [1/5], Step [792/10336], Loss: 1.4277\n",
      "Epoch [1/5], Step [794/10336], Loss: 0.3557\n",
      "Epoch [1/5], Step [796/10336], Loss: 2.2769\n",
      "Epoch [1/5], Step [798/10336], Loss: 6.1807\n",
      "Epoch [1/5], Step [800/10336], Loss: 2.0353\n",
      "Epoch [1/5], Step [802/10336], Loss: 2.1021\n",
      "Epoch [1/5], Step [804/10336], Loss: 1.9586\n",
      "Epoch [1/5], Step [806/10336], Loss: 2.3379\n",
      "Epoch [1/5], Step [808/10336], Loss: 0.5990\n",
      "Epoch [1/5], Step [810/10336], Loss: 2.7909\n",
      "Epoch [1/5], Step [812/10336], Loss: 0.3407\n",
      "Epoch [1/5], Step [814/10336], Loss: 1.4727\n",
      "Epoch [1/5], Step [816/10336], Loss: 0.1658\n",
      "Epoch [1/5], Step [818/10336], Loss: 1.2155\n",
      "Epoch [1/5], Step [820/10336], Loss: 0.7173\n",
      "Epoch [1/5], Step [822/10336], Loss: 1.2977\n",
      "Epoch [1/5], Step [824/10336], Loss: 1.1878\n",
      "Epoch [1/5], Step [826/10336], Loss: 2.0779\n",
      "Epoch [1/5], Step [828/10336], Loss: 4.7594\n",
      "Epoch [1/5], Step [830/10336], Loss: 3.3918\n",
      "Epoch [1/5], Step [832/10336], Loss: 3.5350\n",
      "Epoch [1/5], Step [834/10336], Loss: 2.3045\n",
      "Epoch [1/5], Step [836/10336], Loss: 0.8297\n",
      "Epoch [1/5], Step [838/10336], Loss: 0.4157\n",
      "Epoch [1/5], Step [840/10336], Loss: 5.6887\n",
      "Epoch [1/5], Step [842/10336], Loss: 1.1929\n",
      "Epoch [1/5], Step [844/10336], Loss: 4.3059\n",
      "Epoch [1/5], Step [846/10336], Loss: 3.5574\n",
      "Epoch [1/5], Step [848/10336], Loss: 2.9508\n",
      "Epoch [1/5], Step [850/10336], Loss: 2.0586\n",
      "Epoch [1/5], Step [852/10336], Loss: 0.7431\n",
      "Epoch [1/5], Step [854/10336], Loss: 1.6997\n",
      "Epoch [1/5], Step [856/10336], Loss: 6.2318\n",
      "Epoch [1/5], Step [858/10336], Loss: 4.1658\n",
      "Epoch [1/5], Step [860/10336], Loss: 0.2837\n",
      "Epoch [1/5], Step [862/10336], Loss: 0.4772\n",
      "Epoch [1/5], Step [864/10336], Loss: 0.7079\n",
      "Epoch [1/5], Step [866/10336], Loss: 1.8013\n",
      "Epoch [1/5], Step [868/10336], Loss: 6.0447\n",
      "Epoch [1/5], Step [870/10336], Loss: 4.2441\n",
      "Epoch [1/5], Step [872/10336], Loss: 5.5494\n",
      "Epoch [1/5], Step [874/10336], Loss: 1.8876\n",
      "Epoch [1/5], Step [876/10336], Loss: 3.1639\n",
      "Epoch [1/5], Step [878/10336], Loss: 0.7419\n",
      "Epoch [1/5], Step [880/10336], Loss: 0.5628\n",
      "Epoch [1/5], Step [882/10336], Loss: 2.5590\n",
      "Epoch [1/5], Step [884/10336], Loss: 1.6415\n",
      "Epoch [1/5], Step [886/10336], Loss: 4.6330\n",
      "Epoch [1/5], Step [888/10336], Loss: 1.2150\n",
      "Epoch [1/5], Step [890/10336], Loss: 3.0252\n",
      "Epoch [1/5], Step [892/10336], Loss: 0.6640\n",
      "Epoch [1/5], Step [894/10336], Loss: 3.0248\n",
      "Epoch [1/5], Step [896/10336], Loss: 1.3124\n",
      "Epoch [1/5], Step [898/10336], Loss: 2.8269\n",
      "Epoch [1/5], Step [900/10336], Loss: 1.0386\n",
      "Epoch [1/5], Step [902/10336], Loss: 0.8661\n",
      "Epoch [1/5], Step [904/10336], Loss: 1.4476\n",
      "Epoch [1/5], Step [906/10336], Loss: 0.8016\n",
      "Epoch [1/5], Step [908/10336], Loss: 0.5647\n",
      "Epoch [1/5], Step [910/10336], Loss: 3.1138\n",
      "Epoch [1/5], Step [912/10336], Loss: 0.4538\n",
      "Epoch [1/5], Step [914/10336], Loss: 2.2206\n",
      "Epoch [1/5], Step [916/10336], Loss: 2.0141\n",
      "Epoch [1/5], Step [918/10336], Loss: 0.9569\n",
      "Epoch [1/5], Step [920/10336], Loss: 4.9147\n",
      "Epoch [1/5], Step [922/10336], Loss: 5.3595\n",
      "Epoch [1/5], Step [924/10336], Loss: 1.9018\n",
      "Epoch [1/5], Step [926/10336], Loss: 0.5902\n",
      "Epoch [1/5], Step [928/10336], Loss: 2.7354\n",
      "Epoch [1/5], Step [930/10336], Loss: 5.7979\n",
      "Epoch [1/5], Step [932/10336], Loss: 3.0613\n",
      "Epoch [1/5], Step [934/10336], Loss: 0.6241\n",
      "Epoch [1/5], Step [936/10336], Loss: 1.7364\n",
      "Epoch [1/5], Step [938/10336], Loss: 0.4702\n",
      "Epoch [1/5], Step [940/10336], Loss: 2.9434\n",
      "Epoch [1/5], Step [942/10336], Loss: 0.8910\n",
      "Epoch [1/5], Step [944/10336], Loss: 3.7162\n",
      "Epoch [1/5], Step [946/10336], Loss: 1.9684\n",
      "Epoch [1/5], Step [948/10336], Loss: 0.5513\n",
      "Epoch [1/5], Step [950/10336], Loss: 0.4880\n",
      "Epoch [1/5], Step [952/10336], Loss: 0.6930\n",
      "Epoch [1/5], Step [954/10336], Loss: 0.9037\n",
      "Epoch [1/5], Step [956/10336], Loss: 1.0604\n",
      "Epoch [1/5], Step [958/10336], Loss: 0.4289\n",
      "Epoch [1/5], Step [960/10336], Loss: 0.3201\n",
      "Epoch [1/5], Step [962/10336], Loss: 0.7657\n",
      "Epoch [1/5], Step [964/10336], Loss: 4.9899\n",
      "Epoch [1/5], Step [966/10336], Loss: 0.0476\n",
      "Epoch [1/5], Step [968/10336], Loss: 1.8479\n",
      "Epoch [1/5], Step [970/10336], Loss: 0.3591\n",
      "Epoch [1/5], Step [972/10336], Loss: 5.1740\n",
      "Epoch [1/5], Step [974/10336], Loss: 1.1060\n",
      "Epoch [1/5], Step [976/10336], Loss: 4.8186\n",
      "Epoch [1/5], Step [978/10336], Loss: 3.2652\n",
      "Epoch [1/5], Step [980/10336], Loss: 0.9524\n",
      "Epoch [1/5], Step [982/10336], Loss: 1.0877\n",
      "Epoch [1/5], Step [984/10336], Loss: 2.4930\n",
      "Epoch [1/5], Step [986/10336], Loss: 0.4533\n",
      "Epoch [1/5], Step [988/10336], Loss: 3.9882\n",
      "Epoch [1/5], Step [990/10336], Loss: 0.1047\n",
      "Epoch [1/5], Step [992/10336], Loss: 0.6914\n",
      "Epoch [1/5], Step [994/10336], Loss: 1.0243\n",
      "Epoch [1/5], Step [996/10336], Loss: 0.6146\n",
      "Epoch [1/5], Step [998/10336], Loss: 2.8651\n",
      "Epoch [1/5], Step [1000/10336], Loss: 3.3649\n",
      "Epoch [1/5], Step [1002/10336], Loss: 0.8481\n",
      "Epoch [1/5], Step [1004/10336], Loss: 0.9553\n",
      "Epoch [1/5], Step [1006/10336], Loss: 0.6587\n",
      "Epoch [1/5], Step [1008/10336], Loss: 4.0741\n",
      "Epoch [1/5], Step [1010/10336], Loss: 0.4580\n",
      "Epoch [1/5], Step [1012/10336], Loss: 0.2953\n",
      "Epoch [1/5], Step [1014/10336], Loss: 0.4773\n",
      "Epoch [1/5], Step [1016/10336], Loss: 0.3487\n",
      "Epoch [1/5], Step [1018/10336], Loss: 2.9096\n",
      "Epoch [1/5], Step [1020/10336], Loss: 1.9029\n",
      "Epoch [1/5], Step [1022/10336], Loss: 6.8623\n",
      "Epoch [1/5], Step [1024/10336], Loss: 1.6858\n",
      "Epoch [1/5], Step [1026/10336], Loss: 1.7008\n",
      "Epoch [1/5], Step [1028/10336], Loss: 0.3841\n",
      "Epoch [1/5], Step [1030/10336], Loss: 0.4685\n",
      "Epoch [1/5], Step [1032/10336], Loss: 0.3195\n",
      "Epoch [1/5], Step [1034/10336], Loss: 0.6237\n",
      "Epoch [1/5], Step [1036/10336], Loss: 4.4736\n",
      "Epoch [1/5], Step [1038/10336], Loss: 0.6531\n",
      "Epoch [1/5], Step [1040/10336], Loss: 0.8924\n",
      "Epoch [1/5], Step [1042/10336], Loss: 0.7845\n",
      "Epoch [1/5], Step [1044/10336], Loss: 5.2076\n",
      "Epoch [1/5], Step [1046/10336], Loss: 1.5595\n",
      "Epoch [1/5], Step [1048/10336], Loss: 0.8700\n",
      "Epoch [1/5], Step [1050/10336], Loss: 3.6578\n",
      "Epoch [1/5], Step [1052/10336], Loss: 3.8557\n",
      "Epoch [1/5], Step [1054/10336], Loss: 1.4106\n",
      "Epoch [1/5], Step [1056/10336], Loss: 0.2173\n",
      "Epoch [1/5], Step [1058/10336], Loss: 1.6330\n",
      "Epoch [1/5], Step [1060/10336], Loss: 1.2762\n",
      "Epoch [1/5], Step [1062/10336], Loss: 1.5140\n",
      "Epoch [1/5], Step [1064/10336], Loss: 2.5185\n",
      "Epoch [1/5], Step [1066/10336], Loss: 3.9944\n",
      "Epoch [1/5], Step [1068/10336], Loss: 2.3132\n",
      "Epoch [1/5], Step [1070/10336], Loss: 4.8818\n",
      "Epoch [1/5], Step [1072/10336], Loss: 0.9992\n",
      "Epoch [1/5], Step [1074/10336], Loss: 2.6709\n",
      "Epoch [1/5], Step [1076/10336], Loss: 2.5242\n",
      "Epoch [1/5], Step [1078/10336], Loss: 1.3225\n",
      "Epoch [1/5], Step [1080/10336], Loss: 5.1827\n",
      "Epoch [1/5], Step [1082/10336], Loss: 5.4380\n",
      "Epoch [1/5], Step [1084/10336], Loss: 3.2974\n",
      "Epoch [1/5], Step [1086/10336], Loss: 2.8963\n",
      "Epoch [1/5], Step [1088/10336], Loss: 3.8330\n",
      "Epoch [1/5], Step [1090/10336], Loss: 1.0836\n",
      "Epoch [1/5], Step [1092/10336], Loss: 0.3473\n",
      "Epoch [1/5], Step [1094/10336], Loss: 1.5456\n",
      "Epoch [1/5], Step [1096/10336], Loss: 3.3494\n",
      "Epoch [1/5], Step [1098/10336], Loss: 0.6052\n",
      "Epoch [1/5], Step [1100/10336], Loss: 1.9047\n",
      "Epoch [1/5], Step [1102/10336], Loss: 2.0431\n",
      "Epoch [1/5], Step [1104/10336], Loss: 0.1145\n",
      "Epoch [1/5], Step [1106/10336], Loss: 0.8444\n",
      "Epoch [1/5], Step [1108/10336], Loss: 0.9005\n",
      "Epoch [1/5], Step [1110/10336], Loss: 0.0966\n",
      "Epoch [1/5], Step [1112/10336], Loss: 1.7688\n",
      "Epoch [1/5], Step [1114/10336], Loss: 0.3800\n",
      "Epoch [1/5], Step [1116/10336], Loss: 3.8954\n",
      "Epoch [1/5], Step [1118/10336], Loss: 1.3082\n",
      "Epoch [1/5], Step [1120/10336], Loss: 0.0574\n",
      "Epoch [1/5], Step [1122/10336], Loss: 4.2361\n",
      "Epoch [1/5], Step [1124/10336], Loss: 1.3872\n",
      "Epoch [1/5], Step [1126/10336], Loss: 0.5510\n",
      "Epoch [1/5], Step [1128/10336], Loss: 0.5906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1130/10336], Loss: 1.6087\n",
      "Epoch [1/5], Step [1132/10336], Loss: 2.1634\n",
      "Epoch [1/5], Step [1134/10336], Loss: 1.0803\n",
      "Epoch [1/5], Step [1136/10336], Loss: 0.4561\n",
      "Epoch [1/5], Step [1138/10336], Loss: 0.8204\n",
      "Epoch [1/5], Step [1140/10336], Loss: 3.9196\n",
      "Epoch [1/5], Step [1142/10336], Loss: 2.7514\n",
      "Epoch [1/5], Step [1144/10336], Loss: 1.0717\n",
      "Epoch [1/5], Step [1146/10336], Loss: 0.5141\n",
      "Epoch [1/5], Step [1148/10336], Loss: 0.4776\n",
      "Epoch [1/5], Step [1150/10336], Loss: 0.3548\n",
      "Epoch [1/5], Step [1152/10336], Loss: 0.7041\n",
      "Epoch [1/5], Step [1154/10336], Loss: 0.7534\n",
      "Epoch [1/5], Step [1156/10336], Loss: 0.7816\n",
      "Epoch [1/5], Step [1158/10336], Loss: 1.1415\n",
      "Epoch [1/5], Step [1160/10336], Loss: 4.3918\n",
      "Epoch [1/5], Step [1162/10336], Loss: 5.0874\n",
      "Epoch [1/5], Step [1164/10336], Loss: 0.6086\n",
      "Epoch [1/5], Step [1166/10336], Loss: 0.4757\n",
      "Epoch [1/5], Step [1168/10336], Loss: 2.5412\n",
      "Epoch [1/5], Step [1170/10336], Loss: 1.5532\n",
      "Epoch [1/5], Step [1172/10336], Loss: 2.9925\n",
      "Epoch [1/5], Step [1174/10336], Loss: 1.1133\n",
      "Epoch [1/5], Step [1176/10336], Loss: 3.7448\n",
      "Epoch [1/5], Step [1178/10336], Loss: 0.5634\n",
      "Epoch [1/5], Step [1180/10336], Loss: 2.2879\n",
      "Epoch [1/5], Step [1182/10336], Loss: 0.7490\n",
      "Epoch [1/5], Step [1184/10336], Loss: 4.6779\n",
      "Epoch [1/5], Step [1186/10336], Loss: 3.8326\n",
      "Epoch [1/5], Step [1188/10336], Loss: 0.4472\n",
      "Epoch [1/5], Step [1190/10336], Loss: 0.3389\n",
      "Epoch [1/5], Step [1192/10336], Loss: 2.3743\n",
      "Epoch [1/5], Step [1194/10336], Loss: 1.6003\n",
      "Epoch [1/5], Step [1196/10336], Loss: 1.8145\n",
      "Epoch [1/5], Step [1198/10336], Loss: 1.8349\n",
      "Epoch [1/5], Step [1200/10336], Loss: 1.2197\n",
      "Epoch [1/5], Step [1202/10336], Loss: 0.0866\n",
      "Epoch [1/5], Step [1204/10336], Loss: 1.7215\n",
      "Epoch [1/5], Step [1206/10336], Loss: 1.7952\n",
      "Epoch [1/5], Step [1208/10336], Loss: 0.8775\n",
      "Epoch [1/5], Step [1210/10336], Loss: 0.4971\n",
      "Epoch [1/5], Step [1212/10336], Loss: 5.1021\n",
      "Epoch [1/5], Step [1214/10336], Loss: 0.4039\n",
      "Epoch [1/5], Step [1216/10336], Loss: 0.5176\n",
      "Epoch [1/5], Step [1218/10336], Loss: 0.4195\n",
      "Epoch [1/5], Step [1220/10336], Loss: 0.5266\n",
      "Epoch [1/5], Step [1222/10336], Loss: 0.4257\n",
      "Epoch [1/5], Step [1224/10336], Loss: 1.4502\n",
      "Epoch [1/5], Step [1226/10336], Loss: 1.2779\n",
      "Epoch [1/5], Step [1228/10336], Loss: 0.9252\n",
      "Epoch [1/5], Step [1230/10336], Loss: 0.8262\n",
      "Epoch [1/5], Step [1232/10336], Loss: 4.0310\n",
      "Epoch [1/5], Step [1234/10336], Loss: 0.2155\n",
      "Epoch [1/5], Step [1236/10336], Loss: 0.2525\n",
      "Epoch [1/5], Step [1238/10336], Loss: 4.8145\n",
      "Epoch [1/5], Step [1240/10336], Loss: 1.0879\n",
      "Epoch [1/5], Step [1242/10336], Loss: 0.7503\n",
      "Epoch [1/5], Step [1244/10336], Loss: 0.7274\n",
      "Epoch [1/5], Step [1246/10336], Loss: 0.1068\n",
      "Epoch [1/5], Step [1248/10336], Loss: 1.2849\n",
      "Epoch [1/5], Step [1250/10336], Loss: 1.3342\n",
      "Epoch [1/5], Step [1252/10336], Loss: 1.9026\n",
      "Epoch [1/5], Step [1254/10336], Loss: 0.6389\n",
      "Epoch [1/5], Step [1256/10336], Loss: 1.6093\n",
      "Epoch [1/5], Step [1258/10336], Loss: 1.2521\n",
      "Epoch [1/5], Step [1260/10336], Loss: 2.2712\n",
      "Epoch [1/5], Step [1262/10336], Loss: 3.0089\n",
      "Epoch [1/5], Step [1264/10336], Loss: 2.5200\n",
      "Epoch [1/5], Step [1266/10336], Loss: 1.6228\n",
      "Epoch [1/5], Step [1268/10336], Loss: 0.6402\n",
      "Epoch [1/5], Step [1270/10336], Loss: 1.2523\n",
      "Epoch [1/5], Step [1272/10336], Loss: 0.8795\n",
      "Epoch [1/5], Step [1274/10336], Loss: 4.8149\n",
      "Epoch [1/5], Step [1276/10336], Loss: 1.6986\n",
      "Epoch [1/5], Step [1278/10336], Loss: 1.2399\n",
      "Epoch [1/5], Step [1280/10336], Loss: 0.3937\n",
      "Epoch [1/5], Step [1282/10336], Loss: 0.6929\n",
      "Epoch [1/5], Step [1284/10336], Loss: 1.4568\n",
      "Epoch [1/5], Step [1286/10336], Loss: 2.6472\n",
      "Epoch [1/5], Step [1288/10336], Loss: 0.1518\n",
      "Epoch [1/5], Step [1290/10336], Loss: 0.7554\n",
      "Epoch [1/5], Step [1292/10336], Loss: 1.0824\n",
      "Epoch [1/5], Step [1294/10336], Loss: 1.3518\n",
      "Epoch [1/5], Step [1296/10336], Loss: 0.6013\n",
      "Epoch [1/5], Step [1298/10336], Loss: 1.0266\n",
      "Epoch [1/5], Step [1300/10336], Loss: 2.6880\n",
      "Epoch [1/5], Step [1302/10336], Loss: 1.4668\n",
      "Epoch [1/5], Step [1304/10336], Loss: 0.6358\n",
      "Epoch [1/5], Step [1306/10336], Loss: 0.2298\n",
      "Epoch [1/5], Step [1308/10336], Loss: 0.1123\n",
      "Epoch [1/5], Step [1310/10336], Loss: 1.2447\n",
      "Epoch [1/5], Step [1312/10336], Loss: 3.7887\n",
      "Epoch [1/5], Step [1314/10336], Loss: 1.0444\n",
      "Epoch [1/5], Step [1316/10336], Loss: 1.9096\n",
      "Epoch [1/5], Step [1318/10336], Loss: 0.6687\n",
      "Epoch [1/5], Step [1320/10336], Loss: 1.1089\n",
      "Epoch [1/5], Step [1322/10336], Loss: 0.4281\n",
      "Epoch [1/5], Step [1324/10336], Loss: 2.0532\n",
      "Epoch [1/5], Step [1326/10336], Loss: 2.3129\n",
      "Epoch [1/5], Step [1328/10336], Loss: 0.5782\n",
      "Epoch [1/5], Step [1330/10336], Loss: 3.9065\n",
      "Epoch [1/5], Step [1332/10336], Loss: 1.2599\n",
      "Epoch [1/5], Step [1334/10336], Loss: 2.5508\n",
      "Epoch [1/5], Step [1336/10336], Loss: 4.5784\n",
      "Epoch [1/5], Step [1338/10336], Loss: 0.6153\n",
      "Epoch [1/5], Step [1340/10336], Loss: 4.0405\n",
      "Epoch [1/5], Step [1342/10336], Loss: 1.2696\n",
      "Epoch [1/5], Step [1344/10336], Loss: 0.5995\n",
      "Epoch [1/5], Step [1346/10336], Loss: 1.3311\n",
      "Epoch [1/5], Step [1348/10336], Loss: 1.4494\n",
      "Epoch [1/5], Step [1350/10336], Loss: 0.4405\n",
      "Epoch [1/5], Step [1352/10336], Loss: 1.2767\n",
      "Epoch [1/5], Step [1354/10336], Loss: 2.7335\n",
      "Epoch [1/5], Step [1356/10336], Loss: 0.8614\n",
      "Epoch [1/5], Step [1358/10336], Loss: 2.2694\n",
      "Epoch [1/5], Step [1360/10336], Loss: 5.5663\n",
      "Epoch [1/5], Step [1362/10336], Loss: 0.1197\n",
      "Epoch [1/5], Step [1364/10336], Loss: 1.9861\n",
      "Epoch [1/5], Step [1366/10336], Loss: 3.2302\n",
      "Epoch [1/5], Step [1368/10336], Loss: 5.4546\n",
      "Epoch [1/5], Step [1370/10336], Loss: 4.3046\n",
      "Epoch [1/5], Step [1372/10336], Loss: 0.5749\n",
      "Epoch [1/5], Step [1374/10336], Loss: 2.0089\n",
      "Epoch [1/5], Step [1376/10336], Loss: 1.2870\n",
      "Epoch [1/5], Step [1378/10336], Loss: 0.3885\n",
      "Epoch [1/5], Step [1380/10336], Loss: 1.5641\n",
      "Epoch [1/5], Step [1382/10336], Loss: 0.9117\n",
      "Epoch [1/5], Step [1384/10336], Loss: 1.0980\n",
      "Epoch [1/5], Step [1386/10336], Loss: 0.4549\n",
      "Epoch [1/5], Step [1388/10336], Loss: 3.4438\n",
      "Epoch [1/5], Step [1390/10336], Loss: 1.9060\n",
      "Epoch [1/5], Step [1392/10336], Loss: 0.4092\n",
      "Epoch [1/5], Step [1394/10336], Loss: 1.2190\n",
      "Epoch [1/5], Step [1396/10336], Loss: 1.4984\n",
      "Epoch [1/5], Step [1398/10336], Loss: 4.1909\n",
      "Epoch [1/5], Step [1400/10336], Loss: 0.5852\n",
      "Epoch [1/5], Step [1402/10336], Loss: 4.2365\n",
      "Epoch [1/5], Step [1404/10336], Loss: 0.1363\n",
      "Epoch [1/5], Step [1406/10336], Loss: 0.4085\n",
      "Epoch [1/5], Step [1408/10336], Loss: 0.2733\n",
      "Epoch [1/5], Step [1410/10336], Loss: 0.8228\n",
      "Epoch [1/5], Step [1412/10336], Loss: 0.4367\n",
      "Epoch [1/5], Step [1414/10336], Loss: 0.4397\n",
      "Epoch [1/5], Step [1416/10336], Loss: 1.5207\n",
      "Epoch [1/5], Step [1418/10336], Loss: 0.7450\n",
      "Epoch [1/5], Step [1420/10336], Loss: 5.2022\n",
      "Epoch [1/5], Step [1422/10336], Loss: 0.4968\n",
      "Epoch [1/5], Step [1424/10336], Loss: 4.2382\n",
      "Epoch [1/5], Step [1426/10336], Loss: 3.1215\n",
      "Epoch [1/5], Step [1428/10336], Loss: 1.5102\n",
      "Epoch [1/5], Step [1430/10336], Loss: 1.2554\n",
      "Epoch [1/5], Step [1432/10336], Loss: 0.7676\n",
      "Epoch [1/5], Step [1434/10336], Loss: 0.9806\n",
      "Epoch [1/5], Step [1436/10336], Loss: 0.3578\n",
      "Epoch [1/5], Step [1438/10336], Loss: 0.9119\n",
      "Epoch [1/5], Step [1440/10336], Loss: 0.6019\n",
      "Epoch [1/5], Step [1442/10336], Loss: 1.7156\n",
      "Epoch [1/5], Step [1444/10336], Loss: 5.8503\n",
      "Epoch [1/5], Step [1446/10336], Loss: 2.3161\n",
      "Epoch [1/5], Step [1448/10336], Loss: 6.3847\n",
      "Epoch [1/5], Step [1450/10336], Loss: 0.7520\n",
      "Epoch [1/5], Step [1452/10336], Loss: 0.6563\n",
      "Epoch [1/5], Step [1454/10336], Loss: 1.6449\n",
      "Epoch [1/5], Step [1456/10336], Loss: 1.4553\n",
      "Epoch [1/5], Step [1458/10336], Loss: 1.7001\n",
      "Epoch [1/5], Step [1460/10336], Loss: 3.0602\n",
      "Epoch [1/5], Step [1462/10336], Loss: 0.2133\n",
      "Epoch [1/5], Step [1464/10336], Loss: 0.8227\n",
      "Epoch [1/5], Step [1466/10336], Loss: 0.9067\n",
      "Epoch [1/5], Step [1468/10336], Loss: 0.3459\n",
      "Epoch [1/5], Step [1470/10336], Loss: 2.2405\n",
      "Epoch [1/5], Step [1472/10336], Loss: 4.9573\n",
      "Epoch [1/5], Step [1474/10336], Loss: 3.8235\n",
      "Epoch [1/5], Step [1476/10336], Loss: 0.6460\n",
      "Epoch [1/5], Step [1478/10336], Loss: 2.8641\n",
      "Epoch [1/5], Step [1480/10336], Loss: 1.3652\n",
      "Epoch [1/5], Step [1482/10336], Loss: 0.6657\n",
      "Epoch [1/5], Step [1484/10336], Loss: 3.0046\n",
      "Epoch [1/5], Step [1486/10336], Loss: 0.2851\n",
      "Epoch [1/5], Step [1488/10336], Loss: 1.9542\n",
      "Epoch [1/5], Step [1490/10336], Loss: 0.1484\n",
      "Epoch [1/5], Step [1492/10336], Loss: 0.0349\n",
      "Epoch [1/5], Step [1494/10336], Loss: 0.8194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1496/10336], Loss: 0.0614\n",
      "Epoch [1/5], Step [1498/10336], Loss: 1.2564\n",
      "Epoch [1/5], Step [1500/10336], Loss: 1.9051\n",
      "Epoch [1/5], Step [1502/10336], Loss: 0.3621\n",
      "Epoch [1/5], Step [1504/10336], Loss: 0.6496\n",
      "Epoch [1/5], Step [1506/10336], Loss: 6.0041\n",
      "Epoch [1/5], Step [1508/10336], Loss: 0.0610\n",
      "Epoch [1/5], Step [1510/10336], Loss: 2.8018\n",
      "Epoch [1/5], Step [1512/10336], Loss: 5.4388\n",
      "Epoch [1/5], Step [1514/10336], Loss: 0.6769\n",
      "Epoch [1/5], Step [1516/10336], Loss: 1.5697\n",
      "Epoch [1/5], Step [1518/10336], Loss: 0.7331\n",
      "Epoch [1/5], Step [1520/10336], Loss: 1.1106\n",
      "Epoch [1/5], Step [1522/10336], Loss: 2.9616\n",
      "Epoch [1/5], Step [1524/10336], Loss: 0.4279\n",
      "Epoch [1/5], Step [1526/10336], Loss: 1.1997\n",
      "Epoch [1/5], Step [1528/10336], Loss: 0.1020\n",
      "Epoch [1/5], Step [1530/10336], Loss: 1.2509\n",
      "Epoch [1/5], Step [1532/10336], Loss: 2.2139\n",
      "Epoch [1/5], Step [1534/10336], Loss: 0.7872\n",
      "Epoch [1/5], Step [1536/10336], Loss: 3.3487\n",
      "Epoch [1/5], Step [1538/10336], Loss: 3.2322\n",
      "Epoch [1/5], Step [1540/10336], Loss: 3.0157\n",
      "Epoch [1/5], Step [1542/10336], Loss: 0.4428\n",
      "Epoch [1/5], Step [1544/10336], Loss: 0.2535\n",
      "Epoch [1/5], Step [1546/10336], Loss: 3.6588\n",
      "Epoch [1/5], Step [1548/10336], Loss: 0.8372\n",
      "Epoch [1/5], Step [1550/10336], Loss: 1.0113\n",
      "Epoch [1/5], Step [1552/10336], Loss: 4.4721\n",
      "Epoch [1/5], Step [1554/10336], Loss: 1.6252\n",
      "Epoch [1/5], Step [1556/10336], Loss: 1.1281\n",
      "Epoch [1/5], Step [1558/10336], Loss: 1.2144\n",
      "Epoch [1/5], Step [1560/10336], Loss: 0.8797\n",
      "Epoch [1/5], Step [1562/10336], Loss: 0.6047\n",
      "Epoch [1/5], Step [1564/10336], Loss: 0.9208\n",
      "Epoch [1/5], Step [1566/10336], Loss: 6.9788\n",
      "Epoch [1/5], Step [1568/10336], Loss: 3.5289\n",
      "Epoch [1/5], Step [1570/10336], Loss: 4.1221\n",
      "Epoch [1/5], Step [1572/10336], Loss: 0.5253\n",
      "Epoch [1/5], Step [1574/10336], Loss: 2.6392\n",
      "Epoch [1/5], Step [1576/10336], Loss: 5.4417\n",
      "Epoch [1/5], Step [1578/10336], Loss: 2.7595\n",
      "Epoch [1/5], Step [1580/10336], Loss: 0.8205\n",
      "Epoch [1/5], Step [1582/10336], Loss: 1.3924\n",
      "Epoch [1/5], Step [1584/10336], Loss: 2.1693\n",
      "Epoch [1/5], Step [1586/10336], Loss: 4.3954\n",
      "Epoch [1/5], Step [1588/10336], Loss: 4.3301\n",
      "Epoch [1/5], Step [1590/10336], Loss: 0.8865\n",
      "Epoch [1/5], Step [1592/10336], Loss: 2.7784\n",
      "Epoch [1/5], Step [1594/10336], Loss: 1.4732\n",
      "Epoch [1/5], Step [1596/10336], Loss: 1.8657\n",
      "Epoch [1/5], Step [1598/10336], Loss: 0.7277\n",
      "Epoch [1/5], Step [1600/10336], Loss: 1.0215\n",
      "Epoch [1/5], Step [1602/10336], Loss: 0.0412\n",
      "Epoch [1/5], Step [1604/10336], Loss: 1.4960\n",
      "Epoch [1/5], Step [1606/10336], Loss: 0.8119\n",
      "Epoch [1/5], Step [1608/10336], Loss: 1.6532\n",
      "Epoch [1/5], Step [1610/10336], Loss: 0.9864\n",
      "Epoch [1/5], Step [1612/10336], Loss: 0.6615\n",
      "Epoch [1/5], Step [1614/10336], Loss: 0.3265\n",
      "Epoch [1/5], Step [1616/10336], Loss: 1.6016\n",
      "Epoch [1/5], Step [1618/10336], Loss: 0.0677\n",
      "Epoch [1/5], Step [1620/10336], Loss: 6.0702\n",
      "Epoch [1/5], Step [1622/10336], Loss: 0.8607\n",
      "Epoch [1/5], Step [1624/10336], Loss: 0.9407\n",
      "Epoch [1/5], Step [1626/10336], Loss: 0.2431\n",
      "Epoch [1/5], Step [1628/10336], Loss: 0.1393\n",
      "Epoch [1/5], Step [1630/10336], Loss: 1.1421\n",
      "Epoch [1/5], Step [1632/10336], Loss: 4.1266\n",
      "Epoch [1/5], Step [1634/10336], Loss: 1.5944\n",
      "Epoch [1/5], Step [1636/10336], Loss: 4.1761\n",
      "Epoch [1/5], Step [1638/10336], Loss: 0.5957\n",
      "Epoch [1/5], Step [1640/10336], Loss: 2.8717\n",
      "Epoch [1/5], Step [1642/10336], Loss: 1.1068\n",
      "Epoch [1/5], Step [1644/10336], Loss: 0.7132\n",
      "Epoch [1/5], Step [1646/10336], Loss: 0.2048\n",
      "Epoch [1/5], Step [1648/10336], Loss: 1.0207\n",
      "Epoch [1/5], Step [1650/10336], Loss: 4.5629\n",
      "Epoch [1/5], Step [1652/10336], Loss: 0.7841\n",
      "Epoch [1/5], Step [1654/10336], Loss: 1.1536\n",
      "Epoch [1/5], Step [1656/10336], Loss: 3.9490\n",
      "Epoch [1/5], Step [1658/10336], Loss: 2.3679\n",
      "Epoch [1/5], Step [1660/10336], Loss: 2.2387\n",
      "Epoch [1/5], Step [1662/10336], Loss: 0.6081\n",
      "Epoch [1/5], Step [1664/10336], Loss: 0.9420\n",
      "Epoch [1/5], Step [1666/10336], Loss: 3.2176\n",
      "Epoch [1/5], Step [1668/10336], Loss: 0.5609\n",
      "Epoch [1/5], Step [1670/10336], Loss: 1.1638\n",
      "Epoch [1/5], Step [1672/10336], Loss: 4.3572\n",
      "Epoch [1/5], Step [1674/10336], Loss: 2.9499\n",
      "Epoch [1/5], Step [1676/10336], Loss: 0.6111\n",
      "Epoch [1/5], Step [1678/10336], Loss: 4.0401\n",
      "Epoch [1/5], Step [1680/10336], Loss: 0.7897\n",
      "Epoch [1/5], Step [1682/10336], Loss: 4.3226\n",
      "Epoch [1/5], Step [1684/10336], Loss: 2.2201\n",
      "Epoch [1/5], Step [1686/10336], Loss: 1.9040\n",
      "Epoch [1/5], Step [1688/10336], Loss: 0.4779\n",
      "Epoch [1/5], Step [1690/10336], Loss: 0.5817\n",
      "Epoch [1/5], Step [1692/10336], Loss: 1.3181\n",
      "Epoch [1/5], Step [1694/10336], Loss: 3.3098\n",
      "Epoch [1/5], Step [1696/10336], Loss: 1.2337\n",
      "Epoch [1/5], Step [1698/10336], Loss: 0.3611\n",
      "Epoch [1/5], Step [1700/10336], Loss: 0.6456\n",
      "Epoch [1/5], Step [1702/10336], Loss: 0.3367\n",
      "Epoch [1/5], Step [1704/10336], Loss: 1.1729\n",
      "Epoch [1/5], Step [1706/10336], Loss: 1.7637\n",
      "Epoch [1/5], Step [1708/10336], Loss: 5.5788\n",
      "Epoch [1/5], Step [1710/10336], Loss: 1.4918\n",
      "Epoch [1/5], Step [1712/10336], Loss: 5.7285\n",
      "Epoch [1/5], Step [1714/10336], Loss: 0.9673\n",
      "Epoch [1/5], Step [1716/10336], Loss: 0.6382\n",
      "Epoch [1/5], Step [1718/10336], Loss: 0.5820\n",
      "Epoch [1/5], Step [1720/10336], Loss: 4.8993\n",
      "Epoch [1/5], Step [1722/10336], Loss: 0.5776\n",
      "Epoch [1/5], Step [1724/10336], Loss: 1.7054\n",
      "Epoch [1/5], Step [1726/10336], Loss: 6.7194\n",
      "Epoch [1/5], Step [1728/10336], Loss: 0.8132\n",
      "Epoch [1/5], Step [1730/10336], Loss: 0.9693\n",
      "Epoch [1/5], Step [1732/10336], Loss: 1.4107\n",
      "Epoch [1/5], Step [1734/10336], Loss: 1.0895\n",
      "Epoch [1/5], Step [1736/10336], Loss: 1.4763\n",
      "Epoch [1/5], Step [1738/10336], Loss: 2.1058\n",
      "Epoch [1/5], Step [1740/10336], Loss: 5.7348\n",
      "Epoch [1/5], Step [1742/10336], Loss: 0.6118\n",
      "Epoch [1/5], Step [1744/10336], Loss: 0.7351\n",
      "Epoch [1/5], Step [1746/10336], Loss: 0.6469\n",
      "Epoch [1/5], Step [1748/10336], Loss: 1.1107\n",
      "Epoch [1/5], Step [1750/10336], Loss: 1.3210\n",
      "Epoch [1/5], Step [1752/10336], Loss: 0.5340\n",
      "Epoch [1/5], Step [1754/10336], Loss: 0.4167\n",
      "Epoch [1/5], Step [1756/10336], Loss: 0.1142\n",
      "Epoch [1/5], Step [1758/10336], Loss: 0.7780\n",
      "Epoch [1/5], Step [1760/10336], Loss: 1.2918\n",
      "Epoch [1/5], Step [1762/10336], Loss: 0.5212\n",
      "Epoch [1/5], Step [1764/10336], Loss: 1.8959\n",
      "Epoch [1/5], Step [1766/10336], Loss: 4.7958\n",
      "Epoch [1/5], Step [1768/10336], Loss: 0.6332\n",
      "Epoch [1/5], Step [1770/10336], Loss: 3.0345\n",
      "Epoch [1/5], Step [1772/10336], Loss: 1.2766\n",
      "Epoch [1/5], Step [1774/10336], Loss: 1.3044\n",
      "Epoch [1/5], Step [1776/10336], Loss: 0.5769\n",
      "Epoch [1/5], Step [1778/10336], Loss: 0.1639\n",
      "Epoch [1/5], Step [1780/10336], Loss: 4.6600\n",
      "Epoch [1/5], Step [1782/10336], Loss: 6.2967\n",
      "Epoch [1/5], Step [1784/10336], Loss: 4.7035\n",
      "Epoch [1/5], Step [1786/10336], Loss: 0.3600\n",
      "Epoch [1/5], Step [1788/10336], Loss: 0.8024\n",
      "Epoch [1/5], Step [1790/10336], Loss: 0.5253\n",
      "Epoch [1/5], Step [1792/10336], Loss: 1.3844\n",
      "Epoch [1/5], Step [1794/10336], Loss: 0.2987\n",
      "Epoch [1/5], Step [1796/10336], Loss: 1.9164\n",
      "Epoch [1/5], Step [1798/10336], Loss: 1.8726\n",
      "Epoch [1/5], Step [1800/10336], Loss: 0.9958\n",
      "Epoch [1/5], Step [1802/10336], Loss: 1.8326\n",
      "Epoch [1/5], Step [1804/10336], Loss: 0.3820\n",
      "Epoch [1/5], Step [1806/10336], Loss: 0.8545\n",
      "Epoch [1/5], Step [1808/10336], Loss: 1.4954\n",
      "Epoch [1/5], Step [1810/10336], Loss: 0.6209\n",
      "Epoch [1/5], Step [1812/10336], Loss: 0.6365\n",
      "Epoch [1/5], Step [1814/10336], Loss: 3.2806\n",
      "Epoch [1/5], Step [1816/10336], Loss: 0.8150\n",
      "Epoch [1/5], Step [1818/10336], Loss: 0.2715\n",
      "Epoch [1/5], Step [1820/10336], Loss: 0.6284\n",
      "Epoch [1/5], Step [1822/10336], Loss: 1.4877\n",
      "Epoch [1/5], Step [1824/10336], Loss: 0.5637\n",
      "Epoch [1/5], Step [1826/10336], Loss: 0.4508\n",
      "Epoch [1/5], Step [1828/10336], Loss: 0.2253\n",
      "Epoch [1/5], Step [1830/10336], Loss: 5.5970\n",
      "Epoch [1/5], Step [1832/10336], Loss: 2.0174\n",
      "Epoch [1/5], Step [1834/10336], Loss: 0.1800\n",
      "Epoch [1/5], Step [1836/10336], Loss: 0.5358\n",
      "Epoch [1/5], Step [1838/10336], Loss: 6.4691\n",
      "Epoch [1/5], Step [1840/10336], Loss: 0.9004\n",
      "Epoch [1/5], Step [1842/10336], Loss: 1.2508\n",
      "Epoch [1/5], Step [1844/10336], Loss: 0.4095\n",
      "Epoch [1/5], Step [1846/10336], Loss: 0.3732\n",
      "Epoch [1/5], Step [1848/10336], Loss: 3.9437\n",
      "Epoch [1/5], Step [1850/10336], Loss: 4.6153\n",
      "Epoch [1/5], Step [1852/10336], Loss: 2.2228\n",
      "Epoch [1/5], Step [1854/10336], Loss: 0.4192\n",
      "Epoch [1/5], Step [1856/10336], Loss: 0.6397\n",
      "Epoch [1/5], Step [1858/10336], Loss: 0.2586\n",
      "Epoch [1/5], Step [1860/10336], Loss: 0.4570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1862/10336], Loss: 0.8344\n",
      "Epoch [1/5], Step [1864/10336], Loss: 1.1508\n",
      "Epoch [1/5], Step [1866/10336], Loss: 0.3950\n",
      "Epoch [1/5], Step [1868/10336], Loss: 0.5709\n",
      "Epoch [1/5], Step [1870/10336], Loss: 1.2372\n",
      "Epoch [1/5], Step [1872/10336], Loss: 3.5268\n",
      "Epoch [1/5], Step [1874/10336], Loss: 0.3963\n",
      "Epoch [1/5], Step [1876/10336], Loss: 0.3809\n",
      "Epoch [1/5], Step [1878/10336], Loss: 0.8563\n",
      "Epoch [1/5], Step [1880/10336], Loss: 0.2010\n",
      "Epoch [1/5], Step [1882/10336], Loss: 0.0750\n",
      "Epoch [1/5], Step [1884/10336], Loss: 5.7225\n",
      "Epoch [1/5], Step [1886/10336], Loss: 0.9268\n",
      "Epoch [1/5], Step [1888/10336], Loss: 0.7925\n",
      "Epoch [1/5], Step [1890/10336], Loss: 0.1638\n",
      "Epoch [1/5], Step [1892/10336], Loss: 0.2411\n",
      "Epoch [1/5], Step [1894/10336], Loss: 0.3749\n",
      "Epoch [1/5], Step [1896/10336], Loss: 0.5086\n",
      "Epoch [1/5], Step [1898/10336], Loss: 0.6684\n",
      "Epoch [1/5], Step [1900/10336], Loss: 0.7202\n",
      "Epoch [1/5], Step [1902/10336], Loss: 0.5109\n",
      "Epoch [1/5], Step [1904/10336], Loss: 0.7307\n",
      "Epoch [1/5], Step [1906/10336], Loss: 0.9536\n",
      "Epoch [1/5], Step [1908/10336], Loss: 1.8895\n",
      "Epoch [1/5], Step [1910/10336], Loss: 1.0716\n",
      "Epoch [1/5], Step [1912/10336], Loss: 5.9387\n",
      "Epoch [1/5], Step [1914/10336], Loss: 6.0368\n",
      "Epoch [1/5], Step [1916/10336], Loss: 4.4579\n",
      "Epoch [1/5], Step [1918/10336], Loss: 0.2674\n",
      "Epoch [1/5], Step [1920/10336], Loss: 0.8684\n",
      "Epoch [1/5], Step [1922/10336], Loss: 2.2613\n",
      "Epoch [1/5], Step [1924/10336], Loss: 0.3411\n",
      "Epoch [1/5], Step [1926/10336], Loss: 1.9449\n",
      "Epoch [1/5], Step [1928/10336], Loss: 0.3365\n",
      "Epoch [1/5], Step [1930/10336], Loss: 0.5210\n",
      "Epoch [1/5], Step [1932/10336], Loss: 0.9869\n",
      "Epoch [1/5], Step [1934/10336], Loss: 0.9386\n",
      "Epoch [1/5], Step [1936/10336], Loss: 0.2752\n",
      "Epoch [1/5], Step [1938/10336], Loss: 1.2349\n",
      "Epoch [1/5], Step [1940/10336], Loss: 1.5178\n",
      "Epoch [1/5], Step [1942/10336], Loss: 2.9416\n",
      "Epoch [1/5], Step [1944/10336], Loss: 0.3114\n",
      "Epoch [1/5], Step [1946/10336], Loss: 0.6558\n",
      "Epoch [1/5], Step [1948/10336], Loss: 1.6254\n",
      "Epoch [1/5], Step [1950/10336], Loss: 3.5447\n",
      "Epoch [1/5], Step [1952/10336], Loss: 3.7052\n",
      "Epoch [1/5], Step [1954/10336], Loss: 2.8602\n",
      "Epoch [1/5], Step [1956/10336], Loss: 2.2094\n",
      "Epoch [1/5], Step [1958/10336], Loss: 2.5506\n",
      "Epoch [1/5], Step [1960/10336], Loss: 0.5394\n",
      "Epoch [1/5], Step [1962/10336], Loss: 1.0164\n",
      "Epoch [1/5], Step [1964/10336], Loss: 0.5233\n",
      "Epoch [1/5], Step [1966/10336], Loss: 4.6582\n",
      "Epoch [1/5], Step [1968/10336], Loss: 1.9190\n",
      "Epoch [1/5], Step [1970/10336], Loss: 0.7066\n",
      "Epoch [1/5], Step [1972/10336], Loss: 2.5188\n",
      "Epoch [1/5], Step [1974/10336], Loss: 0.7173\n",
      "Epoch [1/5], Step [1976/10336], Loss: 1.4899\n",
      "Epoch [1/5], Step [1978/10336], Loss: 0.2979\n",
      "Epoch [1/5], Step [1980/10336], Loss: 5.4261\n",
      "Epoch [1/5], Step [1982/10336], Loss: 0.3133\n",
      "Epoch [1/5], Step [1984/10336], Loss: 0.0997\n",
      "Epoch [1/5], Step [1986/10336], Loss: 0.9288\n",
      "Epoch [1/5], Step [1988/10336], Loss: 0.1877\n",
      "Epoch [1/5], Step [1990/10336], Loss: 0.5277\n",
      "Epoch [1/5], Step [1992/10336], Loss: 4.6423\n",
      "Epoch [1/5], Step [1994/10336], Loss: 1.3457\n",
      "Epoch [1/5], Step [1996/10336], Loss: 3.2521\n",
      "Epoch [1/5], Step [1998/10336], Loss: 4.4941\n",
      "Epoch [1/5], Step [2000/10336], Loss: 2.3512\n",
      "Epoch [1/5], Step [2002/10336], Loss: 4.2360\n",
      "Epoch [1/5], Step [2004/10336], Loss: 1.0776\n",
      "Epoch [1/5], Step [2006/10336], Loss: 0.3409\n",
      "Epoch [1/5], Step [2008/10336], Loss: 1.6666\n",
      "Epoch [1/5], Step [2010/10336], Loss: 1.8919\n",
      "Epoch [1/5], Step [2012/10336], Loss: 0.7131\n",
      "Epoch [1/5], Step [2014/10336], Loss: 0.5802\n",
      "Epoch [1/5], Step [2016/10336], Loss: 0.4664\n",
      "Epoch [1/5], Step [2018/10336], Loss: 3.5857\n",
      "Epoch [1/5], Step [2020/10336], Loss: 0.6158\n",
      "Epoch [1/5], Step [2022/10336], Loss: 4.1439\n",
      "Epoch [1/5], Step [2024/10336], Loss: 0.5405\n",
      "Epoch [1/5], Step [2026/10336], Loss: 0.1852\n",
      "Epoch [1/5], Step [2028/10336], Loss: 0.3047\n",
      "Epoch [1/5], Step [2030/10336], Loss: 0.1380\n",
      "Epoch [1/5], Step [2032/10336], Loss: 0.9579\n",
      "Epoch [1/5], Step [2034/10336], Loss: 1.0316\n",
      "Epoch [1/5], Step [2036/10336], Loss: 3.7324\n",
      "Epoch [1/5], Step [2038/10336], Loss: 5.4648\n",
      "Epoch [1/5], Step [2040/10336], Loss: 0.3727\n",
      "Epoch [1/5], Step [2042/10336], Loss: 2.7307\n",
      "Epoch [1/5], Step [2044/10336], Loss: 1.0289\n",
      "Epoch [1/5], Step [2046/10336], Loss: 1.5612\n",
      "Epoch [1/5], Step [2048/10336], Loss: 1.5728\n",
      "Epoch [1/5], Step [2050/10336], Loss: 2.3036\n",
      "Epoch [1/5], Step [2052/10336], Loss: 0.3747\n",
      "Epoch [1/5], Step [2054/10336], Loss: 5.2737\n",
      "Epoch [1/5], Step [2056/10336], Loss: 1.1709\n",
      "Epoch [1/5], Step [2058/10336], Loss: 0.5870\n",
      "Epoch [1/5], Step [2060/10336], Loss: 3.1682\n",
      "Epoch [1/5], Step [2062/10336], Loss: 3.9814\n",
      "Epoch [1/5], Step [2064/10336], Loss: 0.3467\n",
      "Epoch [1/5], Step [2066/10336], Loss: 1.7675\n",
      "Epoch [1/5], Step [2068/10336], Loss: 0.7056\n",
      "Epoch [1/5], Step [2070/10336], Loss: 1.2166\n",
      "Epoch [1/5], Step [2072/10336], Loss: 0.6070\n",
      "Epoch [1/5], Step [2074/10336], Loss: 0.8047\n",
      "Epoch [1/5], Step [2076/10336], Loss: 5.2143\n",
      "Epoch [1/5], Step [2078/10336], Loss: 1.5480\n",
      "Epoch [1/5], Step [2080/10336], Loss: 5.0719\n",
      "Epoch [1/5], Step [2082/10336], Loss: 1.7361\n",
      "Epoch [1/5], Step [2084/10336], Loss: 0.2908\n",
      "Epoch [1/5], Step [2086/10336], Loss: 2.6423\n",
      "Epoch [1/5], Step [2088/10336], Loss: 4.3835\n",
      "Epoch [1/5], Step [2090/10336], Loss: 4.4904\n",
      "Epoch [1/5], Step [2092/10336], Loss: 0.8008\n",
      "Epoch [1/5], Step [2094/10336], Loss: 0.7205\n",
      "Epoch [1/5], Step [2096/10336], Loss: 0.8057\n",
      "Epoch [1/5], Step [2098/10336], Loss: 0.7768\n",
      "Epoch [1/5], Step [2100/10336], Loss: 1.0732\n",
      "Epoch [1/5], Step [2102/10336], Loss: 0.3890\n",
      "Epoch [1/5], Step [2104/10336], Loss: 0.6059\n",
      "Epoch [1/5], Step [2106/10336], Loss: 0.8849\n",
      "Epoch [1/5], Step [2108/10336], Loss: 0.3397\n",
      "Epoch [1/5], Step [2110/10336], Loss: 0.4427\n",
      "Epoch [1/5], Step [2112/10336], Loss: 2.7570\n",
      "Epoch [1/5], Step [2114/10336], Loss: 2.2754\n",
      "Epoch [1/5], Step [2116/10336], Loss: 1.1761\n",
      "Epoch [1/5], Step [2118/10336], Loss: 0.9143\n",
      "Epoch [1/5], Step [2120/10336], Loss: 0.1859\n",
      "Epoch [1/5], Step [2122/10336], Loss: 0.4442\n",
      "Epoch [1/5], Step [2124/10336], Loss: 5.0861\n",
      "Epoch [1/5], Step [2126/10336], Loss: 4.1596\n",
      "Epoch [1/5], Step [2128/10336], Loss: 2.4593\n",
      "Epoch [1/5], Step [2130/10336], Loss: 2.9820\n",
      "Epoch [1/5], Step [2132/10336], Loss: 3.0131\n",
      "Epoch [1/5], Step [2134/10336], Loss: 0.9403\n",
      "Epoch [1/5], Step [2136/10336], Loss: 1.3361\n",
      "Epoch [1/5], Step [2138/10336], Loss: 2.0657\n",
      "Epoch [1/5], Step [2140/10336], Loss: 1.2742\n",
      "Epoch [1/5], Step [2142/10336], Loss: 0.3645\n",
      "Epoch [1/5], Step [2144/10336], Loss: 0.3367\n",
      "Epoch [1/5], Step [2146/10336], Loss: 1.6387\n",
      "Epoch [1/5], Step [2148/10336], Loss: 3.8480\n",
      "Epoch [1/5], Step [2150/10336], Loss: 5.7670\n",
      "Epoch [1/5], Step [2152/10336], Loss: 4.4943\n",
      "Epoch [1/5], Step [2154/10336], Loss: 3.0322\n",
      "Epoch [1/5], Step [2156/10336], Loss: 0.4826\n",
      "Epoch [1/5], Step [2158/10336], Loss: 2.6351\n",
      "Epoch [1/5], Step [2160/10336], Loss: 3.7398\n",
      "Epoch [1/5], Step [2162/10336], Loss: 4.5980\n",
      "Epoch [1/5], Step [2164/10336], Loss: 2.2879\n",
      "Epoch [1/5], Step [2166/10336], Loss: 1.9683\n",
      "Epoch [1/5], Step [2168/10336], Loss: 3.7412\n",
      "Epoch [1/5], Step [2170/10336], Loss: 1.4146\n",
      "Epoch [1/5], Step [2172/10336], Loss: 2.2467\n",
      "Epoch [1/5], Step [2174/10336], Loss: 0.9285\n",
      "Epoch [1/5], Step [2176/10336], Loss: 3.9473\n",
      "Epoch [1/5], Step [2178/10336], Loss: 5.1214\n",
      "Epoch [1/5], Step [2180/10336], Loss: 0.2985\n",
      "Epoch [1/5], Step [2182/10336], Loss: 0.8065\n",
      "Epoch [1/5], Step [2184/10336], Loss: 0.7871\n",
      "Epoch [1/5], Step [2186/10336], Loss: 1.2479\n",
      "Epoch [1/5], Step [2188/10336], Loss: 0.1475\n",
      "Epoch [1/5], Step [2190/10336], Loss: 0.6306\n",
      "Epoch [1/5], Step [2192/10336], Loss: 0.4026\n",
      "Epoch [1/5], Step [2194/10336], Loss: 0.2062\n",
      "Epoch [1/5], Step [2196/10336], Loss: 1.5005\n",
      "Epoch [1/5], Step [2198/10336], Loss: 0.7776\n",
      "Epoch [1/5], Step [2200/10336], Loss: 4.3074\n",
      "Epoch [1/5], Step [2202/10336], Loss: 2.2137\n",
      "Epoch [1/5], Step [2204/10336], Loss: 0.3375\n",
      "Epoch [1/5], Step [2206/10336], Loss: 4.7540\n",
      "Epoch [1/5], Step [2208/10336], Loss: 2.8446\n",
      "Epoch [1/5], Step [2210/10336], Loss: 0.5292\n",
      "Epoch [1/5], Step [2212/10336], Loss: 1.0377\n",
      "Epoch [1/5], Step [2214/10336], Loss: 0.9148\n",
      "Epoch [1/5], Step [2216/10336], Loss: 0.4178\n",
      "Epoch [1/5], Step [2218/10336], Loss: 0.3666\n",
      "Epoch [1/5], Step [2220/10336], Loss: 0.6137\n",
      "Epoch [1/5], Step [2222/10336], Loss: 4.6589\n",
      "Epoch [1/5], Step [2224/10336], Loss: 0.2979\n",
      "Epoch [1/5], Step [2226/10336], Loss: 2.1455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2228/10336], Loss: 0.4547\n",
      "Epoch [1/5], Step [2230/10336], Loss: 0.3380\n",
      "Epoch [1/5], Step [2232/10336], Loss: 2.1915\n",
      "Epoch [1/5], Step [2234/10336], Loss: 3.6765\n",
      "Epoch [1/5], Step [2236/10336], Loss: 0.4754\n",
      "Epoch [1/5], Step [2238/10336], Loss: 0.6854\n",
      "Epoch [1/5], Step [2240/10336], Loss: 3.4986\n",
      "Epoch [1/5], Step [2242/10336], Loss: 3.3984\n",
      "Epoch [1/5], Step [2244/10336], Loss: 1.5050\n",
      "Epoch [1/5], Step [2246/10336], Loss: 2.2374\n",
      "Epoch [1/5], Step [2248/10336], Loss: 0.6053\n",
      "Epoch [1/5], Step [2250/10336], Loss: 3.5060\n",
      "Epoch [1/5], Step [2252/10336], Loss: 0.9805\n",
      "Epoch [1/5], Step [2254/10336], Loss: 0.1534\n",
      "Epoch [1/5], Step [2256/10336], Loss: 0.3997\n",
      "Epoch [1/5], Step [2258/10336], Loss: 0.8480\n",
      "Epoch [1/5], Step [2260/10336], Loss: 0.4869\n",
      "Epoch [1/5], Step [2262/10336], Loss: 6.4458\n",
      "Epoch [1/5], Step [2264/10336], Loss: 3.1298\n",
      "Epoch [1/5], Step [2266/10336], Loss: 2.5977\n",
      "Epoch [1/5], Step [2268/10336], Loss: 0.5216\n",
      "Epoch [1/5], Step [2270/10336], Loss: 0.3481\n",
      "Epoch [1/5], Step [2272/10336], Loss: 0.7242\n",
      "Epoch [1/5], Step [2274/10336], Loss: 0.4135\n",
      "Epoch [1/5], Step [2276/10336], Loss: 1.4270\n",
      "Epoch [1/5], Step [2278/10336], Loss: 0.8862\n",
      "Epoch [1/5], Step [2280/10336], Loss: 1.8444\n",
      "Epoch [1/5], Step [2282/10336], Loss: 0.7012\n",
      "Epoch [1/5], Step [2284/10336], Loss: 4.5180\n",
      "Epoch [1/5], Step [2286/10336], Loss: 1.3452\n",
      "Epoch [1/5], Step [2288/10336], Loss: 2.5465\n",
      "Epoch [1/5], Step [2290/10336], Loss: 1.3554\n",
      "Epoch [1/5], Step [2292/10336], Loss: 1.1582\n",
      "Epoch [1/5], Step [2294/10336], Loss: 0.9803\n",
      "Epoch [1/5], Step [2296/10336], Loss: 0.6856\n",
      "Epoch [1/5], Step [2298/10336], Loss: 2.8178\n",
      "Epoch [1/5], Step [2300/10336], Loss: 0.8543\n",
      "Epoch [1/5], Step [2302/10336], Loss: 4.4010\n",
      "Epoch [1/5], Step [2304/10336], Loss: 2.6052\n",
      "Epoch [1/5], Step [2306/10336], Loss: 1.9170\n",
      "Epoch [1/5], Step [2308/10336], Loss: 4.8590\n",
      "Epoch [1/5], Step [2310/10336], Loss: 1.0064\n",
      "Epoch [1/5], Step [2312/10336], Loss: 0.5360\n",
      "Epoch [1/5], Step [2314/10336], Loss: 1.1643\n",
      "Epoch [1/5], Step [2316/10336], Loss: 3.7149\n",
      "Epoch [1/5], Step [2318/10336], Loss: 0.9582\n",
      "Epoch [1/5], Step [2320/10336], Loss: 4.2732\n",
      "Epoch [1/5], Step [2322/10336], Loss: 2.1494\n",
      "Epoch [1/5], Step [2324/10336], Loss: 0.4059\n",
      "Epoch [1/5], Step [2326/10336], Loss: 2.7505\n",
      "Epoch [1/5], Step [2328/10336], Loss: 1.0357\n",
      "Epoch [1/5], Step [2330/10336], Loss: 0.1385\n",
      "Epoch [1/5], Step [2332/10336], Loss: 2.6102\n",
      "Epoch [1/5], Step [2334/10336], Loss: 0.2125\n",
      "Epoch [1/5], Step [2336/10336], Loss: 0.4533\n",
      "Epoch [1/5], Step [2338/10336], Loss: 0.6767\n",
      "Epoch [1/5], Step [2340/10336], Loss: 5.8163\n",
      "Epoch [1/5], Step [2342/10336], Loss: 0.7109\n",
      "Epoch [1/5], Step [2344/10336], Loss: 1.2245\n",
      "Epoch [1/5], Step [2346/10336], Loss: 0.9538\n",
      "Epoch [1/5], Step [2348/10336], Loss: 1.6495\n",
      "Epoch [1/5], Step [2350/10336], Loss: 0.3913\n",
      "Epoch [1/5], Step [2352/10336], Loss: 2.6410\n",
      "Epoch [1/5], Step [2354/10336], Loss: 1.7847\n",
      "Epoch [1/5], Step [2356/10336], Loss: 1.3343\n",
      "Epoch [1/5], Step [2358/10336], Loss: 5.0422\n",
      "Epoch [1/5], Step [2360/10336], Loss: 0.8888\n",
      "Epoch [1/5], Step [2362/10336], Loss: 1.4379\n",
      "Epoch [1/5], Step [2364/10336], Loss: 4.1480\n",
      "Epoch [1/5], Step [2366/10336], Loss: 2.4251\n",
      "Epoch [1/5], Step [2368/10336], Loss: 2.0717\n",
      "Epoch [1/5], Step [2370/10336], Loss: 1.3231\n",
      "Epoch [1/5], Step [2372/10336], Loss: 0.6180\n",
      "Epoch [1/5], Step [2374/10336], Loss: 3.7183\n",
      "Epoch [1/5], Step [2376/10336], Loss: 2.0987\n",
      "Epoch [1/5], Step [2378/10336], Loss: 0.9649\n",
      "Epoch [1/5], Step [2380/10336], Loss: 0.3231\n",
      "Epoch [1/5], Step [2382/10336], Loss: 0.4161\n",
      "Epoch [1/5], Step [2384/10336], Loss: 0.8392\n",
      "Epoch [1/5], Step [2386/10336], Loss: 0.8580\n",
      "Epoch [1/5], Step [2388/10336], Loss: 0.4730\n",
      "Epoch [1/5], Step [2390/10336], Loss: 0.1136\n",
      "Epoch [1/5], Step [2392/10336], Loss: 0.6726\n",
      "Epoch [1/5], Step [2394/10336], Loss: 0.2877\n",
      "Epoch [1/5], Step [2396/10336], Loss: 1.7812\n",
      "Epoch [1/5], Step [2398/10336], Loss: 0.3454\n",
      "Epoch [1/5], Step [2400/10336], Loss: 4.5806\n",
      "Epoch [1/5], Step [2402/10336], Loss: 0.7335\n",
      "Epoch [1/5], Step [2404/10336], Loss: 0.8505\n",
      "Epoch [1/5], Step [2406/10336], Loss: 2.4369\n",
      "Epoch [1/5], Step [2408/10336], Loss: 1.0295\n",
      "Epoch [1/5], Step [2410/10336], Loss: 3.0867\n",
      "Epoch [1/5], Step [2412/10336], Loss: 0.2677\n",
      "Epoch [1/5], Step [2414/10336], Loss: 4.6553\n",
      "Epoch [1/5], Step [2416/10336], Loss: 0.4341\n",
      "Epoch [1/5], Step [2418/10336], Loss: 2.2474\n",
      "Epoch [1/5], Step [2420/10336], Loss: 0.7571\n",
      "Epoch [1/5], Step [2422/10336], Loss: 0.8261\n",
      "Epoch [1/5], Step [2424/10336], Loss: 0.3693\n",
      "Epoch [1/5], Step [2426/10336], Loss: 0.7673\n",
      "Epoch [1/5], Step [2428/10336], Loss: 3.5047\n",
      "Epoch [1/5], Step [2430/10336], Loss: 0.9232\n",
      "Epoch [1/5], Step [2432/10336], Loss: 0.8763\n",
      "Epoch [1/5], Step [2434/10336], Loss: 2.2076\n",
      "Epoch [1/5], Step [2436/10336], Loss: 0.1344\n",
      "Epoch [1/5], Step [2438/10336], Loss: 1.4093\n",
      "Epoch [1/5], Step [2440/10336], Loss: 0.7317\n",
      "Epoch [1/5], Step [2442/10336], Loss: 1.3713\n",
      "Epoch [1/5], Step [2444/10336], Loss: 0.3715\n",
      "Epoch [1/5], Step [2446/10336], Loss: 0.4313\n",
      "Epoch [1/5], Step [2448/10336], Loss: 1.5136\n",
      "Epoch [1/5], Step [2450/10336], Loss: 0.4972\n",
      "Epoch [1/5], Step [2452/10336], Loss: 1.5360\n",
      "Epoch [1/5], Step [2454/10336], Loss: 3.8381\n",
      "Epoch [1/5], Step [2456/10336], Loss: 1.8202\n",
      "Epoch [1/5], Step [2458/10336], Loss: 4.1327\n",
      "Epoch [1/5], Step [2460/10336], Loss: 1.9110\n",
      "Epoch [1/5], Step [2462/10336], Loss: 0.7493\n",
      "Epoch [1/5], Step [2464/10336], Loss: 3.2682\n",
      "Epoch [1/5], Step [2466/10336], Loss: 4.2380\n",
      "Epoch [1/5], Step [2468/10336], Loss: 0.7667\n",
      "Epoch [1/5], Step [2470/10336], Loss: 0.6460\n",
      "Epoch [1/5], Step [2472/10336], Loss: 1.7277\n",
      "Epoch [1/5], Step [2474/10336], Loss: 0.1087\n",
      "Epoch [1/5], Step [2476/10336], Loss: 1.6919\n",
      "Epoch [1/5], Step [2478/10336], Loss: 0.4255\n",
      "Epoch [1/5], Step [2480/10336], Loss: 2.9440\n",
      "Epoch [1/5], Step [2482/10336], Loss: 0.4474\n",
      "Epoch [1/5], Step [2484/10336], Loss: 5.2668\n",
      "Epoch [1/5], Step [2486/10336], Loss: 0.4873\n",
      "Epoch [1/5], Step [2488/10336], Loss: 0.1165\n",
      "Epoch [1/5], Step [2490/10336], Loss: 0.9187\n",
      "Epoch [1/5], Step [2492/10336], Loss: 1.1027\n",
      "Epoch [1/5], Step [2494/10336], Loss: 0.4406\n",
      "Epoch [1/5], Step [2496/10336], Loss: 4.6789\n",
      "Epoch [1/5], Step [2498/10336], Loss: 0.9335\n",
      "Epoch [1/5], Step [2500/10336], Loss: 0.2987\n",
      "Epoch [1/5], Step [2502/10336], Loss: 0.2591\n",
      "Epoch [1/5], Step [2504/10336], Loss: 2.8391\n",
      "Epoch [1/5], Step [2506/10336], Loss: 3.0712\n",
      "Epoch [1/5], Step [2508/10336], Loss: 0.7972\n",
      "Epoch [1/5], Step [2510/10336], Loss: 0.3156\n",
      "Epoch [1/5], Step [2512/10336], Loss: 2.3148\n",
      "Epoch [1/5], Step [2514/10336], Loss: 0.2939\n",
      "Epoch [1/5], Step [2516/10336], Loss: 2.2685\n",
      "Epoch [1/5], Step [2518/10336], Loss: 4.5267\n",
      "Epoch [1/5], Step [2520/10336], Loss: 0.5082\n",
      "Epoch [1/5], Step [2522/10336], Loss: 0.4737\n",
      "Epoch [1/5], Step [2524/10336], Loss: 5.6805\n",
      "Epoch [1/5], Step [2526/10336], Loss: 0.4056\n",
      "Epoch [1/5], Step [2528/10336], Loss: 0.4616\n",
      "Epoch [1/5], Step [2530/10336], Loss: 0.4396\n",
      "Epoch [1/5], Step [2532/10336], Loss: 0.5462\n",
      "Epoch [1/5], Step [2534/10336], Loss: 0.2672\n",
      "Epoch [1/5], Step [2536/10336], Loss: 1.8946\n",
      "Epoch [1/5], Step [2538/10336], Loss: 0.7005\n",
      "Epoch [1/5], Step [2540/10336], Loss: 4.2333\n",
      "Epoch [1/5], Step [2542/10336], Loss: 0.3904\n",
      "Epoch [1/5], Step [2544/10336], Loss: 1.4527\n",
      "Epoch [1/5], Step [2546/10336], Loss: 0.4779\n",
      "Epoch [1/5], Step [2548/10336], Loss: 1.9147\n",
      "Epoch [1/5], Step [2550/10336], Loss: 1.9754\n",
      "Epoch [1/5], Step [2552/10336], Loss: 0.5662\n",
      "Epoch [1/5], Step [2554/10336], Loss: 2.7644\n",
      "Epoch [1/5], Step [2556/10336], Loss: 0.9795\n",
      "Epoch [1/5], Step [2558/10336], Loss: 1.3071\n",
      "Epoch [1/5], Step [2560/10336], Loss: 2.3942\n",
      "Epoch [1/5], Step [2562/10336], Loss: 5.3796\n",
      "Epoch [1/5], Step [2564/10336], Loss: 2.3378\n",
      "Epoch [1/5], Step [2566/10336], Loss: 0.3582\n",
      "Epoch [1/5], Step [2568/10336], Loss: 0.7520\n",
      "Epoch [1/5], Step [2570/10336], Loss: 0.5497\n",
      "Epoch [1/5], Step [2572/10336], Loss: 3.2543\n",
      "Epoch [1/5], Step [2574/10336], Loss: 5.9540\n",
      "Epoch [1/5], Step [2576/10336], Loss: 0.5223\n",
      "Epoch [1/5], Step [2578/10336], Loss: 0.7612\n",
      "Epoch [1/5], Step [2580/10336], Loss: 1.3573\n",
      "Epoch [1/5], Step [2582/10336], Loss: 0.4407\n",
      "Epoch [1/5], Step [2584/10336], Loss: 1.2777\n",
      "Epoch [1/5], Step [2586/10336], Loss: 4.1625\n",
      "Epoch [1/5], Step [2588/10336], Loss: 0.6587\n",
      "Epoch [1/5], Step [2590/10336], Loss: 0.8327\n",
      "Epoch [1/5], Step [2592/10336], Loss: 0.5568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2594/10336], Loss: 0.1463\n",
      "Epoch [1/5], Step [2596/10336], Loss: 0.6708\n",
      "Epoch [1/5], Step [2598/10336], Loss: 0.5260\n",
      "Epoch [1/5], Step [2600/10336], Loss: 1.2332\n",
      "Epoch [1/5], Step [2602/10336], Loss: 0.5680\n",
      "Epoch [1/5], Step [2604/10336], Loss: 0.1183\n",
      "Epoch [1/5], Step [2606/10336], Loss: 1.3957\n",
      "Epoch [1/5], Step [2608/10336], Loss: 0.1854\n",
      "Epoch [1/5], Step [2610/10336], Loss: 0.1178\n",
      "Epoch [1/5], Step [2612/10336], Loss: 0.7409\n",
      "Epoch [1/5], Step [2614/10336], Loss: 1.6285\n",
      "Epoch [1/5], Step [2616/10336], Loss: 0.7626\n",
      "Epoch [1/5], Step [2618/10336], Loss: 0.6171\n",
      "Epoch [1/5], Step [2620/10336], Loss: 1.3989\n",
      "Epoch [1/5], Step [2622/10336], Loss: 1.1459\n",
      "Epoch [1/5], Step [2624/10336], Loss: 1.6856\n",
      "Epoch [1/5], Step [2626/10336], Loss: 0.6736\n",
      "Epoch [1/5], Step [2628/10336], Loss: 0.5235\n",
      "Epoch [1/5], Step [2630/10336], Loss: 1.0694\n",
      "Epoch [1/5], Step [2632/10336], Loss: 0.1832\n",
      "Epoch [1/5], Step [2634/10336], Loss: 0.9202\n",
      "Epoch [1/5], Step [2636/10336], Loss: 0.7970\n",
      "Epoch [1/5], Step [2638/10336], Loss: 0.8187\n",
      "Epoch [1/5], Step [2640/10336], Loss: 1.1969\n",
      "Epoch [1/5], Step [2642/10336], Loss: 0.5977\n",
      "Epoch [1/5], Step [2644/10336], Loss: 0.3156\n",
      "Epoch [1/5], Step [2646/10336], Loss: 0.9375\n",
      "Epoch [1/5], Step [2648/10336], Loss: 0.3616\n",
      "Epoch [1/5], Step [2650/10336], Loss: 0.3262\n",
      "Epoch [1/5], Step [2652/10336], Loss: 0.5118\n",
      "Epoch [1/5], Step [2654/10336], Loss: 6.4939\n",
      "Epoch [1/5], Step [2656/10336], Loss: 5.7158\n",
      "Epoch [1/5], Step [2658/10336], Loss: 0.3908\n",
      "Epoch [1/5], Step [2660/10336], Loss: 0.9377\n",
      "Epoch [1/5], Step [2662/10336], Loss: 0.5398\n",
      "Epoch [1/5], Step [2664/10336], Loss: 0.5588\n",
      "Epoch [1/5], Step [2666/10336], Loss: 5.3640\n",
      "Epoch [1/5], Step [2668/10336], Loss: 0.2264\n",
      "Epoch [1/5], Step [2670/10336], Loss: 1.5295\n",
      "Epoch [1/5], Step [2672/10336], Loss: 1.1942\n",
      "Epoch [1/5], Step [2674/10336], Loss: 0.3393\n",
      "Epoch [1/5], Step [2676/10336], Loss: 1.6418\n",
      "Epoch [1/5], Step [2678/10336], Loss: 1.1504\n",
      "Epoch [1/5], Step [2680/10336], Loss: 0.4299\n",
      "Epoch [1/5], Step [2682/10336], Loss: 0.3786\n",
      "Epoch [1/5], Step [2684/10336], Loss: 0.3466\n",
      "Epoch [1/5], Step [2686/10336], Loss: 0.0564\n",
      "Epoch [1/5], Step [2688/10336], Loss: 3.9645\n",
      "Epoch [1/5], Step [2690/10336], Loss: 3.7300\n",
      "Epoch [1/5], Step [2692/10336], Loss: 0.9452\n",
      "Epoch [1/5], Step [2694/10336], Loss: 1.1157\n",
      "Epoch [1/5], Step [2696/10336], Loss: 0.1437\n",
      "Epoch [1/5], Step [2698/10336], Loss: 1.4076\n",
      "Epoch [1/5], Step [2700/10336], Loss: 2.3228\n",
      "Epoch [1/5], Step [2702/10336], Loss: 0.4252\n",
      "Epoch [1/5], Step [2704/10336], Loss: 3.1182\n",
      "Epoch [1/5], Step [2706/10336], Loss: 5.1336\n",
      "Epoch [1/5], Step [2708/10336], Loss: 5.0278\n",
      "Epoch [1/5], Step [2710/10336], Loss: 1.0918\n",
      "Epoch [1/5], Step [2712/10336], Loss: 1.2758\n",
      "Epoch [1/5], Step [2714/10336], Loss: 2.4951\n",
      "Epoch [1/5], Step [2716/10336], Loss: 0.9329\n",
      "Epoch [1/5], Step [2718/10336], Loss: 0.5777\n",
      "Epoch [1/5], Step [2720/10336], Loss: 1.1832\n",
      "Epoch [1/5], Step [2722/10336], Loss: 3.6186\n",
      "Epoch [1/5], Step [2724/10336], Loss: 0.2641\n",
      "Epoch [1/5], Step [2726/10336], Loss: 0.2273\n",
      "Epoch [1/5], Step [2728/10336], Loss: 0.8538\n",
      "Epoch [1/5], Step [2730/10336], Loss: 0.0824\n",
      "Epoch [1/5], Step [2732/10336], Loss: 0.5703\n",
      "Epoch [1/5], Step [2734/10336], Loss: 3.1243\n",
      "Epoch [1/5], Step [2736/10336], Loss: 2.0371\n",
      "Epoch [1/5], Step [2738/10336], Loss: 0.4079\n",
      "Epoch [1/5], Step [2740/10336], Loss: 0.3011\n",
      "Epoch [1/5], Step [2742/10336], Loss: 0.6162\n",
      "Epoch [1/5], Step [2744/10336], Loss: 3.3101\n",
      "Epoch [1/5], Step [2746/10336], Loss: 1.9794\n",
      "Epoch [1/5], Step [2748/10336], Loss: 1.6833\n",
      "Epoch [1/5], Step [2750/10336], Loss: 0.8692\n",
      "Epoch [1/5], Step [2752/10336], Loss: 3.5009\n",
      "Epoch [1/5], Step [2754/10336], Loss: 0.0664\n",
      "Epoch [1/5], Step [2756/10336], Loss: 1.5352\n",
      "Epoch [1/5], Step [2758/10336], Loss: 0.8640\n",
      "Epoch [1/5], Step [2760/10336], Loss: 4.3833\n",
      "Epoch [1/5], Step [2762/10336], Loss: 0.0295\n",
      "Epoch [1/5], Step [2764/10336], Loss: 0.6694\n",
      "Epoch [1/5], Step [2766/10336], Loss: 1.5283\n",
      "Epoch [1/5], Step [2768/10336], Loss: 0.5859\n",
      "Epoch [1/5], Step [2770/10336], Loss: 0.3884\n",
      "Epoch [1/5], Step [2772/10336], Loss: 4.6266\n",
      "Epoch [1/5], Step [2774/10336], Loss: 3.3614\n",
      "Epoch [1/5], Step [2776/10336], Loss: 1.8108\n",
      "Epoch [1/5], Step [2778/10336], Loss: 5.1100\n",
      "Epoch [1/5], Step [2780/10336], Loss: 0.3699\n",
      "Epoch [1/5], Step [2782/10336], Loss: 0.3848\n",
      "Epoch [1/5], Step [2784/10336], Loss: 4.3059\n",
      "Epoch [1/5], Step [2786/10336], Loss: 0.5999\n",
      "Epoch [1/5], Step [2788/10336], Loss: 0.1800\n",
      "Epoch [1/5], Step [2790/10336], Loss: 1.7717\n",
      "Epoch [1/5], Step [2792/10336], Loss: 0.5216\n",
      "Epoch [1/5], Step [2794/10336], Loss: 0.6493\n",
      "Epoch [1/5], Step [2796/10336], Loss: 0.3707\n",
      "Epoch [1/5], Step [2798/10336], Loss: 0.4344\n",
      "Epoch [1/5], Step [2800/10336], Loss: 0.3316\n",
      "Epoch [1/5], Step [2802/10336], Loss: 3.9966\n",
      "Epoch [1/5], Step [2804/10336], Loss: 0.4150\n",
      "Epoch [1/5], Step [2806/10336], Loss: 6.4263\n",
      "Epoch [1/5], Step [2808/10336], Loss: 3.9604\n",
      "Epoch [1/5], Step [2810/10336], Loss: 4.4346\n",
      "Epoch [1/5], Step [2812/10336], Loss: 2.7412\n",
      "Epoch [1/5], Step [2814/10336], Loss: 1.8184\n",
      "Epoch [1/5], Step [2816/10336], Loss: 0.3345\n",
      "Epoch [1/5], Step [2818/10336], Loss: 0.4412\n",
      "Epoch [1/5], Step [2820/10336], Loss: 0.3356\n",
      "Epoch [1/5], Step [2822/10336], Loss: 1.9338\n",
      "Epoch [1/5], Step [2824/10336], Loss: 4.1760\n",
      "Epoch [1/5], Step [2826/10336], Loss: 0.9224\n",
      "Epoch [1/5], Step [2828/10336], Loss: 0.4685\n",
      "Epoch [1/5], Step [2830/10336], Loss: 0.0899\n",
      "Epoch [1/5], Step [2832/10336], Loss: 0.1318\n",
      "Epoch [1/5], Step [2834/10336], Loss: 6.0675\n",
      "Epoch [1/5], Step [2836/10336], Loss: 5.3471\n",
      "Epoch [1/5], Step [2838/10336], Loss: 0.9323\n",
      "Epoch [1/5], Step [2840/10336], Loss: 0.4535\n",
      "Epoch [1/5], Step [2842/10336], Loss: 0.0552\n",
      "Epoch [1/5], Step [2844/10336], Loss: 0.8690\n",
      "Epoch [1/5], Step [2846/10336], Loss: 1.1708\n",
      "Epoch [1/5], Step [2848/10336], Loss: 0.7105\n",
      "Epoch [1/5], Step [2850/10336], Loss: 2.2351\n",
      "Epoch [1/5], Step [2852/10336], Loss: 0.2074\n",
      "Epoch [1/5], Step [2854/10336], Loss: 0.3523\n",
      "Epoch [1/5], Step [2856/10336], Loss: 3.0482\n",
      "Epoch [1/5], Step [2858/10336], Loss: 1.4549\n",
      "Epoch [1/5], Step [2860/10336], Loss: 0.3230\n",
      "Epoch [1/5], Step [2862/10336], Loss: 0.6578\n",
      "Epoch [1/5], Step [2864/10336], Loss: 0.2006\n",
      "Epoch [1/5], Step [2866/10336], Loss: 3.1848\n",
      "Epoch [1/5], Step [2868/10336], Loss: 3.3543\n",
      "Epoch [1/5], Step [2870/10336], Loss: 2.2180\n",
      "Epoch [1/5], Step [2872/10336], Loss: 0.5861\n",
      "Epoch [1/5], Step [2874/10336], Loss: 3.8300\n",
      "Epoch [1/5], Step [2876/10336], Loss: 0.4050\n",
      "Epoch [1/5], Step [2878/10336], Loss: 5.5708\n",
      "Epoch [1/5], Step [2880/10336], Loss: 0.7878\n",
      "Epoch [1/5], Step [2882/10336], Loss: 1.1608\n",
      "Epoch [1/5], Step [2884/10336], Loss: 0.7227\n",
      "Epoch [1/5], Step [2886/10336], Loss: 4.2151\n",
      "Epoch [1/5], Step [2888/10336], Loss: 3.8174\n",
      "Epoch [1/5], Step [2890/10336], Loss: 1.4027\n",
      "Epoch [1/5], Step [2892/10336], Loss: 0.2718\n",
      "Epoch [1/5], Step [2894/10336], Loss: 0.1400\n",
      "Epoch [1/5], Step [2896/10336], Loss: 0.1559\n",
      "Epoch [1/5], Step [2898/10336], Loss: 4.5309\n",
      "Epoch [1/5], Step [2900/10336], Loss: 2.1979\n",
      "Epoch [1/5], Step [2902/10336], Loss: 0.4909\n",
      "Epoch [1/5], Step [2904/10336], Loss: 0.3734\n",
      "Epoch [1/5], Step [2906/10336], Loss: 2.0819\n",
      "Epoch [1/5], Step [2908/10336], Loss: 3.8394\n",
      "Epoch [1/5], Step [2910/10336], Loss: 2.6168\n",
      "Epoch [1/5], Step [2912/10336], Loss: 1.0053\n",
      "Epoch [1/5], Step [2914/10336], Loss: 0.3216\n",
      "Epoch [1/5], Step [2916/10336], Loss: 1.3486\n",
      "Epoch [1/5], Step [2918/10336], Loss: 1.6915\n",
      "Epoch [1/5], Step [2920/10336], Loss: 0.4880\n",
      "Epoch [1/5], Step [2922/10336], Loss: 0.0271\n",
      "Epoch [1/5], Step [2924/10336], Loss: 4.7825\n",
      "Epoch [1/5], Step [2926/10336], Loss: 0.2280\n",
      "Epoch [1/5], Step [2928/10336], Loss: 0.4219\n",
      "Epoch [1/5], Step [2930/10336], Loss: 4.1752\n",
      "Epoch [1/5], Step [2932/10336], Loss: 1.2506\n",
      "Epoch [1/5], Step [2934/10336], Loss: 1.5950\n",
      "Epoch [1/5], Step [2936/10336], Loss: 1.2469\n",
      "Epoch [1/5], Step [2938/10336], Loss: 1.5810\n",
      "Epoch [1/5], Step [2940/10336], Loss: 0.9166\n",
      "Epoch [1/5], Step [2942/10336], Loss: 1.0311\n",
      "Epoch [1/5], Step [2944/10336], Loss: 0.2430\n",
      "Epoch [1/5], Step [2946/10336], Loss: 0.8907\n",
      "Epoch [1/5], Step [2948/10336], Loss: 0.5735\n",
      "Epoch [1/5], Step [2950/10336], Loss: 2.3104\n",
      "Epoch [1/5], Step [2952/10336], Loss: 4.6466\n",
      "Epoch [1/5], Step [2954/10336], Loss: 0.3709\n",
      "Epoch [1/5], Step [2956/10336], Loss: 0.5818\n",
      "Epoch [1/5], Step [2958/10336], Loss: 0.3094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2960/10336], Loss: 1.8476\n",
      "Epoch [1/5], Step [2962/10336], Loss: 2.7398\n",
      "Epoch [1/5], Step [2964/10336], Loss: 0.4133\n",
      "Epoch [1/5], Step [2966/10336], Loss: 1.3403\n",
      "Epoch [1/5], Step [2968/10336], Loss: 1.5058\n",
      "Epoch [1/5], Step [2970/10336], Loss: 2.3005\n",
      "Epoch [1/5], Step [2972/10336], Loss: 0.9253\n",
      "Epoch [1/5], Step [2974/10336], Loss: 0.5796\n",
      "Epoch [1/5], Step [2976/10336], Loss: 3.3611\n",
      "Epoch [1/5], Step [2978/10336], Loss: 1.5191\n",
      "Epoch [1/5], Step [2980/10336], Loss: 3.8515\n",
      "Epoch [1/5], Step [2982/10336], Loss: 0.3583\n",
      "Epoch [1/5], Step [2984/10336], Loss: 1.2043\n",
      "Epoch [1/5], Step [2986/10336], Loss: 0.8561\n",
      "Epoch [1/5], Step [2988/10336], Loss: 0.0644\n",
      "Epoch [1/5], Step [2990/10336], Loss: 1.6081\n",
      "Epoch [1/5], Step [2992/10336], Loss: 1.3939\n",
      "Epoch [1/5], Step [2994/10336], Loss: 1.0647\n",
      "Epoch [1/5], Step [2996/10336], Loss: 0.8337\n",
      "Epoch [1/5], Step [2998/10336], Loss: 0.5496\n",
      "Epoch [1/5], Step [3000/10336], Loss: 1.2405\n",
      "Epoch [1/5], Step [3002/10336], Loss: 0.9542\n",
      "Epoch [1/5], Step [3004/10336], Loss: 0.4779\n",
      "Epoch [1/5], Step [3006/10336], Loss: 5.0102\n",
      "Epoch [1/5], Step [3008/10336], Loss: 0.4682\n",
      "Epoch [1/5], Step [3010/10336], Loss: 0.9485\n",
      "Epoch [1/5], Step [3012/10336], Loss: 0.3788\n",
      "Epoch [1/5], Step [3014/10336], Loss: 1.2155\n",
      "Epoch [1/5], Step [3016/10336], Loss: 4.5863\n",
      "Epoch [1/5], Step [3018/10336], Loss: 3.1802\n",
      "Epoch [1/5], Step [3020/10336], Loss: 0.2897\n",
      "Epoch [1/5], Step [3022/10336], Loss: 1.0874\n",
      "Epoch [1/5], Step [3024/10336], Loss: 0.3287\n",
      "Epoch [1/5], Step [3026/10336], Loss: 0.7128\n",
      "Epoch [1/5], Step [3028/10336], Loss: 0.8086\n",
      "Epoch [1/5], Step [3030/10336], Loss: 2.4295\n",
      "Epoch [1/5], Step [3032/10336], Loss: 0.4041\n",
      "Epoch [1/5], Step [3034/10336], Loss: 1.6680\n",
      "Epoch [1/5], Step [3036/10336], Loss: 3.0727\n",
      "Epoch [1/5], Step [3038/10336], Loss: 3.1626\n",
      "Epoch [1/5], Step [3040/10336], Loss: 1.5032\n",
      "Epoch [1/5], Step [3042/10336], Loss: 0.5413\n",
      "Epoch [1/5], Step [3044/10336], Loss: 0.7129\n",
      "Epoch [1/5], Step [3046/10336], Loss: 1.3226\n",
      "Epoch [1/5], Step [3048/10336], Loss: 2.0227\n",
      "Epoch [1/5], Step [3050/10336], Loss: 1.2992\n",
      "Epoch [1/5], Step [3052/10336], Loss: 0.9060\n",
      "Epoch [1/5], Step [3054/10336], Loss: 0.7382\n",
      "Epoch [1/5], Step [3056/10336], Loss: 0.6918\n",
      "Epoch [1/5], Step [3058/10336], Loss: 1.2424\n",
      "Epoch [1/5], Step [3060/10336], Loss: 0.4132\n",
      "Epoch [1/5], Step [3062/10336], Loss: 2.2225\n",
      "Epoch [1/5], Step [3064/10336], Loss: 0.0950\n",
      "Epoch [1/5], Step [3066/10336], Loss: 0.4175\n",
      "Epoch [1/5], Step [3068/10336], Loss: 4.6603\n",
      "Epoch [1/5], Step [3070/10336], Loss: 4.2339\n",
      "Epoch [1/5], Step [3072/10336], Loss: 1.1717\n",
      "Epoch [1/5], Step [3074/10336], Loss: 1.1718\n",
      "Epoch [1/5], Step [3076/10336], Loss: 3.7169\n",
      "Epoch [1/5], Step [3078/10336], Loss: 1.2594\n",
      "Epoch [1/5], Step [3080/10336], Loss: 2.0210\n",
      "Epoch [1/5], Step [3082/10336], Loss: 1.5191\n",
      "Epoch [1/5], Step [3084/10336], Loss: 0.7623\n",
      "Epoch [1/5], Step [3086/10336], Loss: 4.7922\n",
      "Epoch [1/5], Step [3088/10336], Loss: 0.8337\n",
      "Epoch [1/5], Step [3090/10336], Loss: 4.0307\n",
      "Epoch [1/5], Step [3092/10336], Loss: 0.7170\n",
      "Epoch [1/5], Step [3094/10336], Loss: 5.2529\n",
      "Epoch [1/5], Step [3096/10336], Loss: 5.0117\n",
      "Epoch [1/5], Step [3098/10336], Loss: 0.6633\n",
      "Epoch [1/5], Step [3100/10336], Loss: 1.1318\n",
      "Epoch [1/5], Step [3102/10336], Loss: 0.6661\n",
      "Epoch [1/5], Step [3104/10336], Loss: 1.6676\n",
      "Epoch [1/5], Step [3106/10336], Loss: 1.5156\n",
      "Epoch [1/5], Step [3108/10336], Loss: 0.0687\n",
      "Epoch [1/5], Step [3110/10336], Loss: 0.8887\n",
      "Epoch [1/5], Step [3112/10336], Loss: 0.1698\n",
      "Epoch [1/5], Step [3114/10336], Loss: 0.2396\n",
      "Epoch [1/5], Step [3116/10336], Loss: 0.2043\n",
      "Epoch [1/5], Step [3118/10336], Loss: 5.1400\n",
      "Epoch [1/5], Step [3120/10336], Loss: 0.2987\n",
      "Epoch [1/5], Step [3122/10336], Loss: 0.4719\n",
      "Epoch [1/5], Step [3124/10336], Loss: 0.0334\n",
      "Epoch [1/5], Step [3126/10336], Loss: 0.4208\n",
      "Epoch [1/5], Step [3128/10336], Loss: 0.7270\n",
      "Epoch [1/5], Step [3130/10336], Loss: 0.2278\n",
      "Epoch [1/5], Step [3132/10336], Loss: 0.3753\n",
      "Epoch [1/5], Step [3134/10336], Loss: 0.9448\n",
      "Epoch [1/5], Step [3136/10336], Loss: 1.1780\n",
      "Epoch [1/5], Step [3138/10336], Loss: 0.2978\n",
      "Epoch [1/5], Step [3140/10336], Loss: 0.4010\n",
      "Epoch [1/5], Step [3142/10336], Loss: 1.1715\n",
      "Epoch [1/5], Step [3144/10336], Loss: 0.0116\n",
      "Epoch [1/5], Step [3146/10336], Loss: 1.5290\n",
      "Epoch [1/5], Step [3148/10336], Loss: 0.1079\n",
      "Epoch [1/5], Step [3150/10336], Loss: 0.4519\n",
      "Epoch [1/5], Step [3152/10336], Loss: 3.9848\n",
      "Epoch [1/5], Step [3154/10336], Loss: 0.5377\n",
      "Epoch [1/5], Step [3156/10336], Loss: 4.8868\n",
      "Epoch [1/5], Step [3158/10336], Loss: 2.6311\n",
      "Epoch [1/5], Step [3160/10336], Loss: 1.6699\n",
      "Epoch [1/5], Step [3162/10336], Loss: 0.1419\n",
      "Epoch [1/5], Step [3164/10336], Loss: 0.0338\n",
      "Epoch [1/5], Step [3166/10336], Loss: 2.1947\n",
      "Epoch [1/5], Step [3168/10336], Loss: 0.1161\n",
      "Epoch [1/5], Step [3170/10336], Loss: 0.5348\n",
      "Epoch [1/5], Step [3172/10336], Loss: 0.4126\n",
      "Epoch [1/5], Step [3174/10336], Loss: 1.4699\n",
      "Epoch [1/5], Step [3176/10336], Loss: 0.5318\n",
      "Epoch [1/5], Step [3178/10336], Loss: 0.7290\n",
      "Epoch [1/5], Step [3180/10336], Loss: 0.4028\n",
      "Epoch [1/5], Step [3182/10336], Loss: 2.1698\n",
      "Epoch [1/5], Step [3184/10336], Loss: 0.5215\n",
      "Epoch [1/5], Step [3186/10336], Loss: 1.1148\n",
      "Epoch [1/5], Step [3188/10336], Loss: 0.3736\n",
      "Epoch [1/5], Step [3190/10336], Loss: 0.4173\n",
      "Epoch [1/5], Step [3192/10336], Loss: 0.0922\n",
      "Epoch [1/5], Step [3194/10336], Loss: 1.5743\n",
      "Epoch [1/5], Step [3196/10336], Loss: 4.3570\n",
      "Epoch [1/5], Step [3198/10336], Loss: 0.3222\n",
      "Epoch [1/5], Step [3200/10336], Loss: 0.1280\n",
      "Epoch [1/5], Step [3202/10336], Loss: 0.8941\n",
      "Epoch [1/5], Step [3204/10336], Loss: 0.3949\n",
      "Epoch [1/5], Step [3206/10336], Loss: 0.0379\n",
      "Epoch [1/5], Step [3208/10336], Loss: 1.5217\n",
      "Epoch [1/5], Step [3210/10336], Loss: 0.8687\n",
      "Epoch [1/5], Step [3212/10336], Loss: 5.2462\n",
      "Epoch [1/5], Step [3214/10336], Loss: 0.1010\n",
      "Epoch [1/5], Step [3216/10336], Loss: 0.7693\n",
      "Epoch [1/5], Step [3218/10336], Loss: 1.2017\n",
      "Epoch [1/5], Step [3220/10336], Loss: 0.6798\n",
      "Epoch [1/5], Step [3222/10336], Loss: 0.3362\n",
      "Epoch [1/5], Step [3224/10336], Loss: 0.0607\n",
      "Epoch [1/5], Step [3226/10336], Loss: 5.4306\n",
      "Epoch [1/5], Step [3228/10336], Loss: 1.4389\n",
      "Epoch [1/5], Step [3230/10336], Loss: 0.8542\n",
      "Epoch [1/5], Step [3232/10336], Loss: 1.2210\n",
      "Epoch [1/5], Step [3234/10336], Loss: 6.4175\n",
      "Epoch [1/5], Step [3236/10336], Loss: 0.6549\n",
      "Epoch [1/5], Step [3238/10336], Loss: 0.2461\n",
      "Epoch [1/5], Step [3240/10336], Loss: 0.2923\n",
      "Epoch [1/5], Step [3242/10336], Loss: 0.2837\n",
      "Epoch [1/5], Step [3244/10336], Loss: 1.3456\n",
      "Epoch [1/5], Step [3246/10336], Loss: 0.7068\n",
      "Epoch [1/5], Step [3248/10336], Loss: 0.5033\n",
      "Epoch [1/5], Step [3250/10336], Loss: 0.0809\n",
      "Epoch [1/5], Step [3252/10336], Loss: 0.9149\n",
      "Epoch [1/5], Step [3254/10336], Loss: 1.4866\n",
      "Epoch [1/5], Step [3256/10336], Loss: 0.1964\n",
      "Epoch [1/5], Step [3258/10336], Loss: 0.3381\n",
      "Epoch [1/5], Step [3260/10336], Loss: 1.3645\n",
      "Epoch [1/5], Step [3262/10336], Loss: 0.6308\n",
      "Epoch [1/5], Step [3264/10336], Loss: 3.9888\n",
      "Epoch [1/5], Step [3266/10336], Loss: 0.6719\n",
      "Epoch [1/5], Step [3268/10336], Loss: 3.0761\n",
      "Epoch [1/5], Step [3270/10336], Loss: 1.8638\n",
      "Epoch [1/5], Step [3272/10336], Loss: 0.2626\n",
      "Epoch [1/5], Step [3274/10336], Loss: 1.2490\n",
      "Epoch [1/5], Step [3276/10336], Loss: 1.3356\n",
      "Epoch [1/5], Step [3278/10336], Loss: 0.1692\n",
      "Epoch [1/5], Step [3280/10336], Loss: 1.7835\n",
      "Epoch [1/5], Step [3282/10336], Loss: 0.5276\n",
      "Epoch [1/5], Step [3284/10336], Loss: 4.1381\n",
      "Epoch [1/5], Step [3286/10336], Loss: 0.3590\n",
      "Epoch [1/5], Step [3288/10336], Loss: 4.0235\n",
      "Epoch [1/5], Step [3290/10336], Loss: 0.5866\n",
      "Epoch [1/5], Step [3292/10336], Loss: 1.6227\n",
      "Epoch [1/5], Step [3294/10336], Loss: 1.7130\n",
      "Epoch [1/5], Step [3296/10336], Loss: 0.5847\n",
      "Epoch [1/5], Step [3298/10336], Loss: 0.9556\n",
      "Epoch [1/5], Step [3300/10336], Loss: 0.6709\n",
      "Epoch [1/5], Step [3302/10336], Loss: 4.2945\n",
      "Epoch [1/5], Step [3304/10336], Loss: 2.5443\n",
      "Epoch [1/5], Step [3306/10336], Loss: 4.5267\n",
      "Epoch [1/5], Step [3308/10336], Loss: 0.5283\n",
      "Epoch [1/5], Step [3310/10336], Loss: 0.3430\n",
      "Epoch [1/5], Step [3312/10336], Loss: 2.1939\n",
      "Epoch [1/5], Step [3314/10336], Loss: 0.2668\n",
      "Epoch [1/5], Step [3316/10336], Loss: 0.3043\n",
      "Epoch [1/5], Step [3318/10336], Loss: 2.4997\n",
      "Epoch [1/5], Step [3320/10336], Loss: 4.3521\n",
      "Epoch [1/5], Step [3322/10336], Loss: 0.4089\n",
      "Epoch [1/5], Step [3324/10336], Loss: 0.6026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [3326/10336], Loss: 3.8136\n",
      "Epoch [1/5], Step [3328/10336], Loss: 0.0656\n",
      "Epoch [1/5], Step [3330/10336], Loss: 3.4835\n",
      "Epoch [1/5], Step [3332/10336], Loss: 0.9752\n",
      "Epoch [1/5], Step [3334/10336], Loss: 0.4647\n",
      "Epoch [1/5], Step [3336/10336], Loss: 1.5618\n",
      "Epoch [1/5], Step [3338/10336], Loss: 0.6919\n",
      "Epoch [1/5], Step [3340/10336], Loss: 0.2691\n",
      "Epoch [1/5], Step [3342/10336], Loss: 0.1047\n",
      "Epoch [1/5], Step [3344/10336], Loss: 1.2666\n",
      "Epoch [1/5], Step [3346/10336], Loss: 1.2344\n",
      "Epoch [1/5], Step [3348/10336], Loss: 0.6603\n",
      "Epoch [1/5], Step [3350/10336], Loss: 0.8332\n",
      "Epoch [1/5], Step [3352/10336], Loss: 4.8576\n",
      "Epoch [1/5], Step [3354/10336], Loss: 0.2468\n",
      "Epoch [1/5], Step [3356/10336], Loss: 5.6299\n",
      "Epoch [1/5], Step [3358/10336], Loss: 1.7791\n",
      "Epoch [1/5], Step [3360/10336], Loss: 0.2731\n",
      "Epoch [1/5], Step [3362/10336], Loss: 1.2254\n",
      "Epoch [1/5], Step [3364/10336], Loss: 3.2268\n",
      "Epoch [1/5], Step [3366/10336], Loss: 0.8503\n",
      "Epoch [1/5], Step [3368/10336], Loss: 0.9711\n",
      "Epoch [1/5], Step [3370/10336], Loss: 0.4666\n",
      "Epoch [1/5], Step [3372/10336], Loss: 0.3711\n",
      "Epoch [1/5], Step [3374/10336], Loss: 4.8376\n",
      "Epoch [1/5], Step [3376/10336], Loss: 4.7163\n",
      "Epoch [1/5], Step [3378/10336], Loss: 0.2981\n",
      "Epoch [1/5], Step [3380/10336], Loss: 0.3128\n",
      "Epoch [1/5], Step [3382/10336], Loss: 0.0580\n",
      "Epoch [1/5], Step [3384/10336], Loss: 4.1539\n",
      "Epoch [1/5], Step [3386/10336], Loss: 0.3776\n",
      "Epoch [1/5], Step [3388/10336], Loss: 3.5191\n",
      "Epoch [1/5], Step [3390/10336], Loss: 0.6221\n",
      "Epoch [1/5], Step [3392/10336], Loss: 0.7646\n",
      "Epoch [1/5], Step [3394/10336], Loss: 1.0878\n",
      "Epoch [1/5], Step [3396/10336], Loss: 2.1158\n",
      "Epoch [1/5], Step [3398/10336], Loss: 3.3169\n",
      "Epoch [1/5], Step [3400/10336], Loss: 0.5820\n",
      "Epoch [1/5], Step [3402/10336], Loss: 3.7272\n",
      "Epoch [1/5], Step [3404/10336], Loss: 4.0565\n",
      "Epoch [1/5], Step [3406/10336], Loss: 1.3753\n",
      "Epoch [1/5], Step [3408/10336], Loss: 1.1849\n",
      "Epoch [1/5], Step [3410/10336], Loss: 1.6021\n",
      "Epoch [1/5], Step [3412/10336], Loss: 0.7917\n",
      "Epoch [1/5], Step [3414/10336], Loss: 5.4846\n",
      "Epoch [1/5], Step [3416/10336], Loss: 0.4224\n",
      "Epoch [1/5], Step [3418/10336], Loss: 4.8656\n",
      "Epoch [1/5], Step [3420/10336], Loss: 0.2623\n",
      "Epoch [1/5], Step [3422/10336], Loss: 0.3811\n",
      "Epoch [1/5], Step [3424/10336], Loss: 0.4902\n",
      "Epoch [1/5], Step [3426/10336], Loss: 2.5365\n",
      "Epoch [1/5], Step [3428/10336], Loss: 0.6968\n",
      "Epoch [1/5], Step [3430/10336], Loss: 0.7066\n",
      "Epoch [1/5], Step [3432/10336], Loss: 1.8565\n",
      "Epoch [1/5], Step [3434/10336], Loss: 3.1305\n",
      "Epoch [1/5], Step [3436/10336], Loss: 0.7099\n",
      "Epoch [1/5], Step [3438/10336], Loss: 0.6055\n",
      "Epoch [1/5], Step [3440/10336], Loss: 2.5351\n",
      "Epoch [1/5], Step [3442/10336], Loss: 0.2198\n",
      "Epoch [1/5], Step [3444/10336], Loss: 3.0550\n",
      "Epoch [1/5], Step [3446/10336], Loss: 1.8643\n",
      "Epoch [1/5], Step [3448/10336], Loss: 0.7312\n",
      "Epoch [1/5], Step [3450/10336], Loss: 1.5862\n",
      "Epoch [1/5], Step [3452/10336], Loss: 0.3696\n",
      "Epoch [1/5], Step [3454/10336], Loss: 0.5977\n",
      "Epoch [1/5], Step [3456/10336], Loss: 0.1605\n",
      "Epoch [1/5], Step [3458/10336], Loss: 0.2424\n",
      "Epoch [1/5], Step [3460/10336], Loss: 0.9338\n",
      "Epoch [1/5], Step [3462/10336], Loss: 0.6055\n",
      "Epoch [1/5], Step [3464/10336], Loss: 0.1338\n",
      "Epoch [1/5], Step [3466/10336], Loss: 0.6920\n",
      "Epoch [1/5], Step [3468/10336], Loss: 5.4219\n",
      "Epoch [1/5], Step [3470/10336], Loss: 0.7283\n",
      "Epoch [1/5], Step [3472/10336], Loss: 0.0528\n",
      "Epoch [1/5], Step [3474/10336], Loss: 0.5960\n",
      "Epoch [1/5], Step [3476/10336], Loss: 0.3183\n",
      "Epoch [1/5], Step [3478/10336], Loss: 5.4034\n",
      "Epoch [1/5], Step [3480/10336], Loss: 2.8721\n",
      "Epoch [1/5], Step [3482/10336], Loss: 4.1632\n",
      "Epoch [1/5], Step [3484/10336], Loss: 1.0447\n",
      "Epoch [1/5], Step [3486/10336], Loss: 0.7364\n",
      "Epoch [1/5], Step [3488/10336], Loss: 2.3128\n",
      "Epoch [1/5], Step [3490/10336], Loss: 0.5559\n",
      "Epoch [1/5], Step [3492/10336], Loss: 4.1672\n",
      "Epoch [1/5], Step [3494/10336], Loss: 2.7260\n",
      "Epoch [1/5], Step [3496/10336], Loss: 0.3442\n",
      "Epoch [1/5], Step [3498/10336], Loss: 0.9503\n",
      "Epoch [1/5], Step [3500/10336], Loss: 0.4827\n",
      "Epoch [1/5], Step [3502/10336], Loss: 0.8228\n",
      "Epoch [1/5], Step [3504/10336], Loss: 0.2467\n",
      "Epoch [1/5], Step [3506/10336], Loss: 0.8242\n",
      "Epoch [1/5], Step [3508/10336], Loss: 0.2020\n",
      "Epoch [1/5], Step [3510/10336], Loss: 2.8642\n",
      "Epoch [1/5], Step [3512/10336], Loss: 1.8853\n",
      "Epoch [1/5], Step [3514/10336], Loss: 0.8149\n",
      "Epoch [1/5], Step [3516/10336], Loss: 0.9524\n",
      "Epoch [1/5], Step [3518/10336], Loss: 2.0193\n",
      "Epoch [1/5], Step [3520/10336], Loss: 0.7375\n",
      "Epoch [1/5], Step [3522/10336], Loss: 1.2644\n",
      "Epoch [1/5], Step [3524/10336], Loss: 0.7038\n",
      "Epoch [1/5], Step [3526/10336], Loss: 3.0880\n",
      "Epoch [1/5], Step [3528/10336], Loss: 1.0863\n",
      "Epoch [1/5], Step [3530/10336], Loss: 1.6189\n",
      "Epoch [1/5], Step [3532/10336], Loss: 1.2792\n",
      "Epoch [1/5], Step [3534/10336], Loss: 1.3915\n",
      "Epoch [1/5], Step [3536/10336], Loss: 0.8275\n",
      "Epoch [1/5], Step [3538/10336], Loss: 2.2213\n",
      "Epoch [1/5], Step [3540/10336], Loss: 0.2924\n",
      "Epoch [1/5], Step [3542/10336], Loss: 0.2897\n",
      "Epoch [1/5], Step [3544/10336], Loss: 1.6538\n",
      "Epoch [1/5], Step [3546/10336], Loss: 0.9010\n",
      "Epoch [1/5], Step [3548/10336], Loss: 0.5235\n",
      "Epoch [1/5], Step [3550/10336], Loss: 0.9507\n",
      "Epoch [1/5], Step [3552/10336], Loss: 2.0760\n",
      "Epoch [1/5], Step [3554/10336], Loss: 2.6544\n",
      "Epoch [1/5], Step [3556/10336], Loss: 0.4669\n",
      "Epoch [1/5], Step [3558/10336], Loss: 0.5338\n",
      "Epoch [1/5], Step [3560/10336], Loss: 2.4293\n",
      "Epoch [1/5], Step [3562/10336], Loss: 0.2080\n",
      "Epoch [1/5], Step [3564/10336], Loss: 0.3800\n",
      "Epoch [1/5], Step [3566/10336], Loss: 0.1433\n",
      "Epoch [1/5], Step [3568/10336], Loss: 0.5373\n",
      "Epoch [1/5], Step [3570/10336], Loss: 0.7276\n",
      "Epoch [1/5], Step [3572/10336], Loss: 5.3904\n",
      "Epoch [1/5], Step [3574/10336], Loss: 1.4038\n",
      "Epoch [1/5], Step [3576/10336], Loss: 3.8907\n",
      "Epoch [1/5], Step [3578/10336], Loss: 1.1268\n",
      "Epoch [1/5], Step [3580/10336], Loss: 3.0397\n",
      "Epoch [1/5], Step [3582/10336], Loss: 1.1003\n",
      "Epoch [1/5], Step [3584/10336], Loss: 4.0401\n",
      "Epoch [1/5], Step [3586/10336], Loss: 1.7441\n",
      "Epoch [1/5], Step [3588/10336], Loss: 1.8589\n",
      "Epoch [1/5], Step [3590/10336], Loss: 5.2951\n",
      "Epoch [1/5], Step [3592/10336], Loss: 0.2792\n",
      "Epoch [1/5], Step [3594/10336], Loss: 0.3330\n",
      "Epoch [1/5], Step [3596/10336], Loss: 0.3077\n",
      "Epoch [1/5], Step [3598/10336], Loss: 5.7831\n",
      "Epoch [1/5], Step [3600/10336], Loss: 0.3001\n",
      "Epoch [1/5], Step [3602/10336], Loss: 0.4492\n",
      "Epoch [1/5], Step [3604/10336], Loss: 0.8059\n",
      "Epoch [1/5], Step [3606/10336], Loss: 4.1870\n",
      "Epoch [1/5], Step [3608/10336], Loss: 0.6449\n",
      "Epoch [1/5], Step [3610/10336], Loss: 0.5246\n",
      "Epoch [1/5], Step [3612/10336], Loss: 1.0730\n",
      "Epoch [1/5], Step [3614/10336], Loss: 0.7252\n",
      "Epoch [1/5], Step [3616/10336], Loss: 0.3333\n",
      "Epoch [1/5], Step [3618/10336], Loss: 1.1345\n",
      "Epoch [1/5], Step [3620/10336], Loss: 3.7503\n",
      "Epoch [1/5], Step [3622/10336], Loss: 3.5295\n",
      "Epoch [1/5], Step [3624/10336], Loss: 2.3246\n",
      "Epoch [1/5], Step [3626/10336], Loss: 0.4050\n",
      "Epoch [1/5], Step [3628/10336], Loss: 2.2162\n",
      "Epoch [1/5], Step [3630/10336], Loss: 2.7114\n",
      "Epoch [1/5], Step [3632/10336], Loss: 3.4102\n",
      "Epoch [1/5], Step [3634/10336], Loss: 4.1143\n",
      "Epoch [1/5], Step [3636/10336], Loss: 4.3633\n",
      "Epoch [1/5], Step [3638/10336], Loss: 0.9850\n",
      "Epoch [1/5], Step [3640/10336], Loss: 0.1781\n",
      "Epoch [1/5], Step [3642/10336], Loss: 0.5410\n",
      "Epoch [1/5], Step [3644/10336], Loss: 0.2901\n",
      "Epoch [1/5], Step [3646/10336], Loss: 0.4223\n",
      "Epoch [1/5], Step [3648/10336], Loss: 0.4678\n",
      "Epoch [1/5], Step [3650/10336], Loss: 2.7240\n",
      "Epoch [1/5], Step [3652/10336], Loss: 0.3473\n",
      "Epoch [1/5], Step [3654/10336], Loss: 2.7303\n",
      "Epoch [1/5], Step [3656/10336], Loss: 4.0816\n",
      "Epoch [1/5], Step [3658/10336], Loss: 0.5516\n",
      "Epoch [1/5], Step [3660/10336], Loss: 1.4491\n",
      "Epoch [1/5], Step [3662/10336], Loss: 1.6018\n",
      "Epoch [1/5], Step [3664/10336], Loss: 0.9048\n",
      "Epoch [1/5], Step [3666/10336], Loss: 0.3835\n",
      "Epoch [1/5], Step [3668/10336], Loss: 3.4604\n",
      "Epoch [1/5], Step [3670/10336], Loss: 0.7053\n",
      "Epoch [1/5], Step [3672/10336], Loss: 0.2202\n",
      "Epoch [1/5], Step [3674/10336], Loss: 1.4828\n",
      "Epoch [1/5], Step [3676/10336], Loss: 0.5151\n",
      "Epoch [1/5], Step [3678/10336], Loss: 0.6766\n",
      "Epoch [1/5], Step [3680/10336], Loss: 0.7947\n",
      "Epoch [1/5], Step [3682/10336], Loss: 3.7095\n",
      "Epoch [1/5], Step [3684/10336], Loss: 3.6724\n",
      "Epoch [1/5], Step [3686/10336], Loss: 1.2444\n",
      "Epoch [1/5], Step [3688/10336], Loss: 0.4157\n",
      "Epoch [1/5], Step [3690/10336], Loss: 1.0376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [3692/10336], Loss: 1.5129\n",
      "Epoch [1/5], Step [3694/10336], Loss: 0.7109\n",
      "Epoch [1/5], Step [3696/10336], Loss: 1.0295\n",
      "Epoch [1/5], Step [3698/10336], Loss: 0.2647\n",
      "Epoch [1/5], Step [3700/10336], Loss: 3.3707\n",
      "Epoch [1/5], Step [3702/10336], Loss: 0.6313\n",
      "Epoch [1/5], Step [3704/10336], Loss: 0.1209\n",
      "Epoch [1/5], Step [3706/10336], Loss: 0.9623\n",
      "Epoch [1/5], Step [3708/10336], Loss: 0.8193\n",
      "Epoch [1/5], Step [3710/10336], Loss: 3.1845\n",
      "Epoch [1/5], Step [3712/10336], Loss: 0.5205\n",
      "Epoch [1/5], Step [3714/10336], Loss: 0.1368\n",
      "Epoch [1/5], Step [3716/10336], Loss: 0.5648\n",
      "Epoch [1/5], Step [3718/10336], Loss: 0.6502\n",
      "Epoch [1/5], Step [3720/10336], Loss: 0.4643\n",
      "Epoch [1/5], Step [3722/10336], Loss: 1.0832\n",
      "Epoch [1/5], Step [3724/10336], Loss: 4.3803\n",
      "Epoch [1/5], Step [3726/10336], Loss: 0.2557\n",
      "Epoch [1/5], Step [3728/10336], Loss: 1.2207\n",
      "Epoch [1/5], Step [3730/10336], Loss: 0.3529\n",
      "Epoch [1/5], Step [3732/10336], Loss: 0.4342\n",
      "Epoch [1/5], Step [3734/10336], Loss: 0.8357\n",
      "Epoch [1/5], Step [3736/10336], Loss: 1.4687\n",
      "Epoch [1/5], Step [3738/10336], Loss: 0.2104\n",
      "Epoch [1/5], Step [3740/10336], Loss: 0.4165\n",
      "Epoch [1/5], Step [3742/10336], Loss: 3.0803\n",
      "Epoch [1/5], Step [3744/10336], Loss: 1.5951\n",
      "Epoch [1/5], Step [3746/10336], Loss: 3.9667\n",
      "Epoch [1/5], Step [3748/10336], Loss: 0.6083\n",
      "Epoch [1/5], Step [3750/10336], Loss: 0.4764\n",
      "Epoch [1/5], Step [3752/10336], Loss: 0.9605\n",
      "Epoch [1/5], Step [3754/10336], Loss: 0.3982\n",
      "Epoch [1/5], Step [3756/10336], Loss: 4.0962\n",
      "Epoch [1/5], Step [3758/10336], Loss: 0.8699\n",
      "Epoch [1/5], Step [3760/10336], Loss: 2.9003\n",
      "Epoch [1/5], Step [3762/10336], Loss: 0.0609\n",
      "Epoch [1/5], Step [3764/10336], Loss: 0.4558\n",
      "Epoch [1/5], Step [3766/10336], Loss: 2.9495\n",
      "Epoch [1/5], Step [3768/10336], Loss: 4.2258\n",
      "Epoch [1/5], Step [3770/10336], Loss: 3.5937\n",
      "Epoch [1/5], Step [3772/10336], Loss: 2.2924\n",
      "Epoch [1/5], Step [3774/10336], Loss: 1.0958\n",
      "Epoch [1/5], Step [3776/10336], Loss: 0.1214\n",
      "Epoch [1/5], Step [3778/10336], Loss: 0.3354\n",
      "Epoch [1/5], Step [3780/10336], Loss: 0.6390\n",
      "Epoch [1/5], Step [3782/10336], Loss: 4.1033\n",
      "Epoch [1/5], Step [3784/10336], Loss: 0.2740\n",
      "Epoch [1/5], Step [3786/10336], Loss: 0.2969\n",
      "Epoch [1/5], Step [3788/10336], Loss: 1.2800\n",
      "Epoch [1/5], Step [3790/10336], Loss: 0.5428\n",
      "Epoch [1/5], Step [3792/10336], Loss: 0.4616\n",
      "Epoch [1/5], Step [3794/10336], Loss: 0.0955\n",
      "Epoch [1/5], Step [3796/10336], Loss: 2.7654\n",
      "Epoch [1/5], Step [3798/10336], Loss: 1.2405\n",
      "Epoch [1/5], Step [3800/10336], Loss: 0.2458\n",
      "Epoch [1/5], Step [3802/10336], Loss: 0.4555\n",
      "Epoch [1/5], Step [3804/10336], Loss: 0.4310\n",
      "Epoch [1/5], Step [3806/10336], Loss: 0.2551\n",
      "Epoch [1/5], Step [3808/10336], Loss: 2.5354\n",
      "Epoch [1/5], Step [3810/10336], Loss: 0.3257\n",
      "Epoch [1/5], Step [3812/10336], Loss: 0.2527\n",
      "Epoch [1/5], Step [3814/10336], Loss: 1.4504\n",
      "Epoch [1/5], Step [3816/10336], Loss: 0.1278\n",
      "Epoch [1/5], Step [3818/10336], Loss: 2.4066\n",
      "Epoch [1/5], Step [3820/10336], Loss: 2.5997\n",
      "Epoch [1/5], Step [3822/10336], Loss: 0.6097\n",
      "Epoch [1/5], Step [3824/10336], Loss: 0.5682\n",
      "Epoch [1/5], Step [3826/10336], Loss: 1.6351\n",
      "Epoch [1/5], Step [3828/10336], Loss: 0.8331\n",
      "Epoch [1/5], Step [3830/10336], Loss: 0.4447\n",
      "Epoch [1/5], Step [3832/10336], Loss: 0.9005\n",
      "Epoch [1/5], Step [3834/10336], Loss: 1.7248\n",
      "Epoch [1/5], Step [3836/10336], Loss: 0.1469\n",
      "Epoch [1/5], Step [3838/10336], Loss: 3.8463\n",
      "Epoch [1/5], Step [3840/10336], Loss: 0.4232\n",
      "Epoch [1/5], Step [3842/10336], Loss: 1.6082\n",
      "Epoch [1/5], Step [3844/10336], Loss: 0.9038\n",
      "Epoch [1/5], Step [3846/10336], Loss: 1.9700\n",
      "Epoch [1/5], Step [3848/10336], Loss: 1.8894\n",
      "Epoch [1/5], Step [3850/10336], Loss: 4.9372\n",
      "Epoch [1/5], Step [3852/10336], Loss: 3.0506\n",
      "Epoch [1/5], Step [3854/10336], Loss: 4.0303\n",
      "Epoch [1/5], Step [3856/10336], Loss: 1.5910\n",
      "Epoch [1/5], Step [3858/10336], Loss: 0.7288\n",
      "Epoch [1/5], Step [3860/10336], Loss: 0.7572\n",
      "Epoch [1/5], Step [3862/10336], Loss: 0.7880\n",
      "Epoch [1/5], Step [3864/10336], Loss: 5.7444\n",
      "Epoch [1/5], Step [3866/10336], Loss: 3.5998\n",
      "Epoch [1/5], Step [3868/10336], Loss: 4.0707\n",
      "Epoch [1/5], Step [3870/10336], Loss: 0.6356\n",
      "Epoch [1/5], Step [3872/10336], Loss: 0.3005\n",
      "Epoch [1/5], Step [3874/10336], Loss: 0.3806\n",
      "Epoch [1/5], Step [3876/10336], Loss: 4.7293\n",
      "Epoch [1/5], Step [3878/10336], Loss: 0.3935\n",
      "Epoch [1/5], Step [3880/10336], Loss: 0.2965\n",
      "Epoch [1/5], Step [3882/10336], Loss: 1.1652\n",
      "Epoch [1/5], Step [3884/10336], Loss: 2.9363\n",
      "Epoch [1/5], Step [3886/10336], Loss: 0.5608\n",
      "Epoch [1/5], Step [3888/10336], Loss: 1.9950\n",
      "Epoch [1/5], Step [3890/10336], Loss: 0.9704\n",
      "Epoch [1/5], Step [3892/10336], Loss: 0.0724\n",
      "Epoch [1/5], Step [3894/10336], Loss: 3.3777\n",
      "Epoch [1/5], Step [3896/10336], Loss: 0.5798\n",
      "Epoch [1/5], Step [3898/10336], Loss: 0.5035\n",
      "Epoch [1/5], Step [3900/10336], Loss: 1.3277\n",
      "Epoch [1/5], Step [3902/10336], Loss: 0.4560\n",
      "Epoch [1/5], Step [3904/10336], Loss: 3.8537\n",
      "Epoch [1/5], Step [3906/10336], Loss: 1.7326\n",
      "Epoch [1/5], Step [3908/10336], Loss: 0.2543\n",
      "Epoch [1/5], Step [3910/10336], Loss: 0.0581\n",
      "Epoch [1/5], Step [3912/10336], Loss: 3.5641\n",
      "Epoch [1/5], Step [3914/10336], Loss: 0.6881\n",
      "Epoch [1/5], Step [3916/10336], Loss: 4.1985\n",
      "Epoch [1/5], Step [3918/10336], Loss: 0.4277\n",
      "Epoch [1/5], Step [3920/10336], Loss: 0.7140\n",
      "Epoch [1/5], Step [3922/10336], Loss: 0.2961\n",
      "Epoch [1/5], Step [3924/10336], Loss: 0.4549\n",
      "Epoch [1/5], Step [3926/10336], Loss: 3.3365\n",
      "Epoch [1/5], Step [3928/10336], Loss: 1.7046\n",
      "Epoch [1/5], Step [3930/10336], Loss: 2.7603\n",
      "Epoch [1/5], Step [3932/10336], Loss: 0.4148\n",
      "Epoch [1/5], Step [3934/10336], Loss: 1.0015\n",
      "Epoch [1/5], Step [3936/10336], Loss: 1.2571\n",
      "Epoch [1/5], Step [3938/10336], Loss: 1.7264\n",
      "Epoch [1/5], Step [3940/10336], Loss: 0.3517\n",
      "Epoch [1/5], Step [3942/10336], Loss: 1.1772\n",
      "Epoch [1/5], Step [3944/10336], Loss: 0.8832\n",
      "Epoch [1/5], Step [3946/10336], Loss: 0.7903\n",
      "Epoch [1/5], Step [3948/10336], Loss: 0.0723\n",
      "Epoch [1/5], Step [3950/10336], Loss: 3.5426\n",
      "Epoch [1/5], Step [3952/10336], Loss: 0.7766\n",
      "Epoch [1/5], Step [3954/10336], Loss: 0.7934\n",
      "Epoch [1/5], Step [3956/10336], Loss: 0.9828\n",
      "Epoch [1/5], Step [3958/10336], Loss: 0.4495\n",
      "Epoch [1/5], Step [3960/10336], Loss: 3.2637\n",
      "Epoch [1/5], Step [3962/10336], Loss: 4.3268\n",
      "Epoch [1/5], Step [3964/10336], Loss: 0.3864\n",
      "Epoch [1/5], Step [3966/10336], Loss: 0.3191\n",
      "Epoch [1/5], Step [3968/10336], Loss: 0.7267\n",
      "Epoch [1/5], Step [3970/10336], Loss: 1.0240\n",
      "Epoch [1/5], Step [3972/10336], Loss: 0.3460\n",
      "Epoch [1/5], Step [3974/10336], Loss: 0.0770\n",
      "Epoch [1/5], Step [3976/10336], Loss: 1.0819\n",
      "Epoch [1/5], Step [3978/10336], Loss: 0.1720\n",
      "Epoch [1/5], Step [3980/10336], Loss: 0.6057\n",
      "Epoch [1/5], Step [3982/10336], Loss: 4.4160\n",
      "Epoch [1/5], Step [3984/10336], Loss: 0.2365\n",
      "Epoch [1/5], Step [3986/10336], Loss: 2.4048\n",
      "Epoch [1/5], Step [3988/10336], Loss: 0.0689\n",
      "Epoch [1/5], Step [3990/10336], Loss: 0.7078\n",
      "Epoch [1/5], Step [3992/10336], Loss: 3.4710\n",
      "Epoch [1/5], Step [3994/10336], Loss: 3.9082\n",
      "Epoch [1/5], Step [3996/10336], Loss: 1.0426\n",
      "Epoch [1/5], Step [3998/10336], Loss: 3.3010\n",
      "Epoch [1/5], Step [4000/10336], Loss: 0.8847\n",
      "Epoch [1/5], Step [4002/10336], Loss: 3.9334\n",
      "Epoch [1/5], Step [4004/10336], Loss: 0.2448\n",
      "Epoch [1/5], Step [4006/10336], Loss: 0.3266\n",
      "Epoch [1/5], Step [4008/10336], Loss: 5.2129\n",
      "Epoch [1/5], Step [4010/10336], Loss: 4.2182\n",
      "Epoch [1/5], Step [4012/10336], Loss: 0.5078\n",
      "Epoch [1/5], Step [4014/10336], Loss: 2.3410\n",
      "Epoch [1/5], Step [4016/10336], Loss: 0.3179\n",
      "Epoch [1/5], Step [4018/10336], Loss: 0.4709\n",
      "Epoch [1/5], Step [4020/10336], Loss: 0.3802\n",
      "Epoch [1/5], Step [4022/10336], Loss: 0.4744\n",
      "Epoch [1/5], Step [4024/10336], Loss: 0.9235\n",
      "Epoch [1/5], Step [4026/10336], Loss: 0.4278\n",
      "Epoch [1/5], Step [4028/10336], Loss: 1.1694\n",
      "Epoch [1/5], Step [4030/10336], Loss: 0.3949\n",
      "Epoch [1/5], Step [4032/10336], Loss: 0.5920\n",
      "Epoch [1/5], Step [4034/10336], Loss: 0.5295\n",
      "Epoch [1/5], Step [4036/10336], Loss: 1.2781\n",
      "Epoch [1/5], Step [4038/10336], Loss: 0.5789\n",
      "Epoch [1/5], Step [4040/10336], Loss: 0.6387\n",
      "Epoch [1/5], Step [4042/10336], Loss: 0.0851\n",
      "Epoch [1/5], Step [4044/10336], Loss: 0.5861\n",
      "Epoch [1/5], Step [4046/10336], Loss: 0.6019\n",
      "Epoch [1/5], Step [4048/10336], Loss: 4.7310\n",
      "Epoch [1/5], Step [4050/10336], Loss: 0.7370\n",
      "Epoch [1/5], Step [4052/10336], Loss: 3.2176\n",
      "Epoch [1/5], Step [4054/10336], Loss: 0.4020\n",
      "Epoch [1/5], Step [4056/10336], Loss: 0.0488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4058/10336], Loss: 2.4356\n",
      "Epoch [1/5], Step [4060/10336], Loss: 0.2581\n",
      "Epoch [1/5], Step [4062/10336], Loss: 0.4952\n",
      "Epoch [1/5], Step [4064/10336], Loss: 4.7817\n",
      "Epoch [1/5], Step [4066/10336], Loss: 0.2666\n",
      "Epoch [1/5], Step [4068/10336], Loss: 0.4892\n",
      "Epoch [1/5], Step [4070/10336], Loss: 0.2456\n",
      "Epoch [1/5], Step [4072/10336], Loss: 4.0388\n",
      "Epoch [1/5], Step [4074/10336], Loss: 2.6674\n",
      "Epoch [1/5], Step [4076/10336], Loss: 3.1299\n",
      "Epoch [1/5], Step [4078/10336], Loss: 0.0682\n",
      "Epoch [1/5], Step [4080/10336], Loss: 3.5660\n",
      "Epoch [1/5], Step [4082/10336], Loss: 1.0910\n",
      "Epoch [1/5], Step [4084/10336], Loss: 0.7327\n",
      "Epoch [1/5], Step [4086/10336], Loss: 0.7163\n",
      "Epoch [1/5], Step [4088/10336], Loss: 3.9845\n",
      "Epoch [1/5], Step [4090/10336], Loss: 0.5222\n",
      "Epoch [1/5], Step [4092/10336], Loss: 4.6057\n",
      "Epoch [1/5], Step [4094/10336], Loss: 0.3742\n",
      "Epoch [1/5], Step [4096/10336], Loss: 0.4752\n",
      "Epoch [1/5], Step [4098/10336], Loss: 0.3044\n",
      "Epoch [1/5], Step [4100/10336], Loss: 1.3507\n",
      "Epoch [1/5], Step [4102/10336], Loss: 3.5113\n",
      "Epoch [1/5], Step [4104/10336], Loss: 0.0739\n",
      "Epoch [1/5], Step [4106/10336], Loss: 1.8929\n",
      "Epoch [1/5], Step [4108/10336], Loss: 0.5314\n",
      "Epoch [1/5], Step [4110/10336], Loss: 1.7198\n",
      "Epoch [1/5], Step [4112/10336], Loss: 4.9438\n",
      "Epoch [1/5], Step [4114/10336], Loss: 0.7080\n",
      "Epoch [1/5], Step [4116/10336], Loss: 1.0400\n",
      "Epoch [1/5], Step [4118/10336], Loss: 0.3005\n",
      "Epoch [1/5], Step [4120/10336], Loss: 2.8251\n",
      "Epoch [1/5], Step [4122/10336], Loss: 1.2925\n",
      "Epoch [1/5], Step [4124/10336], Loss: 4.2419\n",
      "Epoch [1/5], Step [4126/10336], Loss: 1.9974\n",
      "Epoch [1/5], Step [4128/10336], Loss: 1.9362\n",
      "Epoch [1/5], Step [4130/10336], Loss: 0.2736\n",
      "Epoch [1/5], Step [4132/10336], Loss: 1.7988\n",
      "Epoch [1/5], Step [4134/10336], Loss: 0.3414\n",
      "Epoch [1/5], Step [4136/10336], Loss: 0.4119\n",
      "Epoch [1/5], Step [4138/10336], Loss: 0.7738\n",
      "Epoch [1/5], Step [4140/10336], Loss: 0.0348\n",
      "Epoch [1/5], Step [4142/10336], Loss: 0.1185\n",
      "Epoch [1/5], Step [4144/10336], Loss: 1.1105\n",
      "Epoch [1/5], Step [4146/10336], Loss: 3.1744\n",
      "Epoch [1/5], Step [4148/10336], Loss: 1.1784\n",
      "Epoch [1/5], Step [4150/10336], Loss: 0.7273\n",
      "Epoch [1/5], Step [4152/10336], Loss: 0.2039\n",
      "Epoch [1/5], Step [4154/10336], Loss: 4.1334\n",
      "Epoch [1/5], Step [4156/10336], Loss: 0.2745\n",
      "Epoch [1/5], Step [4158/10336], Loss: 0.2367\n",
      "Epoch [1/5], Step [4160/10336], Loss: 0.7871\n",
      "Epoch [1/5], Step [4162/10336], Loss: 0.4272\n",
      "Epoch [1/5], Step [4164/10336], Loss: 0.6727\n",
      "Epoch [1/5], Step [4166/10336], Loss: 0.4904\n",
      "Epoch [1/5], Step [4168/10336], Loss: 0.7304\n",
      "Epoch [1/5], Step [4170/10336], Loss: 1.5230\n",
      "Epoch [1/5], Step [4172/10336], Loss: 0.5384\n",
      "Epoch [1/5], Step [4174/10336], Loss: 0.4327\n",
      "Epoch [1/5], Step [4176/10336], Loss: 3.8970\n",
      "Epoch [1/5], Step [4178/10336], Loss: 0.7734\n",
      "Epoch [1/5], Step [4180/10336], Loss: 4.0226\n",
      "Epoch [1/5], Step [4182/10336], Loss: 0.6625\n",
      "Epoch [1/5], Step [4184/10336], Loss: 0.4063\n",
      "Epoch [1/5], Step [4186/10336], Loss: 1.0372\n",
      "Epoch [1/5], Step [4188/10336], Loss: 0.5707\n",
      "Epoch [1/5], Step [4190/10336], Loss: 0.2480\n",
      "Epoch [1/5], Step [4192/10336], Loss: 0.9619\n",
      "Epoch [1/5], Step [4194/10336], Loss: 0.1039\n",
      "Epoch [1/5], Step [4196/10336], Loss: 1.0620\n",
      "Epoch [1/5], Step [4198/10336], Loss: 0.5380\n",
      "Epoch [1/5], Step [4200/10336], Loss: 2.0170\n",
      "Epoch [1/5], Step [4202/10336], Loss: 0.2415\n",
      "Epoch [1/5], Step [4204/10336], Loss: 0.3912\n",
      "Epoch [1/5], Step [4206/10336], Loss: 3.4243\n",
      "Epoch [1/5], Step [4208/10336], Loss: 1.7773\n",
      "Epoch [1/5], Step [4210/10336], Loss: 1.4227\n",
      "Epoch [1/5], Step [4212/10336], Loss: 1.0072\n",
      "Epoch [1/5], Step [4214/10336], Loss: 1.2106\n",
      "Epoch [1/5], Step [4216/10336], Loss: 0.1082\n",
      "Epoch [1/5], Step [4218/10336], Loss: 1.2898\n",
      "Epoch [1/5], Step [4220/10336], Loss: 2.9356\n",
      "Epoch [1/5], Step [4222/10336], Loss: 0.4303\n",
      "Epoch [1/5], Step [4224/10336], Loss: 0.5682\n",
      "Epoch [1/5], Step [4226/10336], Loss: 4.4171\n",
      "Epoch [1/5], Step [4228/10336], Loss: 0.2271\n",
      "Epoch [1/5], Step [4230/10336], Loss: 4.1043\n",
      "Epoch [1/5], Step [4232/10336], Loss: 0.2277\n",
      "Epoch [1/5], Step [4234/10336], Loss: 0.2670\n",
      "Epoch [1/5], Step [4236/10336], Loss: 1.7823\n",
      "Epoch [1/5], Step [4238/10336], Loss: 1.9848\n",
      "Epoch [1/5], Step [4240/10336], Loss: 5.0354\n",
      "Epoch [1/5], Step [4242/10336], Loss: 0.3978\n",
      "Epoch [1/5], Step [4244/10336], Loss: 0.5948\n",
      "Epoch [1/5], Step [4246/10336], Loss: 0.4254\n",
      "Epoch [1/5], Step [4248/10336], Loss: 1.2117\n",
      "Epoch [1/5], Step [4250/10336], Loss: 0.0277\n",
      "Epoch [1/5], Step [4252/10336], Loss: 3.5309\n",
      "Epoch [1/5], Step [4254/10336], Loss: 3.6718\n",
      "Epoch [1/5], Step [4256/10336], Loss: 0.3371\n",
      "Epoch [1/5], Step [4258/10336], Loss: 0.7302\n",
      "Epoch [1/5], Step [4260/10336], Loss: 2.6457\n",
      "Epoch [1/5], Step [4262/10336], Loss: 0.8904\n",
      "Epoch [1/5], Step [4264/10336], Loss: 0.0573\n",
      "Epoch [1/5], Step [4266/10336], Loss: 4.5704\n",
      "Epoch [1/5], Step [4268/10336], Loss: 0.2300\n",
      "Epoch [1/5], Step [4270/10336], Loss: 0.0268\n",
      "Epoch [1/5], Step [4272/10336], Loss: 0.3247\n",
      "Epoch [1/5], Step [4274/10336], Loss: 0.9944\n",
      "Epoch [1/5], Step [4276/10336], Loss: 0.9521\n",
      "Epoch [1/5], Step [4278/10336], Loss: 0.1674\n",
      "Epoch [1/5], Step [4280/10336], Loss: 1.1814\n",
      "Epoch [1/5], Step [4282/10336], Loss: 6.0857\n",
      "Epoch [1/5], Step [4284/10336], Loss: 0.4782\n",
      "Epoch [1/5], Step [4286/10336], Loss: 0.0975\n",
      "Epoch [1/5], Step [4288/10336], Loss: 0.2614\n",
      "Epoch [1/5], Step [4290/10336], Loss: 0.0761\n",
      "Epoch [1/5], Step [4292/10336], Loss: 0.2040\n",
      "Epoch [1/5], Step [4294/10336], Loss: 0.1485\n",
      "Epoch [1/5], Step [4296/10336], Loss: 0.3332\n",
      "Epoch [1/5], Step [4298/10336], Loss: 0.8754\n",
      "Epoch [1/5], Step [4300/10336], Loss: 3.5735\n",
      "Epoch [1/5], Step [4302/10336], Loss: 1.7231\n",
      "Epoch [1/5], Step [4304/10336], Loss: 0.8521\n",
      "Epoch [1/5], Step [4306/10336], Loss: 4.3246\n",
      "Epoch [1/5], Step [4308/10336], Loss: 0.9191\n",
      "Epoch [1/5], Step [4310/10336], Loss: 0.1993\n",
      "Epoch [1/5], Step [4312/10336], Loss: 1.5392\n",
      "Epoch [1/5], Step [4314/10336], Loss: 1.1348\n",
      "Epoch [1/5], Step [4316/10336], Loss: 0.4847\n",
      "Epoch [1/5], Step [4318/10336], Loss: 1.0759\n",
      "Epoch [1/5], Step [4320/10336], Loss: 0.0941\n",
      "Epoch [1/5], Step [4322/10336], Loss: 5.5419\n",
      "Epoch [1/5], Step [4324/10336], Loss: 1.2829\n",
      "Epoch [1/5], Step [4326/10336], Loss: 0.4147\n",
      "Epoch [1/5], Step [4328/10336], Loss: 0.0108\n",
      "Epoch [1/5], Step [4330/10336], Loss: 0.2066\n",
      "Epoch [1/5], Step [4332/10336], Loss: 0.9359\n",
      "Epoch [1/5], Step [4334/10336], Loss: 0.5185\n",
      "Epoch [1/5], Step [4336/10336], Loss: 0.0588\n",
      "Epoch [1/5], Step [4338/10336], Loss: 0.9816\n",
      "Epoch [1/5], Step [4340/10336], Loss: 3.1167\n",
      "Epoch [1/5], Step [4342/10336], Loss: 3.6312\n",
      "Epoch [1/5], Step [4344/10336], Loss: 0.4210\n",
      "Epoch [1/5], Step [4346/10336], Loss: 3.0948\n",
      "Epoch [1/5], Step [4348/10336], Loss: 1.9107\n",
      "Epoch [1/5], Step [4350/10336], Loss: 0.6334\n",
      "Epoch [1/5], Step [4352/10336], Loss: 0.3491\n",
      "Epoch [1/5], Step [4354/10336], Loss: 1.9954\n",
      "Epoch [1/5], Step [4356/10336], Loss: 0.7520\n",
      "Epoch [1/5], Step [4358/10336], Loss: 2.5394\n",
      "Epoch [1/5], Step [4360/10336], Loss: 0.7282\n",
      "Epoch [1/5], Step [4362/10336], Loss: 2.9400\n",
      "Epoch [1/5], Step [4364/10336], Loss: 0.0198\n",
      "Epoch [1/5], Step [4366/10336], Loss: 2.6196\n",
      "Epoch [1/5], Step [4368/10336], Loss: 0.4875\n",
      "Epoch [1/5], Step [4370/10336], Loss: 2.6504\n",
      "Epoch [1/5], Step [4372/10336], Loss: 3.3009\n",
      "Epoch [1/5], Step [4374/10336], Loss: 4.1227\n",
      "Epoch [1/5], Step [4376/10336], Loss: 0.0860\n",
      "Epoch [1/5], Step [4378/10336], Loss: 0.2733\n",
      "Epoch [1/5], Step [4380/10336], Loss: 2.5264\n",
      "Epoch [1/5], Step [4382/10336], Loss: 1.4288\n",
      "Epoch [1/5], Step [4384/10336], Loss: 1.3608\n",
      "Epoch [1/5], Step [4386/10336], Loss: 0.6555\n",
      "Epoch [1/5], Step [4388/10336], Loss: 1.4726\n",
      "Epoch [1/5], Step [4390/10336], Loss: 1.6581\n",
      "Epoch [1/5], Step [4392/10336], Loss: 1.3501\n",
      "Epoch [1/5], Step [4394/10336], Loss: 0.9858\n",
      "Epoch [1/5], Step [4396/10336], Loss: 0.4897\n",
      "Epoch [1/5], Step [4398/10336], Loss: 0.6715\n",
      "Epoch [1/5], Step [4400/10336], Loss: 0.6733\n",
      "Epoch [1/5], Step [4402/10336], Loss: 0.4824\n",
      "Epoch [1/5], Step [4404/10336], Loss: 0.2160\n",
      "Epoch [1/5], Step [4406/10336], Loss: 0.1059\n",
      "Epoch [1/5], Step [4408/10336], Loss: 6.1285\n",
      "Epoch [1/5], Step [4410/10336], Loss: 0.4229\n",
      "Epoch [1/5], Step [4412/10336], Loss: 0.6957\n",
      "Epoch [1/5], Step [4414/10336], Loss: 0.0490\n",
      "Epoch [1/5], Step [4416/10336], Loss: 0.2768\n",
      "Epoch [1/5], Step [4418/10336], Loss: 0.8081\n",
      "Epoch [1/5], Step [4420/10336], Loss: 0.8507\n",
      "Epoch [1/5], Step [4422/10336], Loss: 0.9678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4424/10336], Loss: 2.4961\n",
      "Epoch [1/5], Step [4426/10336], Loss: 1.0151\n",
      "Epoch [1/5], Step [4428/10336], Loss: 1.1944\n",
      "Epoch [1/5], Step [4430/10336], Loss: 0.5541\n",
      "Epoch [1/5], Step [4432/10336], Loss: 0.8772\n",
      "Epoch [1/5], Step [4434/10336], Loss: 0.3254\n",
      "Epoch [1/5], Step [4436/10336], Loss: 0.3254\n",
      "Epoch [1/5], Step [4438/10336], Loss: 1.9654\n",
      "Epoch [1/5], Step [4440/10336], Loss: 0.4118\n",
      "Epoch [1/5], Step [4442/10336], Loss: 0.2545\n",
      "Epoch [1/5], Step [4444/10336], Loss: 2.1123\n",
      "Epoch [1/5], Step [4446/10336], Loss: 0.1388\n",
      "Epoch [1/5], Step [4448/10336], Loss: 0.4335\n",
      "Epoch [1/5], Step [4450/10336], Loss: 0.7051\n",
      "Epoch [1/5], Step [4452/10336], Loss: 2.0425\n",
      "Epoch [1/5], Step [4454/10336], Loss: 5.3640\n",
      "Epoch [1/5], Step [4456/10336], Loss: 0.2784\n",
      "Epoch [1/5], Step [4458/10336], Loss: 1.0227\n",
      "Epoch [1/5], Step [4460/10336], Loss: 2.2591\n",
      "Epoch [1/5], Step [4462/10336], Loss: 0.8876\n",
      "Epoch [1/5], Step [4464/10336], Loss: 5.8528\n",
      "Epoch [1/5], Step [4466/10336], Loss: 2.0420\n",
      "Epoch [1/5], Step [4468/10336], Loss: 4.3846\n",
      "Epoch [1/5], Step [4470/10336], Loss: 3.3335\n",
      "Epoch [1/5], Step [4472/10336], Loss: 4.1807\n",
      "Epoch [1/5], Step [4474/10336], Loss: 0.6403\n",
      "Epoch [1/5], Step [4476/10336], Loss: 3.9338\n",
      "Epoch [1/5], Step [4478/10336], Loss: 1.5491\n",
      "Epoch [1/5], Step [4480/10336], Loss: 1.2036\n",
      "Epoch [1/5], Step [4482/10336], Loss: 1.3687\n",
      "Epoch [1/5], Step [4484/10336], Loss: 0.4627\n",
      "Epoch [1/5], Step [4486/10336], Loss: 0.0632\n",
      "Epoch [1/5], Step [4488/10336], Loss: 1.7081\n",
      "Epoch [1/5], Step [4490/10336], Loss: 0.6708\n",
      "Epoch [1/5], Step [4492/10336], Loss: 0.8954\n",
      "Epoch [1/5], Step [4494/10336], Loss: 3.9281\n",
      "Epoch [1/5], Step [4496/10336], Loss: 0.3428\n",
      "Epoch [1/5], Step [4498/10336], Loss: 2.8335\n",
      "Epoch [1/5], Step [4500/10336], Loss: 0.0259\n",
      "Epoch [1/5], Step [4502/10336], Loss: 3.6589\n",
      "Epoch [1/5], Step [4504/10336], Loss: 0.7716\n",
      "Epoch [1/5], Step [4506/10336], Loss: 2.7360\n",
      "Epoch [1/5], Step [4508/10336], Loss: 0.7085\n",
      "Epoch [1/5], Step [4510/10336], Loss: 0.6332\n",
      "Epoch [1/5], Step [4512/10336], Loss: 0.8037\n",
      "Epoch [1/5], Step [4514/10336], Loss: 0.5478\n",
      "Epoch [1/5], Step [4516/10336], Loss: 0.6704\n",
      "Epoch [1/5], Step [4518/10336], Loss: 0.6884\n",
      "Epoch [1/5], Step [4520/10336], Loss: 0.3330\n",
      "Epoch [1/5], Step [4522/10336], Loss: 0.2652\n",
      "Epoch [1/5], Step [4524/10336], Loss: 0.4918\n",
      "Epoch [1/5], Step [4526/10336], Loss: 0.4235\n",
      "Epoch [1/5], Step [4528/10336], Loss: 1.9108\n",
      "Epoch [1/5], Step [4530/10336], Loss: 1.3601\n",
      "Epoch [1/5], Step [4532/10336], Loss: 4.7531\n",
      "Epoch [1/5], Step [4534/10336], Loss: 0.6564\n",
      "Epoch [1/5], Step [4536/10336], Loss: 0.4334\n",
      "Epoch [1/5], Step [4538/10336], Loss: 2.8385\n",
      "Epoch [1/5], Step [4540/10336], Loss: 1.6291\n",
      "Epoch [1/5], Step [4542/10336], Loss: 0.3378\n",
      "Epoch [1/5], Step [4544/10336], Loss: 0.6563\n",
      "Epoch [1/5], Step [4546/10336], Loss: 1.6865\n",
      "Epoch [1/5], Step [4548/10336], Loss: 0.3384\n",
      "Epoch [1/5], Step [4550/10336], Loss: 0.6089\n",
      "Epoch [1/5], Step [4552/10336], Loss: 1.8490\n",
      "Epoch [1/5], Step [4554/10336], Loss: 0.2426\n",
      "Epoch [1/5], Step [4556/10336], Loss: 0.2534\n",
      "Epoch [1/5], Step [4558/10336], Loss: 0.3952\n",
      "Epoch [1/5], Step [4560/10336], Loss: 1.8852\n",
      "Epoch [1/5], Step [4562/10336], Loss: 0.1849\n",
      "Epoch [1/5], Step [4564/10336], Loss: 0.2285\n",
      "Epoch [1/5], Step [4566/10336], Loss: 0.5462\n",
      "Epoch [1/5], Step [4568/10336], Loss: 0.3306\n",
      "Epoch [1/5], Step [4570/10336], Loss: 5.1107\n",
      "Epoch [1/5], Step [4572/10336], Loss: 0.3417\n",
      "Epoch [1/5], Step [4574/10336], Loss: 0.1358\n",
      "Epoch [1/5], Step [4576/10336], Loss: 1.6267\n",
      "Epoch [1/5], Step [4578/10336], Loss: 0.5411\n",
      "Epoch [1/5], Step [4580/10336], Loss: 0.6555\n",
      "Epoch [1/5], Step [4582/10336], Loss: 0.0232\n",
      "Epoch [1/5], Step [4584/10336], Loss: 2.6439\n",
      "Epoch [1/5], Step [4586/10336], Loss: 0.2158\n",
      "Epoch [1/5], Step [4588/10336], Loss: 0.7042\n",
      "Epoch [1/5], Step [4590/10336], Loss: 2.1995\n",
      "Epoch [1/5], Step [4592/10336], Loss: 3.7509\n",
      "Epoch [1/5], Step [4594/10336], Loss: 0.8201\n",
      "Epoch [1/5], Step [4596/10336], Loss: 0.4274\n",
      "Epoch [1/5], Step [4598/10336], Loss: 0.6485\n",
      "Epoch [1/5], Step [4600/10336], Loss: 2.0113\n",
      "Epoch [1/5], Step [4602/10336], Loss: 4.1076\n",
      "Epoch [1/5], Step [4604/10336], Loss: 0.0733\n",
      "Epoch [1/5], Step [4606/10336], Loss: 0.7704\n",
      "Epoch [1/5], Step [4608/10336], Loss: 0.3275\n",
      "Epoch [1/5], Step [4610/10336], Loss: 5.1594\n",
      "Epoch [1/5], Step [4612/10336], Loss: 0.6068\n",
      "Epoch [1/5], Step [4614/10336], Loss: 1.1645\n",
      "Epoch [1/5], Step [4616/10336], Loss: 4.5322\n",
      "Epoch [1/5], Step [4618/10336], Loss: 7.5398\n",
      "Epoch [1/5], Step [4620/10336], Loss: 0.5387\n",
      "Epoch [1/5], Step [4622/10336], Loss: 0.3559\n",
      "Epoch [1/5], Step [4624/10336], Loss: 0.2240\n",
      "Epoch [1/5], Step [4626/10336], Loss: 2.5002\n",
      "Epoch [1/5], Step [4628/10336], Loss: 0.5843\n",
      "Epoch [1/5], Step [4630/10336], Loss: 0.7286\n",
      "Epoch [1/5], Step [4632/10336], Loss: 0.6921\n",
      "Epoch [1/5], Step [4634/10336], Loss: 0.2914\n",
      "Epoch [1/5], Step [4636/10336], Loss: 0.7028\n",
      "Epoch [1/5], Step [4638/10336], Loss: 0.2775\n",
      "Epoch [1/5], Step [4640/10336], Loss: 0.9648\n",
      "Epoch [1/5], Step [4642/10336], Loss: 0.4564\n",
      "Epoch [1/5], Step [4644/10336], Loss: 2.7386\n",
      "Epoch [1/5], Step [4646/10336], Loss: 4.7435\n",
      "Epoch [1/5], Step [4648/10336], Loss: 0.3671\n",
      "Epoch [1/5], Step [4650/10336], Loss: 4.5612\n",
      "Epoch [1/5], Step [4652/10336], Loss: 0.1258\n",
      "Epoch [1/5], Step [4654/10336], Loss: 0.9636\n",
      "Epoch [1/5], Step [4656/10336], Loss: 0.8470\n",
      "Epoch [1/5], Step [4658/10336], Loss: 0.0636\n",
      "Epoch [1/5], Step [4660/10336], Loss: 0.1842\n",
      "Epoch [1/5], Step [4662/10336], Loss: 0.2163\n",
      "Epoch [1/5], Step [4664/10336], Loss: 1.4347\n",
      "Epoch [1/5], Step [4666/10336], Loss: 3.2789\n",
      "Epoch [1/5], Step [4668/10336], Loss: 2.9926\n",
      "Epoch [1/5], Step [4670/10336], Loss: 2.5907\n",
      "Epoch [1/5], Step [4672/10336], Loss: 0.2907\n",
      "Epoch [1/5], Step [4674/10336], Loss: 4.0530\n",
      "Epoch [1/5], Step [4676/10336], Loss: 4.2790\n",
      "Epoch [1/5], Step [4678/10336], Loss: 1.8863\n",
      "Epoch [1/5], Step [4680/10336], Loss: 3.7994\n",
      "Epoch [1/5], Step [4682/10336], Loss: 0.1612\n",
      "Epoch [1/5], Step [4684/10336], Loss: 1.1295\n",
      "Epoch [1/5], Step [4686/10336], Loss: 0.1240\n",
      "Epoch [1/5], Step [4688/10336], Loss: 2.8999\n",
      "Epoch [1/5], Step [4690/10336], Loss: 4.5172\n",
      "Epoch [1/5], Step [4692/10336], Loss: 2.9516\n",
      "Epoch [1/5], Step [4694/10336], Loss: 3.0277\n",
      "Epoch [1/5], Step [4696/10336], Loss: 0.3616\n",
      "Epoch [1/5], Step [4698/10336], Loss: 1.7450\n",
      "Epoch [1/5], Step [4700/10336], Loss: 0.4989\n",
      "Epoch [1/5], Step [4702/10336], Loss: 0.9964\n",
      "Epoch [1/5], Step [4704/10336], Loss: 0.8603\n",
      "Epoch [1/5], Step [4706/10336], Loss: 0.6604\n",
      "Epoch [1/5], Step [4708/10336], Loss: 0.2935\n",
      "Epoch [1/5], Step [4710/10336], Loss: 0.2347\n",
      "Epoch [1/5], Step [4712/10336], Loss: 0.4617\n",
      "Epoch [1/5], Step [4714/10336], Loss: 0.0254\n",
      "Epoch [1/5], Step [4716/10336], Loss: 0.9320\n",
      "Epoch [1/5], Step [4718/10336], Loss: 0.2348\n",
      "Epoch [1/5], Step [4720/10336], Loss: 0.4404\n",
      "Epoch [1/5], Step [4722/10336], Loss: 1.4869\n",
      "Epoch [1/5], Step [4724/10336], Loss: 3.6785\n",
      "Epoch [1/5], Step [4726/10336], Loss: 0.9850\n",
      "Epoch [1/5], Step [4728/10336], Loss: 1.2146\n",
      "Epoch [1/5], Step [4730/10336], Loss: 1.8791\n",
      "Epoch [1/5], Step [4732/10336], Loss: 2.9037\n",
      "Epoch [1/5], Step [4734/10336], Loss: 0.1372\n",
      "Epoch [1/5], Step [4736/10336], Loss: 2.5454\n",
      "Epoch [1/5], Step [4738/10336], Loss: 0.9941\n",
      "Epoch [1/5], Step [4740/10336], Loss: 0.2906\n",
      "Epoch [1/5], Step [4742/10336], Loss: 0.3293\n",
      "Epoch [1/5], Step [4744/10336], Loss: 0.4139\n",
      "Epoch [1/5], Step [4746/10336], Loss: 0.9859\n",
      "Epoch [1/5], Step [4748/10336], Loss: 2.3851\n",
      "Epoch [1/5], Step [4750/10336], Loss: 2.9428\n",
      "Epoch [1/5], Step [4752/10336], Loss: 4.1847\n",
      "Epoch [1/5], Step [4754/10336], Loss: 0.9291\n",
      "Epoch [1/5], Step [4756/10336], Loss: 3.7178\n",
      "Epoch [1/5], Step [4758/10336], Loss: 0.5289\n",
      "Epoch [1/5], Step [4760/10336], Loss: 0.1456\n",
      "Epoch [1/5], Step [4762/10336], Loss: 3.1947\n",
      "Epoch [1/5], Step [4764/10336], Loss: 0.6027\n",
      "Epoch [1/5], Step [4766/10336], Loss: 0.2375\n",
      "Epoch [1/5], Step [4768/10336], Loss: 0.8752\n",
      "Epoch [1/5], Step [4770/10336], Loss: 0.0746\n",
      "Epoch [1/5], Step [4772/10336], Loss: 0.4725\n",
      "Epoch [1/5], Step [4774/10336], Loss: 0.3193\n",
      "Epoch [1/5], Step [4776/10336], Loss: 0.6079\n",
      "Epoch [1/5], Step [4778/10336], Loss: 0.2651\n",
      "Epoch [1/5], Step [4780/10336], Loss: 0.5580\n",
      "Epoch [1/5], Step [4782/10336], Loss: 0.8749\n",
      "Epoch [1/5], Step [4784/10336], Loss: 0.0833\n",
      "Epoch [1/5], Step [4786/10336], Loss: 0.0466\n",
      "Epoch [1/5], Step [4788/10336], Loss: 0.2951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4790/10336], Loss: 0.7287\n",
      "Epoch [1/5], Step [4792/10336], Loss: 2.4720\n",
      "Epoch [1/5], Step [4794/10336], Loss: 0.6532\n",
      "Epoch [1/5], Step [4796/10336], Loss: 0.1287\n",
      "Epoch [1/5], Step [4798/10336], Loss: 0.9552\n",
      "Epoch [1/5], Step [4800/10336], Loss: 3.2519\n",
      "Epoch [1/5], Step [4802/10336], Loss: 1.1031\n",
      "Epoch [1/5], Step [4804/10336], Loss: 0.3337\n",
      "Epoch [1/5], Step [4806/10336], Loss: 0.5555\n",
      "Epoch [1/5], Step [4808/10336], Loss: 2.1835\n",
      "Epoch [1/5], Step [4810/10336], Loss: 1.8829\n",
      "Epoch [1/5], Step [4812/10336], Loss: 4.6591\n",
      "Epoch [1/5], Step [4814/10336], Loss: 0.3022\n",
      "Epoch [1/5], Step [4816/10336], Loss: 0.5750\n",
      "Epoch [1/5], Step [4818/10336], Loss: 4.4202\n",
      "Epoch [1/5], Step [4820/10336], Loss: 0.4571\n",
      "Epoch [1/5], Step [4822/10336], Loss: 1.7211\n",
      "Epoch [1/5], Step [4824/10336], Loss: 1.1422\n",
      "Epoch [1/5], Step [4826/10336], Loss: 2.1132\n",
      "Epoch [1/5], Step [4828/10336], Loss: 3.6681\n",
      "Epoch [1/5], Step [4830/10336], Loss: 0.7884\n",
      "Epoch [1/5], Step [4832/10336], Loss: 2.2072\n",
      "Epoch [1/5], Step [4834/10336], Loss: 2.2177\n",
      "Epoch [1/5], Step [4836/10336], Loss: 0.4519\n",
      "Epoch [1/5], Step [4838/10336], Loss: 1.7069\n",
      "Epoch [1/5], Step [4840/10336], Loss: 2.0813\n",
      "Epoch [1/5], Step [4842/10336], Loss: 0.3549\n",
      "Epoch [1/5], Step [4844/10336], Loss: 0.3549\n",
      "Epoch [1/5], Step [4846/10336], Loss: 0.0616\n",
      "Epoch [1/5], Step [4848/10336], Loss: 0.1709\n",
      "Epoch [1/5], Step [4850/10336], Loss: 0.4884\n",
      "Epoch [1/5], Step [4852/10336], Loss: 0.7162\n",
      "Epoch [1/5], Step [4854/10336], Loss: 0.6977\n",
      "Epoch [1/5], Step [4856/10336], Loss: 0.0298\n",
      "Epoch [1/5], Step [4858/10336], Loss: 3.2086\n",
      "Epoch [1/5], Step [4860/10336], Loss: 2.1368\n",
      "Epoch [1/5], Step [4862/10336], Loss: 0.3242\n",
      "Epoch [1/5], Step [4864/10336], Loss: 0.0665\n",
      "Epoch [1/5], Step [4866/10336], Loss: 3.9134\n",
      "Epoch [1/5], Step [4868/10336], Loss: 0.2438\n",
      "Epoch [1/5], Step [4870/10336], Loss: 1.0591\n",
      "Epoch [1/5], Step [4872/10336], Loss: 0.7365\n",
      "Epoch [1/5], Step [4874/10336], Loss: 0.1558\n",
      "Epoch [1/5], Step [4876/10336], Loss: 0.2858\n",
      "Epoch [1/5], Step [4878/10336], Loss: 0.6063\n",
      "Epoch [1/5], Step [4880/10336], Loss: 3.8082\n",
      "Epoch [1/5], Step [4882/10336], Loss: 2.5297\n",
      "Epoch [1/5], Step [4884/10336], Loss: 0.1004\n",
      "Epoch [1/5], Step [4886/10336], Loss: 1.4351\n",
      "Epoch [1/5], Step [4888/10336], Loss: 1.0346\n",
      "Epoch [1/5], Step [4890/10336], Loss: 4.0845\n",
      "Epoch [1/5], Step [4892/10336], Loss: 0.2025\n",
      "Epoch [1/5], Step [4894/10336], Loss: 0.8449\n",
      "Epoch [1/5], Step [4896/10336], Loss: 0.9909\n",
      "Epoch [1/5], Step [4898/10336], Loss: 1.0107\n",
      "Epoch [1/5], Step [4900/10336], Loss: 1.9282\n",
      "Epoch [1/5], Step [4902/10336], Loss: 2.4936\n",
      "Epoch [1/5], Step [4904/10336], Loss: 0.6539\n",
      "Epoch [1/5], Step [4906/10336], Loss: 0.9764\n",
      "Epoch [1/5], Step [4908/10336], Loss: 1.6794\n",
      "Epoch [1/5], Step [4910/10336], Loss: 2.5815\n",
      "Epoch [1/5], Step [4912/10336], Loss: 0.2918\n",
      "Epoch [1/5], Step [4914/10336], Loss: 0.3238\n",
      "Epoch [1/5], Step [4916/10336], Loss: 1.1235\n",
      "Epoch [1/5], Step [4918/10336], Loss: 0.2156\n",
      "Epoch [1/5], Step [4920/10336], Loss: 0.0601\n",
      "Epoch [1/5], Step [4922/10336], Loss: 0.2591\n",
      "Epoch [1/5], Step [4924/10336], Loss: 0.6019\n",
      "Epoch [1/5], Step [4926/10336], Loss: 0.9825\n",
      "Epoch [1/5], Step [4928/10336], Loss: 0.9465\n",
      "Epoch [1/5], Step [4930/10336], Loss: 1.1257\n",
      "Epoch [1/5], Step [4932/10336], Loss: 0.2466\n",
      "Epoch [1/5], Step [4934/10336], Loss: 0.8163\n",
      "Epoch [1/5], Step [4936/10336], Loss: 0.4917\n",
      "Epoch [1/5], Step [4938/10336], Loss: 4.1067\n",
      "Epoch [1/5], Step [4940/10336], Loss: 1.5537\n",
      "Epoch [1/5], Step [4942/10336], Loss: 0.4193\n",
      "Epoch [1/5], Step [4944/10336], Loss: 0.4614\n",
      "Epoch [1/5], Step [4946/10336], Loss: 2.7806\n",
      "Epoch [1/5], Step [4948/10336], Loss: 1.6046\n",
      "Epoch [1/5], Step [4950/10336], Loss: 0.3951\n",
      "Epoch [1/5], Step [4952/10336], Loss: 0.7370\n",
      "Epoch [1/5], Step [4954/10336], Loss: 0.5905\n",
      "Epoch [1/5], Step [4956/10336], Loss: 1.0100\n",
      "Epoch [1/5], Step [4958/10336], Loss: 2.1779\n",
      "Epoch [1/5], Step [4960/10336], Loss: 0.2002\n",
      "Epoch [1/5], Step [4962/10336], Loss: 1.0916\n",
      "Epoch [1/5], Step [4964/10336], Loss: 0.2915\n",
      "Epoch [1/5], Step [4966/10336], Loss: 0.7956\n",
      "Epoch [1/5], Step [4968/10336], Loss: 0.6713\n",
      "Epoch [1/5], Step [4970/10336], Loss: 1.0464\n",
      "Epoch [1/5], Step [4972/10336], Loss: 2.5897\n",
      "Epoch [1/5], Step [4974/10336], Loss: 0.4038\n",
      "Epoch [1/5], Step [4976/10336], Loss: 1.9053\n",
      "Epoch [1/5], Step [4978/10336], Loss: 0.4052\n",
      "Epoch [1/5], Step [4980/10336], Loss: 0.1217\n",
      "Epoch [1/5], Step [4982/10336], Loss: 4.5031\n",
      "Epoch [1/5], Step [4984/10336], Loss: 1.4137\n",
      "Epoch [1/5], Step [4986/10336], Loss: 1.3694\n",
      "Epoch [1/5], Step [4988/10336], Loss: 4.4272\n",
      "Epoch [1/5], Step [4990/10336], Loss: 3.7390\n",
      "Epoch [1/5], Step [4992/10336], Loss: 1.3373\n",
      "Epoch [1/5], Step [4994/10336], Loss: 0.3772\n",
      "Epoch [1/5], Step [4996/10336], Loss: 0.5147\n",
      "Epoch [1/5], Step [4998/10336], Loss: 0.7176\n",
      "Epoch [1/5], Step [5000/10336], Loss: 3.5869\n",
      "Epoch [1/5], Step [5002/10336], Loss: 1.9840\n",
      "Epoch [1/5], Step [5004/10336], Loss: 0.6197\n",
      "Epoch [1/5], Step [5006/10336], Loss: 3.5149\n",
      "Epoch [1/5], Step [5008/10336], Loss: 0.0298\n",
      "Epoch [1/5], Step [5010/10336], Loss: 1.3846\n",
      "Epoch [1/5], Step [5012/10336], Loss: 1.3939\n",
      "Epoch [1/5], Step [5014/10336], Loss: 0.5818\n",
      "Epoch [1/5], Step [5016/10336], Loss: 3.0011\n",
      "Epoch [1/5], Step [5018/10336], Loss: 1.0519\n",
      "Epoch [1/5], Step [5020/10336], Loss: 1.9508\n",
      "Epoch [1/5], Step [5022/10336], Loss: 0.3629\n",
      "Epoch [1/5], Step [5024/10336], Loss: 1.6036\n",
      "Epoch [1/5], Step [5026/10336], Loss: 0.2641\n",
      "Epoch [1/5], Step [5028/10336], Loss: 0.4116\n",
      "Epoch [1/5], Step [5030/10336], Loss: 3.6864\n",
      "Epoch [1/5], Step [5032/10336], Loss: 0.3833\n",
      "Epoch [1/5], Step [5034/10336], Loss: 0.3106\n",
      "Epoch [1/5], Step [5036/10336], Loss: 3.4528\n",
      "Epoch [1/5], Step [5038/10336], Loss: 0.2200\n",
      "Epoch [1/5], Step [5040/10336], Loss: 1.3783\n",
      "Epoch [1/5], Step [5042/10336], Loss: 0.2802\n",
      "Epoch [1/5], Step [5044/10336], Loss: 0.2386\n",
      "Epoch [1/5], Step [5046/10336], Loss: 0.3522\n",
      "Epoch [1/5], Step [5048/10336], Loss: 3.1302\n",
      "Epoch [1/5], Step [5050/10336], Loss: 1.9607\n",
      "Epoch [1/5], Step [5052/10336], Loss: 1.8656\n",
      "Epoch [1/5], Step [5054/10336], Loss: 3.1657\n",
      "Epoch [1/5], Step [5056/10336], Loss: 0.7904\n",
      "Epoch [1/5], Step [5058/10336], Loss: 0.6998\n",
      "Epoch [1/5], Step [5060/10336], Loss: 2.2459\n",
      "Epoch [1/5], Step [5062/10336], Loss: 1.2553\n",
      "Epoch [1/5], Step [5064/10336], Loss: 0.5255\n",
      "Epoch [1/5], Step [5066/10336], Loss: 0.1364\n",
      "Epoch [1/5], Step [5068/10336], Loss: 1.0617\n",
      "Epoch [1/5], Step [5070/10336], Loss: 0.0597\n",
      "Epoch [1/5], Step [5072/10336], Loss: 3.6629\n",
      "Epoch [1/5], Step [5074/10336], Loss: 0.3767\n",
      "Epoch [1/5], Step [5076/10336], Loss: 0.7468\n",
      "Epoch [1/5], Step [5078/10336], Loss: 0.5367\n",
      "Epoch [1/5], Step [5080/10336], Loss: 0.6512\n",
      "Epoch [1/5], Step [5082/10336], Loss: 1.0580\n",
      "Epoch [1/5], Step [5084/10336], Loss: 1.6406\n",
      "Epoch [1/5], Step [5086/10336], Loss: 1.7121\n",
      "Epoch [1/5], Step [5088/10336], Loss: 0.3176\n",
      "Epoch [1/5], Step [5090/10336], Loss: 1.0150\n",
      "Epoch [1/5], Step [5092/10336], Loss: 1.0654\n",
      "Epoch [1/5], Step [5094/10336], Loss: 1.3919\n",
      "Epoch [1/5], Step [5096/10336], Loss: 3.1115\n",
      "Epoch [1/5], Step [5098/10336], Loss: 0.0205\n",
      "Epoch [1/5], Step [5100/10336], Loss: 0.0565\n",
      "Epoch [1/5], Step [5102/10336], Loss: 0.0318\n",
      "Epoch [1/5], Step [5104/10336], Loss: 0.3832\n",
      "Epoch [1/5], Step [5106/10336], Loss: 0.4253\n",
      "Epoch [1/5], Step [5108/10336], Loss: 2.0950\n",
      "Epoch [1/5], Step [5110/10336], Loss: 0.5148\n",
      "Epoch [1/5], Step [5112/10336], Loss: 1.3702\n",
      "Epoch [1/5], Step [5114/10336], Loss: 0.2889\n",
      "Epoch [1/5], Step [5116/10336], Loss: 0.2720\n",
      "Epoch [1/5], Step [5118/10336], Loss: 1.4043\n",
      "Epoch [1/5], Step [5120/10336], Loss: 2.3012\n",
      "Epoch [1/5], Step [5122/10336], Loss: 4.0497\n",
      "Epoch [1/5], Step [5124/10336], Loss: 0.5505\n",
      "Epoch [1/5], Step [5126/10336], Loss: 2.8371\n",
      "Epoch [1/5], Step [5128/10336], Loss: 0.3628\n",
      "Epoch [1/5], Step [5130/10336], Loss: 0.3038\n",
      "Epoch [1/5], Step [5132/10336], Loss: 1.2780\n",
      "Epoch [1/5], Step [5134/10336], Loss: 1.0472\n",
      "Epoch [1/5], Step [5136/10336], Loss: 5.4414\n",
      "Epoch [1/5], Step [5138/10336], Loss: 0.2815\n",
      "Epoch [1/5], Step [5140/10336], Loss: 0.4485\n",
      "Epoch [1/5], Step [5142/10336], Loss: 0.8513\n",
      "Epoch [1/5], Step [5144/10336], Loss: 0.0514\n",
      "Epoch [1/5], Step [5146/10336], Loss: 0.5230\n",
      "Epoch [1/5], Step [5148/10336], Loss: 2.9166\n",
      "Epoch [1/5], Step [5150/10336], Loss: 0.3704\n",
      "Epoch [1/5], Step [5152/10336], Loss: 1.9536\n",
      "Epoch [1/5], Step [5154/10336], Loss: 0.3655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5156/10336], Loss: 0.2036\n",
      "Epoch [1/5], Step [5158/10336], Loss: 1.4247\n",
      "Epoch [1/5], Step [5160/10336], Loss: 0.0741\n",
      "Epoch [1/5], Step [5162/10336], Loss: 0.3582\n",
      "Epoch [1/5], Step [5164/10336], Loss: 0.2965\n",
      "Epoch [1/5], Step [5166/10336], Loss: 2.8684\n",
      "Epoch [1/5], Step [5168/10336], Loss: 3.7875\n",
      "Epoch [1/5], Step [5170/10336], Loss: 2.7417\n",
      "Epoch [1/5], Step [5172/10336], Loss: 1.4966\n",
      "Epoch [1/5], Step [5174/10336], Loss: 1.1902\n",
      "Epoch [1/5], Step [5176/10336], Loss: 0.6796\n",
      "Epoch [1/5], Step [5178/10336], Loss: 2.8824\n",
      "Epoch [1/5], Step [5180/10336], Loss: 0.3777\n",
      "Epoch [1/5], Step [5182/10336], Loss: 3.8629\n",
      "Epoch [1/5], Step [5184/10336], Loss: 1.1239\n",
      "Epoch [1/5], Step [5186/10336], Loss: 2.3751\n",
      "Epoch [1/5], Step [5188/10336], Loss: 1.2364\n",
      "Epoch [1/5], Step [5190/10336], Loss: 0.3085\n",
      "Epoch [1/5], Step [5192/10336], Loss: 0.9998\n",
      "Epoch [1/5], Step [5194/10336], Loss: 0.5520\n",
      "Epoch [1/5], Step [5196/10336], Loss: 1.1806\n",
      "Epoch [1/5], Step [5198/10336], Loss: 2.0935\n",
      "Epoch [1/5], Step [5200/10336], Loss: 0.3343\n",
      "Epoch [1/5], Step [5202/10336], Loss: 0.5877\n",
      "Epoch [1/5], Step [5204/10336], Loss: 0.4448\n",
      "Epoch [1/5], Step [5206/10336], Loss: 0.3742\n",
      "Epoch [1/5], Step [5208/10336], Loss: 0.6078\n",
      "Epoch [1/5], Step [5210/10336], Loss: 2.7563\n",
      "Epoch [1/5], Step [5212/10336], Loss: 3.7173\n",
      "Epoch [1/5], Step [5214/10336], Loss: 1.1048\n",
      "Epoch [1/5], Step [5216/10336], Loss: 1.0043\n",
      "Epoch [1/5], Step [5218/10336], Loss: 4.2156\n",
      "Epoch [1/5], Step [5220/10336], Loss: 0.0607\n",
      "Epoch [1/5], Step [5222/10336], Loss: 0.5752\n",
      "Epoch [1/5], Step [5224/10336], Loss: 0.3951\n",
      "Epoch [1/5], Step [5226/10336], Loss: 0.8313\n",
      "Epoch [1/5], Step [5228/10336], Loss: 0.3050\n",
      "Epoch [1/5], Step [5230/10336], Loss: 0.0977\n",
      "Epoch [1/5], Step [5232/10336], Loss: 0.2772\n",
      "Epoch [1/5], Step [5234/10336], Loss: 0.4079\n",
      "Epoch [1/5], Step [5236/10336], Loss: 2.8091\n",
      "Epoch [1/5], Step [5238/10336], Loss: 0.4119\n",
      "Epoch [1/5], Step [5240/10336], Loss: 0.4259\n",
      "Epoch [1/5], Step [5242/10336], Loss: 0.2883\n",
      "Epoch [1/5], Step [5244/10336], Loss: 0.7187\n",
      "Epoch [1/5], Step [5246/10336], Loss: 0.1604\n",
      "Epoch [1/5], Step [5248/10336], Loss: 0.3246\n",
      "Epoch [1/5], Step [5250/10336], Loss: 4.8405\n",
      "Epoch [1/5], Step [5252/10336], Loss: 2.0772\n",
      "Epoch [1/5], Step [5254/10336], Loss: 0.5086\n",
      "Epoch [1/5], Step [5256/10336], Loss: 0.6138\n",
      "Epoch [1/5], Step [5258/10336], Loss: 1.3809\n",
      "Epoch [1/5], Step [5260/10336], Loss: 0.2747\n",
      "Epoch [1/5], Step [5262/10336], Loss: 0.5994\n",
      "Epoch [1/5], Step [5264/10336], Loss: 0.6541\n",
      "Epoch [1/5], Step [5266/10336], Loss: 0.7435\n",
      "Epoch [1/5], Step [5268/10336], Loss: 0.7687\n",
      "Epoch [1/5], Step [5270/10336], Loss: 5.4019\n",
      "Epoch [1/5], Step [5272/10336], Loss: 0.8883\n",
      "Epoch [1/5], Step [5274/10336], Loss: 0.0650\n",
      "Epoch [1/5], Step [5276/10336], Loss: 0.3997\n",
      "Epoch [1/5], Step [5278/10336], Loss: 0.4028\n",
      "Epoch [1/5], Step [5280/10336], Loss: 0.1087\n",
      "Epoch [1/5], Step [5282/10336], Loss: 1.1109\n",
      "Epoch [1/5], Step [5284/10336], Loss: 0.0361\n",
      "Epoch [1/5], Step [5286/10336], Loss: 1.2942\n",
      "Epoch [1/5], Step [5288/10336], Loss: 2.1717\n",
      "Epoch [1/5], Step [5290/10336], Loss: 0.8365\n",
      "Epoch [1/5], Step [5292/10336], Loss: 3.4659\n",
      "Epoch [1/5], Step [5294/10336], Loss: 0.2384\n",
      "Epoch [1/5], Step [5296/10336], Loss: 1.3697\n",
      "Epoch [1/5], Step [5298/10336], Loss: 0.4570\n",
      "Epoch [1/5], Step [5300/10336], Loss: 2.1971\n",
      "Epoch [1/5], Step [5302/10336], Loss: 3.3095\n",
      "Epoch [1/5], Step [5304/10336], Loss: 0.6663\n",
      "Epoch [1/5], Step [5306/10336], Loss: 0.1179\n",
      "Epoch [1/5], Step [5308/10336], Loss: 0.1286\n",
      "Epoch [1/5], Step [5310/10336], Loss: 0.5132\n",
      "Epoch [1/5], Step [5312/10336], Loss: 2.3016\n",
      "Epoch [1/5], Step [5314/10336], Loss: 1.0669\n",
      "Epoch [1/5], Step [5316/10336], Loss: 1.1524\n",
      "Epoch [1/5], Step [5318/10336], Loss: 0.4205\n",
      "Epoch [1/5], Step [5320/10336], Loss: 3.2055\n",
      "Epoch [1/5], Step [5322/10336], Loss: 3.2577\n",
      "Epoch [1/5], Step [5324/10336], Loss: 1.0657\n",
      "Epoch [1/5], Step [5326/10336], Loss: 0.6450\n",
      "Epoch [1/5], Step [5328/10336], Loss: 0.9323\n",
      "Epoch [1/5], Step [5330/10336], Loss: 5.1093\n",
      "Epoch [1/5], Step [5332/10336], Loss: 4.7325\n",
      "Epoch [1/5], Step [5334/10336], Loss: 0.8175\n",
      "Epoch [1/5], Step [5336/10336], Loss: 1.0995\n",
      "Epoch [1/5], Step [5338/10336], Loss: 1.6560\n",
      "Epoch [1/5], Step [5340/10336], Loss: 4.3525\n",
      "Epoch [1/5], Step [5342/10336], Loss: 0.5918\n",
      "Epoch [1/5], Step [5344/10336], Loss: 2.2295\n",
      "Epoch [1/5], Step [5346/10336], Loss: 1.2273\n",
      "Epoch [1/5], Step [5348/10336], Loss: 1.7145\n",
      "Epoch [1/5], Step [5350/10336], Loss: 0.7929\n",
      "Epoch [1/5], Step [5352/10336], Loss: 1.9679\n",
      "Epoch [1/5], Step [5354/10336], Loss: 0.8684\n",
      "Epoch [1/5], Step [5356/10336], Loss: 0.9601\n",
      "Epoch [1/5], Step [5358/10336], Loss: 0.5374\n",
      "Epoch [1/5], Step [5360/10336], Loss: 0.2343\n",
      "Epoch [1/5], Step [5362/10336], Loss: 0.5119\n",
      "Epoch [1/5], Step [5364/10336], Loss: 0.5996\n",
      "Epoch [1/5], Step [5366/10336], Loss: 1.9318\n",
      "Epoch [1/5], Step [5368/10336], Loss: 0.9079\n",
      "Epoch [1/5], Step [5370/10336], Loss: 0.3529\n",
      "Epoch [1/5], Step [5372/10336], Loss: 2.2130\n",
      "Epoch [1/5], Step [5374/10336], Loss: 0.3040\n",
      "Epoch [1/5], Step [5376/10336], Loss: 0.3303\n",
      "Epoch [1/5], Step [5378/10336], Loss: 1.3230\n",
      "Epoch [1/5], Step [5380/10336], Loss: 0.0164\n",
      "Epoch [1/5], Step [5382/10336], Loss: 1.9306\n",
      "Epoch [1/5], Step [5384/10336], Loss: 0.2180\n",
      "Epoch [1/5], Step [5386/10336], Loss: 1.6125\n",
      "Epoch [1/5], Step [5388/10336], Loss: 4.8840\n",
      "Epoch [1/5], Step [5390/10336], Loss: 1.0445\n",
      "Epoch [1/5], Step [5392/10336], Loss: 2.6551\n",
      "Epoch [1/5], Step [5394/10336], Loss: 0.3541\n",
      "Epoch [1/5], Step [5396/10336], Loss: 0.4523\n",
      "Epoch [1/5], Step [5398/10336], Loss: 0.8521\n",
      "Epoch [1/5], Step [5400/10336], Loss: 0.3501\n",
      "Epoch [1/5], Step [5402/10336], Loss: 5.0129\n",
      "Epoch [1/5], Step [5404/10336], Loss: 0.2960\n",
      "Epoch [1/5], Step [5406/10336], Loss: 0.4745\n",
      "Epoch [1/5], Step [5408/10336], Loss: 5.1360\n",
      "Epoch [1/5], Step [5410/10336], Loss: 0.5417\n",
      "Epoch [1/5], Step [5412/10336], Loss: 0.6879\n",
      "Epoch [1/5], Step [5414/10336], Loss: 4.0333\n",
      "Epoch [1/5], Step [5416/10336], Loss: 0.3302\n",
      "Epoch [1/5], Step [5418/10336], Loss: 0.7640\n",
      "Epoch [1/5], Step [5420/10336], Loss: 4.5274\n",
      "Epoch [1/5], Step [5422/10336], Loss: 0.6373\n",
      "Epoch [1/5], Step [5424/10336], Loss: 0.7260\n",
      "Epoch [1/5], Step [5426/10336], Loss: 3.9900\n",
      "Epoch [1/5], Step [5428/10336], Loss: 0.1312\n",
      "Epoch [1/5], Step [5430/10336], Loss: 5.3306\n",
      "Epoch [1/5], Step [5432/10336], Loss: 0.4348\n",
      "Epoch [1/5], Step [5434/10336], Loss: 0.1250\n",
      "Epoch [1/5], Step [5436/10336], Loss: 0.3622\n",
      "Epoch [1/5], Step [5438/10336], Loss: 0.1543\n",
      "Epoch [1/5], Step [5440/10336], Loss: 0.9958\n",
      "Epoch [1/5], Step [5442/10336], Loss: 2.5063\n",
      "Epoch [1/5], Step [5444/10336], Loss: 1.3181\n",
      "Epoch [1/5], Step [5446/10336], Loss: 0.1842\n",
      "Epoch [1/5], Step [5448/10336], Loss: 1.9772\n",
      "Epoch [1/5], Step [5450/10336], Loss: 2.1859\n",
      "Epoch [1/5], Step [5452/10336], Loss: 1.3755\n",
      "Epoch [1/5], Step [5454/10336], Loss: 0.9942\n",
      "Epoch [1/5], Step [5456/10336], Loss: 3.9246\n",
      "Epoch [1/5], Step [5458/10336], Loss: 5.7805\n",
      "Epoch [1/5], Step [5460/10336], Loss: 0.3608\n",
      "Epoch [1/5], Step [5462/10336], Loss: 0.5981\n",
      "Epoch [1/5], Step [5464/10336], Loss: 1.1084\n",
      "Epoch [1/5], Step [5466/10336], Loss: 0.2020\n",
      "Epoch [1/5], Step [5468/10336], Loss: 0.2378\n",
      "Epoch [1/5], Step [5470/10336], Loss: 0.0125\n",
      "Epoch [1/5], Step [5472/10336], Loss: 5.0926\n",
      "Epoch [1/5], Step [5474/10336], Loss: 0.6458\n",
      "Epoch [1/5], Step [5476/10336], Loss: 0.1119\n",
      "Epoch [1/5], Step [5478/10336], Loss: 3.5740\n",
      "Epoch [1/5], Step [5480/10336], Loss: 0.0076\n",
      "Epoch [1/5], Step [5482/10336], Loss: 1.0009\n",
      "Epoch [1/5], Step [5484/10336], Loss: 4.3833\n",
      "Epoch [1/5], Step [5486/10336], Loss: 0.3617\n",
      "Epoch [1/5], Step [5488/10336], Loss: 1.2424\n",
      "Epoch [1/5], Step [5490/10336], Loss: 2.6117\n",
      "Epoch [1/5], Step [5492/10336], Loss: 0.7511\n",
      "Epoch [1/5], Step [5494/10336], Loss: 0.3036\n",
      "Epoch [1/5], Step [5496/10336], Loss: 2.5735\n",
      "Epoch [1/5], Step [5498/10336], Loss: 0.4991\n",
      "Epoch [1/5], Step [5500/10336], Loss: 2.1963\n",
      "Epoch [1/5], Step [5502/10336], Loss: 1.4672\n",
      "Epoch [1/5], Step [5504/10336], Loss: 0.9814\n",
      "Epoch [1/5], Step [5506/10336], Loss: 2.8685\n",
      "Epoch [1/5], Step [5508/10336], Loss: 0.1390\n",
      "Epoch [1/5], Step [5510/10336], Loss: 0.9737\n",
      "Epoch [1/5], Step [5512/10336], Loss: 0.2306\n",
      "Epoch [1/5], Step [5514/10336], Loss: 4.0451\n",
      "Epoch [1/5], Step [5516/10336], Loss: 0.0475\n",
      "Epoch [1/5], Step [5518/10336], Loss: 0.2721\n",
      "Epoch [1/5], Step [5520/10336], Loss: 1.4233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5522/10336], Loss: 0.3057\n",
      "Epoch [1/5], Step [5524/10336], Loss: 0.4187\n",
      "Epoch [1/5], Step [5526/10336], Loss: 0.6815\n",
      "Epoch [1/5], Step [5528/10336], Loss: 0.7254\n",
      "Epoch [1/5], Step [5530/10336], Loss: 3.4541\n",
      "Epoch [1/5], Step [5532/10336], Loss: 0.6401\n",
      "Epoch [1/5], Step [5534/10336], Loss: 1.3417\n",
      "Epoch [1/5], Step [5536/10336], Loss: 1.6747\n",
      "Epoch [1/5], Step [5538/10336], Loss: 0.2144\n",
      "Epoch [1/5], Step [5540/10336], Loss: 1.2855\n",
      "Epoch [1/5], Step [5542/10336], Loss: 1.4081\n",
      "Epoch [1/5], Step [5544/10336], Loss: 0.3485\n",
      "Epoch [1/5], Step [5546/10336], Loss: 5.6405\n",
      "Epoch [1/5], Step [5548/10336], Loss: 0.0826\n",
      "Epoch [1/5], Step [5550/10336], Loss: 6.3682\n",
      "Epoch [1/5], Step [5552/10336], Loss: 0.4350\n",
      "Epoch [1/5], Step [5554/10336], Loss: 4.8865\n",
      "Epoch [1/5], Step [5556/10336], Loss: 0.1875\n",
      "Epoch [1/5], Step [5558/10336], Loss: 0.4715\n",
      "Epoch [1/5], Step [5560/10336], Loss: 1.0033\n",
      "Epoch [1/5], Step [5562/10336], Loss: 3.9456\n",
      "Epoch [1/5], Step [5564/10336], Loss: 0.5159\n",
      "Epoch [1/5], Step [5566/10336], Loss: 4.7146\n",
      "Epoch [1/5], Step [5568/10336], Loss: 0.6648\n",
      "Epoch [1/5], Step [5570/10336], Loss: 0.3231\n",
      "Epoch [1/5], Step [5572/10336], Loss: 0.4785\n",
      "Epoch [1/5], Step [5574/10336], Loss: 4.1396\n",
      "Epoch [1/5], Step [5576/10336], Loss: 0.2614\n",
      "Epoch [1/5], Step [5578/10336], Loss: 1.2775\n",
      "Epoch [1/5], Step [5580/10336], Loss: 0.2316\n",
      "Epoch [1/5], Step [5582/10336], Loss: 4.4215\n",
      "Epoch [1/5], Step [5584/10336], Loss: 1.3456\n",
      "Epoch [1/5], Step [5586/10336], Loss: 3.5358\n",
      "Epoch [1/5], Step [5588/10336], Loss: 1.1883\n",
      "Epoch [1/5], Step [5590/10336], Loss: 3.4684\n",
      "Epoch [1/5], Step [5592/10336], Loss: 0.5803\n",
      "Epoch [1/5], Step [5594/10336], Loss: 0.8265\n",
      "Epoch [1/5], Step [5596/10336], Loss: 0.3200\n",
      "Epoch [1/5], Step [5598/10336], Loss: 0.3786\n",
      "Epoch [1/5], Step [5600/10336], Loss: 2.4600\n",
      "Epoch [1/5], Step [5602/10336], Loss: 0.6375\n",
      "Epoch [1/5], Step [5604/10336], Loss: 4.0557\n",
      "Epoch [1/5], Step [5606/10336], Loss: 0.4980\n",
      "Epoch [1/5], Step [5608/10336], Loss: 1.6698\n",
      "Epoch [1/5], Step [5610/10336], Loss: 0.5669\n",
      "Epoch [1/5], Step [5612/10336], Loss: 0.4063\n",
      "Epoch [1/5], Step [5614/10336], Loss: 0.0244\n",
      "Epoch [1/5], Step [5616/10336], Loss: 0.3310\n",
      "Epoch [1/5], Step [5618/10336], Loss: 1.4332\n",
      "Epoch [1/5], Step [5620/10336], Loss: 0.1576\n",
      "Epoch [1/5], Step [5622/10336], Loss: 0.5647\n",
      "Epoch [1/5], Step [5624/10336], Loss: 0.2200\n",
      "Epoch [1/5], Step [5626/10336], Loss: 0.3994\n",
      "Epoch [1/5], Step [5628/10336], Loss: 0.1583\n",
      "Epoch [1/5], Step [5630/10336], Loss: 5.2993\n",
      "Epoch [1/5], Step [5632/10336], Loss: 1.2432\n",
      "Epoch [1/5], Step [5634/10336], Loss: 0.6428\n",
      "Epoch [1/5], Step [5636/10336], Loss: 1.8116\n",
      "Epoch [1/5], Step [5638/10336], Loss: 0.4944\n",
      "Epoch [1/5], Step [5640/10336], Loss: 0.0042\n",
      "Epoch [1/5], Step [5642/10336], Loss: 5.8111\n",
      "Epoch [1/5], Step [5644/10336], Loss: 0.2917\n",
      "Epoch [1/5], Step [5646/10336], Loss: 0.2738\n",
      "Epoch [1/5], Step [5648/10336], Loss: 0.0066\n",
      "Epoch [1/5], Step [5650/10336], Loss: 0.4009\n",
      "Epoch [1/5], Step [5652/10336], Loss: 0.6339\n",
      "Epoch [1/5], Step [5654/10336], Loss: 0.5450\n",
      "Epoch [1/5], Step [5656/10336], Loss: 0.5480\n",
      "Epoch [1/5], Step [5658/10336], Loss: 1.0653\n",
      "Epoch [1/5], Step [5660/10336], Loss: 0.2479\n",
      "Epoch [1/5], Step [5662/10336], Loss: 3.5705\n",
      "Epoch [1/5], Step [5664/10336], Loss: 2.7333\n",
      "Epoch [1/5], Step [5666/10336], Loss: 0.3232\n",
      "Epoch [1/5], Step [5668/10336], Loss: 0.6814\n",
      "Epoch [1/5], Step [5670/10336], Loss: 2.6484\n",
      "Epoch [1/5], Step [5672/10336], Loss: 0.0889\n",
      "Epoch [1/5], Step [5674/10336], Loss: 4.4767\n",
      "Epoch [1/5], Step [5676/10336], Loss: 4.6321\n",
      "Epoch [1/5], Step [5678/10336], Loss: 1.3815\n",
      "Epoch [1/5], Step [5680/10336], Loss: 0.8829\n",
      "Epoch [1/5], Step [5682/10336], Loss: 0.7817\n",
      "Epoch [1/5], Step [5684/10336], Loss: 0.0152\n",
      "Epoch [1/5], Step [5686/10336], Loss: 0.6516\n",
      "Epoch [1/5], Step [5688/10336], Loss: 3.4859\n",
      "Epoch [1/5], Step [5690/10336], Loss: 0.1192\n",
      "Epoch [1/5], Step [5692/10336], Loss: 0.1827\n",
      "Epoch [1/5], Step [5694/10336], Loss: 0.2486\n",
      "Epoch [1/5], Step [5696/10336], Loss: 0.7060\n",
      "Epoch [1/5], Step [5698/10336], Loss: 1.3800\n",
      "Epoch [1/5], Step [5700/10336], Loss: 0.7024\n",
      "Epoch [1/5], Step [5702/10336], Loss: 0.7980\n",
      "Epoch [1/5], Step [5704/10336], Loss: 3.4603\n",
      "Epoch [1/5], Step [5706/10336], Loss: 0.8775\n",
      "Epoch [1/5], Step [5708/10336], Loss: 0.7833\n",
      "Epoch [1/5], Step [5710/10336], Loss: 1.1695\n",
      "Epoch [1/5], Step [5712/10336], Loss: 1.4176\n",
      "Epoch [1/5], Step [5714/10336], Loss: 0.1182\n",
      "Epoch [1/5], Step [5716/10336], Loss: 0.3263\n",
      "Epoch [1/5], Step [5718/10336], Loss: 0.2757\n",
      "Epoch [1/5], Step [5720/10336], Loss: 2.1173\n",
      "Epoch [1/5], Step [5722/10336], Loss: 0.3451\n",
      "Epoch [1/5], Step [5724/10336], Loss: 3.9859\n",
      "Epoch [1/5], Step [5726/10336], Loss: 0.9198\n",
      "Epoch [1/5], Step [5728/10336], Loss: 0.8400\n",
      "Epoch [1/5], Step [5730/10336], Loss: 0.3534\n",
      "Epoch [1/5], Step [5732/10336], Loss: 0.1509\n",
      "Epoch [1/5], Step [5734/10336], Loss: 0.1213\n",
      "Epoch [1/5], Step [5736/10336], Loss: 2.9055\n",
      "Epoch [1/5], Step [5738/10336], Loss: 0.1560\n",
      "Epoch [1/5], Step [5740/10336], Loss: 1.0055\n",
      "Epoch [1/5], Step [5742/10336], Loss: 0.2376\n",
      "Epoch [1/5], Step [5744/10336], Loss: 0.7514\n",
      "Epoch [1/5], Step [5746/10336], Loss: 0.5368\n",
      "Epoch [1/5], Step [5748/10336], Loss: 0.4798\n",
      "Epoch [1/5], Step [5750/10336], Loss: 0.8751\n",
      "Epoch [1/5], Step [5752/10336], Loss: 0.9327\n",
      "Epoch [1/5], Step [5754/10336], Loss: 0.5121\n",
      "Epoch [1/5], Step [5756/10336], Loss: 1.4006\n",
      "Epoch [1/5], Step [5758/10336], Loss: 0.0402\n",
      "Epoch [1/5], Step [5760/10336], Loss: 4.9659\n",
      "Epoch [1/5], Step [5762/10336], Loss: 2.8795\n",
      "Epoch [1/5], Step [5764/10336], Loss: 1.2835\n",
      "Epoch [1/5], Step [5766/10336], Loss: 1.7189\n",
      "Epoch [1/5], Step [5768/10336], Loss: 0.6544\n",
      "Epoch [1/5], Step [5770/10336], Loss: 0.3077\n",
      "Epoch [1/5], Step [5772/10336], Loss: 0.5992\n",
      "Epoch [1/5], Step [5774/10336], Loss: 0.3316\n",
      "Epoch [1/5], Step [5776/10336], Loss: 0.1953\n",
      "Epoch [1/5], Step [5778/10336], Loss: 0.3613\n",
      "Epoch [1/5], Step [5780/10336], Loss: 0.3966\n",
      "Epoch [1/5], Step [5782/10336], Loss: 0.4498\n",
      "Epoch [1/5], Step [5784/10336], Loss: 0.2552\n",
      "Epoch [1/5], Step [5786/10336], Loss: 0.2309\n",
      "Epoch [1/5], Step [5788/10336], Loss: 0.3183\n",
      "Epoch [1/5], Step [5790/10336], Loss: 6.1136\n",
      "Epoch [1/5], Step [5792/10336], Loss: 0.7907\n",
      "Epoch [1/5], Step [5794/10336], Loss: 1.2263\n",
      "Epoch [1/5], Step [5796/10336], Loss: 3.1379\n",
      "Epoch [1/5], Step [5798/10336], Loss: 0.2015\n",
      "Epoch [1/5], Step [5800/10336], Loss: 4.5353\n",
      "Epoch [1/5], Step [5802/10336], Loss: 0.6524\n",
      "Epoch [1/5], Step [5804/10336], Loss: 1.6841\n",
      "Epoch [1/5], Step [5806/10336], Loss: 2.2941\n",
      "Epoch [1/5], Step [5808/10336], Loss: 0.3265\n",
      "Epoch [1/5], Step [5810/10336], Loss: 0.1739\n",
      "Epoch [1/5], Step [5812/10336], Loss: 4.0835\n",
      "Epoch [1/5], Step [5814/10336], Loss: 0.2268\n",
      "Epoch [1/5], Step [5816/10336], Loss: 0.3803\n",
      "Epoch [1/5], Step [5818/10336], Loss: 1.0398\n",
      "Epoch [1/5], Step [5820/10336], Loss: 1.9057\n",
      "Epoch [1/5], Step [5822/10336], Loss: 0.4389\n",
      "Epoch [1/5], Step [5824/10336], Loss: 1.8067\n",
      "Epoch [1/5], Step [5826/10336], Loss: 2.6968\n",
      "Epoch [1/5], Step [5828/10336], Loss: 3.7960\n",
      "Epoch [1/5], Step [5830/10336], Loss: 0.4133\n",
      "Epoch [1/5], Step [5832/10336], Loss: 0.9933\n",
      "Epoch [1/5], Step [5834/10336], Loss: 0.2418\n",
      "Epoch [1/5], Step [5836/10336], Loss: 0.3305\n",
      "Epoch [1/5], Step [5838/10336], Loss: 2.8985\n",
      "Epoch [1/5], Step [5840/10336], Loss: 1.0836\n",
      "Epoch [1/5], Step [5842/10336], Loss: 0.0731\n",
      "Epoch [1/5], Step [5844/10336], Loss: 0.1987\n",
      "Epoch [1/5], Step [5846/10336], Loss: 0.1473\n",
      "Epoch [1/5], Step [5848/10336], Loss: 0.4892\n",
      "Epoch [1/5], Step [5850/10336], Loss: 5.2742\n",
      "Epoch [1/5], Step [5852/10336], Loss: 2.5325\n",
      "Epoch [1/5], Step [5854/10336], Loss: 5.7236\n",
      "Epoch [1/5], Step [5856/10336], Loss: 5.6588\n",
      "Epoch [1/5], Step [5858/10336], Loss: 1.2516\n",
      "Epoch [1/5], Step [5860/10336], Loss: 0.3356\n",
      "Epoch [1/5], Step [5862/10336], Loss: 2.8135\n",
      "Epoch [1/5], Step [5864/10336], Loss: 2.1605\n",
      "Epoch [1/5], Step [5866/10336], Loss: 0.3444\n",
      "Epoch [1/5], Step [5868/10336], Loss: 1.5380\n",
      "Epoch [1/5], Step [5870/10336], Loss: 3.1169\n",
      "Epoch [1/5], Step [5872/10336], Loss: 0.1006\n",
      "Epoch [1/5], Step [5874/10336], Loss: 3.0077\n",
      "Epoch [1/5], Step [5876/10336], Loss: 0.5007\n",
      "Epoch [1/5], Step [5878/10336], Loss: 0.4371\n",
      "Epoch [1/5], Step [5880/10336], Loss: 0.3032\n",
      "Epoch [1/5], Step [5882/10336], Loss: 0.2519\n",
      "Epoch [1/5], Step [5884/10336], Loss: 0.6125\n",
      "Epoch [1/5], Step [5886/10336], Loss: 2.6255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5888/10336], Loss: 0.6706\n",
      "Epoch [1/5], Step [5890/10336], Loss: 0.5129\n",
      "Epoch [1/5], Step [5892/10336], Loss: 1.1219\n",
      "Epoch [1/5], Step [5894/10336], Loss: 0.5244\n",
      "Epoch [1/5], Step [5896/10336], Loss: 0.1148\n",
      "Epoch [1/5], Step [5898/10336], Loss: 3.8793\n",
      "Epoch [1/5], Step [5900/10336], Loss: 5.9558\n",
      "Epoch [1/5], Step [5902/10336], Loss: 2.4027\n",
      "Epoch [1/5], Step [5904/10336], Loss: 0.8822\n",
      "Epoch [1/5], Step [5906/10336], Loss: 0.3822\n",
      "Epoch [1/5], Step [5908/10336], Loss: 0.1290\n",
      "Epoch [1/5], Step [5910/10336], Loss: 0.4139\n",
      "Epoch [1/5], Step [5912/10336], Loss: 1.2532\n",
      "Epoch [1/5], Step [5914/10336], Loss: 0.3797\n",
      "Epoch [1/5], Step [5916/10336], Loss: 0.4343\n",
      "Epoch [1/5], Step [5918/10336], Loss: 0.3171\n",
      "Epoch [1/5], Step [5920/10336], Loss: 0.4695\n",
      "Epoch [1/5], Step [5922/10336], Loss: 1.7833\n",
      "Epoch [1/5], Step [5924/10336], Loss: 5.7248\n",
      "Epoch [1/5], Step [5926/10336], Loss: 0.3747\n",
      "Epoch [1/5], Step [5928/10336], Loss: 1.7614\n",
      "Epoch [1/5], Step [5930/10336], Loss: 1.8654\n",
      "Epoch [1/5], Step [5932/10336], Loss: 0.1935\n",
      "Epoch [1/5], Step [5934/10336], Loss: 0.8296\n",
      "Epoch [1/5], Step [5936/10336], Loss: 0.0104\n",
      "Epoch [1/5], Step [5938/10336], Loss: 0.9134\n",
      "Epoch [1/5], Step [5940/10336], Loss: 0.2044\n",
      "Epoch [1/5], Step [5942/10336], Loss: 1.0553\n",
      "Epoch [1/5], Step [5944/10336], Loss: 3.3609\n",
      "Epoch [1/5], Step [5946/10336], Loss: 0.5718\n",
      "Epoch [1/5], Step [5948/10336], Loss: 0.4874\n",
      "Epoch [1/5], Step [5950/10336], Loss: 0.7793\n",
      "Epoch [1/5], Step [5952/10336], Loss: 0.7183\n",
      "Epoch [1/5], Step [5954/10336], Loss: 0.2355\n",
      "Epoch [1/5], Step [5956/10336], Loss: 3.4002\n",
      "Epoch [1/5], Step [5958/10336], Loss: 0.1456\n",
      "Epoch [1/5], Step [5960/10336], Loss: 1.2727\n",
      "Epoch [1/5], Step [5962/10336], Loss: 0.1552\n",
      "Epoch [1/5], Step [5964/10336], Loss: 0.7401\n",
      "Epoch [1/5], Step [5966/10336], Loss: 1.2313\n",
      "Epoch [1/5], Step [5968/10336], Loss: 1.5443\n",
      "Epoch [1/5], Step [5970/10336], Loss: 1.2319\n",
      "Epoch [1/5], Step [5972/10336], Loss: 4.9074\n",
      "Epoch [1/5], Step [5974/10336], Loss: 0.3794\n",
      "Epoch [1/5], Step [5976/10336], Loss: 0.0605\n",
      "Epoch [1/5], Step [5978/10336], Loss: 0.4047\n",
      "Epoch [1/5], Step [5980/10336], Loss: 0.7922\n",
      "Epoch [1/5], Step [5982/10336], Loss: 0.5085\n",
      "Epoch [1/5], Step [5984/10336], Loss: 1.1273\n",
      "Epoch [1/5], Step [5986/10336], Loss: 0.1946\n",
      "Epoch [1/5], Step [5988/10336], Loss: 2.9817\n",
      "Epoch [1/5], Step [5990/10336], Loss: 0.5900\n",
      "Epoch [1/5], Step [5992/10336], Loss: 0.3642\n",
      "Epoch [1/5], Step [5994/10336], Loss: 0.4458\n",
      "Epoch [1/5], Step [5996/10336], Loss: 0.7655\n",
      "Epoch [1/5], Step [5998/10336], Loss: 0.0791\n",
      "Epoch [1/5], Step [6000/10336], Loss: 3.3841\n",
      "Epoch [1/5], Step [6002/10336], Loss: 4.4720\n",
      "Epoch [1/5], Step [6004/10336], Loss: 0.1691\n",
      "Epoch [1/5], Step [6006/10336], Loss: 0.3396\n",
      "Epoch [1/5], Step [6008/10336], Loss: 1.4675\n",
      "Epoch [1/5], Step [6010/10336], Loss: 4.0210\n",
      "Epoch [1/5], Step [6012/10336], Loss: 0.2966\n",
      "Epoch [1/5], Step [6014/10336], Loss: 0.6011\n",
      "Epoch [1/5], Step [6016/10336], Loss: 0.9314\n",
      "Epoch [1/5], Step [6018/10336], Loss: 3.1258\n",
      "Epoch [1/5], Step [6020/10336], Loss: 1.5244\n",
      "Epoch [1/5], Step [6022/10336], Loss: 0.3265\n",
      "Epoch [1/5], Step [6024/10336], Loss: 0.5591\n",
      "Epoch [1/5], Step [6026/10336], Loss: 1.0304\n",
      "Epoch [1/5], Step [6028/10336], Loss: 2.6598\n",
      "Epoch [1/5], Step [6030/10336], Loss: 0.5150\n",
      "Epoch [1/5], Step [6032/10336], Loss: 2.9862\n",
      "Epoch [1/5], Step [6034/10336], Loss: 0.2342\n",
      "Epoch [1/5], Step [6036/10336], Loss: 0.3790\n",
      "Epoch [1/5], Step [6038/10336], Loss: 2.4539\n",
      "Epoch [1/5], Step [6040/10336], Loss: 0.6666\n",
      "Epoch [1/5], Step [6042/10336], Loss: 6.5821\n",
      "Epoch [1/5], Step [6044/10336], Loss: 0.2410\n",
      "Epoch [1/5], Step [6046/10336], Loss: 0.1176\n",
      "Epoch [1/5], Step [6048/10336], Loss: 3.2761\n",
      "Epoch [1/5], Step [6050/10336], Loss: 0.2710\n",
      "Epoch [1/5], Step [6052/10336], Loss: 1.0215\n",
      "Epoch [1/5], Step [6054/10336], Loss: 0.3876\n",
      "Epoch [1/5], Step [6056/10336], Loss: 0.4573\n",
      "Epoch [1/5], Step [6058/10336], Loss: 4.0088\n",
      "Epoch [1/5], Step [6060/10336], Loss: 0.9784\n",
      "Epoch [1/5], Step [6062/10336], Loss: 0.3974\n",
      "Epoch [1/5], Step [6064/10336], Loss: 2.3921\n",
      "Epoch [1/5], Step [6066/10336], Loss: 0.6709\n",
      "Epoch [1/5], Step [6068/10336], Loss: 0.0240\n",
      "Epoch [1/5], Step [6070/10336], Loss: 1.1772\n",
      "Epoch [1/5], Step [6072/10336], Loss: 0.3133\n",
      "Epoch [1/5], Step [6074/10336], Loss: 1.4136\n",
      "Epoch [1/5], Step [6076/10336], Loss: 1.3211\n",
      "Epoch [1/5], Step [6078/10336], Loss: 0.0236\n",
      "Epoch [1/5], Step [6080/10336], Loss: 4.5039\n",
      "Epoch [1/5], Step [6082/10336], Loss: 0.7350\n",
      "Epoch [1/5], Step [6084/10336], Loss: 0.8753\n",
      "Epoch [1/5], Step [6086/10336], Loss: 0.3730\n",
      "Epoch [1/5], Step [6088/10336], Loss: 1.1196\n",
      "Epoch [1/5], Step [6090/10336], Loss: 0.0724\n",
      "Epoch [1/5], Step [6092/10336], Loss: 3.7919\n",
      "Epoch [1/5], Step [6094/10336], Loss: 0.9104\n",
      "Epoch [1/5], Step [6096/10336], Loss: 0.3058\n",
      "Epoch [1/5], Step [6098/10336], Loss: 0.2773\n",
      "Epoch [1/5], Step [6100/10336], Loss: 3.3099\n",
      "Epoch [1/5], Step [6102/10336], Loss: 0.2040\n",
      "Epoch [1/5], Step [6104/10336], Loss: 0.8674\n",
      "Epoch [1/5], Step [6106/10336], Loss: 0.2799\n",
      "Epoch [1/5], Step [6108/10336], Loss: 0.3565\n",
      "Epoch [1/5], Step [6110/10336], Loss: 3.4949\n",
      "Epoch [1/5], Step [6112/10336], Loss: 0.2099\n",
      "Epoch [1/5], Step [6114/10336], Loss: 0.0791\n",
      "Epoch [1/5], Step [6116/10336], Loss: 0.0647\n",
      "Epoch [1/5], Step [6118/10336], Loss: 3.0420\n",
      "Epoch [1/5], Step [6120/10336], Loss: 0.5254\n",
      "Epoch [1/5], Step [6122/10336], Loss: 0.1423\n",
      "Epoch [1/5], Step [6124/10336], Loss: 0.0623\n",
      "Epoch [1/5], Step [6126/10336], Loss: 0.4329\n",
      "Epoch [1/5], Step [6128/10336], Loss: 0.7876\n",
      "Epoch [1/5], Step [6130/10336], Loss: 4.0936\n",
      "Epoch [1/5], Step [6132/10336], Loss: 0.3180\n",
      "Epoch [1/5], Step [6134/10336], Loss: 1.2871\n",
      "Epoch [1/5], Step [6136/10336], Loss: 0.2440\n",
      "Epoch [1/5], Step [6138/10336], Loss: 0.4247\n",
      "Epoch [1/5], Step [6140/10336], Loss: 1.5901\n",
      "Epoch [1/5], Step [6142/10336], Loss: 3.8159\n",
      "Epoch [1/5], Step [6144/10336], Loss: 3.5591\n",
      "Epoch [1/5], Step [6146/10336], Loss: 0.1256\n",
      "Epoch [1/5], Step [6148/10336], Loss: 0.3641\n",
      "Epoch [1/5], Step [6150/10336], Loss: 0.8194\n",
      "Epoch [1/5], Step [6152/10336], Loss: 0.2516\n",
      "Epoch [1/5], Step [6154/10336], Loss: 0.4210\n",
      "Epoch [1/5], Step [6156/10336], Loss: 0.2434\n",
      "Epoch [1/5], Step [6158/10336], Loss: 0.4803\n",
      "Epoch [1/5], Step [6160/10336], Loss: 2.8509\n",
      "Epoch [1/5], Step [6162/10336], Loss: 5.7998\n",
      "Epoch [1/5], Step [6164/10336], Loss: 0.2633\n",
      "Epoch [1/5], Step [6166/10336], Loss: 0.8191\n",
      "Epoch [1/5], Step [6168/10336], Loss: 0.4104\n",
      "Epoch [1/5], Step [6170/10336], Loss: 0.5907\n",
      "Epoch [1/5], Step [6172/10336], Loss: 0.3892\n",
      "Epoch [1/5], Step [6174/10336], Loss: 3.6360\n",
      "Epoch [1/5], Step [6176/10336], Loss: 4.2358\n",
      "Epoch [1/5], Step [6178/10336], Loss: 4.3947\n",
      "Epoch [1/5], Step [6180/10336], Loss: 0.1910\n",
      "Epoch [1/5], Step [6182/10336], Loss: 0.6894\n",
      "Epoch [1/5], Step [6184/10336], Loss: 0.3962\n",
      "Epoch [1/5], Step [6186/10336], Loss: 0.1264\n",
      "Epoch [1/5], Step [6188/10336], Loss: 0.4114\n",
      "Epoch [1/5], Step [6190/10336], Loss: 0.6316\n",
      "Epoch [1/5], Step [6192/10336], Loss: 1.7895\n",
      "Epoch [1/5], Step [6194/10336], Loss: 0.4502\n",
      "Epoch [1/5], Step [6196/10336], Loss: 0.7279\n",
      "Epoch [1/5], Step [6198/10336], Loss: 1.9899\n",
      "Epoch [1/5], Step [6200/10336], Loss: 0.5654\n",
      "Epoch [1/5], Step [6202/10336], Loss: 4.6547\n",
      "Epoch [1/5], Step [6204/10336], Loss: 0.4092\n",
      "Epoch [1/5], Step [6206/10336], Loss: 0.2371\n",
      "Epoch [1/5], Step [6208/10336], Loss: 0.4837\n",
      "Epoch [1/5], Step [6210/10336], Loss: 0.1756\n",
      "Epoch [1/5], Step [6212/10336], Loss: 3.3523\n",
      "Epoch [1/5], Step [6214/10336], Loss: 0.7355\n",
      "Epoch [1/5], Step [6216/10336], Loss: 0.6474\n",
      "Epoch [1/5], Step [6218/10336], Loss: 0.2898\n",
      "Epoch [1/5], Step [6220/10336], Loss: 2.2872\n",
      "Epoch [1/5], Step [6222/10336], Loss: 0.8346\n",
      "Epoch [1/5], Step [6224/10336], Loss: 0.0953\n",
      "Epoch [1/5], Step [6226/10336], Loss: 0.3360\n",
      "Epoch [1/5], Step [6228/10336], Loss: 0.9790\n",
      "Epoch [1/5], Step [6230/10336], Loss: 0.0468\n",
      "Epoch [1/5], Step [6232/10336], Loss: 2.3117\n",
      "Epoch [1/5], Step [6234/10336], Loss: 0.5973\n",
      "Epoch [1/5], Step [6236/10336], Loss: 0.7492\n",
      "Epoch [1/5], Step [6238/10336], Loss: 0.4578\n",
      "Epoch [1/5], Step [6240/10336], Loss: 1.0228\n",
      "Epoch [1/5], Step [6242/10336], Loss: 0.2056\n",
      "Epoch [1/5], Step [6244/10336], Loss: 5.7465\n",
      "Epoch [1/5], Step [6246/10336], Loss: 0.4855\n",
      "Epoch [1/5], Step [6248/10336], Loss: 0.9025\n",
      "Epoch [1/5], Step [6250/10336], Loss: 0.3617\n",
      "Epoch [1/5], Step [6252/10336], Loss: 4.4986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [6254/10336], Loss: 0.6924\n",
      "Epoch [1/5], Step [6256/10336], Loss: 0.5837\n",
      "Epoch [1/5], Step [6258/10336], Loss: 0.1483\n",
      "Epoch [1/5], Step [6260/10336], Loss: 3.3899\n",
      "Epoch [1/5], Step [6262/10336], Loss: 3.1436\n",
      "Epoch [1/5], Step [6264/10336], Loss: 0.5860\n",
      "Epoch [1/5], Step [6266/10336], Loss: 0.4969\n",
      "Epoch [1/5], Step [6268/10336], Loss: 0.0406\n",
      "Epoch [1/5], Step [6270/10336], Loss: 0.6734\n",
      "Epoch [1/5], Step [6272/10336], Loss: 0.3580\n",
      "Epoch [1/5], Step [6274/10336], Loss: 0.6946\n",
      "Epoch [1/5], Step [6276/10336], Loss: 1.2177\n",
      "Epoch [1/5], Step [6278/10336], Loss: 0.3920\n",
      "Epoch [1/5], Step [6280/10336], Loss: 1.1142\n",
      "Epoch [1/5], Step [6282/10336], Loss: 0.5893\n",
      "Epoch [1/5], Step [6284/10336], Loss: 0.3842\n",
      "Epoch [1/5], Step [6286/10336], Loss: 0.3808\n",
      "Epoch [1/5], Step [6288/10336], Loss: 0.4087\n",
      "Epoch [1/5], Step [6290/10336], Loss: 3.4966\n",
      "Epoch [1/5], Step [6292/10336], Loss: 5.0903\n",
      "Epoch [1/5], Step [6294/10336], Loss: 3.2436\n",
      "Epoch [1/5], Step [6296/10336], Loss: 0.1661\n",
      "Epoch [1/5], Step [6298/10336], Loss: 2.4474\n",
      "Epoch [1/5], Step [6300/10336], Loss: 0.6313\n",
      "Epoch [1/5], Step [6302/10336], Loss: 1.7831\n",
      "Epoch [1/5], Step [6304/10336], Loss: 2.0326\n",
      "Epoch [1/5], Step [6306/10336], Loss: 1.7115\n",
      "Epoch [1/5], Step [6308/10336], Loss: 0.7989\n",
      "Epoch [1/5], Step [6310/10336], Loss: 1.8461\n",
      "Epoch [1/5], Step [6312/10336], Loss: 0.2373\n",
      "Epoch [1/5], Step [6314/10336], Loss: 4.0830\n",
      "Epoch [1/5], Step [6316/10336], Loss: 0.2399\n",
      "Epoch [1/5], Step [6318/10336], Loss: 0.3152\n",
      "Epoch [1/5], Step [6320/10336], Loss: 0.2953\n",
      "Epoch [1/5], Step [6322/10336], Loss: 1.0643\n",
      "Epoch [1/5], Step [6324/10336], Loss: 0.2606\n",
      "Epoch [1/5], Step [6326/10336], Loss: 0.5015\n",
      "Epoch [1/5], Step [6328/10336], Loss: 3.6734\n",
      "Epoch [1/5], Step [6330/10336], Loss: 4.1842\n",
      "Epoch [1/5], Step [6332/10336], Loss: 1.4484\n",
      "Epoch [1/5], Step [6334/10336], Loss: 0.2429\n",
      "Epoch [1/5], Step [6336/10336], Loss: 2.3828\n",
      "Epoch [1/5], Step [6338/10336], Loss: 0.4879\n",
      "Epoch [1/5], Step [6340/10336], Loss: 0.6321\n",
      "Epoch [1/5], Step [6342/10336], Loss: 0.5043\n",
      "Epoch [1/5], Step [6344/10336], Loss: 0.0442\n",
      "Epoch [1/5], Step [6346/10336], Loss: 0.7375\n",
      "Epoch [1/5], Step [6348/10336], Loss: 0.0118\n",
      "Epoch [1/5], Step [6350/10336], Loss: 3.8028\n",
      "Epoch [1/5], Step [6352/10336], Loss: 2.2604\n",
      "Epoch [1/5], Step [6354/10336], Loss: 0.2864\n",
      "Epoch [1/5], Step [6356/10336], Loss: 0.3349\n",
      "Epoch [1/5], Step [6358/10336], Loss: 0.7231\n",
      "Epoch [1/5], Step [6360/10336], Loss: 0.6092\n",
      "Epoch [1/5], Step [6362/10336], Loss: 2.0669\n",
      "Epoch [1/5], Step [6364/10336], Loss: 4.4686\n",
      "Epoch [1/5], Step [6366/10336], Loss: 0.2994\n",
      "Epoch [1/5], Step [6368/10336], Loss: 1.1920\n",
      "Epoch [1/5], Step [6370/10336], Loss: 0.3075\n",
      "Epoch [1/5], Step [6372/10336], Loss: 0.5585\n",
      "Epoch [1/5], Step [6374/10336], Loss: 1.3326\n",
      "Epoch [1/5], Step [6376/10336], Loss: 0.2526\n",
      "Epoch [1/5], Step [6378/10336], Loss: 0.0558\n",
      "Epoch [1/5], Step [6380/10336], Loss: 2.2826\n",
      "Epoch [1/5], Step [6382/10336], Loss: 4.4501\n",
      "Epoch [1/5], Step [6384/10336], Loss: 0.7275\n",
      "Epoch [1/5], Step [6386/10336], Loss: 0.3446\n",
      "Epoch [1/5], Step [6388/10336], Loss: 1.7857\n",
      "Epoch [1/5], Step [6390/10336], Loss: 0.2916\n",
      "Epoch [1/5], Step [6392/10336], Loss: 3.1123\n",
      "Epoch [1/5], Step [6394/10336], Loss: 3.7223\n",
      "Epoch [1/5], Step [6396/10336], Loss: 3.9687\n",
      "Epoch [1/5], Step [6398/10336], Loss: 0.5653\n",
      "Epoch [1/5], Step [6400/10336], Loss: 0.5520\n",
      "Epoch [1/5], Step [6402/10336], Loss: 1.3248\n",
      "Epoch [1/5], Step [6404/10336], Loss: 3.4333\n",
      "Epoch [1/5], Step [6406/10336], Loss: 0.3815\n",
      "Epoch [1/5], Step [6408/10336], Loss: 1.6509\n",
      "Epoch [1/5], Step [6410/10336], Loss: 1.5532\n",
      "Epoch [1/5], Step [6412/10336], Loss: 0.2352\n",
      "Epoch [1/5], Step [6414/10336], Loss: 0.2724\n",
      "Epoch [1/5], Step [6416/10336], Loss: 0.8556\n",
      "Epoch [1/5], Step [6418/10336], Loss: 0.3680\n",
      "Epoch [1/5], Step [6420/10336], Loss: 0.3245\n",
      "Epoch [1/5], Step [6422/10336], Loss: 0.1535\n",
      "Epoch [1/5], Step [6424/10336], Loss: 3.3746\n",
      "Epoch [1/5], Step [6426/10336], Loss: 1.2692\n",
      "Epoch [1/5], Step [6428/10336], Loss: 0.4815\n",
      "Epoch [1/5], Step [6430/10336], Loss: 0.6791\n",
      "Epoch [1/5], Step [6432/10336], Loss: 0.4307\n",
      "Epoch [1/5], Step [6434/10336], Loss: 0.2801\n",
      "Epoch [1/5], Step [6436/10336], Loss: 0.9740\n",
      "Epoch [1/5], Step [6438/10336], Loss: 0.9739\n",
      "Epoch [1/5], Step [6440/10336], Loss: 0.4868\n",
      "Epoch [1/5], Step [6442/10336], Loss: 0.5273\n",
      "Epoch [1/5], Step [6444/10336], Loss: 0.3376\n",
      "Epoch [1/5], Step [6446/10336], Loss: 0.8704\n",
      "Epoch [1/5], Step [6448/10336], Loss: 0.7813\n",
      "Epoch [1/5], Step [6450/10336], Loss: 0.0334\n",
      "Epoch [1/5], Step [6452/10336], Loss: 0.3822\n",
      "Epoch [1/5], Step [6454/10336], Loss: 3.7835\n",
      "Epoch [1/5], Step [6456/10336], Loss: 3.4324\n",
      "Epoch [1/5], Step [6458/10336], Loss: 0.3164\n",
      "Epoch [1/5], Step [6460/10336], Loss: 0.8277\n",
      "Epoch [1/5], Step [6462/10336], Loss: 0.3671\n",
      "Epoch [1/5], Step [6464/10336], Loss: 1.2914\n",
      "Epoch [1/5], Step [6466/10336], Loss: 0.5984\n",
      "Epoch [1/5], Step [6468/10336], Loss: 0.2668\n",
      "Epoch [1/5], Step [6470/10336], Loss: 0.8111\n",
      "Epoch [1/5], Step [6472/10336], Loss: 0.2705\n",
      "Epoch [1/5], Step [6474/10336], Loss: 0.2512\n",
      "Epoch [1/5], Step [6476/10336], Loss: 4.8282\n",
      "Epoch [1/5], Step [6478/10336], Loss: 2.4228\n",
      "Epoch [1/5], Step [6480/10336], Loss: 0.5639\n",
      "Epoch [1/5], Step [6482/10336], Loss: 0.5289\n",
      "Epoch [1/5], Step [6484/10336], Loss: 0.0757\n",
      "Epoch [1/5], Step [6486/10336], Loss: 0.5026\n",
      "Epoch [1/5], Step [6488/10336], Loss: 1.1574\n",
      "Epoch [1/5], Step [6490/10336], Loss: 0.4081\n",
      "Epoch [1/5], Step [6492/10336], Loss: 3.0877\n",
      "Epoch [1/5], Step [6494/10336], Loss: 0.8774\n",
      "Epoch [1/5], Step [6496/10336], Loss: 0.4077\n",
      "Epoch [1/5], Step [6498/10336], Loss: 1.8918\n",
      "Epoch [1/5], Step [6500/10336], Loss: 0.1429\n",
      "Epoch [1/5], Step [6502/10336], Loss: 3.3317\n",
      "Epoch [1/5], Step [6504/10336], Loss: 0.0183\n",
      "Epoch [1/5], Step [6506/10336], Loss: 0.6832\n",
      "Epoch [1/5], Step [6508/10336], Loss: 4.7167\n",
      "Epoch [1/5], Step [6510/10336], Loss: 0.2136\n",
      "Epoch [1/5], Step [6512/10336], Loss: 0.5265\n",
      "Epoch [1/5], Step [6514/10336], Loss: 0.1495\n",
      "Epoch [1/5], Step [6516/10336], Loss: 0.2665\n",
      "Epoch [1/5], Step [6518/10336], Loss: 0.0607\n",
      "Epoch [1/5], Step [6520/10336], Loss: 3.9408\n",
      "Epoch [1/5], Step [6522/10336], Loss: 1.0278\n",
      "Epoch [1/5], Step [6524/10336], Loss: 0.2733\n",
      "Epoch [1/5], Step [6526/10336], Loss: 3.0203\n",
      "Epoch [1/5], Step [6528/10336], Loss: 1.9305\n",
      "Epoch [1/5], Step [6530/10336], Loss: 2.6390\n",
      "Epoch [1/5], Step [6532/10336], Loss: 0.1852\n",
      "Epoch [1/5], Step [6534/10336], Loss: 2.3521\n",
      "Epoch [1/5], Step [6536/10336], Loss: 0.1829\n",
      "Epoch [1/5], Step [6538/10336], Loss: 0.0186\n",
      "Epoch [1/5], Step [6540/10336], Loss: 0.0061\n",
      "Epoch [1/5], Step [6542/10336], Loss: 0.4711\n",
      "Epoch [1/5], Step [6544/10336], Loss: 3.2625\n",
      "Epoch [1/5], Step [6546/10336], Loss: 3.1503\n",
      "Epoch [1/5], Step [6548/10336], Loss: 0.0214\n",
      "Epoch [1/5], Step [6550/10336], Loss: 0.7731\n",
      "Epoch [1/5], Step [6552/10336], Loss: 0.4157\n",
      "Epoch [1/5], Step [6554/10336], Loss: 1.1675\n",
      "Epoch [1/5], Step [6556/10336], Loss: 0.4645\n",
      "Epoch [1/5], Step [6558/10336], Loss: 5.2875\n",
      "Epoch [1/5], Step [6560/10336], Loss: 4.7833\n",
      "Epoch [1/5], Step [6562/10336], Loss: 0.9639\n",
      "Epoch [1/5], Step [6564/10336], Loss: 0.4491\n",
      "Epoch [1/5], Step [6566/10336], Loss: 1.0527\n",
      "Epoch [1/5], Step [6568/10336], Loss: 2.1355\n",
      "Epoch [1/5], Step [6570/10336], Loss: 3.0944\n",
      "Epoch [1/5], Step [6572/10336], Loss: 0.1391\n",
      "Epoch [1/5], Step [6574/10336], Loss: 0.4363\n",
      "Epoch [1/5], Step [6576/10336], Loss: 0.8437\n",
      "Epoch [1/5], Step [6578/10336], Loss: 0.9815\n",
      "Epoch [1/5], Step [6580/10336], Loss: 7.1418\n",
      "Epoch [1/5], Step [6582/10336], Loss: 1.1495\n",
      "Epoch [1/5], Step [6584/10336], Loss: 2.9514\n",
      "Epoch [1/5], Step [6586/10336], Loss: 3.2373\n",
      "Epoch [1/5], Step [6588/10336], Loss: 3.4439\n",
      "Epoch [1/5], Step [6590/10336], Loss: 0.3523\n",
      "Epoch [1/5], Step [6592/10336], Loss: 1.2133\n",
      "Epoch [1/5], Step [6594/10336], Loss: 1.0064\n",
      "Epoch [1/5], Step [6596/10336], Loss: 0.3437\n",
      "Epoch [1/5], Step [6598/10336], Loss: 0.0475\n",
      "Epoch [1/5], Step [6600/10336], Loss: 0.4510\n",
      "Epoch [1/5], Step [6602/10336], Loss: 3.7193\n",
      "Epoch [1/5], Step [6604/10336], Loss: 2.5727\n",
      "Epoch [1/5], Step [6606/10336], Loss: 3.6037\n",
      "Epoch [1/5], Step [6608/10336], Loss: 0.1993\n",
      "Epoch [1/5], Step [6610/10336], Loss: 0.5965\n",
      "Epoch [1/5], Step [6612/10336], Loss: 0.4693\n",
      "Epoch [1/5], Step [6614/10336], Loss: 0.9100\n",
      "Epoch [1/5], Step [6616/10336], Loss: 1.4751\n",
      "Epoch [1/5], Step [6618/10336], Loss: 0.5868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [6620/10336], Loss: 0.7731\n",
      "Epoch [1/5], Step [6622/10336], Loss: 0.0327\n",
      "Epoch [1/5], Step [6624/10336], Loss: 2.1753\n",
      "Epoch [1/5], Step [6626/10336], Loss: 4.5032\n",
      "Epoch [1/5], Step [6628/10336], Loss: 2.5931\n",
      "Epoch [1/5], Step [6630/10336], Loss: 2.9952\n",
      "Epoch [1/5], Step [6632/10336], Loss: 3.7504\n",
      "Epoch [1/5], Step [6634/10336], Loss: 1.4897\n",
      "Epoch [1/5], Step [6636/10336], Loss: 2.1893\n",
      "Epoch [1/5], Step [6638/10336], Loss: 0.6369\n",
      "Epoch [1/5], Step [6640/10336], Loss: 0.9914\n",
      "Epoch [1/5], Step [6642/10336], Loss: 3.6786\n",
      "Epoch [1/5], Step [6644/10336], Loss: 0.2621\n",
      "Epoch [1/5], Step [6646/10336], Loss: 1.7617\n",
      "Epoch [1/5], Step [6648/10336], Loss: 2.6595\n",
      "Epoch [1/5], Step [6650/10336], Loss: 0.3096\n",
      "Epoch [1/5], Step [6652/10336], Loss: 0.2945\n",
      "Epoch [1/5], Step [6654/10336], Loss: 0.4888\n",
      "Epoch [1/5], Step [6656/10336], Loss: 0.2133\n",
      "Epoch [1/5], Step [6658/10336], Loss: 1.0550\n",
      "Epoch [1/5], Step [6660/10336], Loss: 1.0745\n",
      "Epoch [1/5], Step [6662/10336], Loss: 0.3286\n",
      "Epoch [1/5], Step [6664/10336], Loss: 0.0281\n",
      "Epoch [1/5], Step [6666/10336], Loss: 6.8092\n",
      "Epoch [1/5], Step [6668/10336], Loss: 0.5840\n",
      "Epoch [1/5], Step [6670/10336], Loss: 0.7986\n",
      "Epoch [1/5], Step [6672/10336], Loss: 0.8786\n",
      "Epoch [1/5], Step [6674/10336], Loss: 4.5173\n",
      "Epoch [1/5], Step [6676/10336], Loss: 0.8624\n",
      "Epoch [1/5], Step [6678/10336], Loss: 0.0886\n",
      "Epoch [1/5], Step [6680/10336], Loss: 5.1631\n",
      "Epoch [1/5], Step [6682/10336], Loss: 0.1889\n",
      "Epoch [1/5], Step [6684/10336], Loss: 0.0332\n",
      "Epoch [1/5], Step [6686/10336], Loss: 0.7489\n",
      "Epoch [1/5], Step [6688/10336], Loss: 0.0259\n",
      "Epoch [1/5], Step [6690/10336], Loss: 1.4603\n",
      "Epoch [1/5], Step [6692/10336], Loss: 0.0737\n",
      "Epoch [1/5], Step [6694/10336], Loss: 4.1229\n",
      "Epoch [1/5], Step [6696/10336], Loss: 0.1046\n",
      "Epoch [1/5], Step [6698/10336], Loss: 0.4250\n",
      "Epoch [1/5], Step [6700/10336], Loss: 0.3121\n",
      "Epoch [1/5], Step [6702/10336], Loss: 0.5149\n",
      "Epoch [1/5], Step [6704/10336], Loss: 0.8321\n",
      "Epoch [1/5], Step [6706/10336], Loss: 0.2663\n",
      "Epoch [1/5], Step [6708/10336], Loss: 3.7478\n",
      "Epoch [1/5], Step [6710/10336], Loss: 4.6994\n",
      "Epoch [1/5], Step [6712/10336], Loss: 3.9923\n",
      "Epoch [1/5], Step [6714/10336], Loss: 1.0659\n",
      "Epoch [1/5], Step [6716/10336], Loss: 1.7372\n",
      "Epoch [1/5], Step [6718/10336], Loss: 0.2481\n",
      "Epoch [1/5], Step [6720/10336], Loss: 0.5984\n",
      "Epoch [1/5], Step [6722/10336], Loss: 0.1263\n",
      "Epoch [1/5], Step [6724/10336], Loss: 1.0777\n",
      "Epoch [1/5], Step [6726/10336], Loss: 0.3055\n",
      "Epoch [1/5], Step [6728/10336], Loss: 0.3993\n",
      "Epoch [1/5], Step [6730/10336], Loss: 0.3150\n",
      "Epoch [1/5], Step [6732/10336], Loss: 1.2755\n",
      "Epoch [1/5], Step [6734/10336], Loss: 1.0464\n",
      "Epoch [1/5], Step [6736/10336], Loss: 0.0313\n",
      "Epoch [1/5], Step [6738/10336], Loss: 0.0106\n",
      "Epoch [1/5], Step [6740/10336], Loss: 4.9556\n",
      "Epoch [1/5], Step [6742/10336], Loss: 2.0608\n",
      "Epoch [1/5], Step [6744/10336], Loss: 0.0694\n",
      "Epoch [1/5], Step [6746/10336], Loss: 2.2052\n",
      "Epoch [1/5], Step [6748/10336], Loss: 0.0973\n",
      "Epoch [1/5], Step [6750/10336], Loss: 5.7756\n",
      "Epoch [1/5], Step [6752/10336], Loss: 1.0261\n",
      "Epoch [1/5], Step [6754/10336], Loss: 0.5616\n",
      "Epoch [1/5], Step [6756/10336], Loss: 2.1982\n",
      "Epoch [1/5], Step [6758/10336], Loss: 0.9242\n",
      "Epoch [1/5], Step [6760/10336], Loss: 0.0297\n",
      "Epoch [1/5], Step [6762/10336], Loss: 0.5908\n",
      "Epoch [1/5], Step [6764/10336], Loss: 2.0254\n",
      "Epoch [1/5], Step [6766/10336], Loss: 0.2791\n",
      "Epoch [1/5], Step [6768/10336], Loss: 0.0471\n",
      "Epoch [1/5], Step [6770/10336], Loss: 0.2230\n",
      "Epoch [1/5], Step [6772/10336], Loss: 0.0490\n",
      "Epoch [1/5], Step [6774/10336], Loss: 1.0493\n",
      "Epoch [1/5], Step [6776/10336], Loss: 3.1332\n",
      "Epoch [1/5], Step [6778/10336], Loss: 2.3751\n",
      "Epoch [1/5], Step [6780/10336], Loss: 0.2749\n",
      "Epoch [1/5], Step [6782/10336], Loss: 4.0030\n",
      "Epoch [1/5], Step [6784/10336], Loss: 0.1425\n",
      "Epoch [1/5], Step [6786/10336], Loss: 0.4910\n",
      "Epoch [1/5], Step [6788/10336], Loss: 0.3518\n",
      "Epoch [1/5], Step [6790/10336], Loss: 0.6020\n",
      "Epoch [1/5], Step [6792/10336], Loss: 0.5027\n",
      "Epoch [1/5], Step [6794/10336], Loss: 0.8706\n",
      "Epoch [1/5], Step [6796/10336], Loss: 0.3724\n",
      "Epoch [1/5], Step [6798/10336], Loss: 0.2439\n",
      "Epoch [1/5], Step [6800/10336], Loss: 0.3643\n",
      "Epoch [1/5], Step [6802/10336], Loss: 0.2877\n",
      "Epoch [1/5], Step [6804/10336], Loss: 1.0892\n",
      "Epoch [1/5], Step [6806/10336], Loss: 0.2015\n",
      "Epoch [1/5], Step [6808/10336], Loss: 0.3413\n",
      "Epoch [1/5], Step [6810/10336], Loss: 0.2723\n",
      "Epoch [1/5], Step [6812/10336], Loss: 0.2484\n",
      "Epoch [1/5], Step [6814/10336], Loss: 3.2442\n",
      "Epoch [1/5], Step [6816/10336], Loss: 4.1578\n",
      "Epoch [1/5], Step [6818/10336], Loss: 2.9812\n",
      "Epoch [1/5], Step [6820/10336], Loss: 1.6561\n",
      "Epoch [1/5], Step [6822/10336], Loss: 0.8716\n",
      "Epoch [1/5], Step [6824/10336], Loss: 0.9001\n",
      "Epoch [1/5], Step [6826/10336], Loss: 0.2876\n",
      "Epoch [1/5], Step [6828/10336], Loss: 3.9464\n",
      "Epoch [1/5], Step [6830/10336], Loss: 0.4821\n",
      "Epoch [1/5], Step [6832/10336], Loss: 0.2351\n",
      "Epoch [1/5], Step [6834/10336], Loss: 1.6743\n",
      "Epoch [1/5], Step [6836/10336], Loss: 1.9463\n",
      "Epoch [1/5], Step [6838/10336], Loss: 0.0555\n",
      "Epoch [1/5], Step [6840/10336], Loss: 4.3030\n",
      "Epoch [1/5], Step [6842/10336], Loss: 2.5909\n",
      "Epoch [1/5], Step [6844/10336], Loss: 0.1913\n",
      "Epoch [1/5], Step [6846/10336], Loss: 0.3233\n",
      "Epoch [1/5], Step [6848/10336], Loss: 0.6653\n",
      "Epoch [1/5], Step [6850/10336], Loss: 1.3840\n",
      "Epoch [1/5], Step [6852/10336], Loss: 0.4757\n",
      "Epoch [1/5], Step [6854/10336], Loss: 3.0369\n",
      "Epoch [1/5], Step [6856/10336], Loss: 0.2457\n",
      "Epoch [1/5], Step [6858/10336], Loss: 2.1193\n",
      "Epoch [1/5], Step [6860/10336], Loss: 0.0116\n",
      "Epoch [1/5], Step [6862/10336], Loss: 0.0263\n",
      "Epoch [1/5], Step [6864/10336], Loss: 1.3208\n",
      "Epoch [1/5], Step [6866/10336], Loss: 0.7115\n",
      "Epoch [1/5], Step [6868/10336], Loss: 0.6134\n",
      "Epoch [1/5], Step [6870/10336], Loss: 0.4019\n",
      "Epoch [1/5], Step [6872/10336], Loss: 0.5722\n",
      "Epoch [1/5], Step [6874/10336], Loss: 0.7501\n",
      "Epoch [1/5], Step [6876/10336], Loss: 0.0870\n",
      "Epoch [1/5], Step [6878/10336], Loss: 3.4463\n",
      "Epoch [1/5], Step [6880/10336], Loss: 0.6978\n",
      "Epoch [1/5], Step [6882/10336], Loss: 0.6546\n",
      "Epoch [1/5], Step [6884/10336], Loss: 3.1871\n",
      "Epoch [1/5], Step [6886/10336], Loss: 0.3648\n",
      "Epoch [1/5], Step [6888/10336], Loss: 0.5704\n",
      "Epoch [1/5], Step [6890/10336], Loss: 0.1561\n",
      "Epoch [1/5], Step [6892/10336], Loss: 1.4689\n",
      "Epoch [1/5], Step [6894/10336], Loss: 2.3385\n",
      "Epoch [1/5], Step [6896/10336], Loss: 0.9047\n",
      "Epoch [1/5], Step [6898/10336], Loss: 0.1724\n",
      "Epoch [1/5], Step [6900/10336], Loss: 0.3119\n",
      "Epoch [1/5], Step [6902/10336], Loss: 0.2238\n",
      "Epoch [1/5], Step [6904/10336], Loss: 3.2408\n",
      "Epoch [1/5], Step [6906/10336], Loss: 0.7030\n",
      "Epoch [1/5], Step [6908/10336], Loss: 0.4238\n",
      "Epoch [1/5], Step [6910/10336], Loss: 1.4331\n",
      "Epoch [1/5], Step [6912/10336], Loss: 0.8724\n",
      "Epoch [1/5], Step [6914/10336], Loss: 2.0634\n",
      "Epoch [1/5], Step [6916/10336], Loss: 0.6561\n",
      "Epoch [1/5], Step [6918/10336], Loss: 0.2265\n",
      "Epoch [1/5], Step [6920/10336], Loss: 4.7947\n",
      "Epoch [1/5], Step [6922/10336], Loss: 3.2413\n",
      "Epoch [1/5], Step [6924/10336], Loss: 1.0331\n",
      "Epoch [1/5], Step [6926/10336], Loss: 2.8593\n",
      "Epoch [1/5], Step [6928/10336], Loss: 2.0412\n",
      "Epoch [1/5], Step [6930/10336], Loss: 0.5365\n",
      "Epoch [1/5], Step [6932/10336], Loss: 0.2192\n",
      "Epoch [1/5], Step [6934/10336], Loss: 0.2891\n",
      "Epoch [1/5], Step [6936/10336], Loss: 0.9418\n",
      "Epoch [1/5], Step [6938/10336], Loss: 0.3708\n",
      "Epoch [1/5], Step [6940/10336], Loss: 3.9218\n",
      "Epoch [1/5], Step [6942/10336], Loss: 0.2142\n",
      "Epoch [1/5], Step [6944/10336], Loss: 3.5151\n",
      "Epoch [1/5], Step [6946/10336], Loss: 0.6614\n",
      "Epoch [1/5], Step [6948/10336], Loss: 0.3287\n",
      "Epoch [1/5], Step [6950/10336], Loss: 0.4609\n",
      "Epoch [1/5], Step [6952/10336], Loss: 1.7201\n",
      "Epoch [1/5], Step [6954/10336], Loss: 0.1502\n",
      "Epoch [1/5], Step [6956/10336], Loss: 0.7205\n",
      "Epoch [1/5], Step [6958/10336], Loss: 1.0267\n",
      "Epoch [1/5], Step [6960/10336], Loss: 0.3625\n",
      "Epoch [1/5], Step [6962/10336], Loss: 3.8020\n",
      "Epoch [1/5], Step [6964/10336], Loss: 0.5411\n",
      "Epoch [1/5], Step [6966/10336], Loss: 3.5504\n",
      "Epoch [1/5], Step [6968/10336], Loss: 0.5058\n",
      "Epoch [1/5], Step [6970/10336], Loss: 0.0945\n",
      "Epoch [1/5], Step [6972/10336], Loss: 0.1638\n",
      "Epoch [1/5], Step [6974/10336], Loss: 2.2283\n",
      "Epoch [1/5], Step [6976/10336], Loss: 0.2036\n",
      "Epoch [1/5], Step [6978/10336], Loss: 0.1774\n",
      "Epoch [1/5], Step [6980/10336], Loss: 0.5920\n",
      "Epoch [1/5], Step [6982/10336], Loss: 0.8037\n",
      "Epoch [1/5], Step [6984/10336], Loss: 0.2673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [6986/10336], Loss: 0.4476\n",
      "Epoch [1/5], Step [6988/10336], Loss: 0.0107\n",
      "Epoch [1/5], Step [6990/10336], Loss: 1.8680\n",
      "Epoch [1/5], Step [6992/10336], Loss: 0.4810\n",
      "Epoch [1/5], Step [6994/10336], Loss: 1.2017\n",
      "Epoch [1/5], Step [6996/10336], Loss: 1.9202\n",
      "Epoch [1/5], Step [6998/10336], Loss: 0.2516\n",
      "Epoch [1/5], Step [7000/10336], Loss: 1.9560\n",
      "Epoch [1/5], Step [7002/10336], Loss: 0.1477\n",
      "Epoch [1/5], Step [7004/10336], Loss: 1.0908\n",
      "Epoch [1/5], Step [7006/10336], Loss: 0.2991\n",
      "Epoch [1/5], Step [7008/10336], Loss: 0.5416\n",
      "Epoch [1/5], Step [7010/10336], Loss: 3.8632\n",
      "Epoch [1/5], Step [7012/10336], Loss: 1.2687\n",
      "Epoch [1/5], Step [7014/10336], Loss: 0.3674\n",
      "Epoch [1/5], Step [7016/10336], Loss: 1.2617\n",
      "Epoch [1/5], Step [7018/10336], Loss: 1.0195\n",
      "Epoch [1/5], Step [7020/10336], Loss: 0.4100\n",
      "Epoch [1/5], Step [7022/10336], Loss: 0.3986\n",
      "Epoch [1/5], Step [7024/10336], Loss: 0.1546\n",
      "Epoch [1/5], Step [7026/10336], Loss: 0.7859\n",
      "Epoch [1/5], Step [7028/10336], Loss: 3.3359\n",
      "Epoch [1/5], Step [7030/10336], Loss: 2.4869\n",
      "Epoch [1/5], Step [7032/10336], Loss: 4.3234\n",
      "Epoch [1/5], Step [7034/10336], Loss: 2.5127\n",
      "Epoch [1/5], Step [7036/10336], Loss: 0.2632\n",
      "Epoch [1/5], Step [7038/10336], Loss: 0.1429\n",
      "Epoch [1/5], Step [7040/10336], Loss: 0.1206\n",
      "Epoch [1/5], Step [7042/10336], Loss: 0.7416\n",
      "Epoch [1/5], Step [7044/10336], Loss: 0.2865\n",
      "Epoch [1/5], Step [7046/10336], Loss: 1.4030\n",
      "Epoch [1/5], Step [7048/10336], Loss: 0.2993\n",
      "Epoch [1/5], Step [7050/10336], Loss: 1.3550\n",
      "Epoch [1/5], Step [7052/10336], Loss: 0.5745\n",
      "Epoch [1/5], Step [7054/10336], Loss: 1.5416\n",
      "Epoch [1/5], Step [7056/10336], Loss: 3.5607\n",
      "Epoch [1/5], Step [7058/10336], Loss: 0.0420\n",
      "Epoch [1/5], Step [7060/10336], Loss: 0.8271\n",
      "Epoch [1/5], Step [7062/10336], Loss: 0.6799\n",
      "Epoch [1/5], Step [7064/10336], Loss: 0.3373\n",
      "Epoch [1/5], Step [7066/10336], Loss: 4.4201\n",
      "Epoch [1/5], Step [7068/10336], Loss: 1.9783\n",
      "Epoch [1/5], Step [7070/10336], Loss: 0.2885\n",
      "Epoch [1/5], Step [7072/10336], Loss: 0.3115\n",
      "Epoch [1/5], Step [7074/10336], Loss: 0.0084\n",
      "Epoch [1/5], Step [7076/10336], Loss: 4.8124\n",
      "Epoch [1/5], Step [7078/10336], Loss: 0.7248\n",
      "Epoch [1/5], Step [7080/10336], Loss: 0.8292\n",
      "Epoch [1/5], Step [7082/10336], Loss: 0.0950\n",
      "Epoch [1/5], Step [7084/10336], Loss: 0.4704\n",
      "Epoch [1/5], Step [7086/10336], Loss: 0.4299\n",
      "Epoch [1/5], Step [7088/10336], Loss: 0.1604\n",
      "Epoch [1/5], Step [7090/10336], Loss: 0.6239\n",
      "Epoch [1/5], Step [7092/10336], Loss: 1.2324\n",
      "Epoch [1/5], Step [7094/10336], Loss: 0.2623\n",
      "Epoch [1/5], Step [7096/10336], Loss: 1.0552\n",
      "Epoch [1/5], Step [7098/10336], Loss: 0.9843\n",
      "Epoch [1/5], Step [7100/10336], Loss: 4.4594\n",
      "Epoch [1/5], Step [7102/10336], Loss: 0.0946\n",
      "Epoch [1/5], Step [7104/10336], Loss: 0.1547\n",
      "Epoch [1/5], Step [7106/10336], Loss: 5.6840\n",
      "Epoch [1/5], Step [7108/10336], Loss: 1.2616\n",
      "Epoch [1/5], Step [7110/10336], Loss: 1.0344\n",
      "Epoch [1/5], Step [7112/10336], Loss: 3.7881\n",
      "Epoch [1/5], Step [7114/10336], Loss: 0.2041\n",
      "Epoch [1/5], Step [7116/10336], Loss: 0.2414\n",
      "Epoch [1/5], Step [7118/10336], Loss: 0.7425\n",
      "Epoch [1/5], Step [7120/10336], Loss: 2.9456\n",
      "Epoch [1/5], Step [7122/10336], Loss: 0.0988\n",
      "Epoch [1/5], Step [7124/10336], Loss: 0.0564\n",
      "Epoch [1/5], Step [7126/10336], Loss: 4.7664\n",
      "Epoch [1/5], Step [7128/10336], Loss: 0.9102\n",
      "Epoch [1/5], Step [7130/10336], Loss: 0.3355\n",
      "Epoch [1/5], Step [7132/10336], Loss: 0.2204\n",
      "Epoch [1/5], Step [7134/10336], Loss: 0.4502\n",
      "Epoch [1/5], Step [7136/10336], Loss: 4.6751\n",
      "Epoch [1/5], Step [7138/10336], Loss: 0.2895\n",
      "Epoch [1/5], Step [7140/10336], Loss: 0.6047\n",
      "Epoch [1/5], Step [7142/10336], Loss: 0.9341\n",
      "Epoch [1/5], Step [7144/10336], Loss: 0.1885\n",
      "Epoch [1/5], Step [7146/10336], Loss: 0.4377\n",
      "Epoch [1/5], Step [7148/10336], Loss: 4.3564\n",
      "Epoch [1/5], Step [7150/10336], Loss: 3.3767\n",
      "Epoch [1/5], Step [7152/10336], Loss: 1.1723\n",
      "Epoch [1/5], Step [7154/10336], Loss: 0.8630\n",
      "Epoch [1/5], Step [7156/10336], Loss: 3.5492\n",
      "Epoch [1/5], Step [7158/10336], Loss: 0.6990\n",
      "Epoch [1/5], Step [7160/10336], Loss: 3.1900\n",
      "Epoch [1/5], Step [7162/10336], Loss: 0.5836\n",
      "Epoch [1/5], Step [7164/10336], Loss: 0.1376\n",
      "Epoch [1/5], Step [7166/10336], Loss: 0.2874\n",
      "Epoch [1/5], Step [7168/10336], Loss: 0.2534\n",
      "Epoch [1/5], Step [7170/10336], Loss: 0.0041\n",
      "Epoch [1/5], Step [7172/10336], Loss: 2.6354\n",
      "Epoch [1/5], Step [7174/10336], Loss: 0.2293\n",
      "Epoch [1/5], Step [7176/10336], Loss: 0.5115\n",
      "Epoch [1/5], Step [7178/10336], Loss: 0.0447\n",
      "Epoch [1/5], Step [7180/10336], Loss: 0.0525\n",
      "Epoch [1/5], Step [7182/10336], Loss: 0.1396\n",
      "Epoch [1/5], Step [7184/10336], Loss: 0.4710\n",
      "Epoch [1/5], Step [7186/10336], Loss: 0.0704\n",
      "Epoch [1/5], Step [7188/10336], Loss: 0.0079\n",
      "Epoch [1/5], Step [7190/10336], Loss: 0.0493\n",
      "Epoch [1/5], Step [7192/10336], Loss: 0.4316\n",
      "Epoch [1/5], Step [7194/10336], Loss: 0.2897\n",
      "Epoch [1/5], Step [7196/10336], Loss: 1.1696\n",
      "Epoch [1/5], Step [7198/10336], Loss: 1.4320\n",
      "Epoch [1/5], Step [7200/10336], Loss: 0.3588\n",
      "Epoch [1/5], Step [7202/10336], Loss: 0.4076\n",
      "Epoch [1/5], Step [7204/10336], Loss: 0.3713\n",
      "Epoch [1/5], Step [7206/10336], Loss: 1.6224\n",
      "Epoch [1/5], Step [7208/10336], Loss: 0.5345\n",
      "Epoch [1/5], Step [7210/10336], Loss: 0.9742\n",
      "Epoch [1/5], Step [7212/10336], Loss: 0.2725\n",
      "Epoch [1/5], Step [7214/10336], Loss: 0.1147\n",
      "Epoch [1/5], Step [7216/10336], Loss: 1.3418\n",
      "Epoch [1/5], Step [7218/10336], Loss: 0.8498\n",
      "Epoch [1/5], Step [7220/10336], Loss: 0.0812\n",
      "Epoch [1/5], Step [7222/10336], Loss: 0.6259\n",
      "Epoch [1/5], Step [7224/10336], Loss: 0.1497\n",
      "Epoch [1/5], Step [7226/10336], Loss: 3.6477\n",
      "Epoch [1/5], Step [7228/10336], Loss: 0.0130\n",
      "Epoch [1/5], Step [7230/10336], Loss: 4.8594\n",
      "Epoch [1/5], Step [7232/10336], Loss: 1.3929\n",
      "Epoch [1/5], Step [7234/10336], Loss: 0.2344\n",
      "Epoch [1/5], Step [7236/10336], Loss: 5.0125\n",
      "Epoch [1/5], Step [7238/10336], Loss: 0.2946\n",
      "Epoch [1/5], Step [7240/10336], Loss: 0.3138\n",
      "Epoch [1/5], Step [7242/10336], Loss: 0.3217\n",
      "Epoch [1/5], Step [7244/10336], Loss: 0.7221\n",
      "Epoch [1/5], Step [7246/10336], Loss: 0.6754\n",
      "Epoch [1/5], Step [7248/10336], Loss: 1.0350\n",
      "Epoch [1/5], Step [7250/10336], Loss: 0.0506\n",
      "Epoch [1/5], Step [7252/10336], Loss: 1.4877\n",
      "Epoch [1/5], Step [7254/10336], Loss: 2.2252\n",
      "Epoch [1/5], Step [7256/10336], Loss: 1.2695\n",
      "Epoch [1/5], Step [7258/10336], Loss: 3.8539\n",
      "Epoch [1/5], Step [7260/10336], Loss: 3.1460\n",
      "Epoch [1/5], Step [7262/10336], Loss: 0.9831\n",
      "Epoch [1/5], Step [7264/10336], Loss: 1.2415\n",
      "Epoch [1/5], Step [7266/10336], Loss: 0.2653\n",
      "Epoch [1/5], Step [7268/10336], Loss: 0.5051\n",
      "Epoch [1/5], Step [7270/10336], Loss: 0.1729\n",
      "Epoch [1/5], Step [7272/10336], Loss: 0.7084\n",
      "Epoch [1/5], Step [7274/10336], Loss: 1.6239\n",
      "Epoch [1/5], Step [7276/10336], Loss: 0.5738\n",
      "Epoch [1/5], Step [7278/10336], Loss: 0.7257\n",
      "Epoch [1/5], Step [7280/10336], Loss: 2.0450\n",
      "Epoch [1/5], Step [7282/10336], Loss: 0.1311\n",
      "Epoch [1/5], Step [7284/10336], Loss: 0.0394\n",
      "Epoch [1/5], Step [7286/10336], Loss: 0.9009\n",
      "Epoch [1/5], Step [7288/10336], Loss: 4.6642\n",
      "Epoch [1/5], Step [7290/10336], Loss: 0.0892\n",
      "Epoch [1/5], Step [7292/10336], Loss: 1.5717\n",
      "Epoch [1/5], Step [7294/10336], Loss: 1.1069\n",
      "Epoch [1/5], Step [7296/10336], Loss: 0.0719\n",
      "Epoch [1/5], Step [7298/10336], Loss: 0.2044\n",
      "Epoch [1/5], Step [7300/10336], Loss: 2.5337\n",
      "Epoch [1/5], Step [7302/10336], Loss: 2.5686\n",
      "Epoch [1/5], Step [7304/10336], Loss: 0.5252\n",
      "Epoch [1/5], Step [7306/10336], Loss: 0.2971\n",
      "Epoch [1/5], Step [7308/10336], Loss: 1.7467\n",
      "Epoch [1/5], Step [7310/10336], Loss: 0.5501\n",
      "Epoch [1/5], Step [7312/10336], Loss: 4.2940\n",
      "Epoch [1/5], Step [7314/10336], Loss: 0.0189\n",
      "Epoch [1/5], Step [7316/10336], Loss: 1.7892\n",
      "Epoch [1/5], Step [7318/10336], Loss: 0.4265\n",
      "Epoch [1/5], Step [7320/10336], Loss: 0.5643\n",
      "Epoch [1/5], Step [7322/10336], Loss: 0.9449\n",
      "Epoch [1/5], Step [7324/10336], Loss: 0.6742\n",
      "Epoch [1/5], Step [7326/10336], Loss: 0.0538\n",
      "Epoch [1/5], Step [7328/10336], Loss: 0.0960\n",
      "Epoch [1/5], Step [7330/10336], Loss: 3.2542\n",
      "Epoch [1/5], Step [7332/10336], Loss: 2.3585\n",
      "Epoch [1/5], Step [7334/10336], Loss: 1.1569\n",
      "Epoch [1/5], Step [7336/10336], Loss: 0.0380\n",
      "Epoch [1/5], Step [7338/10336], Loss: 0.6819\n",
      "Epoch [1/5], Step [7340/10336], Loss: 3.0674\n",
      "Epoch [1/5], Step [7342/10336], Loss: 0.2407\n",
      "Epoch [1/5], Step [7344/10336], Loss: 1.4088\n",
      "Epoch [1/5], Step [7346/10336], Loss: 0.3616\n",
      "Epoch [1/5], Step [7348/10336], Loss: 0.2202\n",
      "Epoch [1/5], Step [7350/10336], Loss: 0.5420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [7352/10336], Loss: 0.6928\n",
      "Epoch [1/5], Step [7354/10336], Loss: 3.5865\n",
      "Epoch [1/5], Step [7356/10336], Loss: 0.2684\n",
      "Epoch [1/5], Step [7358/10336], Loss: 0.3464\n",
      "Epoch [1/5], Step [7360/10336], Loss: 0.3421\n",
      "Epoch [1/5], Step [7362/10336], Loss: 0.8801\n",
      "Epoch [1/5], Step [7364/10336], Loss: 0.3581\n",
      "Epoch [1/5], Step [7366/10336], Loss: 1.3409\n",
      "Epoch [1/5], Step [7368/10336], Loss: 0.3865\n",
      "Epoch [1/5], Step [7370/10336], Loss: 3.8567\n",
      "Epoch [1/5], Step [7372/10336], Loss: 0.0601\n",
      "Epoch [1/5], Step [7374/10336], Loss: 0.3225\n",
      "Epoch [1/5], Step [7376/10336], Loss: 0.0460\n",
      "Epoch [1/5], Step [7378/10336], Loss: 1.0510\n",
      "Epoch [1/5], Step [7380/10336], Loss: 0.3414\n",
      "Epoch [1/5], Step [7382/10336], Loss: 0.2157\n",
      "Epoch [1/5], Step [7384/10336], Loss: 0.3681\n",
      "Epoch [1/5], Step [7386/10336], Loss: 0.3250\n",
      "Epoch [1/5], Step [7388/10336], Loss: 2.6231\n",
      "Epoch [1/5], Step [7390/10336], Loss: 3.1097\n",
      "Epoch [1/5], Step [7392/10336], Loss: 0.4151\n",
      "Epoch [1/5], Step [7394/10336], Loss: 0.9569\n",
      "Epoch [1/5], Step [7396/10336], Loss: 2.6537\n",
      "Epoch [1/5], Step [7398/10336], Loss: 0.5459\n",
      "Epoch [1/5], Step [7400/10336], Loss: 1.0052\n",
      "Epoch [1/5], Step [7402/10336], Loss: 0.3016\n",
      "Epoch [1/5], Step [7404/10336], Loss: 0.3562\n",
      "Epoch [1/5], Step [7406/10336], Loss: 1.9392\n",
      "Epoch [1/5], Step [7408/10336], Loss: 0.0063\n",
      "Epoch [1/5], Step [7410/10336], Loss: 0.8775\n",
      "Epoch [1/5], Step [7412/10336], Loss: 0.3238\n",
      "Epoch [1/5], Step [7414/10336], Loss: 0.4787\n",
      "Epoch [1/5], Step [7416/10336], Loss: 1.9710\n",
      "Epoch [1/5], Step [7418/10336], Loss: 0.4019\n",
      "Epoch [1/5], Step [7420/10336], Loss: 3.7200\n",
      "Epoch [1/5], Step [7422/10336], Loss: 0.5513\n",
      "Epoch [1/5], Step [7424/10336], Loss: 0.6016\n",
      "Epoch [1/5], Step [7426/10336], Loss: 0.0055\n",
      "Epoch [1/5], Step [7428/10336], Loss: 1.1747\n",
      "Epoch [1/5], Step [7430/10336], Loss: 0.0755\n",
      "Epoch [1/5], Step [7432/10336], Loss: 4.9894\n",
      "Epoch [1/5], Step [7434/10336], Loss: 0.9248\n",
      "Epoch [1/5], Step [7436/10336], Loss: 0.2207\n",
      "Epoch [1/5], Step [7438/10336], Loss: 0.2585\n",
      "Epoch [1/5], Step [7440/10336], Loss: 0.0904\n",
      "Epoch [1/5], Step [7442/10336], Loss: 0.5095\n",
      "Epoch [1/5], Step [7444/10336], Loss: 0.8650\n",
      "Epoch [1/5], Step [7446/10336], Loss: 0.8161\n",
      "Epoch [1/5], Step [7448/10336], Loss: 3.2532\n",
      "Epoch [1/5], Step [7450/10336], Loss: 0.4723\n",
      "Epoch [1/5], Step [7452/10336], Loss: 1.6692\n",
      "Epoch [1/5], Step [7454/10336], Loss: 0.2257\n",
      "Epoch [1/5], Step [7456/10336], Loss: 0.1160\n",
      "Epoch [1/5], Step [7458/10336], Loss: 3.5683\n",
      "Epoch [1/5], Step [7460/10336], Loss: 1.7743\n",
      "Epoch [1/5], Step [7462/10336], Loss: 0.7597\n",
      "Epoch [1/5], Step [7464/10336], Loss: 2.2419\n",
      "Epoch [1/5], Step [7466/10336], Loss: 0.2204\n",
      "Epoch [1/5], Step [7468/10336], Loss: 2.7459\n",
      "Epoch [1/5], Step [7470/10336], Loss: 5.0595\n",
      "Epoch [1/5], Step [7472/10336], Loss: 0.3308\n",
      "Epoch [1/5], Step [7474/10336], Loss: 0.5402\n",
      "Epoch [1/5], Step [7476/10336], Loss: 0.3323\n",
      "Epoch [1/5], Step [7478/10336], Loss: 0.4611\n",
      "Epoch [1/5], Step [7480/10336], Loss: 1.4566\n",
      "Epoch [1/5], Step [7482/10336], Loss: 3.1282\n",
      "Epoch [1/5], Step [7484/10336], Loss: 3.5876\n",
      "Epoch [1/5], Step [7486/10336], Loss: 3.9846\n",
      "Epoch [1/5], Step [7488/10336], Loss: 0.4015\n",
      "Epoch [1/5], Step [7490/10336], Loss: 0.2029\n",
      "Epoch [1/5], Step [7492/10336], Loss: 0.5836\n",
      "Epoch [1/5], Step [7494/10336], Loss: 0.7522\n",
      "Epoch [1/5], Step [7496/10336], Loss: 3.8841\n",
      "Epoch [1/5], Step [7498/10336], Loss: 0.8298\n",
      "Epoch [1/5], Step [7500/10336], Loss: 4.7033\n",
      "Epoch [1/5], Step [7502/10336], Loss: 0.2394\n",
      "Epoch [1/5], Step [7504/10336], Loss: 2.9101\n",
      "Epoch [1/5], Step [7506/10336], Loss: 0.1612\n",
      "Epoch [1/5], Step [7508/10336], Loss: 0.8113\n",
      "Epoch [1/5], Step [7510/10336], Loss: 0.9092\n",
      "Epoch [1/5], Step [7512/10336], Loss: 0.5429\n",
      "Epoch [1/5], Step [7514/10336], Loss: 0.0173\n",
      "Epoch [1/5], Step [7516/10336], Loss: 3.1140\n",
      "Epoch [1/5], Step [7518/10336], Loss: 0.3317\n",
      "Epoch [1/5], Step [7520/10336], Loss: 0.3861\n",
      "Epoch [1/5], Step [7522/10336], Loss: 0.2731\n",
      "Epoch [1/5], Step [7524/10336], Loss: 0.3874\n",
      "Epoch [1/5], Step [7526/10336], Loss: 0.3758\n",
      "Epoch [1/5], Step [7528/10336], Loss: 1.6124\n",
      "Epoch [1/5], Step [7530/10336], Loss: 1.1882\n",
      "Epoch [1/5], Step [7532/10336], Loss: 0.0100\n",
      "Epoch [1/5], Step [7534/10336], Loss: 4.4310\n",
      "Epoch [1/5], Step [7536/10336], Loss: 0.5125\n",
      "Epoch [1/5], Step [7538/10336], Loss: 4.2102\n",
      "Epoch [1/5], Step [7540/10336], Loss: 0.2725\n",
      "Epoch [1/5], Step [7542/10336], Loss: 2.7788\n",
      "Epoch [1/5], Step [7544/10336], Loss: 4.5143\n",
      "Epoch [1/5], Step [7546/10336], Loss: 1.3157\n",
      "Epoch [1/5], Step [7548/10336], Loss: 3.4640\n",
      "Epoch [1/5], Step [7550/10336], Loss: 2.2598\n",
      "Epoch [1/5], Step [7552/10336], Loss: 0.6755\n",
      "Epoch [1/5], Step [7554/10336], Loss: 1.3834\n",
      "Epoch [1/5], Step [7556/10336], Loss: 0.4633\n",
      "Epoch [1/5], Step [7558/10336], Loss: 0.0975\n",
      "Epoch [1/5], Step [7560/10336], Loss: 2.8703\n",
      "Epoch [1/5], Step [7562/10336], Loss: 1.6113\n",
      "Epoch [1/5], Step [7564/10336], Loss: 0.5880\n",
      "Epoch [1/5], Step [7566/10336], Loss: 1.0903\n",
      "Epoch [1/5], Step [7568/10336], Loss: 4.3494\n",
      "Epoch [1/5], Step [7570/10336], Loss: 1.3791\n",
      "Epoch [1/5], Step [7572/10336], Loss: 1.2067\n",
      "Epoch [1/5], Step [7574/10336], Loss: 3.8748\n",
      "Epoch [1/5], Step [7576/10336], Loss: 0.5275\n",
      "Epoch [1/5], Step [7578/10336], Loss: 0.4780\n",
      "Epoch [1/5], Step [7580/10336], Loss: 0.2874\n",
      "Epoch [1/5], Step [7582/10336], Loss: 0.2253\n",
      "Epoch [1/5], Step [7584/10336], Loss: 1.5771\n",
      "Epoch [1/5], Step [7586/10336], Loss: 0.2211\n",
      "Epoch [1/5], Step [7588/10336], Loss: 0.3162\n",
      "Epoch [1/5], Step [7590/10336], Loss: 4.2448\n",
      "Epoch [1/5], Step [7592/10336], Loss: 1.1213\n",
      "Epoch [1/5], Step [7594/10336], Loss: 0.3522\n",
      "Epoch [1/5], Step [7596/10336], Loss: 0.2469\n",
      "Epoch [1/5], Step [7598/10336], Loss: 0.5259\n",
      "Epoch [1/5], Step [7600/10336], Loss: 3.8993\n",
      "Epoch [1/5], Step [7602/10336], Loss: 0.3148\n",
      "Epoch [1/5], Step [7604/10336], Loss: 0.1907\n",
      "Epoch [1/5], Step [7606/10336], Loss: 0.2887\n",
      "Epoch [1/5], Step [7608/10336], Loss: 2.4533\n",
      "Epoch [1/5], Step [7610/10336], Loss: 2.0723\n",
      "Epoch [1/5], Step [7612/10336], Loss: 0.2403\n",
      "Epoch [1/5], Step [7614/10336], Loss: 0.1090\n",
      "Epoch [1/5], Step [7616/10336], Loss: 0.3900\n",
      "Epoch [1/5], Step [7618/10336], Loss: 0.2791\n",
      "Epoch [1/5], Step [7620/10336], Loss: 0.6912\n",
      "Epoch [1/5], Step [7622/10336], Loss: 2.3368\n",
      "Epoch [1/5], Step [7624/10336], Loss: 0.3868\n",
      "Epoch [1/5], Step [7626/10336], Loss: 0.6747\n",
      "Epoch [1/5], Step [7628/10336], Loss: 1.3621\n",
      "Epoch [1/5], Step [7630/10336], Loss: 0.3093\n",
      "Epoch [1/5], Step [7632/10336], Loss: 4.0842\n",
      "Epoch [1/5], Step [7634/10336], Loss: 0.8669\n",
      "Epoch [1/5], Step [7636/10336], Loss: 0.0800\n",
      "Epoch [1/5], Step [7638/10336], Loss: 0.7553\n",
      "Epoch [1/5], Step [7640/10336], Loss: 3.2090\n",
      "Epoch [1/5], Step [7642/10336], Loss: 0.0239\n",
      "Epoch [1/5], Step [7644/10336], Loss: 0.2575\n",
      "Epoch [1/5], Step [7646/10336], Loss: 2.9924\n",
      "Epoch [1/5], Step [7648/10336], Loss: 3.1482\n",
      "Epoch [1/5], Step [7650/10336], Loss: 0.3253\n",
      "Epoch [1/5], Step [7652/10336], Loss: 0.4737\n",
      "Epoch [1/5], Step [7654/10336], Loss: 0.3571\n",
      "Epoch [1/5], Step [7656/10336], Loss: 0.0556\n",
      "Epoch [1/5], Step [7658/10336], Loss: 0.6718\n",
      "Epoch [1/5], Step [7660/10336], Loss: 1.1935\n",
      "Epoch [1/5], Step [7662/10336], Loss: 0.4723\n",
      "Epoch [1/5], Step [7664/10336], Loss: 3.1813\n",
      "Epoch [1/5], Step [7666/10336], Loss: 4.4918\n",
      "Epoch [1/5], Step [7668/10336], Loss: 3.4460\n",
      "Epoch [1/5], Step [7670/10336], Loss: 0.1498\n",
      "Epoch [1/5], Step [7672/10336], Loss: 0.6889\n",
      "Epoch [1/5], Step [7674/10336], Loss: 0.0572\n",
      "Epoch [1/5], Step [7676/10336], Loss: 0.0297\n",
      "Epoch [1/5], Step [7678/10336], Loss: 0.4546\n",
      "Epoch [1/5], Step [7680/10336], Loss: 0.2244\n",
      "Epoch [1/5], Step [7682/10336], Loss: 0.2372\n",
      "Epoch [1/5], Step [7684/10336], Loss: 0.2475\n",
      "Epoch [1/5], Step [7686/10336], Loss: 4.4335\n",
      "Epoch [1/5], Step [7688/10336], Loss: 0.0557\n",
      "Epoch [1/5], Step [7690/10336], Loss: 0.5902\n",
      "Epoch [1/5], Step [7692/10336], Loss: 1.9639\n",
      "Epoch [1/5], Step [7694/10336], Loss: 0.0937\n",
      "Epoch [1/5], Step [7696/10336], Loss: 0.0110\n",
      "Epoch [1/5], Step [7698/10336], Loss: 2.0458\n",
      "Epoch [1/5], Step [7700/10336], Loss: 2.6664\n",
      "Epoch [1/5], Step [7702/10336], Loss: 0.1770\n",
      "Epoch [1/5], Step [7704/10336], Loss: 0.4934\n",
      "Epoch [1/5], Step [7706/10336], Loss: 0.1066\n",
      "Epoch [1/5], Step [7708/10336], Loss: 0.3629\n",
      "Epoch [1/5], Step [7710/10336], Loss: 0.5388\n",
      "Epoch [1/5], Step [7712/10336], Loss: 0.1611\n",
      "Epoch [1/5], Step [7714/10336], Loss: 0.2818\n",
      "Epoch [1/5], Step [7716/10336], Loss: 0.9056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [7718/10336], Loss: 0.7555\n",
      "Epoch [1/5], Step [7720/10336], Loss: 3.2470\n",
      "Epoch [1/5], Step [7722/10336], Loss: 2.5943\n",
      "Epoch [1/5], Step [7724/10336], Loss: 4.0112\n",
      "Epoch [1/5], Step [7726/10336], Loss: 5.4225\n",
      "Epoch [1/5], Step [7728/10336], Loss: 0.7202\n",
      "Epoch [1/5], Step [7730/10336], Loss: 1.6161\n",
      "Epoch [1/5], Step [7732/10336], Loss: 0.0604\n",
      "Epoch [1/5], Step [7734/10336], Loss: 0.3941\n",
      "Epoch [1/5], Step [7736/10336], Loss: 0.2769\n",
      "Epoch [1/5], Step [7738/10336], Loss: 1.2997\n",
      "Epoch [1/5], Step [7740/10336], Loss: 0.7133\n",
      "Epoch [1/5], Step [7742/10336], Loss: 0.5668\n",
      "Epoch [1/5], Step [7744/10336], Loss: 0.3896\n",
      "Epoch [1/5], Step [7746/10336], Loss: 0.6204\n",
      "Epoch [1/5], Step [7748/10336], Loss: 0.1522\n",
      "Epoch [1/5], Step [7750/10336], Loss: 0.6839\n",
      "Epoch [1/5], Step [7752/10336], Loss: 0.3576\n",
      "Epoch [1/5], Step [7754/10336], Loss: 0.4169\n",
      "Epoch [1/5], Step [7756/10336], Loss: 1.0254\n",
      "Epoch [1/5], Step [7758/10336], Loss: 3.4563\n",
      "Epoch [1/5], Step [7760/10336], Loss: 0.1730\n",
      "Epoch [1/5], Step [7762/10336], Loss: 0.4013\n",
      "Epoch [1/5], Step [7764/10336], Loss: 2.8503\n",
      "Epoch [1/5], Step [7766/10336], Loss: 2.2891\n",
      "Epoch [1/5], Step [7768/10336], Loss: 0.2349\n",
      "Epoch [1/5], Step [7770/10336], Loss: 0.5882\n",
      "Epoch [1/5], Step [7772/10336], Loss: 0.4040\n",
      "Epoch [1/5], Step [7774/10336], Loss: 0.0322\n",
      "Epoch [1/5], Step [7776/10336], Loss: 0.0943\n",
      "Epoch [1/5], Step [7778/10336], Loss: 0.4528\n",
      "Epoch [1/5], Step [7780/10336], Loss: 0.4583\n",
      "Epoch [1/5], Step [7782/10336], Loss: 1.6873\n",
      "Epoch [1/5], Step [7784/10336], Loss: 0.7246\n",
      "Epoch [1/5], Step [7786/10336], Loss: 0.7728\n",
      "Epoch [1/5], Step [7788/10336], Loss: 2.9897\n",
      "Epoch [1/5], Step [7790/10336], Loss: 0.6555\n",
      "Epoch [1/5], Step [7792/10336], Loss: 0.2553\n",
      "Epoch [1/5], Step [7794/10336], Loss: 0.4087\n",
      "Epoch [1/5], Step [7796/10336], Loss: 1.0381\n",
      "Epoch [1/5], Step [7798/10336], Loss: 1.7830\n",
      "Epoch [1/5], Step [7800/10336], Loss: 0.6411\n",
      "Epoch [1/5], Step [7802/10336], Loss: 2.9550\n",
      "Epoch [1/5], Step [7804/10336], Loss: 0.5757\n",
      "Epoch [1/5], Step [7806/10336], Loss: 1.1600\n",
      "Epoch [1/5], Step [7808/10336], Loss: 0.1693\n",
      "Epoch [1/5], Step [7810/10336], Loss: 1.1573\n",
      "Epoch [1/5], Step [7812/10336], Loss: 2.2105\n",
      "Epoch [1/5], Step [7814/10336], Loss: 0.7642\n",
      "Epoch [1/5], Step [7816/10336], Loss: 3.6869\n",
      "Epoch [1/5], Step [7818/10336], Loss: 1.1911\n",
      "Epoch [1/5], Step [7820/10336], Loss: 1.0451\n",
      "Epoch [1/5], Step [7822/10336], Loss: 4.6988\n",
      "Epoch [1/5], Step [7824/10336], Loss: 1.9701\n",
      "Epoch [1/5], Step [7826/10336], Loss: 0.5084\n",
      "Epoch [1/5], Step [7828/10336], Loss: 0.0885\n",
      "Epoch [1/5], Step [7830/10336], Loss: 0.1342\n",
      "Epoch [1/5], Step [7832/10336], Loss: 4.4002\n",
      "Epoch [1/5], Step [7834/10336], Loss: 6.6170\n",
      "Epoch [1/5], Step [7836/10336], Loss: 0.3363\n",
      "Epoch [1/5], Step [7838/10336], Loss: 2.9910\n",
      "Epoch [1/5], Step [7840/10336], Loss: 0.2609\n",
      "Epoch [1/5], Step [7842/10336], Loss: 0.4391\n",
      "Epoch [1/5], Step [7844/10336], Loss: 2.1337\n",
      "Epoch [1/5], Step [7846/10336], Loss: 1.2087\n",
      "Epoch [1/5], Step [7848/10336], Loss: 0.5291\n",
      "Epoch [1/5], Step [7850/10336], Loss: 0.5058\n",
      "Epoch [1/5], Step [7852/10336], Loss: 0.6096\n",
      "Epoch [1/5], Step [7854/10336], Loss: 1.4694\n",
      "Epoch [1/5], Step [7856/10336], Loss: 0.3854\n",
      "Epoch [1/5], Step [7858/10336], Loss: 1.0830\n",
      "Epoch [1/5], Step [7860/10336], Loss: 4.3666\n",
      "Epoch [1/5], Step [7862/10336], Loss: 0.5941\n",
      "Epoch [1/5], Step [7864/10336], Loss: 4.6241\n",
      "Epoch [1/5], Step [7866/10336], Loss: 1.4579\n",
      "Epoch [1/5], Step [7868/10336], Loss: 3.7752\n",
      "Epoch [1/5], Step [7870/10336], Loss: 0.0469\n",
      "Epoch [1/5], Step [7872/10336], Loss: 0.1316\n",
      "Epoch [1/5], Step [7874/10336], Loss: 0.6452\n",
      "Epoch [1/5], Step [7876/10336], Loss: 1.5361\n",
      "Epoch [1/5], Step [7878/10336], Loss: 3.5363\n",
      "Epoch [1/5], Step [7880/10336], Loss: 0.4398\n",
      "Epoch [1/5], Step [7882/10336], Loss: 4.1414\n",
      "Epoch [1/5], Step [7884/10336], Loss: 4.1953\n",
      "Epoch [1/5], Step [7886/10336], Loss: 0.4493\n",
      "Epoch [1/5], Step [7888/10336], Loss: 0.3552\n",
      "Epoch [1/5], Step [7890/10336], Loss: 0.3750\n",
      "Epoch [1/5], Step [7892/10336], Loss: 0.0859\n",
      "Epoch [1/5], Step [7894/10336], Loss: 0.2124\n",
      "Epoch [1/5], Step [7896/10336], Loss: 1.4785\n",
      "Epoch [1/5], Step [7898/10336], Loss: 0.3262\n",
      "Epoch [1/5], Step [7900/10336], Loss: 0.1745\n",
      "Epoch [1/5], Step [7902/10336], Loss: 0.2886\n",
      "Epoch [1/5], Step [7904/10336], Loss: 0.5170\n",
      "Epoch [1/5], Step [7906/10336], Loss: 3.6208\n",
      "Epoch [1/5], Step [7908/10336], Loss: 0.8398\n",
      "Epoch [1/5], Step [7910/10336], Loss: 0.2058\n",
      "Epoch [1/5], Step [7912/10336], Loss: 0.3256\n",
      "Epoch [1/5], Step [7914/10336], Loss: 2.2341\n",
      "Epoch [1/5], Step [7916/10336], Loss: 2.3293\n",
      "Epoch [1/5], Step [7918/10336], Loss: 0.4067\n",
      "Epoch [1/5], Step [7920/10336], Loss: 3.4787\n",
      "Epoch [1/5], Step [7922/10336], Loss: 0.8171\n",
      "Epoch [1/5], Step [7924/10336], Loss: 0.0522\n",
      "Epoch [1/5], Step [7926/10336], Loss: 0.0292\n",
      "Epoch [1/5], Step [7928/10336], Loss: 0.5012\n",
      "Epoch [1/5], Step [7930/10336], Loss: 0.1662\n",
      "Epoch [1/5], Step [7932/10336], Loss: 0.3877\n",
      "Epoch [1/5], Step [7934/10336], Loss: 1.3273\n",
      "Epoch [1/5], Step [7936/10336], Loss: 1.3096\n",
      "Epoch [1/5], Step [7938/10336], Loss: 0.3617\n",
      "Epoch [1/5], Step [7940/10336], Loss: 0.4356\n",
      "Epoch [1/5], Step [7942/10336], Loss: 2.0484\n",
      "Epoch [1/5], Step [7944/10336], Loss: 3.3913\n",
      "Epoch [1/5], Step [7946/10336], Loss: 0.2377\n",
      "Epoch [1/5], Step [7948/10336], Loss: 2.5179\n",
      "Epoch [1/5], Step [7950/10336], Loss: 0.9365\n",
      "Epoch [1/5], Step [7952/10336], Loss: 0.3127\n",
      "Epoch [1/5], Step [7954/10336], Loss: 0.0603\n",
      "Epoch [1/5], Step [7956/10336], Loss: 2.8300\n",
      "Epoch [1/5], Step [7958/10336], Loss: 0.8747\n",
      "Epoch [1/5], Step [7960/10336], Loss: 0.2456\n",
      "Epoch [1/5], Step [7962/10336], Loss: 0.4985\n",
      "Epoch [1/5], Step [7964/10336], Loss: 0.6338\n",
      "Epoch [1/5], Step [7966/10336], Loss: 0.4562\n",
      "Epoch [1/5], Step [7968/10336], Loss: 2.8396\n",
      "Epoch [1/5], Step [7970/10336], Loss: 0.2714\n",
      "Epoch [1/5], Step [7972/10336], Loss: 0.1891\n",
      "Epoch [1/5], Step [7974/10336], Loss: 0.5124\n",
      "Epoch [1/5], Step [7976/10336], Loss: 0.7568\n",
      "Epoch [1/5], Step [7978/10336], Loss: 0.0358\n",
      "Epoch [1/5], Step [7980/10336], Loss: 0.6238\n",
      "Epoch [1/5], Step [7982/10336], Loss: 1.8639\n",
      "Epoch [1/5], Step [7984/10336], Loss: 0.2019\n",
      "Epoch [1/5], Step [7986/10336], Loss: 2.1087\n",
      "Epoch [1/5], Step [7988/10336], Loss: 0.4421\n",
      "Epoch [1/5], Step [7990/10336], Loss: 2.7772\n",
      "Epoch [1/5], Step [7992/10336], Loss: 0.1573\n",
      "Epoch [1/5], Step [7994/10336], Loss: 0.2184\n",
      "Epoch [1/5], Step [7996/10336], Loss: 0.2404\n",
      "Epoch [1/5], Step [7998/10336], Loss: 0.1876\n",
      "Epoch [1/5], Step [8000/10336], Loss: 0.0530\n",
      "Epoch [1/5], Step [8002/10336], Loss: 0.2737\n",
      "Epoch [1/5], Step [8004/10336], Loss: 0.0044\n",
      "Epoch [1/5], Step [8006/10336], Loss: 1.1185\n",
      "Epoch [1/5], Step [8008/10336], Loss: 0.3658\n",
      "Epoch [1/5], Step [8010/10336], Loss: 0.4948\n",
      "Epoch [1/5], Step [8012/10336], Loss: 1.1251\n",
      "Epoch [1/5], Step [8014/10336], Loss: 1.2764\n",
      "Epoch [1/5], Step [8016/10336], Loss: 0.3288\n",
      "Epoch [1/5], Step [8018/10336], Loss: 0.8864\n",
      "Epoch [1/5], Step [8020/10336], Loss: 0.5452\n",
      "Epoch [1/5], Step [8022/10336], Loss: 0.0589\n",
      "Epoch [1/5], Step [8024/10336], Loss: 1.0819\n",
      "Epoch [1/5], Step [8026/10336], Loss: 0.2272\n",
      "Epoch [1/5], Step [8028/10336], Loss: 0.3784\n",
      "Epoch [1/5], Step [8030/10336], Loss: 2.3672\n",
      "Epoch [1/5], Step [8032/10336], Loss: 1.4011\n",
      "Epoch [1/5], Step [8034/10336], Loss: 1.4457\n",
      "Epoch [1/5], Step [8036/10336], Loss: 0.0564\n",
      "Epoch [1/5], Step [8038/10336], Loss: 0.6805\n",
      "Epoch [1/5], Step [8040/10336], Loss: 0.2889\n",
      "Epoch [1/5], Step [8042/10336], Loss: 0.1684\n",
      "Epoch [1/5], Step [8044/10336], Loss: 0.6486\n",
      "Epoch [1/5], Step [8046/10336], Loss: 3.8935\n",
      "Epoch [1/5], Step [8048/10336], Loss: 1.8384\n",
      "Epoch [1/5], Step [8050/10336], Loss: 1.2109\n",
      "Epoch [1/5], Step [8052/10336], Loss: 0.3258\n",
      "Epoch [1/5], Step [8054/10336], Loss: 0.2903\n",
      "Epoch [1/5], Step [8056/10336], Loss: 0.2321\n",
      "Epoch [1/5], Step [8058/10336], Loss: 0.1547\n",
      "Epoch [1/5], Step [8060/10336], Loss: 1.8205\n",
      "Epoch [1/5], Step [8062/10336], Loss: 4.4635\n",
      "Epoch [1/5], Step [8064/10336], Loss: 1.8851\n",
      "Epoch [1/5], Step [8066/10336], Loss: 0.3923\n",
      "Epoch [1/5], Step [8068/10336], Loss: 0.6668\n",
      "Epoch [1/5], Step [8070/10336], Loss: 3.3111\n",
      "Epoch [1/5], Step [8072/10336], Loss: 3.3415\n",
      "Epoch [1/5], Step [8074/10336], Loss: 4.0740\n",
      "Epoch [1/5], Step [8076/10336], Loss: 0.2211\n",
      "Epoch [1/5], Step [8078/10336], Loss: 0.5433\n",
      "Epoch [1/5], Step [8080/10336], Loss: 5.8057\n",
      "Epoch [1/5], Step [8082/10336], Loss: 0.0352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [8084/10336], Loss: 0.5082\n",
      "Epoch [1/5], Step [8086/10336], Loss: 0.1732\n",
      "Epoch [1/5], Step [8088/10336], Loss: 5.3080\n",
      "Epoch [1/5], Step [8090/10336], Loss: 0.1184\n",
      "Epoch [1/5], Step [8092/10336], Loss: 1.6033\n",
      "Epoch [1/5], Step [8094/10336], Loss: 0.5291\n",
      "Epoch [1/5], Step [8096/10336], Loss: 0.5313\n",
      "Epoch [1/5], Step [8098/10336], Loss: 3.3764\n",
      "Epoch [1/5], Step [8100/10336], Loss: 3.0073\n",
      "Epoch [1/5], Step [8102/10336], Loss: 3.4183\n",
      "Epoch [1/5], Step [8104/10336], Loss: 2.5935\n",
      "Epoch [1/5], Step [8106/10336], Loss: 0.8245\n",
      "Epoch [1/5], Step [8108/10336], Loss: 1.5467\n",
      "Epoch [1/5], Step [8110/10336], Loss: 0.5596\n",
      "Epoch [1/5], Step [8112/10336], Loss: 2.4273\n",
      "Epoch [1/5], Step [8114/10336], Loss: 3.5975\n",
      "Epoch [1/5], Step [8116/10336], Loss: 0.6815\n",
      "Epoch [1/5], Step [8118/10336], Loss: 2.1265\n",
      "Epoch [1/5], Step [8120/10336], Loss: 0.1648\n",
      "Epoch [1/5], Step [8122/10336], Loss: 0.4603\n",
      "Epoch [1/5], Step [8124/10336], Loss: 0.2471\n",
      "Epoch [1/5], Step [8126/10336], Loss: 1.8079\n",
      "Epoch [1/5], Step [8128/10336], Loss: 0.0467\n",
      "Epoch [1/5], Step [8130/10336], Loss: 1.7328\n",
      "Epoch [1/5], Step [8132/10336], Loss: 0.7834\n",
      "Epoch [1/5], Step [8134/10336], Loss: 0.6860\n",
      "Epoch [1/5], Step [8136/10336], Loss: 4.5270\n",
      "Epoch [1/5], Step [8138/10336], Loss: 0.7510\n",
      "Epoch [1/5], Step [8140/10336], Loss: 0.2613\n",
      "Epoch [1/5], Step [8142/10336], Loss: 0.1866\n",
      "Epoch [1/5], Step [8144/10336], Loss: 0.5845\n",
      "Epoch [1/5], Step [8146/10336], Loss: 2.0177\n",
      "Epoch [1/5], Step [8148/10336], Loss: 4.0741\n",
      "Epoch [1/5], Step [8150/10336], Loss: 0.1015\n",
      "Epoch [1/5], Step [8152/10336], Loss: 0.4779\n",
      "Epoch [1/5], Step [8154/10336], Loss: 2.8397\n",
      "Epoch [1/5], Step [8156/10336], Loss: 0.0707\n",
      "Epoch [1/5], Step [8158/10336], Loss: 0.3368\n",
      "Epoch [1/5], Step [8160/10336], Loss: 0.1867\n",
      "Epoch [1/5], Step [8162/10336], Loss: 0.3317\n",
      "Epoch [1/5], Step [8164/10336], Loss: 0.5541\n",
      "Epoch [1/5], Step [8166/10336], Loss: 2.2792\n",
      "Epoch [1/5], Step [8168/10336], Loss: 0.0793\n",
      "Epoch [1/5], Step [8170/10336], Loss: 0.3214\n",
      "Epoch [1/5], Step [8172/10336], Loss: 0.0395\n",
      "Epoch [1/5], Step [8174/10336], Loss: 0.3179\n",
      "Epoch [1/5], Step [8176/10336], Loss: 0.0133\n",
      "Epoch [1/5], Step [8178/10336], Loss: 2.8387\n",
      "Epoch [1/5], Step [8180/10336], Loss: 0.5282\n",
      "Epoch [1/5], Step [8182/10336], Loss: 0.0982\n",
      "Epoch [1/5], Step [8184/10336], Loss: 2.2077\n",
      "Epoch [1/5], Step [8186/10336], Loss: 5.4851\n",
      "Epoch [1/5], Step [8188/10336], Loss: 0.5318\n",
      "Epoch [1/5], Step [8190/10336], Loss: 0.0508\n",
      "Epoch [1/5], Step [8192/10336], Loss: 2.6113\n",
      "Epoch [1/5], Step [8194/10336], Loss: 3.8591\n",
      "Epoch [1/5], Step [8196/10336], Loss: 0.7160\n",
      "Epoch [1/5], Step [8198/10336], Loss: 0.6729\n",
      "Epoch [1/5], Step [8200/10336], Loss: 0.2630\n",
      "Epoch [1/5], Step [8202/10336], Loss: 0.4893\n",
      "Epoch [1/5], Step [8204/10336], Loss: 0.1602\n",
      "Epoch [1/5], Step [8206/10336], Loss: 1.2853\n",
      "Epoch [1/5], Step [8208/10336], Loss: 0.9150\n",
      "Epoch [1/5], Step [8210/10336], Loss: 0.2714\n",
      "Epoch [1/5], Step [8212/10336], Loss: 1.0084\n",
      "Epoch [1/5], Step [8214/10336], Loss: 0.4520\n",
      "Epoch [1/5], Step [8216/10336], Loss: 0.8513\n",
      "Epoch [1/5], Step [8218/10336], Loss: 3.1097\n",
      "Epoch [1/5], Step [8220/10336], Loss: 0.2006\n",
      "Epoch [1/5], Step [8222/10336], Loss: 0.0130\n",
      "Epoch [1/5], Step [8224/10336], Loss: 0.1945\n",
      "Epoch [1/5], Step [8226/10336], Loss: 0.4136\n",
      "Epoch [1/5], Step [8228/10336], Loss: 0.0486\n",
      "Epoch [1/5], Step [8230/10336], Loss: 0.5787\n",
      "Epoch [1/5], Step [8232/10336], Loss: 0.4091\n",
      "Epoch [1/5], Step [8234/10336], Loss: 0.2716\n",
      "Epoch [1/5], Step [8236/10336], Loss: 0.7173\n",
      "Epoch [1/5], Step [8238/10336], Loss: 0.9998\n",
      "Epoch [1/5], Step [8240/10336], Loss: 0.0139\n",
      "Epoch [1/5], Step [8242/10336], Loss: 0.5863\n",
      "Epoch [1/5], Step [8244/10336], Loss: 0.3834\n",
      "Epoch [1/5], Step [8246/10336], Loss: 0.5292\n",
      "Epoch [1/5], Step [8248/10336], Loss: 0.4912\n",
      "Epoch [1/5], Step [8250/10336], Loss: 0.0689\n",
      "Epoch [1/5], Step [8252/10336], Loss: 0.3263\n",
      "Epoch [1/5], Step [8254/10336], Loss: 0.5658\n",
      "Epoch [1/5], Step [8256/10336], Loss: 4.9774\n",
      "Epoch [1/5], Step [8258/10336], Loss: 0.3150\n",
      "Epoch [1/5], Step [8260/10336], Loss: 1.0843\n",
      "Epoch [1/5], Step [8262/10336], Loss: 0.6333\n",
      "Epoch [1/5], Step [8264/10336], Loss: 0.0468\n",
      "Epoch [1/5], Step [8266/10336], Loss: 0.6947\n",
      "Epoch [1/5], Step [8268/10336], Loss: 0.7369\n",
      "Epoch [1/5], Step [8270/10336], Loss: 0.1864\n",
      "Epoch [1/5], Step [8272/10336], Loss: 0.0031\n",
      "Epoch [1/5], Step [8274/10336], Loss: 0.9531\n",
      "Epoch [1/5], Step [8276/10336], Loss: 3.9580\n",
      "Epoch [1/5], Step [8278/10336], Loss: 0.3039\n",
      "Epoch [1/5], Step [8280/10336], Loss: 2.8492\n",
      "Epoch [1/5], Step [8282/10336], Loss: 0.2169\n",
      "Epoch [1/5], Step [8284/10336], Loss: 1.0247\n",
      "Epoch [1/5], Step [8286/10336], Loss: 0.6088\n",
      "Epoch [1/5], Step [8288/10336], Loss: 1.1025\n",
      "Epoch [1/5], Step [8290/10336], Loss: 0.3096\n",
      "Epoch [1/5], Step [8292/10336], Loss: 1.3811\n",
      "Epoch [1/5], Step [8294/10336], Loss: 3.2513\n",
      "Epoch [1/5], Step [8296/10336], Loss: 0.1918\n",
      "Epoch [1/5], Step [8298/10336], Loss: 4.6187\n",
      "Epoch [1/5], Step [8300/10336], Loss: 0.4486\n",
      "Epoch [1/5], Step [8302/10336], Loss: 0.3784\n",
      "Epoch [1/5], Step [8304/10336], Loss: 0.2313\n",
      "Epoch [1/5], Step [8306/10336], Loss: 0.2815\n",
      "Epoch [1/5], Step [8308/10336], Loss: 0.2247\n",
      "Epoch [1/5], Step [8310/10336], Loss: 0.5580\n",
      "Epoch [1/5], Step [8312/10336], Loss: 4.2647\n",
      "Epoch [1/5], Step [8314/10336], Loss: 1.4288\n",
      "Epoch [1/5], Step [8316/10336], Loss: 0.3050\n",
      "Epoch [1/5], Step [8318/10336], Loss: 0.3664\n",
      "Epoch [1/5], Step [8320/10336], Loss: 0.2972\n",
      "Epoch [1/5], Step [8322/10336], Loss: 0.4210\n",
      "Epoch [1/5], Step [8324/10336], Loss: 0.8317\n",
      "Epoch [1/5], Step [8326/10336], Loss: 2.2169\n",
      "Epoch [1/5], Step [8328/10336], Loss: 0.0070\n",
      "Epoch [1/5], Step [8330/10336], Loss: 1.7085\n",
      "Epoch [1/5], Step [8332/10336], Loss: 2.1604\n",
      "Epoch [1/5], Step [8334/10336], Loss: 0.4776\n",
      "Epoch [1/5], Step [8336/10336], Loss: 0.1427\n",
      "Epoch [1/5], Step [8338/10336], Loss: 0.3407\n",
      "Epoch [1/5], Step [8340/10336], Loss: 0.0967\n",
      "Epoch [1/5], Step [8342/10336], Loss: 0.2650\n",
      "Epoch [1/5], Step [8344/10336], Loss: 0.0438\n",
      "Epoch [1/5], Step [8346/10336], Loss: 0.8908\n",
      "Epoch [1/5], Step [8348/10336], Loss: 0.0979\n",
      "Epoch [1/5], Step [8350/10336], Loss: 0.8545\n",
      "Epoch [1/5], Step [8352/10336], Loss: 0.0078\n",
      "Epoch [1/5], Step [8354/10336], Loss: 0.2612\n",
      "Epoch [1/5], Step [8356/10336], Loss: 5.0490\n",
      "Epoch [1/5], Step [8358/10336], Loss: 3.0464\n",
      "Epoch [1/5], Step [8360/10336], Loss: 0.0060\n",
      "Epoch [1/5], Step [8362/10336], Loss: 0.1810\n",
      "Epoch [1/5], Step [8364/10336], Loss: 3.4851\n",
      "Epoch [1/5], Step [8366/10336], Loss: 0.0992\n",
      "Epoch [1/5], Step [8368/10336], Loss: 0.0151\n",
      "Epoch [1/5], Step [8370/10336], Loss: 1.2729\n",
      "Epoch [1/5], Step [8372/10336], Loss: 0.9706\n",
      "Epoch [1/5], Step [8374/10336], Loss: 4.0148\n",
      "Epoch [1/5], Step [8376/10336], Loss: 0.2405\n",
      "Epoch [1/5], Step [8378/10336], Loss: 1.0781\n",
      "Epoch [1/5], Step [8380/10336], Loss: 2.5601\n",
      "Epoch [1/5], Step [8382/10336], Loss: 0.0717\n",
      "Epoch [1/5], Step [8384/10336], Loss: 0.3364\n",
      "Epoch [1/5], Step [8386/10336], Loss: 1.1183\n",
      "Epoch [1/5], Step [8388/10336], Loss: 3.5955\n",
      "Epoch [1/5], Step [8390/10336], Loss: 0.8616\n",
      "Epoch [1/5], Step [8392/10336], Loss: 0.9412\n",
      "Epoch [1/5], Step [8394/10336], Loss: 0.2851\n",
      "Epoch [1/5], Step [8396/10336], Loss: 0.2188\n",
      "Epoch [1/5], Step [8398/10336], Loss: 1.8938\n",
      "Epoch [1/5], Step [8400/10336], Loss: 0.1230\n",
      "Epoch [1/5], Step [8402/10336], Loss: 4.4995\n",
      "Epoch [1/5], Step [8404/10336], Loss: 3.5636\n",
      "Epoch [1/5], Step [8406/10336], Loss: 0.3810\n",
      "Epoch [1/5], Step [8408/10336], Loss: 0.4461\n",
      "Epoch [1/5], Step [8410/10336], Loss: 0.5543\n",
      "Epoch [1/5], Step [8412/10336], Loss: 0.5806\n",
      "Epoch [1/5], Step [8414/10336], Loss: 0.3092\n",
      "Epoch [1/5], Step [8416/10336], Loss: 0.9904\n",
      "Epoch [1/5], Step [8418/10336], Loss: 0.3233\n",
      "Epoch [1/5], Step [8420/10336], Loss: 2.8228\n",
      "Epoch [1/5], Step [8422/10336], Loss: 0.5229\n",
      "Epoch [1/5], Step [8424/10336], Loss: 0.2444\n",
      "Epoch [1/5], Step [8426/10336], Loss: 4.6577\n",
      "Epoch [1/5], Step [8428/10336], Loss: 0.7363\n",
      "Epoch [1/5], Step [8430/10336], Loss: 1.3845\n",
      "Epoch [1/5], Step [8432/10336], Loss: 0.8883\n",
      "Epoch [1/5], Step [8434/10336], Loss: 1.1411\n",
      "Epoch [1/5], Step [8436/10336], Loss: 2.5620\n",
      "Epoch [1/5], Step [8438/10336], Loss: 0.2680\n",
      "Epoch [1/5], Step [8440/10336], Loss: 3.9371\n",
      "Epoch [1/5], Step [8442/10336], Loss: 1.7706\n",
      "Epoch [1/5], Step [8444/10336], Loss: 1.0002\n",
      "Epoch [1/5], Step [8446/10336], Loss: 0.3871\n",
      "Epoch [1/5], Step [8448/10336], Loss: 0.4548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [8450/10336], Loss: 0.8530\n",
      "Epoch [1/5], Step [8452/10336], Loss: 0.2373\n",
      "Epoch [1/5], Step [8454/10336], Loss: 0.2353\n",
      "Epoch [1/5], Step [8456/10336], Loss: 1.4807\n",
      "Epoch [1/5], Step [8458/10336], Loss: 0.3279\n",
      "Epoch [1/5], Step [8460/10336], Loss: 2.3034\n",
      "Epoch [1/5], Step [8462/10336], Loss: 0.4889\n",
      "Epoch [1/5], Step [8464/10336], Loss: 0.3504\n",
      "Epoch [1/5], Step [8466/10336], Loss: 1.2430\n",
      "Epoch [1/5], Step [8468/10336], Loss: 0.4480\n",
      "Epoch [1/5], Step [8470/10336], Loss: 4.2067\n",
      "Epoch [1/5], Step [8472/10336], Loss: 3.5553\n",
      "Epoch [1/5], Step [8474/10336], Loss: 3.0843\n",
      "Epoch [1/5], Step [8476/10336], Loss: 1.7104\n",
      "Epoch [1/5], Step [8478/10336], Loss: 0.4755\n",
      "Epoch [1/5], Step [8480/10336], Loss: 0.6911\n",
      "Epoch [1/5], Step [8482/10336], Loss: 1.7684\n",
      "Epoch [1/5], Step [8484/10336], Loss: 0.0866\n",
      "Epoch [1/5], Step [8486/10336], Loss: 0.3965\n",
      "Epoch [1/5], Step [8488/10336], Loss: 0.0040\n",
      "Epoch [1/5], Step [8490/10336], Loss: 1.1965\n",
      "Epoch [1/5], Step [8492/10336], Loss: 0.8929\n",
      "Epoch [1/5], Step [8494/10336], Loss: 1.4602\n",
      "Epoch [1/5], Step [8496/10336], Loss: 0.3145\n",
      "Epoch [1/5], Step [8498/10336], Loss: 0.2904\n",
      "Epoch [1/5], Step [8500/10336], Loss: 5.2169\n",
      "Epoch [1/5], Step [8502/10336], Loss: 0.2411\n",
      "Epoch [1/5], Step [8504/10336], Loss: 0.6648\n",
      "Epoch [1/5], Step [8506/10336], Loss: 3.7378\n",
      "Epoch [1/5], Step [8508/10336], Loss: 0.0109\n",
      "Epoch [1/5], Step [8510/10336], Loss: 0.2239\n",
      "Epoch [1/5], Step [8512/10336], Loss: 0.4972\n",
      "Epoch [1/5], Step [8514/10336], Loss: 0.1810\n",
      "Epoch [1/5], Step [8516/10336], Loss: 1.2604\n",
      "Epoch [1/5], Step [8518/10336], Loss: 0.2256\n",
      "Epoch [1/5], Step [8520/10336], Loss: 0.0480\n",
      "Epoch [1/5], Step [8522/10336], Loss: 0.3334\n",
      "Epoch [1/5], Step [8524/10336], Loss: 0.6725\n",
      "Epoch [1/5], Step [8526/10336], Loss: 0.2267\n",
      "Epoch [1/5], Step [8528/10336], Loss: 0.0580\n",
      "Epoch [1/5], Step [8530/10336], Loss: 0.0897\n",
      "Epoch [1/5], Step [8532/10336], Loss: 0.4784\n",
      "Epoch [1/5], Step [8534/10336], Loss: 0.2311\n",
      "Epoch [1/5], Step [8536/10336], Loss: 0.3124\n",
      "Epoch [1/5], Step [8538/10336], Loss: 0.1863\n",
      "Epoch [1/5], Step [8540/10336], Loss: 0.0262\n",
      "Epoch [1/5], Step [8542/10336], Loss: 0.0122\n",
      "Epoch [1/5], Step [8544/10336], Loss: 0.4142\n",
      "Epoch [1/5], Step [8546/10336], Loss: 0.0067\n",
      "Epoch [1/5], Step [8548/10336], Loss: 1.1480\n",
      "Epoch [1/5], Step [8550/10336], Loss: 0.3518\n",
      "Epoch [1/5], Step [8552/10336], Loss: 0.2261\n",
      "Epoch [1/5], Step [8554/10336], Loss: 0.1985\n",
      "Epoch [1/5], Step [8556/10336], Loss: 3.0540\n",
      "Epoch [1/5], Step [8558/10336], Loss: 0.6703\n",
      "Epoch [1/5], Step [8560/10336], Loss: 0.4643\n",
      "Epoch [1/5], Step [8562/10336], Loss: 4.3600\n",
      "Epoch [1/5], Step [8564/10336], Loss: 0.3257\n",
      "Epoch [1/5], Step [8566/10336], Loss: 0.4714\n",
      "Epoch [1/5], Step [8568/10336], Loss: 0.3571\n",
      "Epoch [1/5], Step [8570/10336], Loss: 0.6394\n",
      "Epoch [1/5], Step [8572/10336], Loss: 0.0294\n",
      "Epoch [1/5], Step [8574/10336], Loss: 0.1675\n",
      "Epoch [1/5], Step [8576/10336], Loss: 0.3545\n",
      "Epoch [1/5], Step [8578/10336], Loss: 3.2212\n",
      "Epoch [1/5], Step [8580/10336], Loss: 0.5712\n",
      "Epoch [1/5], Step [8582/10336], Loss: 1.0675\n",
      "Epoch [1/5], Step [8584/10336], Loss: 0.0079\n",
      "Epoch [1/5], Step [8586/10336], Loss: 0.0041\n",
      "Epoch [1/5], Step [8588/10336], Loss: 3.1734\n",
      "Epoch [1/5], Step [8590/10336], Loss: 0.7585\n",
      "Epoch [1/5], Step [8592/10336], Loss: 0.2413\n",
      "Epoch [1/5], Step [8594/10336], Loss: 0.3516\n",
      "Epoch [1/5], Step [8596/10336], Loss: 0.3701\n",
      "Epoch [1/5], Step [8598/10336], Loss: 0.2577\n",
      "Epoch [1/5], Step [8600/10336], Loss: 0.0509\n",
      "Epoch [1/5], Step [8602/10336], Loss: 0.2683\n",
      "Epoch [1/5], Step [8604/10336], Loss: 0.8665\n",
      "Epoch [1/5], Step [8606/10336], Loss: 0.2240\n",
      "Epoch [1/5], Step [8608/10336], Loss: 0.0882\n",
      "Epoch [1/5], Step [8610/10336], Loss: 0.3926\n",
      "Epoch [1/5], Step [8612/10336], Loss: 1.1960\n",
      "Epoch [1/5], Step [8614/10336], Loss: 0.0079\n",
      "Epoch [1/5], Step [8616/10336], Loss: 0.3852\n",
      "Epoch [1/5], Step [8618/10336], Loss: 0.2989\n",
      "Epoch [1/5], Step [8620/10336], Loss: 0.6099\n",
      "Epoch [1/5], Step [8622/10336], Loss: 0.9034\n",
      "Epoch [1/5], Step [8624/10336], Loss: 0.2417\n",
      "Epoch [1/5], Step [8626/10336], Loss: 0.0408\n",
      "Epoch [1/5], Step [8628/10336], Loss: 4.0286\n",
      "Epoch [1/5], Step [8630/10336], Loss: 1.2143\n",
      "Epoch [1/5], Step [8632/10336], Loss: 5.1187\n",
      "Epoch [1/5], Step [8634/10336], Loss: 0.3395\n",
      "Epoch [1/5], Step [8636/10336], Loss: 0.7838\n",
      "Epoch [1/5], Step [8638/10336], Loss: 1.5330\n",
      "Epoch [1/5], Step [8640/10336], Loss: 0.1318\n",
      "Epoch [1/5], Step [8642/10336], Loss: 0.0257\n",
      "Epoch [1/5], Step [8644/10336], Loss: 2.8359\n",
      "Epoch [1/5], Step [8646/10336], Loss: 1.2420\n",
      "Epoch [1/5], Step [8648/10336], Loss: 0.3123\n",
      "Epoch [1/5], Step [8650/10336], Loss: 2.9961\n",
      "Epoch [1/5], Step [8652/10336], Loss: 2.0503\n",
      "Epoch [1/5], Step [8654/10336], Loss: 0.3935\n",
      "Epoch [1/5], Step [8656/10336], Loss: 0.8872\n",
      "Epoch [1/5], Step [8658/10336], Loss: 0.2368\n",
      "Epoch [1/5], Step [8660/10336], Loss: 1.9554\n",
      "Epoch [1/5], Step [8662/10336], Loss: 0.9065\n",
      "Epoch [1/5], Step [8664/10336], Loss: 1.0405\n",
      "Epoch [1/5], Step [8666/10336], Loss: 0.2702\n",
      "Epoch [1/5], Step [8668/10336], Loss: 0.0719\n",
      "Epoch [1/5], Step [8670/10336], Loss: 0.3136\n",
      "Epoch [1/5], Step [8672/10336], Loss: 0.3608\n",
      "Epoch [1/5], Step [8674/10336], Loss: 0.0905\n",
      "Epoch [1/5], Step [8676/10336], Loss: 0.3531\n",
      "Epoch [1/5], Step [8678/10336], Loss: 0.9726\n",
      "Epoch [1/5], Step [8680/10336], Loss: 0.1358\n",
      "Epoch [1/5], Step [8682/10336], Loss: 0.3690\n",
      "Epoch [1/5], Step [8684/10336], Loss: 1.9693\n",
      "Epoch [1/5], Step [8686/10336], Loss: 1.2070\n",
      "Epoch [1/5], Step [8688/10336], Loss: 0.5541\n",
      "Epoch [1/5], Step [8690/10336], Loss: 0.0479\n",
      "Epoch [1/5], Step [8692/10336], Loss: 2.2896\n",
      "Epoch [1/5], Step [8694/10336], Loss: 0.1647\n",
      "Epoch [1/5], Step [8696/10336], Loss: 1.0655\n",
      "Epoch [1/5], Step [8698/10336], Loss: 0.4135\n",
      "Epoch [1/5], Step [8700/10336], Loss: 0.6699\n",
      "Epoch [1/5], Step [8702/10336], Loss: 0.5408\n",
      "Epoch [1/5], Step [8704/10336], Loss: 3.5130\n",
      "Epoch [1/5], Step [8706/10336], Loss: 0.0930\n",
      "Epoch [1/5], Step [8708/10336], Loss: 0.9249\n",
      "Epoch [1/5], Step [8710/10336], Loss: 1.4424\n",
      "Epoch [1/5], Step [8712/10336], Loss: 1.0290\n",
      "Epoch [1/5], Step [8714/10336], Loss: 0.2681\n",
      "Epoch [1/5], Step [8716/10336], Loss: 0.9511\n",
      "Epoch [1/5], Step [8718/10336], Loss: 1.3277\n",
      "Epoch [1/5], Step [8720/10336], Loss: 0.8162\n",
      "Epoch [1/5], Step [8722/10336], Loss: 0.9539\n",
      "Epoch [1/5], Step [8724/10336], Loss: 0.3596\n",
      "Epoch [1/5], Step [8726/10336], Loss: 0.7320\n",
      "Epoch [1/5], Step [8728/10336], Loss: 1.4170\n",
      "Epoch [1/5], Step [8730/10336], Loss: 0.3830\n",
      "Epoch [1/5], Step [8732/10336], Loss: 4.4853\n",
      "Epoch [1/5], Step [8734/10336], Loss: 0.5503\n",
      "Epoch [1/5], Step [8736/10336], Loss: 2.8186\n",
      "Epoch [1/5], Step [8738/10336], Loss: 0.4309\n",
      "Epoch [1/5], Step [8740/10336], Loss: 3.1893\n",
      "Epoch [1/5], Step [8742/10336], Loss: 0.2397\n",
      "Epoch [1/5], Step [8744/10336], Loss: 0.1621\n",
      "Epoch [1/5], Step [8746/10336], Loss: 0.7981\n",
      "Epoch [1/5], Step [8748/10336], Loss: 0.2436\n",
      "Epoch [1/5], Step [8750/10336], Loss: 0.2607\n",
      "Epoch [1/5], Step [8752/10336], Loss: 0.2445\n",
      "Epoch [1/5], Step [8754/10336], Loss: 0.2587\n",
      "Epoch [1/5], Step [8756/10336], Loss: 1.8074\n",
      "Epoch [1/5], Step [8758/10336], Loss: 0.1271\n",
      "Epoch [1/5], Step [8760/10336], Loss: 0.1947\n",
      "Epoch [1/5], Step [8762/10336], Loss: 1.0451\n",
      "Epoch [1/5], Step [8764/10336], Loss: 1.2583\n",
      "Epoch [1/5], Step [8766/10336], Loss: 0.0442\n",
      "Epoch [1/5], Step [8768/10336], Loss: 1.0764\n",
      "Epoch [1/5], Step [8770/10336], Loss: 3.3506\n",
      "Epoch [1/5], Step [8772/10336], Loss: 1.3556\n",
      "Epoch [1/5], Step [8774/10336], Loss: 0.4361\n",
      "Epoch [1/5], Step [8776/10336], Loss: 0.1534\n",
      "Epoch [1/5], Step [8778/10336], Loss: 0.2670\n",
      "Epoch [1/5], Step [8780/10336], Loss: 3.0178\n",
      "Epoch [1/5], Step [8782/10336], Loss: 0.1617\n",
      "Epoch [1/5], Step [8784/10336], Loss: 0.3245\n",
      "Epoch [1/5], Step [8786/10336], Loss: 1.8326\n",
      "Epoch [1/5], Step [8788/10336], Loss: 0.0189\n",
      "Epoch [1/5], Step [8790/10336], Loss: 1.1186\n",
      "Epoch [1/5], Step [8792/10336], Loss: 3.0338\n",
      "Epoch [1/5], Step [8794/10336], Loss: 0.9966\n",
      "Epoch [1/5], Step [8796/10336], Loss: 0.8792\n",
      "Epoch [1/5], Step [8798/10336], Loss: 0.2249\n",
      "Epoch [1/5], Step [8800/10336], Loss: 0.7718\n",
      "Epoch [1/5], Step [8802/10336], Loss: 0.5763\n",
      "Epoch [1/5], Step [8804/10336], Loss: 0.2576\n",
      "Epoch [1/5], Step [8806/10336], Loss: 2.4016\n",
      "Epoch [1/5], Step [8808/10336], Loss: 0.7512\n",
      "Epoch [1/5], Step [8810/10336], Loss: 0.2737\n",
      "Epoch [1/5], Step [8812/10336], Loss: 0.2919\n",
      "Epoch [1/5], Step [8814/10336], Loss: 4.6931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [8816/10336], Loss: 0.0692\n",
      "Epoch [1/5], Step [8818/10336], Loss: 1.8475\n",
      "Epoch [1/5], Step [8820/10336], Loss: 0.6829\n",
      "Epoch [1/5], Step [8822/10336], Loss: 1.5959\n",
      "Epoch [1/5], Step [8824/10336], Loss: 0.3395\n",
      "Epoch [1/5], Step [8826/10336], Loss: 0.5195\n",
      "Epoch [1/5], Step [8828/10336], Loss: 0.2891\n",
      "Epoch [1/5], Step [8830/10336], Loss: 2.8225\n",
      "Epoch [1/5], Step [8832/10336], Loss: 0.3680\n",
      "Epoch [1/5], Step [8834/10336], Loss: 0.2187\n",
      "Epoch [1/5], Step [8836/10336], Loss: 1.1045\n",
      "Epoch [1/5], Step [8838/10336], Loss: 0.3715\n",
      "Epoch [1/5], Step [8840/10336], Loss: 0.6595\n",
      "Epoch [1/5], Step [8842/10336], Loss: 0.1808\n",
      "Epoch [1/5], Step [8844/10336], Loss: 0.3839\n",
      "Epoch [1/5], Step [8846/10336], Loss: 1.7325\n",
      "Epoch [1/5], Step [8848/10336], Loss: 1.5974\n",
      "Epoch [1/5], Step [8850/10336], Loss: 1.2449\n",
      "Epoch [1/5], Step [8852/10336], Loss: 0.0290\n",
      "Epoch [1/5], Step [8854/10336], Loss: 0.4917\n",
      "Epoch [1/5], Step [8856/10336], Loss: 0.5027\n",
      "Epoch [1/5], Step [8858/10336], Loss: 0.2357\n",
      "Epoch [1/5], Step [8860/10336], Loss: 0.4078\n",
      "Epoch [1/5], Step [8862/10336], Loss: 0.3385\n",
      "Epoch [1/5], Step [8864/10336], Loss: 4.3174\n",
      "Epoch [1/5], Step [8866/10336], Loss: 0.7688\n",
      "Epoch [1/5], Step [8868/10336], Loss: 2.6393\n",
      "Epoch [1/5], Step [8870/10336], Loss: 1.2025\n",
      "Epoch [1/5], Step [8872/10336], Loss: 5.0820\n",
      "Epoch [1/5], Step [8874/10336], Loss: 0.1667\n",
      "Epoch [1/5], Step [8876/10336], Loss: 0.0680\n",
      "Epoch [1/5], Step [8878/10336], Loss: 6.1000\n",
      "Epoch [1/5], Step [8880/10336], Loss: 2.0937\n",
      "Epoch [1/5], Step [8882/10336], Loss: 0.3725\n",
      "Epoch [1/5], Step [8884/10336], Loss: 0.2414\n",
      "Epoch [1/5], Step [8886/10336], Loss: 0.9307\n",
      "Epoch [1/5], Step [8888/10336], Loss: 0.9135\n",
      "Epoch [1/5], Step [8890/10336], Loss: 3.9913\n",
      "Epoch [1/5], Step [8892/10336], Loss: 0.3168\n",
      "Epoch [1/5], Step [8894/10336], Loss: 0.0385\n",
      "Epoch [1/5], Step [8896/10336], Loss: 1.5551\n",
      "Epoch [1/5], Step [8898/10336], Loss: 2.8034\n",
      "Epoch [1/5], Step [8900/10336], Loss: 0.1831\n",
      "Epoch [1/5], Step [8902/10336], Loss: 2.0042\n",
      "Epoch [1/5], Step [8904/10336], Loss: 1.7537\n",
      "Epoch [1/5], Step [8906/10336], Loss: 0.2223\n",
      "Epoch [1/5], Step [8908/10336], Loss: 4.8758\n",
      "Epoch [1/5], Step [8910/10336], Loss: 0.4085\n",
      "Epoch [1/5], Step [8912/10336], Loss: 0.7681\n",
      "Epoch [1/5], Step [8914/10336], Loss: 0.1840\n",
      "Epoch [1/5], Step [8916/10336], Loss: 0.0074\n",
      "Epoch [1/5], Step [8918/10336], Loss: 1.6892\n",
      "Epoch [1/5], Step [8920/10336], Loss: 3.4698\n",
      "Epoch [1/5], Step [8922/10336], Loss: 0.6278\n",
      "Epoch [1/5], Step [8924/10336], Loss: 0.2132\n",
      "Epoch [1/5], Step [8926/10336], Loss: 0.2982\n",
      "Epoch [1/5], Step [8928/10336], Loss: 0.3105\n",
      "Epoch [1/5], Step [8930/10336], Loss: 1.1936\n",
      "Epoch [1/5], Step [8932/10336], Loss: 0.9921\n",
      "Epoch [1/5], Step [8934/10336], Loss: 2.1460\n",
      "Epoch [1/5], Step [8936/10336], Loss: 0.3531\n",
      "Epoch [1/5], Step [8938/10336], Loss: 0.5611\n",
      "Epoch [1/5], Step [8940/10336], Loss: 1.9579\n",
      "Epoch [1/5], Step [8942/10336], Loss: 0.3972\n",
      "Epoch [1/5], Step [8944/10336], Loss: 0.1044\n",
      "Epoch [1/5], Step [8946/10336], Loss: 3.7929\n",
      "Epoch [1/5], Step [8948/10336], Loss: 0.4488\n",
      "Epoch [1/5], Step [8950/10336], Loss: 1.7682\n",
      "Epoch [1/5], Step [8952/10336], Loss: 0.6012\n",
      "Epoch [1/5], Step [8954/10336], Loss: 3.3678\n",
      "Epoch [1/5], Step [8956/10336], Loss: 0.4180\n",
      "Epoch [1/5], Step [8958/10336], Loss: 0.5248\n",
      "Epoch [1/5], Step [8960/10336], Loss: 2.0281\n",
      "Epoch [1/5], Step [8962/10336], Loss: 0.2749\n",
      "Epoch [1/5], Step [8964/10336], Loss: 0.2847\n",
      "Epoch [1/5], Step [8966/10336], Loss: 3.3419\n",
      "Epoch [1/5], Step [8968/10336], Loss: 0.2671\n",
      "Epoch [1/5], Step [8970/10336], Loss: 0.7051\n",
      "Epoch [1/5], Step [8972/10336], Loss: 0.5833\n",
      "Epoch [1/5], Step [8974/10336], Loss: 0.4529\n",
      "Epoch [1/5], Step [8976/10336], Loss: 0.4189\n",
      "Epoch [1/5], Step [8978/10336], Loss: 0.2915\n",
      "Epoch [1/5], Step [8980/10336], Loss: 3.0345\n",
      "Epoch [1/5], Step [8982/10336], Loss: 0.1863\n",
      "Epoch [1/5], Step [8984/10336], Loss: 0.3588\n",
      "Epoch [1/5], Step [8986/10336], Loss: 4.6328\n",
      "Epoch [1/5], Step [8988/10336], Loss: 0.9612\n",
      "Epoch [1/5], Step [8990/10336], Loss: 0.2079\n",
      "Epoch [1/5], Step [8992/10336], Loss: 1.7202\n",
      "Epoch [1/5], Step [8994/10336], Loss: 3.3743\n",
      "Epoch [1/5], Step [8996/10336], Loss: 0.4261\n",
      "Epoch [1/5], Step [8998/10336], Loss: 1.6055\n",
      "Epoch [1/5], Step [9000/10336], Loss: 0.4204\n",
      "Epoch [1/5], Step [9002/10336], Loss: 0.2940\n",
      "Epoch [1/5], Step [9004/10336], Loss: 3.3751\n",
      "Epoch [1/5], Step [9006/10336], Loss: 0.9783\n",
      "Epoch [1/5], Step [9008/10336], Loss: 0.2600\n",
      "Epoch [1/5], Step [9010/10336], Loss: 1.7658\n",
      "Epoch [1/5], Step [9012/10336], Loss: 0.1638\n",
      "Epoch [1/5], Step [9014/10336], Loss: 4.7799\n",
      "Epoch [1/5], Step [9016/10336], Loss: 0.8445\n",
      "Epoch [1/5], Step [9018/10336], Loss: 3.6063\n",
      "Epoch [1/5], Step [9020/10336], Loss: 1.1161\n",
      "Epoch [1/5], Step [9022/10336], Loss: 0.0239\n",
      "Epoch [1/5], Step [9024/10336], Loss: 3.9754\n",
      "Epoch [1/5], Step [9026/10336], Loss: 3.9833\n",
      "Epoch [1/5], Step [9028/10336], Loss: 0.7155\n",
      "Epoch [1/5], Step [9030/10336], Loss: 0.8003\n",
      "Epoch [1/5], Step [9032/10336], Loss: 0.3389\n",
      "Epoch [1/5], Step [9034/10336], Loss: 2.6658\n",
      "Epoch [1/5], Step [9036/10336], Loss: 0.3692\n",
      "Epoch [1/5], Step [9038/10336], Loss: 1.3118\n",
      "Epoch [1/5], Step [9040/10336], Loss: 1.3976\n",
      "Epoch [1/5], Step [9042/10336], Loss: 0.7371\n",
      "Epoch [1/5], Step [9044/10336], Loss: 0.0207\n",
      "Epoch [1/5], Step [9046/10336], Loss: 0.4947\n",
      "Epoch [1/5], Step [9048/10336], Loss: 0.1649\n",
      "Epoch [1/5], Step [9050/10336], Loss: 0.4585\n",
      "Epoch [1/5], Step [9052/10336], Loss: 0.1707\n",
      "Epoch [1/5], Step [9054/10336], Loss: 0.1967\n",
      "Epoch [1/5], Step [9056/10336], Loss: 1.1547\n",
      "Epoch [1/5], Step [9058/10336], Loss: 5.5786\n",
      "Epoch [1/5], Step [9060/10336], Loss: 0.6613\n",
      "Epoch [1/5], Step [9062/10336], Loss: 2.9076\n",
      "Epoch [1/5], Step [9064/10336], Loss: 4.3593\n",
      "Epoch [1/5], Step [9066/10336], Loss: 3.1010\n",
      "Epoch [1/5], Step [9068/10336], Loss: 0.6292\n",
      "Epoch [1/5], Step [9070/10336], Loss: 0.0502\n",
      "Epoch [1/5], Step [9072/10336], Loss: 0.8676\n",
      "Epoch [1/5], Step [9074/10336], Loss: 5.3289\n",
      "Epoch [1/5], Step [9076/10336], Loss: 0.3156\n",
      "Epoch [1/5], Step [9078/10336], Loss: 0.2327\n",
      "Epoch [1/5], Step [9080/10336], Loss: 0.3792\n",
      "Epoch [1/5], Step [9082/10336], Loss: 1.5016\n",
      "Epoch [1/5], Step [9084/10336], Loss: 0.1462\n",
      "Epoch [1/5], Step [9086/10336], Loss: 3.6512\n",
      "Epoch [1/5], Step [9088/10336], Loss: 0.2138\n",
      "Epoch [1/5], Step [9090/10336], Loss: 0.5200\n",
      "Epoch [1/5], Step [9092/10336], Loss: 1.5985\n",
      "Epoch [1/5], Step [9094/10336], Loss: 0.5952\n",
      "Epoch [1/5], Step [9096/10336], Loss: 0.3981\n",
      "Epoch [1/5], Step [9098/10336], Loss: 0.6548\n",
      "Epoch [1/5], Step [9100/10336], Loss: 2.5059\n",
      "Epoch [1/5], Step [9102/10336], Loss: 0.1129\n",
      "Epoch [1/5], Step [9104/10336], Loss: 4.1542\n",
      "Epoch [1/5], Step [9106/10336], Loss: 0.1926\n",
      "Epoch [1/5], Step [9108/10336], Loss: 4.7891\n",
      "Epoch [1/5], Step [9110/10336], Loss: 0.0144\n",
      "Epoch [1/5], Step [9112/10336], Loss: 3.2130\n",
      "Epoch [1/5], Step [9114/10336], Loss: 1.2417\n",
      "Epoch [1/5], Step [9116/10336], Loss: 1.7542\n",
      "Epoch [1/5], Step [9118/10336], Loss: 0.2505\n",
      "Epoch [1/5], Step [9120/10336], Loss: 0.6517\n",
      "Epoch [1/5], Step [9122/10336], Loss: 0.2293\n",
      "Epoch [1/5], Step [9124/10336], Loss: 0.2446\n",
      "Epoch [1/5], Step [9126/10336], Loss: 0.6510\n",
      "Epoch [1/5], Step [9128/10336], Loss: 0.7327\n",
      "Epoch [1/5], Step [9130/10336], Loss: 3.2857\n",
      "Epoch [1/5], Step [9132/10336], Loss: 1.8810\n",
      "Epoch [1/5], Step [9134/10336], Loss: 0.0058\n",
      "Epoch [1/5], Step [9136/10336], Loss: 0.4696\n",
      "Epoch [1/5], Step [9138/10336], Loss: 0.0167\n",
      "Epoch [1/5], Step [9140/10336], Loss: 0.5972\n",
      "Epoch [1/5], Step [9142/10336], Loss: 0.1837\n",
      "Epoch [1/5], Step [9144/10336], Loss: 0.0304\n",
      "Epoch [1/5], Step [9146/10336], Loss: 1.0965\n",
      "Epoch [1/5], Step [9148/10336], Loss: 3.8253\n",
      "Epoch [1/5], Step [9150/10336], Loss: 0.3457\n",
      "Epoch [1/5], Step [9152/10336], Loss: 0.0315\n",
      "Epoch [1/5], Step [9154/10336], Loss: 2.6580\n",
      "Epoch [1/5], Step [9156/10336], Loss: 0.4342\n",
      "Epoch [1/5], Step [9158/10336], Loss: 0.5489\n",
      "Epoch [1/5], Step [9160/10336], Loss: 0.2347\n",
      "Epoch [1/5], Step [9162/10336], Loss: 1.7921\n",
      "Epoch [1/5], Step [9164/10336], Loss: 0.3463\n",
      "Epoch [1/5], Step [9166/10336], Loss: 0.6208\n",
      "Epoch [1/5], Step [9168/10336], Loss: 5.0538\n",
      "Epoch [1/5], Step [9170/10336], Loss: 1.4325\n",
      "Epoch [1/5], Step [9172/10336], Loss: 4.5234\n",
      "Epoch [1/5], Step [9174/10336], Loss: 0.0926\n",
      "Epoch [1/5], Step [9176/10336], Loss: 0.8212\n",
      "Epoch [1/5], Step [9178/10336], Loss: 0.3822\n",
      "Epoch [1/5], Step [9180/10336], Loss: 4.9490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [9182/10336], Loss: 4.3472\n",
      "Epoch [1/5], Step [9184/10336], Loss: 0.1613\n",
      "Epoch [1/5], Step [9186/10336], Loss: 2.5331\n",
      "Epoch [1/5], Step [9188/10336], Loss: 1.0517\n",
      "Epoch [1/5], Step [9190/10336], Loss: 3.3770\n",
      "Epoch [1/5], Step [9192/10336], Loss: 0.1181\n",
      "Epoch [1/5], Step [9194/10336], Loss: 0.3855\n",
      "Epoch [1/5], Step [9196/10336], Loss: 0.3134\n",
      "Epoch [1/5], Step [9198/10336], Loss: 1.2919\n",
      "Epoch [1/5], Step [9200/10336], Loss: 0.5378\n",
      "Epoch [1/5], Step [9202/10336], Loss: 1.7870\n",
      "Epoch [1/5], Step [9204/10336], Loss: 1.3432\n",
      "Epoch [1/5], Step [9206/10336], Loss: 0.4479\n",
      "Epoch [1/5], Step [9208/10336], Loss: 4.0213\n",
      "Epoch [1/5], Step [9210/10336], Loss: 0.1900\n",
      "Epoch [1/5], Step [9212/10336], Loss: 0.1383\n",
      "Epoch [1/5], Step [9214/10336], Loss: 0.3928\n",
      "Epoch [1/5], Step [9216/10336], Loss: 0.2135\n",
      "Epoch [1/5], Step [9218/10336], Loss: 0.1649\n",
      "Epoch [1/5], Step [9220/10336], Loss: 0.5075\n",
      "Epoch [1/5], Step [9222/10336], Loss: 4.2524\n",
      "Epoch [1/5], Step [9224/10336], Loss: 0.2639\n",
      "Epoch [1/5], Step [9226/10336], Loss: 2.6780\n",
      "Epoch [1/5], Step [9228/10336], Loss: 0.8402\n",
      "Epoch [1/5], Step [9230/10336], Loss: 0.0320\n",
      "Epoch [1/5], Step [9232/10336], Loss: 4.6579\n",
      "Epoch [1/5], Step [9234/10336], Loss: 0.8417\n",
      "Epoch [1/5], Step [9236/10336], Loss: 2.9289\n",
      "Epoch [1/5], Step [9238/10336], Loss: 0.1199\n",
      "Epoch [1/5], Step [9240/10336], Loss: 1.1469\n",
      "Epoch [1/5], Step [9242/10336], Loss: 1.5102\n",
      "Epoch [1/5], Step [9244/10336], Loss: 0.5279\n",
      "Epoch [1/5], Step [9246/10336], Loss: 0.5139\n",
      "Epoch [1/5], Step [9248/10336], Loss: 0.4852\n",
      "Epoch [1/5], Step [9250/10336], Loss: 2.0572\n",
      "Epoch [1/5], Step [9252/10336], Loss: 0.1623\n",
      "Epoch [1/5], Step [9254/10336], Loss: 0.5125\n",
      "Epoch [1/5], Step [9256/10336], Loss: 4.9711\n",
      "Epoch [1/5], Step [9258/10336], Loss: 0.4003\n",
      "Epoch [1/5], Step [9260/10336], Loss: 0.3403\n",
      "Epoch [1/5], Step [9262/10336], Loss: 0.0732\n",
      "Epoch [1/5], Step [9264/10336], Loss: 0.4041\n",
      "Epoch [1/5], Step [9266/10336], Loss: 0.3341\n",
      "Epoch [1/5], Step [9268/10336], Loss: 0.3571\n",
      "Epoch [1/5], Step [9270/10336], Loss: 0.1265\n",
      "Epoch [1/5], Step [9272/10336], Loss: 0.2730\n",
      "Epoch [1/5], Step [9274/10336], Loss: 0.3995\n",
      "Epoch [1/5], Step [9276/10336], Loss: 0.0452\n",
      "Epoch [1/5], Step [9278/10336], Loss: 0.9510\n",
      "Epoch [1/5], Step [9280/10336], Loss: 0.1743\n",
      "Epoch [1/5], Step [9282/10336], Loss: 0.3934\n",
      "Epoch [1/5], Step [9284/10336], Loss: 0.6363\n",
      "Epoch [1/5], Step [9286/10336], Loss: 4.0605\n",
      "Epoch [1/5], Step [9288/10336], Loss: 3.3085\n",
      "Epoch [1/5], Step [9290/10336], Loss: 0.6326\n",
      "Epoch [1/5], Step [9292/10336], Loss: 0.4943\n",
      "Epoch [1/5], Step [9294/10336], Loss: 0.1277\n",
      "Epoch [1/5], Step [9296/10336], Loss: 2.9195\n",
      "Epoch [1/5], Step [9298/10336], Loss: 1.3518\n",
      "Epoch [1/5], Step [9300/10336], Loss: 4.5534\n",
      "Epoch [1/5], Step [9302/10336], Loss: 0.3422\n",
      "Epoch [1/5], Step [9304/10336], Loss: 2.3575\n",
      "Epoch [1/5], Step [9306/10336], Loss: 0.7847\n",
      "Epoch [1/5], Step [9308/10336], Loss: 1.5582\n",
      "Epoch [1/5], Step [9310/10336], Loss: 0.6583\n",
      "Epoch [1/5], Step [9312/10336], Loss: 1.6134\n",
      "Epoch [1/5], Step [9314/10336], Loss: 2.6903\n",
      "Epoch [1/5], Step [9316/10336], Loss: 2.6297\n",
      "Epoch [1/5], Step [9318/10336], Loss: 0.8883\n",
      "Epoch [1/5], Step [9320/10336], Loss: 0.5104\n",
      "Epoch [1/5], Step [9322/10336], Loss: 0.2555\n",
      "Epoch [1/5], Step [9324/10336], Loss: 0.2539\n",
      "Epoch [1/5], Step [9326/10336], Loss: 0.2353\n",
      "Epoch [1/5], Step [9328/10336], Loss: 0.3346\n",
      "Epoch [1/5], Step [9330/10336], Loss: 0.3804\n",
      "Epoch [1/5], Step [9332/10336], Loss: 0.1024\n",
      "Epoch [1/5], Step [9334/10336], Loss: 0.7007\n",
      "Epoch [1/5], Step [9336/10336], Loss: 0.3391\n",
      "Epoch [1/5], Step [9338/10336], Loss: 3.8675\n",
      "Epoch [1/5], Step [9340/10336], Loss: 0.0517\n",
      "Epoch [1/5], Step [9342/10336], Loss: 2.2540\n",
      "Epoch [1/5], Step [9344/10336], Loss: 0.2629\n",
      "Epoch [1/5], Step [9346/10336], Loss: 0.3695\n",
      "Epoch [1/5], Step [9348/10336], Loss: 0.1912\n",
      "Epoch [1/5], Step [9350/10336], Loss: 0.7502\n",
      "Epoch [1/5], Step [9352/10336], Loss: 2.4100\n",
      "Epoch [1/5], Step [9354/10336], Loss: 1.4550\n",
      "Epoch [1/5], Step [9356/10336], Loss: 0.2588\n",
      "Epoch [1/5], Step [9358/10336], Loss: 0.0433\n",
      "Epoch [1/5], Step [9360/10336], Loss: 2.6292\n",
      "Epoch [1/5], Step [9362/10336], Loss: 3.3728\n",
      "Epoch [1/5], Step [9364/10336], Loss: 0.9591\n",
      "Epoch [1/5], Step [9366/10336], Loss: 0.3997\n",
      "Epoch [1/5], Step [9368/10336], Loss: 0.1456\n",
      "Epoch [1/5], Step [9370/10336], Loss: 1.0503\n",
      "Epoch [1/5], Step [9372/10336], Loss: 0.8685\n",
      "Epoch [1/5], Step [9374/10336], Loss: 0.2693\n",
      "Epoch [1/5], Step [9376/10336], Loss: 0.1551\n",
      "Epoch [1/5], Step [9378/10336], Loss: 0.0729\n",
      "Epoch [1/5], Step [9380/10336], Loss: 4.8931\n",
      "Epoch [1/5], Step [9382/10336], Loss: 0.1375\n",
      "Epoch [1/5], Step [9384/10336], Loss: 0.8383\n",
      "Epoch [1/5], Step [9386/10336], Loss: 3.8114\n",
      "Epoch [1/5], Step [9388/10336], Loss: 0.3291\n",
      "Epoch [1/5], Step [9390/10336], Loss: 2.1551\n",
      "Epoch [1/5], Step [9392/10336], Loss: 0.7852\n",
      "Epoch [1/5], Step [9394/10336], Loss: 0.7535\n",
      "Epoch [1/5], Step [9396/10336], Loss: 1.6819\n",
      "Epoch [1/5], Step [9398/10336], Loss: 3.9583\n",
      "Epoch [1/5], Step [9400/10336], Loss: 0.3977\n",
      "Epoch [1/5], Step [9402/10336], Loss: 1.0898\n",
      "Epoch [1/5], Step [9404/10336], Loss: 0.2661\n",
      "Epoch [1/5], Step [9406/10336], Loss: 2.9503\n",
      "Epoch [1/5], Step [9408/10336], Loss: 3.0271\n",
      "Epoch [1/5], Step [9410/10336], Loss: 0.3840\n",
      "Epoch [1/5], Step [9412/10336], Loss: 1.0734\n",
      "Epoch [1/5], Step [9414/10336], Loss: 2.0079\n",
      "Epoch [1/5], Step [9416/10336], Loss: 4.3805\n",
      "Epoch [1/5], Step [9418/10336], Loss: 1.1671\n",
      "Epoch [1/5], Step [9420/10336], Loss: 2.0642\n",
      "Epoch [1/5], Step [9422/10336], Loss: 0.1037\n",
      "Epoch [1/5], Step [9424/10336], Loss: 3.3330\n",
      "Epoch [1/5], Step [9426/10336], Loss: 0.2675\n",
      "Epoch [1/5], Step [9428/10336], Loss: 0.9017\n",
      "Epoch [1/5], Step [9430/10336], Loss: 0.1574\n",
      "Epoch [1/5], Step [9432/10336], Loss: 0.4488\n",
      "Epoch [1/5], Step [9434/10336], Loss: 1.3474\n",
      "Epoch [1/5], Step [9436/10336], Loss: 0.2250\n",
      "Epoch [1/5], Step [9438/10336], Loss: 0.1687\n",
      "Epoch [1/5], Step [9440/10336], Loss: 0.2444\n",
      "Epoch [1/5], Step [9442/10336], Loss: 0.2971\n",
      "Epoch [1/5], Step [9444/10336], Loss: 0.4854\n",
      "Epoch [1/5], Step [9446/10336], Loss: 2.0029\n",
      "Epoch [1/5], Step [9448/10336], Loss: 0.5009\n",
      "Epoch [1/5], Step [9450/10336], Loss: 1.1628\n",
      "Epoch [1/5], Step [9452/10336], Loss: 0.0963\n",
      "Epoch [1/5], Step [9454/10336], Loss: 0.4174\n",
      "Epoch [1/5], Step [9456/10336], Loss: 0.5326\n",
      "Epoch [1/5], Step [9458/10336], Loss: 0.8837\n",
      "Epoch [1/5], Step [9460/10336], Loss: 0.2777\n",
      "Epoch [1/5], Step [9462/10336], Loss: 0.3228\n",
      "Epoch [1/5], Step [9464/10336], Loss: 0.0566\n",
      "Epoch [1/5], Step [9466/10336], Loss: 0.4799\n",
      "Epoch [1/5], Step [9468/10336], Loss: 0.3920\n",
      "Epoch [1/5], Step [9470/10336], Loss: 0.6616\n",
      "Epoch [1/5], Step [9472/10336], Loss: 4.0573\n",
      "Epoch [1/5], Step [9474/10336], Loss: 0.6375\n",
      "Epoch [1/5], Step [9476/10336], Loss: 0.9818\n",
      "Epoch [1/5], Step [9478/10336], Loss: 0.1853\n",
      "Epoch [1/5], Step [9480/10336], Loss: 0.0657\n",
      "Epoch [1/5], Step [9482/10336], Loss: 1.2818\n",
      "Epoch [1/5], Step [9484/10336], Loss: 4.6958\n",
      "Epoch [1/5], Step [9486/10336], Loss: 0.3400\n",
      "Epoch [1/5], Step [9488/10336], Loss: 0.0391\n",
      "Epoch [1/5], Step [9490/10336], Loss: 0.8653\n",
      "Epoch [1/5], Step [9492/10336], Loss: 0.2146\n",
      "Epoch [1/5], Step [9494/10336], Loss: 1.9667\n",
      "Epoch [1/5], Step [9496/10336], Loss: 0.7202\n",
      "Epoch [1/5], Step [9498/10336], Loss: 1.3978\n",
      "Epoch [1/5], Step [9500/10336], Loss: 0.2413\n",
      "Epoch [1/5], Step [9502/10336], Loss: 0.9726\n",
      "Epoch [1/5], Step [9504/10336], Loss: 0.5666\n",
      "Epoch [1/5], Step [9506/10336], Loss: 4.1150\n",
      "Epoch [1/5], Step [9508/10336], Loss: 0.6418\n",
      "Epoch [1/5], Step [9510/10336], Loss: 1.1038\n",
      "Epoch [1/5], Step [9512/10336], Loss: 4.1265\n",
      "Epoch [1/5], Step [9514/10336], Loss: 0.4533\n",
      "Epoch [1/5], Step [9516/10336], Loss: 0.8875\n",
      "Epoch [1/5], Step [9518/10336], Loss: 0.4446\n",
      "Epoch [1/5], Step [9520/10336], Loss: 0.0955\n",
      "Epoch [1/5], Step [9522/10336], Loss: 0.4706\n",
      "Epoch [1/5], Step [9524/10336], Loss: 1.0715\n",
      "Epoch [1/5], Step [9526/10336], Loss: 2.4919\n",
      "Epoch [1/5], Step [9528/10336], Loss: 1.0713\n",
      "Epoch [1/5], Step [9530/10336], Loss: 1.3244\n",
      "Epoch [1/5], Step [9532/10336], Loss: 0.3610\n",
      "Epoch [1/5], Step [9534/10336], Loss: 1.5439\n",
      "Epoch [1/5], Step [9536/10336], Loss: 0.1800\n",
      "Epoch [1/5], Step [9538/10336], Loss: 4.5474\n",
      "Epoch [1/5], Step [9540/10336], Loss: 0.6944\n",
      "Epoch [1/5], Step [9542/10336], Loss: 1.2331\n",
      "Epoch [1/5], Step [9544/10336], Loss: 0.8416\n",
      "Epoch [1/5], Step [9546/10336], Loss: 0.3285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [9548/10336], Loss: 0.0289\n",
      "Epoch [1/5], Step [9550/10336], Loss: 0.3740\n",
      "Epoch [1/5], Step [9552/10336], Loss: 0.0042\n",
      "Epoch [1/5], Step [9554/10336], Loss: 1.6927\n",
      "Epoch [1/5], Step [9556/10336], Loss: 0.0656\n",
      "Epoch [1/5], Step [9558/10336], Loss: 0.2749\n",
      "Epoch [1/5], Step [9560/10336], Loss: 0.3619\n",
      "Epoch [1/5], Step [9562/10336], Loss: 0.0101\n",
      "Epoch [1/5], Step [9564/10336], Loss: 0.5644\n",
      "Epoch [1/5], Step [9566/10336], Loss: 5.3249\n",
      "Epoch [1/5], Step [9568/10336], Loss: 0.4738\n",
      "Epoch [1/5], Step [9570/10336], Loss: 2.9674\n",
      "Epoch [1/5], Step [9572/10336], Loss: 2.9557\n",
      "Epoch [1/5], Step [9574/10336], Loss: 0.2742\n",
      "Epoch [1/5], Step [9576/10336], Loss: 2.8413\n",
      "Epoch [1/5], Step [9578/10336], Loss: 0.1932\n",
      "Epoch [1/5], Step [9580/10336], Loss: 0.0408\n",
      "Epoch [1/5], Step [9582/10336], Loss: 0.2209\n",
      "Epoch [1/5], Step [9584/10336], Loss: 0.2831\n",
      "Epoch [1/5], Step [9586/10336], Loss: 4.0426\n",
      "Epoch [1/5], Step [9588/10336], Loss: 0.2537\n",
      "Epoch [1/5], Step [9590/10336], Loss: 2.7737\n",
      "Epoch [1/5], Step [9592/10336], Loss: 0.6083\n",
      "Epoch [1/5], Step [9594/10336], Loss: 0.5108\n",
      "Epoch [1/5], Step [9596/10336], Loss: 1.2215\n",
      "Epoch [1/5], Step [9598/10336], Loss: 0.8544\n",
      "Epoch [1/5], Step [9600/10336], Loss: 3.5630\n",
      "Epoch [1/5], Step [9602/10336], Loss: 0.9375\n",
      "Epoch [1/5], Step [9604/10336], Loss: 0.0519\n",
      "Epoch [1/5], Step [9606/10336], Loss: 0.0183\n",
      "Epoch [1/5], Step [9608/10336], Loss: 1.6502\n",
      "Epoch [1/5], Step [9610/10336], Loss: 0.2680\n",
      "Epoch [1/5], Step [9612/10336], Loss: 3.5754\n",
      "Epoch [1/5], Step [9614/10336], Loss: 1.2508\n",
      "Epoch [1/5], Step [9616/10336], Loss: 0.7411\n",
      "Epoch [1/5], Step [9618/10336], Loss: 1.7644\n",
      "Epoch [1/5], Step [9620/10336], Loss: 2.1547\n",
      "Epoch [1/5], Step [9622/10336], Loss: 0.4261\n",
      "Epoch [1/5], Step [9624/10336], Loss: 0.2575\n",
      "Epoch [1/5], Step [9626/10336], Loss: 0.0418\n",
      "Epoch [1/5], Step [9628/10336], Loss: 0.2381\n",
      "Epoch [1/5], Step [9630/10336], Loss: 0.3210\n",
      "Epoch [1/5], Step [9632/10336], Loss: 2.4053\n",
      "Epoch [1/5], Step [9634/10336], Loss: 0.3020\n",
      "Epoch [1/5], Step [9636/10336], Loss: 0.2255\n",
      "Epoch [1/5], Step [9638/10336], Loss: 0.1777\n",
      "Epoch [1/5], Step [9640/10336], Loss: 2.8152\n",
      "Epoch [1/5], Step [9642/10336], Loss: 2.5608\n",
      "Epoch [1/5], Step [9644/10336], Loss: 1.3228\n",
      "Epoch [1/5], Step [9646/10336], Loss: 0.5652\n",
      "Epoch [1/5], Step [9648/10336], Loss: 3.8523\n",
      "Epoch [1/5], Step [9650/10336], Loss: 0.5680\n",
      "Epoch [1/5], Step [9652/10336], Loss: 1.0515\n",
      "Epoch [1/5], Step [9654/10336], Loss: 0.4941\n",
      "Epoch [1/5], Step [9656/10336], Loss: 0.3405\n",
      "Epoch [1/5], Step [9658/10336], Loss: 2.0061\n",
      "Epoch [1/5], Step [9660/10336], Loss: 0.2689\n",
      "Epoch [1/5], Step [9662/10336], Loss: 0.3071\n",
      "Epoch [1/5], Step [9664/10336], Loss: 1.1317\n",
      "Epoch [1/5], Step [9666/10336], Loss: 0.5075\n",
      "Epoch [1/5], Step [9668/10336], Loss: 2.0168\n",
      "Epoch [1/5], Step [9670/10336], Loss: 0.1863\n",
      "Epoch [1/5], Step [9672/10336], Loss: 1.0569\n",
      "Epoch [1/5], Step [9674/10336], Loss: 2.4658\n",
      "Epoch [1/5], Step [9676/10336], Loss: 0.7862\n",
      "Epoch [1/5], Step [9678/10336], Loss: 0.1904\n",
      "Epoch [1/5], Step [9680/10336], Loss: 0.2816\n",
      "Epoch [1/5], Step [9682/10336], Loss: 0.2112\n",
      "Epoch [1/5], Step [9684/10336], Loss: 0.3310\n",
      "Epoch [1/5], Step [9686/10336], Loss: 0.6475\n",
      "Epoch [1/5], Step [9688/10336], Loss: 0.0886\n",
      "Epoch [1/5], Step [9690/10336], Loss: 2.8421\n",
      "Epoch [1/5], Step [9692/10336], Loss: 0.0890\n",
      "Epoch [1/5], Step [9694/10336], Loss: 0.4462\n",
      "Epoch [1/5], Step [9696/10336], Loss: 0.5779\n",
      "Epoch [1/5], Step [9698/10336], Loss: 0.5677\n",
      "Epoch [1/5], Step [9700/10336], Loss: 2.4452\n",
      "Epoch [1/5], Step [9702/10336], Loss: 2.2100\n",
      "Epoch [1/5], Step [9704/10336], Loss: 2.5993\n",
      "Epoch [1/5], Step [9706/10336], Loss: 3.3764\n",
      "Epoch [1/5], Step [9708/10336], Loss: 6.1512\n",
      "Epoch [1/5], Step [9710/10336], Loss: 0.3729\n",
      "Epoch [1/5], Step [9712/10336], Loss: 2.6925\n",
      "Epoch [1/5], Step [9714/10336], Loss: 0.3291\n",
      "Epoch [1/5], Step [9716/10336], Loss: 1.0274\n",
      "Epoch [1/5], Step [9718/10336], Loss: 0.4878\n",
      "Epoch [1/5], Step [9720/10336], Loss: 0.6066\n",
      "Epoch [1/5], Step [9722/10336], Loss: 1.1453\n",
      "Epoch [1/5], Step [9724/10336], Loss: 0.2980\n",
      "Epoch [1/5], Step [9726/10336], Loss: 0.9391\n",
      "Epoch [1/5], Step [9728/10336], Loss: 3.5186\n",
      "Epoch [1/5], Step [9730/10336], Loss: 0.5945\n",
      "Epoch [1/5], Step [9732/10336], Loss: 3.0808\n",
      "Epoch [1/5], Step [9734/10336], Loss: 0.3934\n",
      "Epoch [1/5], Step [9736/10336], Loss: 0.3673\n",
      "Epoch [1/5], Step [9738/10336], Loss: 0.1099\n",
      "Epoch [1/5], Step [9740/10336], Loss: 1.7698\n",
      "Epoch [1/5], Step [9742/10336], Loss: 0.1261\n",
      "Epoch [1/5], Step [9744/10336], Loss: 0.7233\n",
      "Epoch [1/5], Step [9746/10336], Loss: 2.8217\n",
      "Epoch [1/5], Step [9748/10336], Loss: 2.7900\n",
      "Epoch [1/5], Step [9750/10336], Loss: 2.8057\n",
      "Epoch [1/5], Step [9752/10336], Loss: 3.8645\n",
      "Epoch [1/5], Step [9754/10336], Loss: 0.5753\n",
      "Epoch [1/5], Step [9756/10336], Loss: 0.3328\n",
      "Epoch [1/5], Step [9758/10336], Loss: 0.2049\n",
      "Epoch [1/5], Step [9760/10336], Loss: 0.4552\n",
      "Epoch [1/5], Step [9762/10336], Loss: 1.0008\n",
      "Epoch [1/5], Step [9764/10336], Loss: 2.8129\n",
      "Epoch [1/5], Step [9766/10336], Loss: 4.0744\n",
      "Epoch [1/5], Step [9768/10336], Loss: 0.1858\n",
      "Epoch [1/5], Step [9770/10336], Loss: 2.0569\n",
      "Epoch [1/5], Step [9772/10336], Loss: 0.1792\n",
      "Epoch [1/5], Step [9774/10336], Loss: 1.3987\n",
      "Epoch [1/5], Step [9776/10336], Loss: 4.3250\n",
      "Epoch [1/5], Step [9778/10336], Loss: 0.0045\n",
      "Epoch [1/5], Step [9780/10336], Loss: 0.3737\n",
      "Epoch [1/5], Step [9782/10336], Loss: 0.8667\n",
      "Epoch [1/5], Step [9784/10336], Loss: 0.5202\n",
      "Epoch [1/5], Step [9786/10336], Loss: 0.3749\n",
      "Epoch [1/5], Step [9788/10336], Loss: 1.2769\n",
      "Epoch [1/5], Step [9790/10336], Loss: 0.3650\n",
      "Epoch [1/5], Step [9792/10336], Loss: 0.6815\n",
      "Epoch [1/5], Step [9794/10336], Loss: 0.1072\n",
      "Epoch [1/5], Step [9796/10336], Loss: 0.0688\n",
      "Epoch [1/5], Step [9798/10336], Loss: 0.0027\n",
      "Epoch [1/5], Step [9800/10336], Loss: 0.0735\n",
      "Epoch [1/5], Step [9802/10336], Loss: 1.9062\n",
      "Epoch [1/5], Step [9804/10336], Loss: 0.3812\n",
      "Epoch [1/5], Step [9806/10336], Loss: 0.4939\n",
      "Epoch [1/5], Step [9808/10336], Loss: 0.2131\n",
      "Epoch [1/5], Step [9810/10336], Loss: 0.5151\n",
      "Epoch [1/5], Step [9812/10336], Loss: 0.3381\n",
      "Epoch [1/5], Step [9814/10336], Loss: 0.2781\n",
      "Epoch [1/5], Step [9816/10336], Loss: 3.4776\n",
      "Epoch [1/5], Step [9818/10336], Loss: 1.5748\n",
      "Epoch [1/5], Step [9820/10336], Loss: 0.2244\n",
      "Epoch [1/5], Step [9822/10336], Loss: 4.5634\n",
      "Epoch [1/5], Step [9824/10336], Loss: 2.8070\n",
      "Epoch [1/5], Step [9826/10336], Loss: 3.0277\n",
      "Epoch [1/5], Step [9828/10336], Loss: 0.2265\n",
      "Epoch [1/5], Step [9830/10336], Loss: 0.2220\n",
      "Epoch [1/5], Step [9832/10336], Loss: 0.9938\n",
      "Epoch [1/5], Step [9834/10336], Loss: 0.2731\n",
      "Epoch [1/5], Step [9836/10336], Loss: 1.1045\n",
      "Epoch [1/5], Step [9838/10336], Loss: 0.1418\n",
      "Epoch [1/5], Step [9840/10336], Loss: 0.3377\n",
      "Epoch [1/5], Step [9842/10336], Loss: 2.1421\n",
      "Epoch [1/5], Step [9844/10336], Loss: 0.5205\n",
      "Epoch [1/5], Step [9846/10336], Loss: 0.0650\n",
      "Epoch [1/5], Step [9848/10336], Loss: 0.7771\n",
      "Epoch [1/5], Step [9850/10336], Loss: 4.7859\n",
      "Epoch [1/5], Step [9852/10336], Loss: 0.2010\n",
      "Epoch [1/5], Step [9854/10336], Loss: 0.1819\n",
      "Epoch [1/5], Step [9856/10336], Loss: 3.7197\n",
      "Epoch [1/5], Step [9858/10336], Loss: 0.1950\n",
      "Epoch [1/5], Step [9860/10336], Loss: 0.2667\n",
      "Epoch [1/5], Step [9862/10336], Loss: 1.1863\n",
      "Epoch [1/5], Step [9864/10336], Loss: 0.1732\n",
      "Epoch [1/5], Step [9866/10336], Loss: 0.2945\n",
      "Epoch [1/5], Step [9868/10336], Loss: 0.0137\n",
      "Epoch [1/5], Step [9870/10336], Loss: 1.2000\n",
      "Epoch [1/5], Step [9872/10336], Loss: 0.0024\n",
      "Epoch [1/5], Step [9874/10336], Loss: 0.6320\n",
      "Epoch [1/5], Step [9876/10336], Loss: 0.2719\n",
      "Epoch [1/5], Step [9878/10336], Loss: 0.0247\n",
      "Epoch [1/5], Step [9880/10336], Loss: 2.2844\n",
      "Epoch [1/5], Step [9882/10336], Loss: 0.8009\n",
      "Epoch [1/5], Step [9884/10336], Loss: 0.9264\n",
      "Epoch [1/5], Step [9886/10336], Loss: 0.6912\n",
      "Epoch [1/5], Step [9888/10336], Loss: 0.2953\n",
      "Epoch [1/5], Step [9890/10336], Loss: 0.1524\n",
      "Epoch [1/5], Step [9892/10336], Loss: 0.0757\n",
      "Epoch [1/5], Step [9894/10336], Loss: 0.2068\n",
      "Epoch [1/5], Step [9896/10336], Loss: 0.2804\n",
      "Epoch [1/5], Step [9898/10336], Loss: 3.5986\n",
      "Epoch [1/5], Step [9900/10336], Loss: 0.8759\n",
      "Epoch [1/5], Step [9902/10336], Loss: 0.0726\n",
      "Epoch [1/5], Step [9904/10336], Loss: 1.1135\n",
      "Epoch [1/5], Step [9906/10336], Loss: 0.1605\n",
      "Epoch [1/5], Step [9908/10336], Loss: 0.5260\n",
      "Epoch [1/5], Step [9910/10336], Loss: 5.4503\n",
      "Epoch [1/5], Step [9912/10336], Loss: 0.3530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [9914/10336], Loss: 1.1922\n",
      "Epoch [1/5], Step [9916/10336], Loss: 1.4776\n",
      "Epoch [1/5], Step [9918/10336], Loss: 0.0476\n",
      "Epoch [1/5], Step [9920/10336], Loss: 0.4426\n",
      "Epoch [1/5], Step [9922/10336], Loss: 3.0941\n",
      "Epoch [1/5], Step [9924/10336], Loss: 3.7223\n",
      "Epoch [1/5], Step [9926/10336], Loss: 0.4331\n",
      "Epoch [1/5], Step [9928/10336], Loss: 5.8055\n",
      "Epoch [1/5], Step [9930/10336], Loss: 3.0732\n",
      "Epoch [1/5], Step [9932/10336], Loss: 0.1676\n",
      "Epoch [1/5], Step [9934/10336], Loss: 0.1688\n",
      "Epoch [1/5], Step [9936/10336], Loss: 2.6084\n",
      "Epoch [1/5], Step [9938/10336], Loss: 0.4065\n",
      "Epoch [1/5], Step [9940/10336], Loss: 1.6797\n",
      "Epoch [1/5], Step [9942/10336], Loss: 0.9780\n",
      "Epoch [1/5], Step [9944/10336], Loss: 0.8750\n",
      "Epoch [1/5], Step [9946/10336], Loss: 0.2360\n",
      "Epoch [1/5], Step [9948/10336], Loss: 0.3387\n",
      "Epoch [1/5], Step [9950/10336], Loss: 3.2150\n",
      "Epoch [1/5], Step [9952/10336], Loss: 0.4452\n",
      "Epoch [1/5], Step [9954/10336], Loss: 0.3236\n",
      "Epoch [1/5], Step [9956/10336], Loss: 0.3422\n",
      "Epoch [1/5], Step [9958/10336], Loss: 0.2250\n",
      "Epoch [1/5], Step [9960/10336], Loss: 2.4329\n",
      "Epoch [1/5], Step [9962/10336], Loss: 0.9929\n",
      "Epoch [1/5], Step [9964/10336], Loss: 2.9854\n",
      "Epoch [1/5], Step [9966/10336], Loss: 1.9847\n",
      "Epoch [1/5], Step [9968/10336], Loss: 0.2837\n",
      "Epoch [1/5], Step [9970/10336], Loss: 0.2930\n",
      "Epoch [1/5], Step [9972/10336], Loss: 1.1250\n",
      "Epoch [1/5], Step [9974/10336], Loss: 0.8443\n",
      "Epoch [1/5], Step [9976/10336], Loss: 0.2569\n",
      "Epoch [1/5], Step [9978/10336], Loss: 0.7654\n",
      "Epoch [1/5], Step [9980/10336], Loss: 2.5981\n",
      "Epoch [1/5], Step [9982/10336], Loss: 0.0336\n",
      "Epoch [1/5], Step [9984/10336], Loss: 0.5266\n",
      "Epoch [1/5], Step [9986/10336], Loss: 0.3846\n",
      "Epoch [1/5], Step [9988/10336], Loss: 1.1557\n",
      "Epoch [1/5], Step [9990/10336], Loss: 0.0536\n",
      "Epoch [1/5], Step [9992/10336], Loss: 3.1278\n",
      "Epoch [1/5], Step [9994/10336], Loss: 0.7408\n",
      "Epoch [1/5], Step [9996/10336], Loss: 1.4784\n",
      "Epoch [1/5], Step [9998/10336], Loss: 1.0434\n",
      "Epoch [1/5], Step [10000/10336], Loss: 1.4467\n",
      "Epoch [1/5], Step [10002/10336], Loss: 1.2566\n",
      "Epoch [1/5], Step [10004/10336], Loss: 1.8333\n",
      "Epoch [1/5], Step [10006/10336], Loss: 2.6165\n",
      "Epoch [1/5], Step [10008/10336], Loss: 0.7488\n",
      "Epoch [1/5], Step [10010/10336], Loss: 2.0701\n",
      "Epoch [1/5], Step [10012/10336], Loss: 0.3369\n",
      "Epoch [1/5], Step [10014/10336], Loss: 0.6730\n",
      "Epoch [1/5], Step [10016/10336], Loss: 0.5727\n",
      "Epoch [1/5], Step [10018/10336], Loss: 0.3412\n",
      "Epoch [1/5], Step [10020/10336], Loss: 0.3767\n",
      "Epoch [1/5], Step [10022/10336], Loss: 0.3151\n",
      "Epoch [1/5], Step [10024/10336], Loss: 0.3130\n",
      "Epoch [1/5], Step [10026/10336], Loss: 0.0525\n",
      "Epoch [1/5], Step [10028/10336], Loss: 1.1841\n",
      "Epoch [1/5], Step [10030/10336], Loss: 0.1338\n",
      "Epoch [1/5], Step [10032/10336], Loss: 1.2207\n",
      "Epoch [1/5], Step [10034/10336], Loss: 0.7630\n",
      "Epoch [1/5], Step [10036/10336], Loss: 0.3889\n",
      "Epoch [1/5], Step [10038/10336], Loss: 0.3589\n",
      "Epoch [1/5], Step [10040/10336], Loss: 0.2359\n",
      "Epoch [1/5], Step [10042/10336], Loss: 2.5315\n",
      "Epoch [1/5], Step [10044/10336], Loss: 0.0218\n",
      "Epoch [1/5], Step [10046/10336], Loss: 0.3697\n",
      "Epoch [1/5], Step [10048/10336], Loss: 0.3755\n",
      "Epoch [1/5], Step [10050/10336], Loss: 0.5180\n",
      "Epoch [1/5], Step [10052/10336], Loss: 2.8580\n",
      "Epoch [1/5], Step [10054/10336], Loss: 6.9023\n",
      "Epoch [1/5], Step [10056/10336], Loss: 0.5017\n",
      "Epoch [1/5], Step [10058/10336], Loss: 0.1209\n",
      "Epoch [1/5], Step [10060/10336], Loss: 0.3460\n",
      "Epoch [1/5], Step [10062/10336], Loss: 3.1636\n",
      "Epoch [1/5], Step [10064/10336], Loss: 0.2314\n",
      "Epoch [1/5], Step [10066/10336], Loss: 4.8486\n",
      "Epoch [1/5], Step [10068/10336], Loss: 1.5020\n",
      "Epoch [1/5], Step [10070/10336], Loss: 0.5830\n",
      "Epoch [1/5], Step [10072/10336], Loss: 0.4361\n",
      "Epoch [1/5], Step [10074/10336], Loss: 2.3758\n",
      "Epoch [1/5], Step [10076/10336], Loss: 2.2048\n",
      "Epoch [1/5], Step [10078/10336], Loss: 1.2769\n",
      "Epoch [1/5], Step [10080/10336], Loss: 0.1562\n",
      "Epoch [1/5], Step [10082/10336], Loss: 0.8133\n",
      "Epoch [1/5], Step [10084/10336], Loss: 0.4527\n",
      "Epoch [1/5], Step [10086/10336], Loss: 0.1379\n",
      "Epoch [1/5], Step [10088/10336], Loss: 0.5003\n",
      "Epoch [1/5], Step [10090/10336], Loss: 0.5883\n",
      "Epoch [1/5], Step [10092/10336], Loss: 1.0480\n",
      "Epoch [1/5], Step [10094/10336], Loss: 0.2887\n",
      "Epoch [1/5], Step [10096/10336], Loss: 0.4422\n",
      "Epoch [1/5], Step [10098/10336], Loss: 0.5175\n",
      "Epoch [1/5], Step [10100/10336], Loss: 0.4867\n",
      "Epoch [1/5], Step [10102/10336], Loss: 0.3079\n",
      "Epoch [1/5], Step [10104/10336], Loss: 4.1908\n",
      "Epoch [1/5], Step [10106/10336], Loss: 2.9244\n",
      "Epoch [1/5], Step [10108/10336], Loss: 0.8562\n",
      "Epoch [1/5], Step [10110/10336], Loss: 1.2828\n",
      "Epoch [1/5], Step [10112/10336], Loss: 0.1227\n",
      "Epoch [1/5], Step [10114/10336], Loss: 3.3290\n",
      "Epoch [1/5], Step [10116/10336], Loss: 0.0405\n",
      "Epoch [1/5], Step [10118/10336], Loss: 2.9494\n",
      "Epoch [1/5], Step [10120/10336], Loss: 0.8766\n",
      "Epoch [1/5], Step [10122/10336], Loss: 0.3683\n",
      "Epoch [1/5], Step [10124/10336], Loss: 0.2373\n",
      "Epoch [1/5], Step [10126/10336], Loss: 0.3716\n",
      "Epoch [1/5], Step [10128/10336], Loss: 0.0176\n",
      "Epoch [1/5], Step [10130/10336], Loss: 2.1662\n",
      "Epoch [1/5], Step [10132/10336], Loss: 0.4858\n",
      "Epoch [1/5], Step [10134/10336], Loss: 0.3133\n",
      "Epoch [1/5], Step [10136/10336], Loss: 2.6874\n",
      "Epoch [1/5], Step [10138/10336], Loss: 0.3036\n",
      "Epoch [1/5], Step [10140/10336], Loss: 1.0399\n",
      "Epoch [1/5], Step [10142/10336], Loss: 1.6703\n",
      "Epoch [1/5], Step [10144/10336], Loss: 0.4379\n",
      "Epoch [1/5], Step [10146/10336], Loss: 2.6702\n",
      "Epoch [1/5], Step [10148/10336], Loss: 0.1802\n",
      "Epoch [1/5], Step [10150/10336], Loss: 0.0311\n",
      "Epoch [1/5], Step [10152/10336], Loss: 0.4916\n",
      "Epoch [1/5], Step [10154/10336], Loss: 0.3327\n",
      "Epoch [1/5], Step [10156/10336], Loss: 0.5500\n",
      "Epoch [1/5], Step [10158/10336], Loss: 0.0583\n",
      "Epoch [1/5], Step [10160/10336], Loss: 0.2669\n",
      "Epoch [1/5], Step [10162/10336], Loss: 0.2502\n",
      "Epoch [1/5], Step [10164/10336], Loss: 1.1424\n",
      "Epoch [1/5], Step [10166/10336], Loss: 0.2302\n",
      "Epoch [1/5], Step [10168/10336], Loss: 0.9150\n",
      "Epoch [1/5], Step [10170/10336], Loss: 0.0209\n",
      "Epoch [1/5], Step [10172/10336], Loss: 0.0386\n",
      "Epoch [1/5], Step [10174/10336], Loss: 0.0032\n",
      "Epoch [1/5], Step [10176/10336], Loss: 0.3395\n",
      "Epoch [1/5], Step [10178/10336], Loss: 0.2926\n",
      "Epoch [1/5], Step [10180/10336], Loss: 4.7062\n",
      "Epoch [1/5], Step [10182/10336], Loss: 0.0015\n",
      "Epoch [1/5], Step [10184/10336], Loss: 0.8620\n",
      "Epoch [1/5], Step [10186/10336], Loss: 0.5094\n",
      "Epoch [1/5], Step [10188/10336], Loss: 0.4547\n",
      "Epoch [1/5], Step [10190/10336], Loss: 0.3907\n",
      "Epoch [1/5], Step [10192/10336], Loss: 2.4623\n",
      "Epoch [1/5], Step [10194/10336], Loss: 0.3400\n",
      "Epoch [1/5], Step [10196/10336], Loss: 0.1189\n",
      "Epoch [1/5], Step [10198/10336], Loss: 0.9893\n",
      "Epoch [1/5], Step [10200/10336], Loss: 0.2220\n",
      "Epoch [1/5], Step [10202/10336], Loss: 0.0269\n",
      "Epoch [1/5], Step [10204/10336], Loss: 1.7050\n",
      "Epoch [1/5], Step [10206/10336], Loss: 0.4170\n",
      "Epoch [1/5], Step [10208/10336], Loss: 0.3999\n",
      "Epoch [1/5], Step [10210/10336], Loss: 3.8343\n",
      "Epoch [1/5], Step [10212/10336], Loss: 0.4581\n",
      "Epoch [1/5], Step [10214/10336], Loss: 0.0587\n",
      "Epoch [1/5], Step [10216/10336], Loss: 0.0973\n",
      "Epoch [1/5], Step [10218/10336], Loss: 0.0755\n",
      "Epoch [1/5], Step [10220/10336], Loss: 0.4601\n",
      "Epoch [1/5], Step [10222/10336], Loss: 0.6458\n",
      "Epoch [1/5], Step [10224/10336], Loss: 0.5616\n",
      "Epoch [1/5], Step [10226/10336], Loss: 0.1219\n",
      "Epoch [1/5], Step [10228/10336], Loss: 0.3916\n",
      "Epoch [1/5], Step [10230/10336], Loss: 0.1626\n",
      "Epoch [1/5], Step [10232/10336], Loss: 0.5940\n",
      "Epoch [1/5], Step [10234/10336], Loss: 2.3154\n",
      "Epoch [1/5], Step [10236/10336], Loss: 1.2488\n",
      "Epoch [1/5], Step [10238/10336], Loss: 0.7065\n",
      "Epoch [1/5], Step [10240/10336], Loss: 0.3346\n",
      "Epoch [1/5], Step [10242/10336], Loss: 2.1375\n",
      "Epoch [1/5], Step [10244/10336], Loss: 0.2084\n",
      "Epoch [1/5], Step [10246/10336], Loss: 0.2867\n",
      "Epoch [1/5], Step [10248/10336], Loss: 0.3595\n",
      "Epoch [1/5], Step [10250/10336], Loss: 0.0929\n",
      "Epoch [1/5], Step [10252/10336], Loss: 0.1368\n",
      "Epoch [1/5], Step [10254/10336], Loss: 0.2670\n",
      "Epoch [1/5], Step [10256/10336], Loss: 0.1220\n",
      "Epoch [1/5], Step [10258/10336], Loss: 0.2295\n",
      "Epoch [1/5], Step [10260/10336], Loss: 0.8848\n",
      "Epoch [1/5], Step [10262/10336], Loss: 0.7454\n",
      "Epoch [1/5], Step [10264/10336], Loss: 0.3146\n",
      "Epoch [1/5], Step [10266/10336], Loss: 0.4694\n",
      "Epoch [1/5], Step [10268/10336], Loss: 1.0309\n",
      "Epoch [1/5], Step [10270/10336], Loss: 0.0125\n",
      "Epoch [1/5], Step [10272/10336], Loss: 2.8120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [10274/10336], Loss: 0.0302\n",
      "Epoch [1/5], Step [10276/10336], Loss: 0.1701\n",
      "Epoch [1/5], Step [10278/10336], Loss: 0.2518\n",
      "Epoch [1/5], Step [10280/10336], Loss: 0.6258\n",
      "Epoch [1/5], Step [10282/10336], Loss: 0.3888\n",
      "Epoch [1/5], Step [10284/10336], Loss: 0.4476\n",
      "Epoch [1/5], Step [10286/10336], Loss: 2.5943\n",
      "Epoch [1/5], Step [10288/10336], Loss: 3.9650\n",
      "Epoch [1/5], Step [10290/10336], Loss: 0.3798\n",
      "Epoch [1/5], Step [10292/10336], Loss: 0.0081\n",
      "Epoch [1/5], Step [10294/10336], Loss: 0.3423\n",
      "Epoch [1/5], Step [10296/10336], Loss: 1.9757\n",
      "Epoch [1/5], Step [10298/10336], Loss: 0.2975\n",
      "Epoch [1/5], Step [10300/10336], Loss: 0.7260\n",
      "Epoch [1/5], Step [10302/10336], Loss: 0.3882\n",
      "Epoch [1/5], Step [10304/10336], Loss: 0.9205\n",
      "Epoch [1/5], Step [10306/10336], Loss: 0.4435\n",
      "Epoch [1/5], Step [10308/10336], Loss: 2.8484\n",
      "Epoch [1/5], Step [10310/10336], Loss: 0.2500\n",
      "Epoch [1/5], Step [10312/10336], Loss: 3.7688\n",
      "Epoch [1/5], Step [10314/10336], Loss: 2.9075\n",
      "Epoch [1/5], Step [10316/10336], Loss: 0.4758\n",
      "Epoch [1/5], Step [10318/10336], Loss: 1.8962\n",
      "Epoch [1/5], Step [10320/10336], Loss: 0.5271\n",
      "Epoch [1/5], Step [10322/10336], Loss: 0.7070\n",
      "Epoch [1/5], Step [10324/10336], Loss: 2.9740\n",
      "Epoch [1/5], Step [10326/10336], Loss: 2.5611\n",
      "Epoch [1/5], Step [10328/10336], Loss: 0.5197\n",
      "Epoch [1/5], Step [10330/10336], Loss: 1.7458\n",
      "Epoch [1/5], Step [10332/10336], Loss: 1.0743\n",
      "Epoch [1/5], Step [10334/10336], Loss: 0.2174\n",
      "Epoch [1/5], Step [10336/10336], Loss: 0.2719\n",
      "Epoch [2/5], Step [2/10336], Loss: 0.4068\n",
      "Epoch [2/5], Step [4/10336], Loss: 0.1259\n",
      "Epoch [2/5], Step [6/10336], Loss: 1.0150\n",
      "Epoch [2/5], Step [8/10336], Loss: 0.1997\n",
      "Epoch [2/5], Step [10/10336], Loss: 2.9480\n",
      "Epoch [2/5], Step [12/10336], Loss: 0.3104\n",
      "Epoch [2/5], Step [14/10336], Loss: 0.2407\n",
      "Epoch [2/5], Step [16/10336], Loss: 1.9033\n",
      "Epoch [2/5], Step [18/10336], Loss: 0.0083\n",
      "Epoch [2/5], Step [20/10336], Loss: 0.3587\n",
      "Epoch [2/5], Step [22/10336], Loss: 0.1749\n",
      "Epoch [2/5], Step [24/10336], Loss: 0.0658\n",
      "Epoch [2/5], Step [26/10336], Loss: 2.9323\n",
      "Epoch [2/5], Step [28/10336], Loss: 1.5728\n",
      "Epoch [2/5], Step [30/10336], Loss: 2.7724\n",
      "Epoch [2/5], Step [32/10336], Loss: 3.0216\n",
      "Epoch [2/5], Step [34/10336], Loss: 0.1526\n",
      "Epoch [2/5], Step [36/10336], Loss: 4.4615\n",
      "Epoch [2/5], Step [38/10336], Loss: 0.5900\n",
      "Epoch [2/5], Step [40/10336], Loss: 1.1692\n",
      "Epoch [2/5], Step [42/10336], Loss: 0.2892\n",
      "Epoch [2/5], Step [44/10336], Loss: 0.2488\n",
      "Epoch [2/5], Step [46/10336], Loss: 0.1014\n",
      "Epoch [2/5], Step [48/10336], Loss: 1.7138\n",
      "Epoch [2/5], Step [50/10336], Loss: 6.3336\n",
      "Epoch [2/5], Step [52/10336], Loss: 1.3845\n",
      "Epoch [2/5], Step [54/10336], Loss: 0.4327\n",
      "Epoch [2/5], Step [56/10336], Loss: 0.2742\n",
      "Epoch [2/5], Step [58/10336], Loss: 3.8206\n",
      "Epoch [2/5], Step [60/10336], Loss: 0.2343\n",
      "Epoch [2/5], Step [62/10336], Loss: 0.3542\n",
      "Epoch [2/5], Step [64/10336], Loss: 0.3111\n",
      "Epoch [2/5], Step [66/10336], Loss: 0.3430\n",
      "Epoch [2/5], Step [68/10336], Loss: 0.1000\n",
      "Epoch [2/5], Step [70/10336], Loss: 0.0306\n",
      "Epoch [2/5], Step [72/10336], Loss: 0.3495\n",
      "Epoch [2/5], Step [74/10336], Loss: 0.2583\n",
      "Epoch [2/5], Step [76/10336], Loss: 1.0103\n",
      "Epoch [2/5], Step [78/10336], Loss: 3.6921\n",
      "Epoch [2/5], Step [80/10336], Loss: 2.8739\n",
      "Epoch [2/5], Step [82/10336], Loss: 1.0740\n",
      "Epoch [2/5], Step [84/10336], Loss: 0.4146\n",
      "Epoch [2/5], Step [86/10336], Loss: 0.3374\n",
      "Epoch [2/5], Step [88/10336], Loss: 0.2527\n",
      "Epoch [2/5], Step [90/10336], Loss: 0.8952\n",
      "Epoch [2/5], Step [92/10336], Loss: 0.9736\n",
      "Epoch [2/5], Step [94/10336], Loss: 0.3423\n",
      "Epoch [2/5], Step [96/10336], Loss: 0.2782\n",
      "Epoch [2/5], Step [98/10336], Loss: 3.2251\n",
      "Epoch [2/5], Step [100/10336], Loss: 0.2090\n",
      "Epoch [2/5], Step [102/10336], Loss: 0.3639\n",
      "Epoch [2/5], Step [104/10336], Loss: 0.0249\n",
      "Epoch [2/5], Step [106/10336], Loss: 3.6078\n",
      "Epoch [2/5], Step [108/10336], Loss: 4.1803\n",
      "Epoch [2/5], Step [110/10336], Loss: 0.3895\n",
      "Epoch [2/5], Step [112/10336], Loss: 3.9020\n",
      "Epoch [2/5], Step [114/10336], Loss: 0.8899\n",
      "Epoch [2/5], Step [116/10336], Loss: 0.4723\n",
      "Epoch [2/5], Step [118/10336], Loss: 0.2991\n",
      "Epoch [2/5], Step [120/10336], Loss: 0.2027\n",
      "Epoch [2/5], Step [122/10336], Loss: 0.5742\n",
      "Epoch [2/5], Step [124/10336], Loss: 0.3593\n",
      "Epoch [2/5], Step [126/10336], Loss: 3.8230\n",
      "Epoch [2/5], Step [128/10336], Loss: 1.2244\n",
      "Epoch [2/5], Step [130/10336], Loss: 4.6784\n",
      "Epoch [2/5], Step [132/10336], Loss: 0.2317\n",
      "Epoch [2/5], Step [134/10336], Loss: 2.8460\n",
      "Epoch [2/5], Step [136/10336], Loss: 1.6249\n",
      "Epoch [2/5], Step [138/10336], Loss: 1.6005\n",
      "Epoch [2/5], Step [140/10336], Loss: 0.9197\n",
      "Epoch [2/5], Step [142/10336], Loss: 0.0967\n",
      "Epoch [2/5], Step [144/10336], Loss: 1.6120\n",
      "Epoch [2/5], Step [146/10336], Loss: 0.4569\n",
      "Epoch [2/5], Step [148/10336], Loss: 4.2722\n",
      "Epoch [2/5], Step [150/10336], Loss: 0.8562\n",
      "Epoch [2/5], Step [152/10336], Loss: 0.5919\n",
      "Epoch [2/5], Step [154/10336], Loss: 0.2736\n",
      "Epoch [2/5], Step [156/10336], Loss: 0.0646\n",
      "Epoch [2/5], Step [158/10336], Loss: 0.4416\n",
      "Epoch [2/5], Step [160/10336], Loss: 0.2309\n",
      "Epoch [2/5], Step [162/10336], Loss: 1.1523\n",
      "Epoch [2/5], Step [164/10336], Loss: 1.7707\n",
      "Epoch [2/5], Step [166/10336], Loss: 0.5141\n",
      "Epoch [2/5], Step [168/10336], Loss: 2.0141\n",
      "Epoch [2/5], Step [170/10336], Loss: 0.2562\n",
      "Epoch [2/5], Step [172/10336], Loss: 0.0059\n",
      "Epoch [2/5], Step [174/10336], Loss: 0.3899\n",
      "Epoch [2/5], Step [176/10336], Loss: 0.1688\n",
      "Epoch [2/5], Step [178/10336], Loss: 0.2916\n",
      "Epoch [2/5], Step [180/10336], Loss: 2.8182\n",
      "Epoch [2/5], Step [182/10336], Loss: 0.3124\n",
      "Epoch [2/5], Step [184/10336], Loss: 3.1319\n",
      "Epoch [2/5], Step [186/10336], Loss: 0.3616\n",
      "Epoch [2/5], Step [188/10336], Loss: 4.7315\n",
      "Epoch [2/5], Step [190/10336], Loss: 0.0934\n",
      "Epoch [2/5], Step [192/10336], Loss: 3.1194\n",
      "Epoch [2/5], Step [194/10336], Loss: 1.6882\n",
      "Epoch [2/5], Step [196/10336], Loss: 1.3055\n",
      "Epoch [2/5], Step [198/10336], Loss: 0.2220\n",
      "Epoch [2/5], Step [200/10336], Loss: 0.0094\n",
      "Epoch [2/5], Step [202/10336], Loss: 0.9153\n",
      "Epoch [2/5], Step [204/10336], Loss: 1.2916\n",
      "Epoch [2/5], Step [206/10336], Loss: 0.8184\n",
      "Epoch [2/5], Step [208/10336], Loss: 1.3463\n",
      "Epoch [2/5], Step [210/10336], Loss: 3.8863\n",
      "Epoch [2/5], Step [212/10336], Loss: 0.2798\n",
      "Epoch [2/5], Step [214/10336], Loss: 0.9426\n",
      "Epoch [2/5], Step [216/10336], Loss: 0.0207\n",
      "Epoch [2/5], Step [218/10336], Loss: 3.6452\n",
      "Epoch [2/5], Step [220/10336], Loss: 1.2830\n",
      "Epoch [2/5], Step [222/10336], Loss: 0.0235\n",
      "Epoch [2/5], Step [224/10336], Loss: 0.4448\n",
      "Epoch [2/5], Step [226/10336], Loss: 0.4433\n",
      "Epoch [2/5], Step [228/10336], Loss: 0.0988\n",
      "Epoch [2/5], Step [230/10336], Loss: 0.4177\n",
      "Epoch [2/5], Step [232/10336], Loss: 0.3292\n",
      "Epoch [2/5], Step [234/10336], Loss: 0.0373\n",
      "Epoch [2/5], Step [236/10336], Loss: 0.2292\n",
      "Epoch [2/5], Step [238/10336], Loss: 0.2469\n",
      "Epoch [2/5], Step [240/10336], Loss: 0.7250\n",
      "Epoch [2/5], Step [242/10336], Loss: 0.0779\n",
      "Epoch [2/5], Step [244/10336], Loss: 0.0346\n",
      "Epoch [2/5], Step [246/10336], Loss: 0.1930\n",
      "Epoch [2/5], Step [248/10336], Loss: 0.0185\n",
      "Epoch [2/5], Step [250/10336], Loss: 0.9139\n",
      "Epoch [2/5], Step [252/10336], Loss: 0.3190\n",
      "Epoch [2/5], Step [254/10336], Loss: 1.2162\n",
      "Epoch [2/5], Step [256/10336], Loss: 0.8397\n",
      "Epoch [2/5], Step [258/10336], Loss: 3.4574\n",
      "Epoch [2/5], Step [260/10336], Loss: 4.6165\n",
      "Epoch [2/5], Step [262/10336], Loss: 0.0052\n",
      "Epoch [2/5], Step [264/10336], Loss: 0.3780\n",
      "Epoch [2/5], Step [266/10336], Loss: 0.1421\n",
      "Epoch [2/5], Step [268/10336], Loss: 0.2121\n",
      "Epoch [2/5], Step [270/10336], Loss: 0.1134\n",
      "Epoch [2/5], Step [272/10336], Loss: 0.3331\n",
      "Epoch [2/5], Step [274/10336], Loss: 1.5945\n",
      "Epoch [2/5], Step [276/10336], Loss: 0.0808\n",
      "Epoch [2/5], Step [278/10336], Loss: 1.5662\n",
      "Epoch [2/5], Step [280/10336], Loss: 0.4421\n",
      "Epoch [2/5], Step [282/10336], Loss: 0.1234\n",
      "Epoch [2/5], Step [284/10336], Loss: 0.5235\n",
      "Epoch [2/5], Step [286/10336], Loss: 0.0830\n",
      "Epoch [2/5], Step [288/10336], Loss: 0.0580\n",
      "Epoch [2/5], Step [290/10336], Loss: 0.4641\n",
      "Epoch [2/5], Step [292/10336], Loss: 0.2036\n",
      "Epoch [2/5], Step [294/10336], Loss: 0.1771\n",
      "Epoch [2/5], Step [296/10336], Loss: 0.5290\n",
      "Epoch [2/5], Step [298/10336], Loss: 1.5467\n",
      "Epoch [2/5], Step [300/10336], Loss: 0.0285\n",
      "Epoch [2/5], Step [302/10336], Loss: 0.5409\n",
      "Epoch [2/5], Step [304/10336], Loss: 1.5015\n",
      "Epoch [2/5], Step [306/10336], Loss: 0.0474\n",
      "Epoch [2/5], Step [308/10336], Loss: 0.8268\n",
      "Epoch [2/5], Step [310/10336], Loss: 3.0335\n",
      "Epoch [2/5], Step [312/10336], Loss: 2.8435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [314/10336], Loss: 0.5560\n",
      "Epoch [2/5], Step [316/10336], Loss: 0.1153\n",
      "Epoch [2/5], Step [318/10336], Loss: 1.9247\n",
      "Epoch [2/5], Step [320/10336], Loss: 0.0534\n",
      "Epoch [2/5], Step [322/10336], Loss: 0.3010\n",
      "Epoch [2/5], Step [324/10336], Loss: 0.2525\n",
      "Epoch [2/5], Step [326/10336], Loss: 1.4785\n",
      "Epoch [2/5], Step [328/10336], Loss: 1.5726\n",
      "Epoch [2/5], Step [330/10336], Loss: 2.1684\n",
      "Epoch [2/5], Step [332/10336], Loss: 0.0694\n",
      "Epoch [2/5], Step [334/10336], Loss: 0.3539\n",
      "Epoch [2/5], Step [336/10336], Loss: 0.7790\n",
      "Epoch [2/5], Step [338/10336], Loss: 2.4713\n",
      "Epoch [2/5], Step [340/10336], Loss: 0.3649\n",
      "Epoch [2/5], Step [342/10336], Loss: 2.2757\n",
      "Epoch [2/5], Step [344/10336], Loss: 1.2332\n",
      "Epoch [2/5], Step [346/10336], Loss: 0.9156\n",
      "Epoch [2/5], Step [348/10336], Loss: 0.0630\n",
      "Epoch [2/5], Step [350/10336], Loss: 0.1490\n",
      "Epoch [2/5], Step [352/10336], Loss: 0.8209\n",
      "Epoch [2/5], Step [354/10336], Loss: 0.3161\n",
      "Epoch [2/5], Step [356/10336], Loss: 0.1473\n",
      "Epoch [2/5], Step [358/10336], Loss: 0.0520\n",
      "Epoch [2/5], Step [360/10336], Loss: 1.8165\n",
      "Epoch [2/5], Step [362/10336], Loss: 1.1239\n",
      "Epoch [2/5], Step [364/10336], Loss: 0.1718\n",
      "Epoch [2/5], Step [366/10336], Loss: 0.2442\n",
      "Epoch [2/5], Step [368/10336], Loss: 0.0077\n",
      "Epoch [2/5], Step [370/10336], Loss: 0.2221\n",
      "Epoch [2/5], Step [372/10336], Loss: 1.0182\n",
      "Epoch [2/5], Step [374/10336], Loss: 1.8039\n",
      "Epoch [2/5], Step [376/10336], Loss: 5.2707\n",
      "Epoch [2/5], Step [378/10336], Loss: 1.6213\n",
      "Epoch [2/5], Step [380/10336], Loss: 6.1663\n",
      "Epoch [2/5], Step [382/10336], Loss: 0.3957\n",
      "Epoch [2/5], Step [384/10336], Loss: 0.0808\n",
      "Epoch [2/5], Step [386/10336], Loss: 0.1710\n",
      "Epoch [2/5], Step [388/10336], Loss: 1.2079\n",
      "Epoch [2/5], Step [390/10336], Loss: 0.8884\n",
      "Epoch [2/5], Step [392/10336], Loss: 0.3275\n",
      "Epoch [2/5], Step [394/10336], Loss: 1.6689\n",
      "Epoch [2/5], Step [396/10336], Loss: 0.3798\n",
      "Epoch [2/5], Step [398/10336], Loss: 3.4498\n",
      "Epoch [2/5], Step [400/10336], Loss: 0.6384\n",
      "Epoch [2/5], Step [402/10336], Loss: 2.7280\n",
      "Epoch [2/5], Step [404/10336], Loss: 0.6716\n",
      "Epoch [2/5], Step [406/10336], Loss: 0.3719\n",
      "Epoch [2/5], Step [408/10336], Loss: 0.0210\n",
      "Epoch [2/5], Step [410/10336], Loss: 0.1905\n",
      "Epoch [2/5], Step [412/10336], Loss: 4.6963\n",
      "Epoch [2/5], Step [414/10336], Loss: 0.2000\n",
      "Epoch [2/5], Step [416/10336], Loss: 3.3628\n",
      "Epoch [2/5], Step [418/10336], Loss: 0.4502\n",
      "Epoch [2/5], Step [420/10336], Loss: 0.5476\n",
      "Epoch [2/5], Step [422/10336], Loss: 0.4521\n",
      "Epoch [2/5], Step [424/10336], Loss: 0.1140\n",
      "Epoch [2/5], Step [426/10336], Loss: 5.7204\n",
      "Epoch [2/5], Step [428/10336], Loss: 4.6766\n",
      "Epoch [2/5], Step [430/10336], Loss: 0.5246\n",
      "Epoch [2/5], Step [432/10336], Loss: 2.5976\n",
      "Epoch [2/5], Step [434/10336], Loss: 0.1654\n",
      "Epoch [2/5], Step [436/10336], Loss: 0.5310\n",
      "Epoch [2/5], Step [438/10336], Loss: 0.5465\n",
      "Epoch [2/5], Step [440/10336], Loss: 0.8355\n",
      "Epoch [2/5], Step [442/10336], Loss: 1.2701\n",
      "Epoch [2/5], Step [444/10336], Loss: 3.0422\n",
      "Epoch [2/5], Step [446/10336], Loss: 2.1246\n",
      "Epoch [2/5], Step [448/10336], Loss: 0.1817\n",
      "Epoch [2/5], Step [450/10336], Loss: 0.3455\n",
      "Epoch [2/5], Step [452/10336], Loss: 2.0458\n",
      "Epoch [2/5], Step [454/10336], Loss: 1.6320\n",
      "Epoch [2/5], Step [456/10336], Loss: 3.5432\n",
      "Epoch [2/5], Step [458/10336], Loss: 0.1039\n",
      "Epoch [2/5], Step [460/10336], Loss: 0.2864\n",
      "Epoch [2/5], Step [462/10336], Loss: 2.6076\n",
      "Epoch [2/5], Step [464/10336], Loss: 0.0772\n",
      "Epoch [2/5], Step [466/10336], Loss: 0.8864\n",
      "Epoch [2/5], Step [468/10336], Loss: 0.2282\n",
      "Epoch [2/5], Step [470/10336], Loss: 0.3117\n",
      "Epoch [2/5], Step [472/10336], Loss: 3.3118\n",
      "Epoch [2/5], Step [474/10336], Loss: 0.2667\n",
      "Epoch [2/5], Step [476/10336], Loss: 2.9637\n",
      "Epoch [2/5], Step [478/10336], Loss: 0.4599\n",
      "Epoch [2/5], Step [480/10336], Loss: 0.2754\n",
      "Epoch [2/5], Step [482/10336], Loss: 3.2418\n",
      "Epoch [2/5], Step [484/10336], Loss: 0.2960\n",
      "Epoch [2/5], Step [486/10336], Loss: 4.5040\n",
      "Epoch [2/5], Step [488/10336], Loss: 0.5841\n",
      "Epoch [2/5], Step [490/10336], Loss: 1.8994\n",
      "Epoch [2/5], Step [492/10336], Loss: 1.1108\n",
      "Epoch [2/5], Step [494/10336], Loss: 0.4858\n",
      "Epoch [2/5], Step [496/10336], Loss: 0.0144\n",
      "Epoch [2/5], Step [498/10336], Loss: 0.3092\n",
      "Epoch [2/5], Step [500/10336], Loss: 0.0042\n",
      "Epoch [2/5], Step [502/10336], Loss: 1.9887\n",
      "Epoch [2/5], Step [504/10336], Loss: 3.2614\n",
      "Epoch [2/5], Step [506/10336], Loss: 0.0232\n",
      "Epoch [2/5], Step [508/10336], Loss: 0.4735\n",
      "Epoch [2/5], Step [510/10336], Loss: 1.3936\n",
      "Epoch [2/5], Step [512/10336], Loss: 1.1442\n",
      "Epoch [2/5], Step [514/10336], Loss: 0.1510\n",
      "Epoch [2/5], Step [516/10336], Loss: 0.0010\n",
      "Epoch [2/5], Step [518/10336], Loss: 0.1556\n",
      "Epoch [2/5], Step [520/10336], Loss: 3.1947\n",
      "Epoch [2/5], Step [522/10336], Loss: 0.5779\n",
      "Epoch [2/5], Step [524/10336], Loss: 0.6244\n",
      "Epoch [2/5], Step [526/10336], Loss: 0.0540\n",
      "Epoch [2/5], Step [528/10336], Loss: 3.7400\n",
      "Epoch [2/5], Step [530/10336], Loss: 0.5707\n",
      "Epoch [2/5], Step [532/10336], Loss: 0.7481\n",
      "Epoch [2/5], Step [534/10336], Loss: 0.0015\n",
      "Epoch [2/5], Step [536/10336], Loss: 1.2997\n",
      "Epoch [2/5], Step [538/10336], Loss: 0.3836\n",
      "Epoch [2/5], Step [540/10336], Loss: 0.0173\n",
      "Epoch [2/5], Step [542/10336], Loss: 0.6639\n",
      "Epoch [2/5], Step [544/10336], Loss: 0.3617\n",
      "Epoch [2/5], Step [546/10336], Loss: 0.6366\n",
      "Epoch [2/5], Step [548/10336], Loss: 0.3155\n",
      "Epoch [2/5], Step [550/10336], Loss: 0.1855\n",
      "Epoch [2/5], Step [552/10336], Loss: 0.0799\n",
      "Epoch [2/5], Step [554/10336], Loss: 0.3347\n",
      "Epoch [2/5], Step [556/10336], Loss: 2.5886\n",
      "Epoch [2/5], Step [558/10336], Loss: 0.3553\n",
      "Epoch [2/5], Step [560/10336], Loss: 2.2830\n",
      "Epoch [2/5], Step [562/10336], Loss: 0.0205\n",
      "Epoch [2/5], Step [564/10336], Loss: 0.2148\n",
      "Epoch [2/5], Step [566/10336], Loss: 0.8233\n",
      "Epoch [2/5], Step [568/10336], Loss: 0.4110\n",
      "Epoch [2/5], Step [570/10336], Loss: 3.2792\n",
      "Epoch [2/5], Step [572/10336], Loss: 0.3320\n",
      "Epoch [2/5], Step [574/10336], Loss: 1.6148\n",
      "Epoch [2/5], Step [576/10336], Loss: 0.1827\n",
      "Epoch [2/5], Step [578/10336], Loss: 2.1004\n",
      "Epoch [2/5], Step [580/10336], Loss: 0.2071\n",
      "Epoch [2/5], Step [582/10336], Loss: 0.3089\n",
      "Epoch [2/5], Step [584/10336], Loss: 0.6386\n",
      "Epoch [2/5], Step [586/10336], Loss: 0.0334\n",
      "Epoch [2/5], Step [588/10336], Loss: 0.2765\n",
      "Epoch [2/5], Step [590/10336], Loss: 0.9078\n",
      "Epoch [2/5], Step [592/10336], Loss: 1.1308\n",
      "Epoch [2/5], Step [594/10336], Loss: 0.5280\n",
      "Epoch [2/5], Step [596/10336], Loss: 4.4651\n",
      "Epoch [2/5], Step [598/10336], Loss: 0.0265\n",
      "Epoch [2/5], Step [600/10336], Loss: 0.2312\n",
      "Epoch [2/5], Step [602/10336], Loss: 1.3432\n",
      "Epoch [2/5], Step [604/10336], Loss: 2.8257\n",
      "Epoch [2/5], Step [606/10336], Loss: 0.2373\n",
      "Epoch [2/5], Step [608/10336], Loss: 0.8797\n",
      "Epoch [2/5], Step [610/10336], Loss: 2.7731\n",
      "Epoch [2/5], Step [612/10336], Loss: 0.0530\n",
      "Epoch [2/5], Step [614/10336], Loss: 1.4551\n",
      "Epoch [2/5], Step [616/10336], Loss: 0.5034\n",
      "Epoch [2/5], Step [618/10336], Loss: 2.7937\n",
      "Epoch [2/5], Step [620/10336], Loss: 0.2154\n",
      "Epoch [2/5], Step [622/10336], Loss: 0.1545\n",
      "Epoch [2/5], Step [624/10336], Loss: 0.0044\n",
      "Epoch [2/5], Step [626/10336], Loss: 0.3862\n",
      "Epoch [2/5], Step [628/10336], Loss: 0.2089\n",
      "Epoch [2/5], Step [630/10336], Loss: 0.9167\n",
      "Epoch [2/5], Step [632/10336], Loss: 0.4366\n",
      "Epoch [2/5], Step [634/10336], Loss: 3.5334\n",
      "Epoch [2/5], Step [636/10336], Loss: 0.8287\n",
      "Epoch [2/5], Step [638/10336], Loss: 0.4046\n",
      "Epoch [2/5], Step [640/10336], Loss: 2.4229\n",
      "Epoch [2/5], Step [642/10336], Loss: 0.6275\n",
      "Epoch [2/5], Step [644/10336], Loss: 3.3245\n",
      "Epoch [2/5], Step [646/10336], Loss: 0.0900\n",
      "Epoch [2/5], Step [648/10336], Loss: 0.5081\n",
      "Epoch [2/5], Step [650/10336], Loss: 1.3140\n",
      "Epoch [2/5], Step [652/10336], Loss: 0.1732\n",
      "Epoch [2/5], Step [654/10336], Loss: 0.2379\n",
      "Epoch [2/5], Step [656/10336], Loss: 0.3253\n",
      "Epoch [2/5], Step [658/10336], Loss: 0.1127\n",
      "Epoch [2/5], Step [660/10336], Loss: 0.0057\n",
      "Epoch [2/5], Step [662/10336], Loss: 5.3150\n",
      "Epoch [2/5], Step [664/10336], Loss: 0.2473\n",
      "Epoch [2/5], Step [666/10336], Loss: 0.4846\n",
      "Epoch [2/5], Step [668/10336], Loss: 2.2127\n",
      "Epoch [2/5], Step [670/10336], Loss: 0.2745\n",
      "Epoch [2/5], Step [672/10336], Loss: 1.6245\n",
      "Epoch [2/5], Step [674/10336], Loss: 0.3896\n",
      "Epoch [2/5], Step [676/10336], Loss: 0.2275\n",
      "Epoch [2/5], Step [678/10336], Loss: 1.5539\n",
      "Epoch [2/5], Step [680/10336], Loss: 0.0073\n",
      "Epoch [2/5], Step [682/10336], Loss: 1.0380\n",
      "Epoch [2/5], Step [684/10336], Loss: 0.2568\n",
      "Epoch [2/5], Step [686/10336], Loss: 3.1876\n",
      "Epoch [2/5], Step [688/10336], Loss: 0.0424\n",
      "Epoch [2/5], Step [690/10336], Loss: 0.2457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [692/10336], Loss: 0.4316\n",
      "Epoch [2/5], Step [694/10336], Loss: 1.0297\n",
      "Epoch [2/5], Step [696/10336], Loss: 0.4462\n",
      "Epoch [2/5], Step [698/10336], Loss: 0.4219\n",
      "Epoch [2/5], Step [700/10336], Loss: 0.3398\n",
      "Epoch [2/5], Step [702/10336], Loss: 0.2011\n",
      "Epoch [2/5], Step [704/10336], Loss: 1.2994\n",
      "Epoch [2/5], Step [706/10336], Loss: 3.2960\n",
      "Epoch [2/5], Step [708/10336], Loss: 5.4361\n",
      "Epoch [2/5], Step [710/10336], Loss: 2.3926\n",
      "Epoch [2/5], Step [712/10336], Loss: 1.0464\n",
      "Epoch [2/5], Step [714/10336], Loss: 0.3680\n",
      "Epoch [2/5], Step [716/10336], Loss: 0.4896\n",
      "Epoch [2/5], Step [718/10336], Loss: 3.9497\n",
      "Epoch [2/5], Step [720/10336], Loss: 1.2874\n",
      "Epoch [2/5], Step [722/10336], Loss: 0.1422\n",
      "Epoch [2/5], Step [724/10336], Loss: 2.3357\n",
      "Epoch [2/5], Step [726/10336], Loss: 0.1489\n",
      "Epoch [2/5], Step [728/10336], Loss: 0.4554\n",
      "Epoch [2/5], Step [730/10336], Loss: 0.2063\n",
      "Epoch [2/5], Step [732/10336], Loss: 2.9648\n",
      "Epoch [2/5], Step [734/10336], Loss: 0.1772\n",
      "Epoch [2/5], Step [736/10336], Loss: 0.0593\n",
      "Epoch [2/5], Step [738/10336], Loss: 0.9596\n",
      "Epoch [2/5], Step [740/10336], Loss: 4.0755\n",
      "Epoch [2/5], Step [742/10336], Loss: 0.4462\n",
      "Epoch [2/5], Step [744/10336], Loss: 0.0353\n",
      "Epoch [2/5], Step [746/10336], Loss: 2.5339\n",
      "Epoch [2/5], Step [748/10336], Loss: 1.5768\n",
      "Epoch [2/5], Step [750/10336], Loss: 0.2424\n",
      "Epoch [2/5], Step [752/10336], Loss: 1.1188\n",
      "Epoch [2/5], Step [754/10336], Loss: 0.5719\n",
      "Epoch [2/5], Step [756/10336], Loss: 0.2161\n",
      "Epoch [2/5], Step [758/10336], Loss: 0.3612\n",
      "Epoch [2/5], Step [760/10336], Loss: 3.4234\n",
      "Epoch [2/5], Step [762/10336], Loss: 0.0021\n",
      "Epoch [2/5], Step [764/10336], Loss: 0.2346\n",
      "Epoch [2/5], Step [766/10336], Loss: 0.9424\n",
      "Epoch [2/5], Step [768/10336], Loss: 0.9252\n",
      "Epoch [2/5], Step [770/10336], Loss: 0.7800\n",
      "Epoch [2/5], Step [772/10336], Loss: 5.0589\n",
      "Epoch [2/5], Step [774/10336], Loss: 2.7562\n",
      "Epoch [2/5], Step [776/10336], Loss: 2.2486\n",
      "Epoch [2/5], Step [778/10336], Loss: 0.2326\n",
      "Epoch [2/5], Step [780/10336], Loss: 0.3393\n",
      "Epoch [2/5], Step [782/10336], Loss: 2.2467\n",
      "Epoch [2/5], Step [784/10336], Loss: 3.8155\n",
      "Epoch [2/5], Step [786/10336], Loss: 0.3580\n",
      "Epoch [2/5], Step [788/10336], Loss: 1.3666\n",
      "Epoch [2/5], Step [790/10336], Loss: 0.5615\n",
      "Epoch [2/5], Step [792/10336], Loss: 3.0938\n",
      "Epoch [2/5], Step [794/10336], Loss: 0.0736\n",
      "Epoch [2/5], Step [796/10336], Loss: 0.3640\n",
      "Epoch [2/5], Step [798/10336], Loss: 0.6015\n",
      "Epoch [2/5], Step [800/10336], Loss: 1.5890\n",
      "Epoch [2/5], Step [802/10336], Loss: 0.0558\n",
      "Epoch [2/5], Step [804/10336], Loss: 0.3453\n",
      "Epoch [2/5], Step [806/10336], Loss: 0.2813\n",
      "Epoch [2/5], Step [808/10336], Loss: 3.7873\n",
      "Epoch [2/5], Step [810/10336], Loss: 0.2981\n",
      "Epoch [2/5], Step [812/10336], Loss: 1.0612\n",
      "Epoch [2/5], Step [814/10336], Loss: 0.6351\n",
      "Epoch [2/5], Step [816/10336], Loss: 1.4325\n",
      "Epoch [2/5], Step [818/10336], Loss: 0.2212\n",
      "Epoch [2/5], Step [820/10336], Loss: 0.3956\n",
      "Epoch [2/5], Step [822/10336], Loss: 0.1769\n",
      "Epoch [2/5], Step [824/10336], Loss: 0.0869\n",
      "Epoch [2/5], Step [826/10336], Loss: 0.3542\n",
      "Epoch [2/5], Step [828/10336], Loss: 0.0446\n",
      "Epoch [2/5], Step [830/10336], Loss: 0.1361\n",
      "Epoch [2/5], Step [832/10336], Loss: 3.6625\n",
      "Epoch [2/5], Step [834/10336], Loss: 1.5188\n",
      "Epoch [2/5], Step [836/10336], Loss: 0.2900\n",
      "Epoch [2/5], Step [838/10336], Loss: 0.2121\n",
      "Epoch [2/5], Step [840/10336], Loss: 1.9939\n",
      "Epoch [2/5], Step [842/10336], Loss: 0.6730\n",
      "Epoch [2/5], Step [844/10336], Loss: 0.7406\n",
      "Epoch [2/5], Step [846/10336], Loss: 0.8884\n",
      "Epoch [2/5], Step [848/10336], Loss: 1.3586\n",
      "Epoch [2/5], Step [850/10336], Loss: 1.5762\n",
      "Epoch [2/5], Step [852/10336], Loss: 0.7135\n",
      "Epoch [2/5], Step [854/10336], Loss: 5.1046\n",
      "Epoch [2/5], Step [856/10336], Loss: 0.2025\n",
      "Epoch [2/5], Step [858/10336], Loss: 0.9341\n",
      "Epoch [2/5], Step [860/10336], Loss: 0.1557\n",
      "Epoch [2/5], Step [862/10336], Loss: 0.7338\n",
      "Epoch [2/5], Step [864/10336], Loss: 0.2809\n",
      "Epoch [2/5], Step [866/10336], Loss: 0.5016\n",
      "Epoch [2/5], Step [868/10336], Loss: 0.0108\n",
      "Epoch [2/5], Step [870/10336], Loss: 0.3285\n",
      "Epoch [2/5], Step [872/10336], Loss: 0.1241\n",
      "Epoch [2/5], Step [874/10336], Loss: 0.8707\n",
      "Epoch [2/5], Step [876/10336], Loss: 3.8281\n",
      "Epoch [2/5], Step [878/10336], Loss: 1.6411\n",
      "Epoch [2/5], Step [880/10336], Loss: 0.8950\n",
      "Epoch [2/5], Step [882/10336], Loss: 0.2502\n",
      "Epoch [2/5], Step [884/10336], Loss: 4.5650\n",
      "Epoch [2/5], Step [886/10336], Loss: 1.2634\n",
      "Epoch [2/5], Step [888/10336], Loss: 0.1011\n",
      "Epoch [2/5], Step [890/10336], Loss: 0.0760\n",
      "Epoch [2/5], Step [892/10336], Loss: 2.0815\n",
      "Epoch [2/5], Step [894/10336], Loss: 0.3045\n",
      "Epoch [2/5], Step [896/10336], Loss: 0.3346\n",
      "Epoch [2/5], Step [898/10336], Loss: 3.0268\n",
      "Epoch [2/5], Step [900/10336], Loss: 0.0240\n",
      "Epoch [2/5], Step [902/10336], Loss: 0.8597\n",
      "Epoch [2/5], Step [904/10336], Loss: 1.4780\n",
      "Epoch [2/5], Step [906/10336], Loss: 0.7245\n",
      "Epoch [2/5], Step [908/10336], Loss: 3.0837\n",
      "Epoch [2/5], Step [910/10336], Loss: 0.6807\n",
      "Epoch [2/5], Step [912/10336], Loss: 6.1698\n",
      "Epoch [2/5], Step [914/10336], Loss: 0.1557\n",
      "Epoch [2/5], Step [916/10336], Loss: 4.7398\n",
      "Epoch [2/5], Step [918/10336], Loss: 0.7131\n",
      "Epoch [2/5], Step [920/10336], Loss: 0.2792\n",
      "Epoch [2/5], Step [922/10336], Loss: 0.1487\n",
      "Epoch [2/5], Step [924/10336], Loss: 0.1929\n",
      "Epoch [2/5], Step [926/10336], Loss: 0.4189\n",
      "Epoch [2/5], Step [928/10336], Loss: 0.4124\n",
      "Epoch [2/5], Step [930/10336], Loss: 0.0331\n",
      "Epoch [2/5], Step [932/10336], Loss: 0.7358\n",
      "Epoch [2/5], Step [934/10336], Loss: 1.9069\n",
      "Epoch [2/5], Step [936/10336], Loss: 2.6339\n",
      "Epoch [2/5], Step [938/10336], Loss: 0.3193\n",
      "Epoch [2/5], Step [940/10336], Loss: 0.6039\n",
      "Epoch [2/5], Step [942/10336], Loss: 3.4077\n",
      "Epoch [2/5], Step [944/10336], Loss: 0.4644\n",
      "Epoch [2/5], Step [946/10336], Loss: 0.1435\n",
      "Epoch [2/5], Step [948/10336], Loss: 1.1225\n",
      "Epoch [2/5], Step [950/10336], Loss: 0.0031\n",
      "Epoch [2/5], Step [952/10336], Loss: 0.4973\n",
      "Epoch [2/5], Step [954/10336], Loss: 0.3630\n",
      "Epoch [2/5], Step [956/10336], Loss: 0.4305\n",
      "Epoch [2/5], Step [958/10336], Loss: 0.4339\n",
      "Epoch [2/5], Step [960/10336], Loss: 0.8575\n",
      "Epoch [2/5], Step [962/10336], Loss: 0.5621\n",
      "Epoch [2/5], Step [964/10336], Loss: 2.0469\n",
      "Epoch [2/5], Step [966/10336], Loss: 0.2368\n",
      "Epoch [2/5], Step [968/10336], Loss: 0.3265\n",
      "Epoch [2/5], Step [970/10336], Loss: 0.0170\n",
      "Epoch [2/5], Step [972/10336], Loss: 0.3680\n",
      "Epoch [2/5], Step [974/10336], Loss: 0.2580\n",
      "Epoch [2/5], Step [976/10336], Loss: 0.5597\n",
      "Epoch [2/5], Step [978/10336], Loss: 0.2093\n",
      "Epoch [2/5], Step [980/10336], Loss: 0.4439\n",
      "Epoch [2/5], Step [982/10336], Loss: 5.0926\n",
      "Epoch [2/5], Step [984/10336], Loss: 0.2320\n",
      "Epoch [2/5], Step [986/10336], Loss: 2.0347\n",
      "Epoch [2/5], Step [988/10336], Loss: 0.0887\n",
      "Epoch [2/5], Step [990/10336], Loss: 0.1810\n",
      "Epoch [2/5], Step [992/10336], Loss: 2.0684\n",
      "Epoch [2/5], Step [994/10336], Loss: 1.4050\n",
      "Epoch [2/5], Step [996/10336], Loss: 1.3589\n",
      "Epoch [2/5], Step [998/10336], Loss: 1.7868\n",
      "Epoch [2/5], Step [1000/10336], Loss: 0.2521\n",
      "Epoch [2/5], Step [1002/10336], Loss: 0.3583\n",
      "Epoch [2/5], Step [1004/10336], Loss: 0.2561\n",
      "Epoch [2/5], Step [1006/10336], Loss: 0.1950\n",
      "Epoch [2/5], Step [1008/10336], Loss: 2.1206\n",
      "Epoch [2/5], Step [1010/10336], Loss: 0.0215\n",
      "Epoch [2/5], Step [1012/10336], Loss: 0.9011\n",
      "Epoch [2/5], Step [1014/10336], Loss: 0.4198\n",
      "Epoch [2/5], Step [1016/10336], Loss: 0.6834\n",
      "Epoch [2/5], Step [1018/10336], Loss: 0.3054\n",
      "Epoch [2/5], Step [1020/10336], Loss: 0.0347\n",
      "Epoch [2/5], Step [1022/10336], Loss: 0.3228\n",
      "Epoch [2/5], Step [1024/10336], Loss: 0.2757\n",
      "Epoch [2/5], Step [1026/10336], Loss: 1.8120\n",
      "Epoch [2/5], Step [1028/10336], Loss: 0.6165\n",
      "Epoch [2/5], Step [1030/10336], Loss: 1.6980\n",
      "Epoch [2/5], Step [1032/10336], Loss: 0.2119\n",
      "Epoch [2/5], Step [1034/10336], Loss: 2.6732\n",
      "Epoch [2/5], Step [1036/10336], Loss: 0.3495\n",
      "Epoch [2/5], Step [1038/10336], Loss: 0.4076\n",
      "Epoch [2/5], Step [1040/10336], Loss: 0.8302\n",
      "Epoch [2/5], Step [1042/10336], Loss: 0.2851\n",
      "Epoch [2/5], Step [1044/10336], Loss: 0.2027\n",
      "Epoch [2/5], Step [1046/10336], Loss: 0.6443\n",
      "Epoch [2/5], Step [1048/10336], Loss: 0.7462\n",
      "Epoch [2/5], Step [1050/10336], Loss: 0.0598\n",
      "Epoch [2/5], Step [1052/10336], Loss: 0.4831\n",
      "Epoch [2/5], Step [1054/10336], Loss: 4.5017\n",
      "Epoch [2/5], Step [1056/10336], Loss: 0.2202\n",
      "Epoch [2/5], Step [1058/10336], Loss: 0.6620\n",
      "Epoch [2/5], Step [1060/10336], Loss: 4.4510\n",
      "Epoch [2/5], Step [1062/10336], Loss: 0.5763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [1064/10336], Loss: 0.8371\n",
      "Epoch [2/5], Step [1066/10336], Loss: 0.7131\n",
      "Epoch [2/5], Step [1068/10336], Loss: 0.1710\n",
      "Epoch [2/5], Step [1070/10336], Loss: 0.3611\n",
      "Epoch [2/5], Step [1072/10336], Loss: 0.8620\n",
      "Epoch [2/5], Step [1074/10336], Loss: 1.0023\n",
      "Epoch [2/5], Step [1076/10336], Loss: 2.3722\n",
      "Epoch [2/5], Step [1078/10336], Loss: 0.2773\n",
      "Epoch [2/5], Step [1080/10336], Loss: 0.1430\n",
      "Epoch [2/5], Step [1082/10336], Loss: 1.5997\n",
      "Epoch [2/5], Step [1084/10336], Loss: 0.6765\n",
      "Epoch [2/5], Step [1086/10336], Loss: 1.0518\n",
      "Epoch [2/5], Step [1088/10336], Loss: 0.4188\n",
      "Epoch [2/5], Step [1090/10336], Loss: 0.7415\n",
      "Epoch [2/5], Step [1092/10336], Loss: 0.0685\n",
      "Epoch [2/5], Step [1094/10336], Loss: 0.6513\n",
      "Epoch [2/5], Step [1096/10336], Loss: 5.1443\n",
      "Epoch [2/5], Step [1098/10336], Loss: 2.5877\n",
      "Epoch [2/5], Step [1100/10336], Loss: 0.2471\n",
      "Epoch [2/5], Step [1102/10336], Loss: 0.0256\n",
      "Epoch [2/5], Step [1104/10336], Loss: 3.1339\n",
      "Epoch [2/5], Step [1106/10336], Loss: 0.7635\n",
      "Epoch [2/5], Step [1108/10336], Loss: 0.1075\n",
      "Epoch [2/5], Step [1110/10336], Loss: 0.2042\n",
      "Epoch [2/5], Step [1112/10336], Loss: 0.6000\n",
      "Epoch [2/5], Step [1114/10336], Loss: 0.6466\n",
      "Epoch [2/5], Step [1116/10336], Loss: 2.8355\n",
      "Epoch [2/5], Step [1118/10336], Loss: 1.0637\n",
      "Epoch [2/5], Step [1120/10336], Loss: 0.0883\n",
      "Epoch [2/5], Step [1122/10336], Loss: 1.0081\n",
      "Epoch [2/5], Step [1124/10336], Loss: 0.0752\n",
      "Epoch [2/5], Step [1126/10336], Loss: 0.6540\n",
      "Epoch [2/5], Step [1128/10336], Loss: 4.0418\n",
      "Epoch [2/5], Step [1130/10336], Loss: 1.7955\n",
      "Epoch [2/5], Step [1132/10336], Loss: 4.3824\n",
      "Epoch [2/5], Step [1134/10336], Loss: 2.4942\n",
      "Epoch [2/5], Step [1136/10336], Loss: 0.2910\n",
      "Epoch [2/5], Step [1138/10336], Loss: 0.2903\n",
      "Epoch [2/5], Step [1140/10336], Loss: 0.6347\n",
      "Epoch [2/5], Step [1142/10336], Loss: 0.3873\n",
      "Epoch [2/5], Step [1144/10336], Loss: 1.1040\n",
      "Epoch [2/5], Step [1146/10336], Loss: 4.0906\n",
      "Epoch [2/5], Step [1148/10336], Loss: 1.2028\n",
      "Epoch [2/5], Step [1150/10336], Loss: 2.3560\n",
      "Epoch [2/5], Step [1152/10336], Loss: 3.5141\n",
      "Epoch [2/5], Step [1154/10336], Loss: 0.3089\n",
      "Epoch [2/5], Step [1156/10336], Loss: 0.4465\n",
      "Epoch [2/5], Step [1158/10336], Loss: 0.9019\n",
      "Epoch [2/5], Step [1160/10336], Loss: 1.9422\n",
      "Epoch [2/5], Step [1162/10336], Loss: 2.8356\n",
      "Epoch [2/5], Step [1164/10336], Loss: 0.2773\n",
      "Epoch [2/5], Step [1166/10336], Loss: 0.1725\n",
      "Epoch [2/5], Step [1168/10336], Loss: 0.3163\n",
      "Epoch [2/5], Step [1170/10336], Loss: 2.2488\n",
      "Epoch [2/5], Step [1172/10336], Loss: 0.2621\n",
      "Epoch [2/5], Step [1174/10336], Loss: 0.2755\n",
      "Epoch [2/5], Step [1176/10336], Loss: 0.8304\n",
      "Epoch [2/5], Step [1178/10336], Loss: 0.2770\n",
      "Epoch [2/5], Step [1180/10336], Loss: 0.0663\n",
      "Epoch [2/5], Step [1182/10336], Loss: 0.3552\n",
      "Epoch [2/5], Step [1184/10336], Loss: 1.2919\n",
      "Epoch [2/5], Step [1186/10336], Loss: 0.0384\n",
      "Epoch [2/5], Step [1188/10336], Loss: 0.3037\n",
      "Epoch [2/5], Step [1190/10336], Loss: 0.1792\n",
      "Epoch [2/5], Step [1192/10336], Loss: 0.0693\n",
      "Epoch [2/5], Step [1194/10336], Loss: 0.2356\n",
      "Epoch [2/5], Step [1196/10336], Loss: 0.1010\n",
      "Epoch [2/5], Step [1198/10336], Loss: 0.2337\n",
      "Epoch [2/5], Step [1200/10336], Loss: 1.2207\n",
      "Epoch [2/5], Step [1202/10336], Loss: 0.9368\n",
      "Epoch [2/5], Step [1204/10336], Loss: 0.3300\n",
      "Epoch [2/5], Step [1206/10336], Loss: 1.9714\n",
      "Epoch [2/5], Step [1208/10336], Loss: 0.4649\n",
      "Epoch [2/5], Step [1210/10336], Loss: 0.2989\n",
      "Epoch [2/5], Step [1212/10336], Loss: 2.0514\n",
      "Epoch [2/5], Step [1214/10336], Loss: 1.6777\n",
      "Epoch [2/5], Step [1216/10336], Loss: 2.9221\n",
      "Epoch [2/5], Step [1218/10336], Loss: 0.0093\n",
      "Epoch [2/5], Step [1220/10336], Loss: 0.3240\n",
      "Epoch [2/5], Step [1222/10336], Loss: 2.4320\n",
      "Epoch [2/5], Step [1224/10336], Loss: 0.0090\n",
      "Epoch [2/5], Step [1226/10336], Loss: 1.7744\n",
      "Epoch [2/5], Step [1228/10336], Loss: 4.4791\n",
      "Epoch [2/5], Step [1230/10336], Loss: 2.6605\n",
      "Epoch [2/5], Step [1232/10336], Loss: 0.6500\n",
      "Epoch [2/5], Step [1234/10336], Loss: 0.4843\n",
      "Epoch [2/5], Step [1236/10336], Loss: 0.4367\n",
      "Epoch [2/5], Step [1238/10336], Loss: 3.1322\n",
      "Epoch [2/5], Step [1240/10336], Loss: 0.0751\n",
      "Epoch [2/5], Step [1242/10336], Loss: 0.2741\n",
      "Epoch [2/5], Step [1244/10336], Loss: 0.3208\n",
      "Epoch [2/5], Step [1246/10336], Loss: 3.4417\n",
      "Epoch [2/5], Step [1248/10336], Loss: 0.0821\n",
      "Epoch [2/5], Step [1250/10336], Loss: 2.5989\n",
      "Epoch [2/5], Step [1252/10336], Loss: 1.5721\n",
      "Epoch [2/5], Step [1254/10336], Loss: 1.6841\n",
      "Epoch [2/5], Step [1256/10336], Loss: 2.0743\n",
      "Epoch [2/5], Step [1258/10336], Loss: 0.4275\n",
      "Epoch [2/5], Step [1260/10336], Loss: 2.7450\n",
      "Epoch [2/5], Step [1262/10336], Loss: 0.3562\n",
      "Epoch [2/5], Step [1264/10336], Loss: 0.1661\n",
      "Epoch [2/5], Step [1266/10336], Loss: 3.8099\n",
      "Epoch [2/5], Step [1268/10336], Loss: 0.5861\n",
      "Epoch [2/5], Step [1270/10336], Loss: 0.2930\n",
      "Epoch [2/5], Step [1272/10336], Loss: 0.3902\n",
      "Epoch [2/5], Step [1274/10336], Loss: 3.2930\n",
      "Epoch [2/5], Step [1276/10336], Loss: 0.0490\n",
      "Epoch [2/5], Step [1278/10336], Loss: 0.3053\n",
      "Epoch [2/5], Step [1280/10336], Loss: 0.6866\n",
      "Epoch [2/5], Step [1282/10336], Loss: 0.1420\n",
      "Epoch [2/5], Step [1284/10336], Loss: 0.6656\n",
      "Epoch [2/5], Step [1286/10336], Loss: 0.4591\n",
      "Epoch [2/5], Step [1288/10336], Loss: 0.2131\n",
      "Epoch [2/5], Step [1290/10336], Loss: 1.9238\n",
      "Epoch [2/5], Step [1292/10336], Loss: 0.0083\n",
      "Epoch [2/5], Step [1294/10336], Loss: 0.7855\n",
      "Epoch [2/5], Step [1296/10336], Loss: 0.0780\n",
      "Epoch [2/5], Step [1298/10336], Loss: 4.2427\n",
      "Epoch [2/5], Step [1300/10336], Loss: 3.0445\n",
      "Epoch [2/5], Step [1302/10336], Loss: 2.7678\n",
      "Epoch [2/5], Step [1304/10336], Loss: 1.0334\n",
      "Epoch [2/5], Step [1306/10336], Loss: 0.1890\n",
      "Epoch [2/5], Step [1308/10336], Loss: 0.2923\n",
      "Epoch [2/5], Step [1310/10336], Loss: 2.6722\n",
      "Epoch [2/5], Step [1312/10336], Loss: 0.2724\n",
      "Epoch [2/5], Step [1314/10336], Loss: 0.2876\n",
      "Epoch [2/5], Step [1316/10336], Loss: 2.0308\n",
      "Epoch [2/5], Step [1318/10336], Loss: 0.3387\n",
      "Epoch [2/5], Step [1320/10336], Loss: 0.5416\n",
      "Epoch [2/5], Step [1322/10336], Loss: 0.6715\n",
      "Epoch [2/5], Step [1324/10336], Loss: 0.2807\n",
      "Epoch [2/5], Step [1326/10336], Loss: 2.5311\n",
      "Epoch [2/5], Step [1328/10336], Loss: 2.0723\n",
      "Epoch [2/5], Step [1330/10336], Loss: 0.2266\n",
      "Epoch [2/5], Step [1332/10336], Loss: 0.1601\n",
      "Epoch [2/5], Step [1334/10336], Loss: 0.9658\n",
      "Epoch [2/5], Step [1336/10336], Loss: 0.2332\n",
      "Epoch [2/5], Step [1338/10336], Loss: 0.0621\n",
      "Epoch [2/5], Step [1340/10336], Loss: 1.2555\n",
      "Epoch [2/5], Step [1342/10336], Loss: 1.5318\n",
      "Epoch [2/5], Step [1344/10336], Loss: 0.0303\n",
      "Epoch [2/5], Step [1346/10336], Loss: 0.9614\n",
      "Epoch [2/5], Step [1348/10336], Loss: 0.3422\n",
      "Epoch [2/5], Step [1350/10336], Loss: 0.1923\n",
      "Epoch [2/5], Step [1352/10336], Loss: 0.5566\n",
      "Epoch [2/5], Step [1354/10336], Loss: 3.2085\n",
      "Epoch [2/5], Step [1356/10336], Loss: 0.8178\n",
      "Epoch [2/5], Step [1358/10336], Loss: 0.2945\n",
      "Epoch [2/5], Step [1360/10336], Loss: 0.3969\n",
      "Epoch [2/5], Step [1362/10336], Loss: 0.3160\n",
      "Epoch [2/5], Step [1364/10336], Loss: 1.7904\n",
      "Epoch [2/5], Step [1366/10336], Loss: 0.7816\n",
      "Epoch [2/5], Step [1368/10336], Loss: 0.2511\n",
      "Epoch [2/5], Step [1370/10336], Loss: 0.1686\n",
      "Epoch [2/5], Step [1372/10336], Loss: 3.6417\n",
      "Epoch [2/5], Step [1374/10336], Loss: 0.3045\n",
      "Epoch [2/5], Step [1376/10336], Loss: 0.1193\n",
      "Epoch [2/5], Step [1378/10336], Loss: 2.3932\n",
      "Epoch [2/5], Step [1380/10336], Loss: 0.1084\n",
      "Epoch [2/5], Step [1382/10336], Loss: 0.1463\n",
      "Epoch [2/5], Step [1384/10336], Loss: 0.4090\n",
      "Epoch [2/5], Step [1386/10336], Loss: 1.1118\n",
      "Epoch [2/5], Step [1388/10336], Loss: 2.7849\n",
      "Epoch [2/5], Step [1390/10336], Loss: 0.3812\n",
      "Epoch [2/5], Step [1392/10336], Loss: 2.0986\n",
      "Epoch [2/5], Step [1394/10336], Loss: 0.3076\n",
      "Epoch [2/5], Step [1396/10336], Loss: 0.6894\n",
      "Epoch [2/5], Step [1398/10336], Loss: 1.9015\n",
      "Epoch [2/5], Step [1400/10336], Loss: 0.3144\n",
      "Epoch [2/5], Step [1402/10336], Loss: 2.8472\n",
      "Epoch [2/5], Step [1404/10336], Loss: 0.4431\n",
      "Epoch [2/5], Step [1406/10336], Loss: 0.6728\n",
      "Epoch [2/5], Step [1408/10336], Loss: 0.1233\n",
      "Epoch [2/5], Step [1410/10336], Loss: 0.6875\n",
      "Epoch [2/5], Step [1412/10336], Loss: 0.7765\n",
      "Epoch [2/5], Step [1414/10336], Loss: 0.6816\n",
      "Epoch [2/5], Step [1416/10336], Loss: 0.7054\n",
      "Epoch [2/5], Step [1418/10336], Loss: 0.6974\n",
      "Epoch [2/5], Step [1420/10336], Loss: 0.2788\n",
      "Epoch [2/5], Step [1422/10336], Loss: 0.3497\n",
      "Epoch [2/5], Step [1424/10336], Loss: 1.2367\n",
      "Epoch [2/5], Step [1426/10336], Loss: 0.5553\n",
      "Epoch [2/5], Step [1428/10336], Loss: 0.3203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [1430/10336], Loss: 0.7497\n",
      "Epoch [2/5], Step [1432/10336], Loss: 0.0149\n",
      "Epoch [2/5], Step [1434/10336], Loss: 0.1130\n",
      "Epoch [2/5], Step [1436/10336], Loss: 0.0660\n",
      "Epoch [2/5], Step [1438/10336], Loss: 0.0442\n",
      "Epoch [2/5], Step [1440/10336], Loss: 3.4523\n",
      "Epoch [2/5], Step [1442/10336], Loss: 0.6005\n",
      "Epoch [2/5], Step [1444/10336], Loss: 0.0146\n",
      "Epoch [2/5], Step [1446/10336], Loss: 1.2488\n",
      "Epoch [2/5], Step [1448/10336], Loss: 0.2428\n",
      "Epoch [2/5], Step [1450/10336], Loss: 5.3715\n",
      "Epoch [2/5], Step [1452/10336], Loss: 0.3561\n",
      "Epoch [2/5], Step [1454/10336], Loss: 0.5363\n",
      "Epoch [2/5], Step [1456/10336], Loss: 1.6674\n",
      "Epoch [2/5], Step [1458/10336], Loss: 0.8303\n",
      "Epoch [2/5], Step [1460/10336], Loss: 0.3879\n",
      "Epoch [2/5], Step [1462/10336], Loss: 3.6080\n",
      "Epoch [2/5], Step [1464/10336], Loss: 2.3511\n",
      "Epoch [2/5], Step [1466/10336], Loss: 0.6793\n",
      "Epoch [2/5], Step [1468/10336], Loss: 0.2234\n",
      "Epoch [2/5], Step [1470/10336], Loss: 0.1651\n",
      "Epoch [2/5], Step [1472/10336], Loss: 0.6805\n",
      "Epoch [2/5], Step [1474/10336], Loss: 0.3922\n",
      "Epoch [2/5], Step [1476/10336], Loss: 3.6868\n",
      "Epoch [2/5], Step [1478/10336], Loss: 0.8114\n",
      "Epoch [2/5], Step [1480/10336], Loss: 1.0622\n",
      "Epoch [2/5], Step [1482/10336], Loss: 0.3338\n",
      "Epoch [2/5], Step [1484/10336], Loss: 0.3772\n",
      "Epoch [2/5], Step [1486/10336], Loss: 3.9687\n",
      "Epoch [2/5], Step [1488/10336], Loss: 0.2014\n",
      "Epoch [2/5], Step [1490/10336], Loss: 0.2239\n",
      "Epoch [2/5], Step [1492/10336], Loss: 0.9947\n",
      "Epoch [2/5], Step [1494/10336], Loss: 1.1513\n",
      "Epoch [2/5], Step [1496/10336], Loss: 0.5030\n",
      "Epoch [2/5], Step [1498/10336], Loss: 0.6744\n",
      "Epoch [2/5], Step [1500/10336], Loss: 0.0284\n",
      "Epoch [2/5], Step [1502/10336], Loss: 0.2100\n",
      "Epoch [2/5], Step [1504/10336], Loss: 0.8847\n",
      "Epoch [2/5], Step [1506/10336], Loss: 3.9830\n",
      "Epoch [2/5], Step [1508/10336], Loss: 0.2835\n",
      "Epoch [2/5], Step [1510/10336], Loss: 1.0797\n",
      "Epoch [2/5], Step [1512/10336], Loss: 5.7571\n",
      "Epoch [2/5], Step [1514/10336], Loss: 1.0267\n",
      "Epoch [2/5], Step [1516/10336], Loss: 0.2468\n",
      "Epoch [2/5], Step [1518/10336], Loss: 0.3457\n",
      "Epoch [2/5], Step [1520/10336], Loss: 0.7154\n",
      "Epoch [2/5], Step [1522/10336], Loss: 0.3750\n",
      "Epoch [2/5], Step [1524/10336], Loss: 0.4414\n",
      "Epoch [2/5], Step [1526/10336], Loss: 1.0230\n",
      "Epoch [2/5], Step [1528/10336], Loss: 0.0031\n",
      "Epoch [2/5], Step [1530/10336], Loss: 0.7647\n",
      "Epoch [2/5], Step [1532/10336], Loss: 0.4449\n",
      "Epoch [2/5], Step [1534/10336], Loss: 0.4902\n",
      "Epoch [2/5], Step [1536/10336], Loss: 0.5847\n",
      "Epoch [2/5], Step [1538/10336], Loss: 0.0344\n",
      "Epoch [2/5], Step [1540/10336], Loss: 0.2984\n",
      "Epoch [2/5], Step [1542/10336], Loss: 0.6818\n",
      "Epoch [2/5], Step [1544/10336], Loss: 0.7244\n",
      "Epoch [2/5], Step [1546/10336], Loss: 0.0021\n",
      "Epoch [2/5], Step [1548/10336], Loss: 0.0431\n",
      "Epoch [2/5], Step [1550/10336], Loss: 0.0374\n",
      "Epoch [2/5], Step [1552/10336], Loss: 0.5095\n",
      "Epoch [2/5], Step [1554/10336], Loss: 0.6134\n",
      "Epoch [2/5], Step [1556/10336], Loss: 1.2850\n",
      "Epoch [2/5], Step [1558/10336], Loss: 0.1560\n",
      "Epoch [2/5], Step [1560/10336], Loss: 0.0122\n",
      "Epoch [2/5], Step [1562/10336], Loss: 3.1306\n",
      "Epoch [2/5], Step [1564/10336], Loss: 0.4622\n",
      "Epoch [2/5], Step [1566/10336], Loss: 3.3093\n",
      "Epoch [2/5], Step [1568/10336], Loss: 0.0014\n",
      "Epoch [2/5], Step [1570/10336], Loss: 1.2762\n",
      "Epoch [2/5], Step [1572/10336], Loss: 0.3776\n",
      "Epoch [2/5], Step [1574/10336], Loss: 2.6836\n",
      "Epoch [2/5], Step [1576/10336], Loss: 0.4671\n",
      "Epoch [2/5], Step [1578/10336], Loss: 0.6374\n",
      "Epoch [2/5], Step [1580/10336], Loss: 0.0615\n",
      "Epoch [2/5], Step [1582/10336], Loss: 0.2415\n",
      "Epoch [2/5], Step [1584/10336], Loss: 0.1575\n",
      "Epoch [2/5], Step [1586/10336], Loss: 0.2103\n",
      "Epoch [2/5], Step [1588/10336], Loss: 3.1242\n",
      "Epoch [2/5], Step [1590/10336], Loss: 0.1947\n",
      "Epoch [2/5], Step [1592/10336], Loss: 0.2064\n",
      "Epoch [2/5], Step [1594/10336], Loss: 0.4939\n",
      "Epoch [2/5], Step [1596/10336], Loss: 0.1208\n",
      "Epoch [2/5], Step [1598/10336], Loss: 0.7863\n",
      "Epoch [2/5], Step [1600/10336], Loss: 0.0538\n",
      "Epoch [2/5], Step [1602/10336], Loss: 4.2608\n",
      "Epoch [2/5], Step [1604/10336], Loss: 0.0580\n",
      "Epoch [2/5], Step [1606/10336], Loss: 0.1924\n",
      "Epoch [2/5], Step [1608/10336], Loss: 0.6541\n",
      "Epoch [2/5], Step [1610/10336], Loss: 0.1219\n",
      "Epoch [2/5], Step [1612/10336], Loss: 0.6228\n",
      "Epoch [2/5], Step [1614/10336], Loss: 0.5309\n",
      "Epoch [2/5], Step [1616/10336], Loss: 2.5619\n",
      "Epoch [2/5], Step [1618/10336], Loss: 0.0539\n",
      "Epoch [2/5], Step [1620/10336], Loss: 2.9606\n",
      "Epoch [2/5], Step [1622/10336], Loss: 0.1225\n",
      "Epoch [2/5], Step [1624/10336], Loss: 0.2425\n",
      "Epoch [2/5], Step [1626/10336], Loss: 2.6455\n",
      "Epoch [2/5], Step [1628/10336], Loss: 0.1358\n",
      "Epoch [2/5], Step [1630/10336], Loss: 1.4920\n",
      "Epoch [2/5], Step [1632/10336], Loss: 1.5787\n",
      "Epoch [2/5], Step [1634/10336], Loss: 4.1996\n",
      "Epoch [2/5], Step [1636/10336], Loss: 2.1094\n",
      "Epoch [2/5], Step [1638/10336], Loss: 0.1212\n",
      "Epoch [2/5], Step [1640/10336], Loss: 1.1204\n",
      "Epoch [2/5], Step [1642/10336], Loss: 0.2295\n",
      "Epoch [2/5], Step [1644/10336], Loss: 0.4276\n",
      "Epoch [2/5], Step [1646/10336], Loss: 0.8737\n",
      "Epoch [2/5], Step [1648/10336], Loss: 0.1291\n",
      "Epoch [2/5], Step [1650/10336], Loss: 1.1472\n",
      "Epoch [2/5], Step [1652/10336], Loss: 3.1083\n",
      "Epoch [2/5], Step [1654/10336], Loss: 0.2427\n",
      "Epoch [2/5], Step [1656/10336], Loss: 0.2416\n",
      "Epoch [2/5], Step [1658/10336], Loss: 0.0053\n",
      "Epoch [2/5], Step [1660/10336], Loss: 4.4219\n",
      "Epoch [2/5], Step [1662/10336], Loss: 0.2705\n",
      "Epoch [2/5], Step [1664/10336], Loss: 0.7676\n",
      "Epoch [2/5], Step [1666/10336], Loss: 0.3135\n",
      "Epoch [2/5], Step [1668/10336], Loss: 0.1383\n",
      "Epoch [2/5], Step [1670/10336], Loss: 0.0440\n",
      "Epoch [2/5], Step [1672/10336], Loss: 0.1733\n",
      "Epoch [2/5], Step [1674/10336], Loss: 1.5663\n",
      "Epoch [2/5], Step [1676/10336], Loss: 1.9607\n",
      "Epoch [2/5], Step [1678/10336], Loss: 1.2736\n",
      "Epoch [2/5], Step [1680/10336], Loss: 0.4207\n",
      "Epoch [2/5], Step [1682/10336], Loss: 0.3915\n",
      "Epoch [2/5], Step [1684/10336], Loss: 0.2518\n",
      "Epoch [2/5], Step [1686/10336], Loss: 0.0227\n",
      "Epoch [2/5], Step [1688/10336], Loss: 1.4145\n",
      "Epoch [2/5], Step [1690/10336], Loss: 0.5058\n",
      "Epoch [2/5], Step [1692/10336], Loss: 0.3650\n",
      "Epoch [2/5], Step [1694/10336], Loss: 1.3068\n",
      "Epoch [2/5], Step [1696/10336], Loss: 0.0114\n",
      "Epoch [2/5], Step [1698/10336], Loss: 3.4302\n",
      "Epoch [2/5], Step [1700/10336], Loss: 0.0975\n",
      "Epoch [2/5], Step [1702/10336], Loss: 0.2145\n",
      "Epoch [2/5], Step [1704/10336], Loss: 0.2284\n",
      "Epoch [2/5], Step [1706/10336], Loss: 0.7141\n",
      "Epoch [2/5], Step [1708/10336], Loss: 0.3095\n",
      "Epoch [2/5], Step [1710/10336], Loss: 0.1311\n",
      "Epoch [2/5], Step [1712/10336], Loss: 0.0148\n",
      "Epoch [2/5], Step [1714/10336], Loss: 0.4396\n",
      "Epoch [2/5], Step [1716/10336], Loss: 3.9003\n",
      "Epoch [2/5], Step [1718/10336], Loss: 0.1953\n",
      "Epoch [2/5], Step [1720/10336], Loss: 2.3173\n",
      "Epoch [2/5], Step [1722/10336], Loss: 3.7767\n",
      "Epoch [2/5], Step [1724/10336], Loss: 0.3427\n",
      "Epoch [2/5], Step [1726/10336], Loss: 2.9934\n",
      "Epoch [2/5], Step [1728/10336], Loss: 0.4779\n",
      "Epoch [2/5], Step [1730/10336], Loss: 0.1720\n",
      "Epoch [2/5], Step [1732/10336], Loss: 0.5404\n",
      "Epoch [2/5], Step [1734/10336], Loss: 3.7672\n",
      "Epoch [2/5], Step [1736/10336], Loss: 0.1589\n",
      "Epoch [2/5], Step [1738/10336], Loss: 3.0791\n",
      "Epoch [2/5], Step [1740/10336], Loss: 0.8328\n",
      "Epoch [2/5], Step [1742/10336], Loss: 0.0395\n",
      "Epoch [2/5], Step [1744/10336], Loss: 4.1896\n",
      "Epoch [2/5], Step [1746/10336], Loss: 0.8795\n",
      "Epoch [2/5], Step [1748/10336], Loss: 0.3053\n",
      "Epoch [2/5], Step [1750/10336], Loss: 4.2026\n",
      "Epoch [2/5], Step [1752/10336], Loss: 2.2636\n",
      "Epoch [2/5], Step [1754/10336], Loss: 0.7193\n",
      "Epoch [2/5], Step [1756/10336], Loss: 0.1280\n",
      "Epoch [2/5], Step [1758/10336], Loss: 0.0125\n",
      "Epoch [2/5], Step [1760/10336], Loss: 0.4537\n",
      "Epoch [2/5], Step [1762/10336], Loss: 0.4287\n",
      "Epoch [2/5], Step [1764/10336], Loss: 0.2501\n",
      "Epoch [2/5], Step [1766/10336], Loss: 3.0389\n",
      "Epoch [2/5], Step [1768/10336], Loss: 0.3005\n",
      "Epoch [2/5], Step [1770/10336], Loss: 1.2484\n",
      "Epoch [2/5], Step [1772/10336], Loss: 1.7365\n",
      "Epoch [2/5], Step [1774/10336], Loss: 1.2946\n",
      "Epoch [2/5], Step [1776/10336], Loss: 0.4222\n",
      "Epoch [2/5], Step [1778/10336], Loss: 0.1546\n",
      "Epoch [2/5], Step [1780/10336], Loss: 0.3380\n",
      "Epoch [2/5], Step [1782/10336], Loss: 0.1004\n",
      "Epoch [2/5], Step [1784/10336], Loss: 0.3046\n",
      "Epoch [2/5], Step [1786/10336], Loss: 0.3088\n",
      "Epoch [2/5], Step [1788/10336], Loss: 1.0285\n",
      "Epoch [2/5], Step [1790/10336], Loss: 0.2279\n",
      "Epoch [2/5], Step [1792/10336], Loss: 1.6929\n",
      "Epoch [2/5], Step [1794/10336], Loss: 0.4049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [1796/10336], Loss: 0.5600\n",
      "Epoch [2/5], Step [1798/10336], Loss: 1.9738\n",
      "Epoch [2/5], Step [1800/10336], Loss: 0.0743\n",
      "Epoch [2/5], Step [1802/10336], Loss: 0.4614\n",
      "Epoch [2/5], Step [1804/10336], Loss: 0.3952\n",
      "Epoch [2/5], Step [1806/10336], Loss: 0.7480\n",
      "Epoch [2/5], Step [1808/10336], Loss: 0.0256\n",
      "Epoch [2/5], Step [1810/10336], Loss: 0.4988\n",
      "Epoch [2/5], Step [1812/10336], Loss: 0.2852\n",
      "Epoch [2/5], Step [1814/10336], Loss: 9.2833\n",
      "Epoch [2/5], Step [1816/10336], Loss: 0.5249\n",
      "Epoch [2/5], Step [1818/10336], Loss: 0.1751\n",
      "Epoch [2/5], Step [1820/10336], Loss: 0.0768\n",
      "Epoch [2/5], Step [1822/10336], Loss: 0.0499\n",
      "Epoch [2/5], Step [1824/10336], Loss: 0.0538\n",
      "Epoch [2/5], Step [1826/10336], Loss: 0.1550\n",
      "Epoch [2/5], Step [1828/10336], Loss: 0.2241\n",
      "Epoch [2/5], Step [1830/10336], Loss: 1.1380\n",
      "Epoch [2/5], Step [1832/10336], Loss: 4.4084\n",
      "Epoch [2/5], Step [1834/10336], Loss: 0.8289\n",
      "Epoch [2/5], Step [1836/10336], Loss: 0.7655\n",
      "Epoch [2/5], Step [1838/10336], Loss: 1.1668\n",
      "Epoch [2/5], Step [1840/10336], Loss: 0.4685\n",
      "Epoch [2/5], Step [1842/10336], Loss: 0.1102\n",
      "Epoch [2/5], Step [1844/10336], Loss: 0.3277\n",
      "Epoch [2/5], Step [1846/10336], Loss: 0.1656\n",
      "Epoch [2/5], Step [1848/10336], Loss: 0.4697\n",
      "Epoch [2/5], Step [1850/10336], Loss: 4.7762\n",
      "Epoch [2/5], Step [1852/10336], Loss: 0.3660\n",
      "Epoch [2/5], Step [1854/10336], Loss: 0.0455\n",
      "Epoch [2/5], Step [1856/10336], Loss: 4.0469\n",
      "Epoch [2/5], Step [1858/10336], Loss: 1.4740\n",
      "Epoch [2/5], Step [1860/10336], Loss: 0.1863\n",
      "Epoch [2/5], Step [1862/10336], Loss: 0.1666\n",
      "Epoch [2/5], Step [1864/10336], Loss: 0.2188\n",
      "Epoch [2/5], Step [1866/10336], Loss: 0.7275\n",
      "Epoch [2/5], Step [1868/10336], Loss: 0.9878\n",
      "Epoch [2/5], Step [1870/10336], Loss: 0.5961\n",
      "Epoch [2/5], Step [1872/10336], Loss: 3.3577\n",
      "Epoch [2/5], Step [1874/10336], Loss: 0.4969\n",
      "Epoch [2/5], Step [1876/10336], Loss: 0.7040\n",
      "Epoch [2/5], Step [1878/10336], Loss: 0.4573\n",
      "Epoch [2/5], Step [1880/10336], Loss: 0.3460\n",
      "Epoch [2/5], Step [1882/10336], Loss: 0.2967\n",
      "Epoch [2/5], Step [1884/10336], Loss: 0.0515\n",
      "Epoch [2/5], Step [1886/10336], Loss: 4.1912\n",
      "Epoch [2/5], Step [1888/10336], Loss: 2.5641\n",
      "Epoch [2/5], Step [1890/10336], Loss: 2.9135\n",
      "Epoch [2/5], Step [1892/10336], Loss: 0.0912\n",
      "Epoch [2/5], Step [1894/10336], Loss: 0.3223\n",
      "Epoch [2/5], Step [1896/10336], Loss: 3.1682\n",
      "Epoch [2/5], Step [1898/10336], Loss: 0.7687\n",
      "Epoch [2/5], Step [1900/10336], Loss: 0.1337\n",
      "Epoch [2/5], Step [1902/10336], Loss: 0.0339\n",
      "Epoch [2/5], Step [1904/10336], Loss: 0.9992\n",
      "Epoch [2/5], Step [1906/10336], Loss: 0.2733\n",
      "Epoch [2/5], Step [1908/10336], Loss: 1.2489\n",
      "Epoch [2/5], Step [1910/10336], Loss: 0.5355\n",
      "Epoch [2/5], Step [1912/10336], Loss: 0.8989\n",
      "Epoch [2/5], Step [1914/10336], Loss: 0.4485\n",
      "Epoch [2/5], Step [1916/10336], Loss: 0.2966\n",
      "Epoch [2/5], Step [1918/10336], Loss: 1.8522\n",
      "Epoch [2/5], Step [1920/10336], Loss: 0.3500\n",
      "Epoch [2/5], Step [1922/10336], Loss: 0.2487\n",
      "Epoch [2/5], Step [1924/10336], Loss: 1.0289\n",
      "Epoch [2/5], Step [1926/10336], Loss: 2.9740\n",
      "Epoch [2/5], Step [1928/10336], Loss: 0.4425\n",
      "Epoch [2/5], Step [1930/10336], Loss: 4.0376\n",
      "Epoch [2/5], Step [1932/10336], Loss: 0.1981\n",
      "Epoch [2/5], Step [1934/10336], Loss: 2.8090\n",
      "Epoch [2/5], Step [1936/10336], Loss: 2.6771\n",
      "Epoch [2/5], Step [1938/10336], Loss: 0.2886\n",
      "Epoch [2/5], Step [1940/10336], Loss: 0.3719\n",
      "Epoch [2/5], Step [1942/10336], Loss: 0.7524\n",
      "Epoch [2/5], Step [1944/10336], Loss: 0.2026\n",
      "Epoch [2/5], Step [1946/10336], Loss: 0.0116\n",
      "Epoch [2/5], Step [1948/10336], Loss: 0.3203\n",
      "Epoch [2/5], Step [1950/10336], Loss: 0.4006\n",
      "Epoch [2/5], Step [1952/10336], Loss: 0.2904\n",
      "Epoch [2/5], Step [1954/10336], Loss: 0.8946\n",
      "Epoch [2/5], Step [1956/10336], Loss: 0.0406\n",
      "Epoch [2/5], Step [1958/10336], Loss: 0.2616\n",
      "Epoch [2/5], Step [1960/10336], Loss: 0.0186\n",
      "Epoch [2/5], Step [1962/10336], Loss: 2.5998\n",
      "Epoch [2/5], Step [1964/10336], Loss: 1.4816\n",
      "Epoch [2/5], Step [1966/10336], Loss: 0.0352\n",
      "Epoch [2/5], Step [1968/10336], Loss: 1.2003\n",
      "Epoch [2/5], Step [1970/10336], Loss: 1.5843\n",
      "Epoch [2/5], Step [1972/10336], Loss: 3.8720\n",
      "Epoch [2/5], Step [1974/10336], Loss: 0.8467\n",
      "Epoch [2/5], Step [1976/10336], Loss: 0.2481\n",
      "Epoch [2/5], Step [1978/10336], Loss: 0.4202\n",
      "Epoch [2/5], Step [1980/10336], Loss: 1.2255\n",
      "Epoch [2/5], Step [1982/10336], Loss: 0.5390\n",
      "Epoch [2/5], Step [1984/10336], Loss: 0.5822\n",
      "Epoch [2/5], Step [1986/10336], Loss: 2.3321\n",
      "Epoch [2/5], Step [1988/10336], Loss: 1.9571\n",
      "Epoch [2/5], Step [1990/10336], Loss: 0.2789\n",
      "Epoch [2/5], Step [1992/10336], Loss: 1.6381\n",
      "Epoch [2/5], Step [1994/10336], Loss: 0.3669\n",
      "Epoch [2/5], Step [1996/10336], Loss: 0.1479\n",
      "Epoch [2/5], Step [1998/10336], Loss: 1.3276\n",
      "Epoch [2/5], Step [2000/10336], Loss: 3.1554\n",
      "Epoch [2/5], Step [2002/10336], Loss: 0.3432\n",
      "Epoch [2/5], Step [2004/10336], Loss: 0.2916\n",
      "Epoch [2/5], Step [2006/10336], Loss: 1.9679\n",
      "Epoch [2/5], Step [2008/10336], Loss: 1.2457\n",
      "Epoch [2/5], Step [2010/10336], Loss: 2.5464\n",
      "Epoch [2/5], Step [2012/10336], Loss: 0.1483\n",
      "Epoch [2/5], Step [2014/10336], Loss: 0.4279\n",
      "Epoch [2/5], Step [2016/10336], Loss: 0.1391\n",
      "Epoch [2/5], Step [2018/10336], Loss: 0.0161\n",
      "Epoch [2/5], Step [2020/10336], Loss: 0.1987\n",
      "Epoch [2/5], Step [2022/10336], Loss: 0.0831\n",
      "Epoch [2/5], Step [2024/10336], Loss: 0.3359\n",
      "Epoch [2/5], Step [2026/10336], Loss: 0.4223\n",
      "Epoch [2/5], Step [2028/10336], Loss: 0.6479\n",
      "Epoch [2/5], Step [2030/10336], Loss: 0.1132\n",
      "Epoch [2/5], Step [2032/10336], Loss: 2.6772\n",
      "Epoch [2/5], Step [2034/10336], Loss: 0.2534\n",
      "Epoch [2/5], Step [2036/10336], Loss: 2.7671\n",
      "Epoch [2/5], Step [2038/10336], Loss: 2.2954\n",
      "Epoch [2/5], Step [2040/10336], Loss: 0.6055\n",
      "Epoch [2/5], Step [2042/10336], Loss: 0.1956\n",
      "Epoch [2/5], Step [2044/10336], Loss: 0.0767\n",
      "Epoch [2/5], Step [2046/10336], Loss: 0.2790\n",
      "Epoch [2/5], Step [2048/10336], Loss: 0.9586\n",
      "Epoch [2/5], Step [2050/10336], Loss: 0.0085\n",
      "Epoch [2/5], Step [2052/10336], Loss: 0.2429\n",
      "Epoch [2/5], Step [2054/10336], Loss: 0.6784\n",
      "Epoch [2/5], Step [2056/10336], Loss: 0.1318\n",
      "Epoch [2/5], Step [2058/10336], Loss: 4.6748\n",
      "Epoch [2/5], Step [2060/10336], Loss: 0.3520\n",
      "Epoch [2/5], Step [2062/10336], Loss: 2.0025\n",
      "Epoch [2/5], Step [2064/10336], Loss: 0.8275\n",
      "Epoch [2/5], Step [2066/10336], Loss: 5.6197\n",
      "Epoch [2/5], Step [2068/10336], Loss: 0.1144\n",
      "Epoch [2/5], Step [2070/10336], Loss: 0.4096\n",
      "Epoch [2/5], Step [2072/10336], Loss: 1.4635\n",
      "Epoch [2/5], Step [2074/10336], Loss: 0.5164\n",
      "Epoch [2/5], Step [2076/10336], Loss: 0.1964\n",
      "Epoch [2/5], Step [2078/10336], Loss: 0.0337\n",
      "Epoch [2/5], Step [2080/10336], Loss: 0.5911\n",
      "Epoch [2/5], Step [2082/10336], Loss: 0.0093\n",
      "Epoch [2/5], Step [2084/10336], Loss: 0.0196\n",
      "Epoch [2/5], Step [2086/10336], Loss: 0.2691\n",
      "Epoch [2/5], Step [2088/10336], Loss: 0.3465\n",
      "Epoch [2/5], Step [2090/10336], Loss: 0.0723\n",
      "Epoch [2/5], Step [2092/10336], Loss: 0.0269\n",
      "Epoch [2/5], Step [2094/10336], Loss: 0.9128\n",
      "Epoch [2/5], Step [2096/10336], Loss: 0.2192\n",
      "Epoch [2/5], Step [2098/10336], Loss: 0.0748\n",
      "Epoch [2/5], Step [2100/10336], Loss: 0.2135\n",
      "Epoch [2/5], Step [2102/10336], Loss: 1.5934\n",
      "Epoch [2/5], Step [2104/10336], Loss: 0.2787\n",
      "Epoch [2/5], Step [2106/10336], Loss: 0.2584\n",
      "Epoch [2/5], Step [2108/10336], Loss: 0.0280\n",
      "Epoch [2/5], Step [2110/10336], Loss: 0.3320\n",
      "Epoch [2/5], Step [2112/10336], Loss: 0.2245\n",
      "Epoch [2/5], Step [2114/10336], Loss: 0.2517\n",
      "Epoch [2/5], Step [2116/10336], Loss: 1.7621\n",
      "Epoch [2/5], Step [2118/10336], Loss: 0.0833\n",
      "Epoch [2/5], Step [2120/10336], Loss: 1.0248\n",
      "Epoch [2/5], Step [2122/10336], Loss: 0.2202\n",
      "Epoch [2/5], Step [2124/10336], Loss: 0.1522\n",
      "Epoch [2/5], Step [2126/10336], Loss: 0.2202\n",
      "Epoch [2/5], Step [2128/10336], Loss: 0.2662\n",
      "Epoch [2/5], Step [2130/10336], Loss: 0.0375\n",
      "Epoch [2/5], Step [2132/10336], Loss: 0.7600\n",
      "Epoch [2/5], Step [2134/10336], Loss: 0.0045\n",
      "Epoch [2/5], Step [2136/10336], Loss: 3.0564\n",
      "Epoch [2/5], Step [2138/10336], Loss: 2.5790\n",
      "Epoch [2/5], Step [2140/10336], Loss: 0.3647\n",
      "Epoch [2/5], Step [2142/10336], Loss: 0.2523\n",
      "Epoch [2/5], Step [2144/10336], Loss: 0.2013\n",
      "Epoch [2/5], Step [2146/10336], Loss: 1.4580\n",
      "Epoch [2/5], Step [2148/10336], Loss: 0.0537\n",
      "Epoch [2/5], Step [2150/10336], Loss: 0.2095\n",
      "Epoch [2/5], Step [2152/10336], Loss: 0.5809\n",
      "Epoch [2/5], Step [2154/10336], Loss: 3.2184\n",
      "Epoch [2/5], Step [2156/10336], Loss: 2.4480\n",
      "Epoch [2/5], Step [2158/10336], Loss: 5.0128\n",
      "Epoch [2/5], Step [2160/10336], Loss: 1.0444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [2162/10336], Loss: 0.2128\n",
      "Epoch [2/5], Step [2164/10336], Loss: 0.5527\n",
      "Epoch [2/5], Step [2166/10336], Loss: 0.6304\n",
      "Epoch [2/5], Step [2168/10336], Loss: 1.9324\n",
      "Epoch [2/5], Step [2170/10336], Loss: 1.3052\n",
      "Epoch [2/5], Step [2172/10336], Loss: 2.8042\n",
      "Epoch [2/5], Step [2174/10336], Loss: 0.1980\n",
      "Epoch [2/5], Step [2176/10336], Loss: 0.4836\n",
      "Epoch [2/5], Step [2178/10336], Loss: 0.2814\n",
      "Epoch [2/5], Step [2180/10336], Loss: 0.3822\n",
      "Epoch [2/5], Step [2182/10336], Loss: 1.2846\n",
      "Epoch [2/5], Step [2184/10336], Loss: 0.4675\n",
      "Epoch [2/5], Step [2186/10336], Loss: 0.0235\n",
      "Epoch [2/5], Step [2188/10336], Loss: 2.2443\n",
      "Epoch [2/5], Step [2190/10336], Loss: 1.6613\n",
      "Epoch [2/5], Step [2192/10336], Loss: 0.0052\n",
      "Epoch [2/5], Step [2194/10336], Loss: 0.2044\n",
      "Epoch [2/5], Step [2196/10336], Loss: 0.2776\n",
      "Epoch [2/5], Step [2198/10336], Loss: 0.6829\n",
      "Epoch [2/5], Step [2200/10336], Loss: 4.5977\n",
      "Epoch [2/5], Step [2202/10336], Loss: 0.0167\n",
      "Epoch [2/5], Step [2204/10336], Loss: 1.1781\n",
      "Epoch [2/5], Step [2206/10336], Loss: 0.2190\n",
      "Epoch [2/5], Step [2208/10336], Loss: 1.5794\n",
      "Epoch [2/5], Step [2210/10336], Loss: 1.7354\n",
      "Epoch [2/5], Step [2212/10336], Loss: 0.5961\n",
      "Epoch [2/5], Step [2214/10336], Loss: 1.9341\n",
      "Epoch [2/5], Step [2216/10336], Loss: 2.9017\n",
      "Epoch [2/5], Step [2218/10336], Loss: 2.1269\n",
      "Epoch [2/5], Step [2220/10336], Loss: 0.0018\n",
      "Epoch [2/5], Step [2222/10336], Loss: 0.1872\n",
      "Epoch [2/5], Step [2224/10336], Loss: 0.7997\n",
      "Epoch [2/5], Step [2226/10336], Loss: 0.1010\n",
      "Epoch [2/5], Step [2228/10336], Loss: 0.1269\n",
      "Epoch [2/5], Step [2230/10336], Loss: 0.1213\n",
      "Epoch [2/5], Step [2232/10336], Loss: 0.0627\n",
      "Epoch [2/5], Step [2234/10336], Loss: 3.3124\n",
      "Epoch [2/5], Step [2236/10336], Loss: 0.3827\n",
      "Epoch [2/5], Step [2238/10336], Loss: 0.0162\n",
      "Epoch [2/5], Step [2240/10336], Loss: 0.4204\n",
      "Epoch [2/5], Step [2242/10336], Loss: 3.7319\n",
      "Epoch [2/5], Step [2244/10336], Loss: 0.6381\n",
      "Epoch [2/5], Step [2246/10336], Loss: 0.2015\n",
      "Epoch [2/5], Step [2248/10336], Loss: 0.1026\n",
      "Epoch [2/5], Step [2250/10336], Loss: 0.0448\n",
      "Epoch [2/5], Step [2252/10336], Loss: 3.2565\n",
      "Epoch [2/5], Step [2254/10336], Loss: 0.4010\n",
      "Epoch [2/5], Step [2256/10336], Loss: 0.4009\n",
      "Epoch [2/5], Step [2258/10336], Loss: 0.5440\n",
      "Epoch [2/5], Step [2260/10336], Loss: 0.2886\n",
      "Epoch [2/5], Step [2262/10336], Loss: 0.3327\n",
      "Epoch [2/5], Step [2264/10336], Loss: 0.4843\n",
      "Epoch [2/5], Step [2266/10336], Loss: 0.0318\n",
      "Epoch [2/5], Step [2268/10336], Loss: 0.1900\n",
      "Epoch [2/5], Step [2270/10336], Loss: 0.3368\n",
      "Epoch [2/5], Step [2272/10336], Loss: 5.5470\n",
      "Epoch [2/5], Step [2274/10336], Loss: 0.7106\n",
      "Epoch [2/5], Step [2276/10336], Loss: 0.0497\n",
      "Epoch [2/5], Step [2278/10336], Loss: 1.2243\n",
      "Epoch [2/5], Step [2280/10336], Loss: 4.4701\n",
      "Epoch [2/5], Step [2282/10336], Loss: 0.2372\n",
      "Epoch [2/5], Step [2284/10336], Loss: 3.6867\n",
      "Epoch [2/5], Step [2286/10336], Loss: 0.9181\n",
      "Epoch [2/5], Step [2288/10336], Loss: 0.9112\n",
      "Epoch [2/5], Step [2290/10336], Loss: 0.2837\n",
      "Epoch [2/5], Step [2292/10336], Loss: 0.1743\n",
      "Epoch [2/5], Step [2294/10336], Loss: 0.5988\n",
      "Epoch [2/5], Step [2296/10336], Loss: 0.2998\n",
      "Epoch [2/5], Step [2298/10336], Loss: 3.3160\n",
      "Epoch [2/5], Step [2300/10336], Loss: 0.4973\n",
      "Epoch [2/5], Step [2302/10336], Loss: 3.7374\n",
      "Epoch [2/5], Step [2304/10336], Loss: 3.6644\n",
      "Epoch [2/5], Step [2306/10336], Loss: 0.1781\n",
      "Epoch [2/5], Step [2308/10336], Loss: 1.0460\n",
      "Epoch [2/5], Step [2310/10336], Loss: 1.4353\n",
      "Epoch [2/5], Step [2312/10336], Loss: 0.1462\n",
      "Epoch [2/5], Step [2314/10336], Loss: 0.3120\n",
      "Epoch [2/5], Step [2316/10336], Loss: 0.0807\n",
      "Epoch [2/5], Step [2318/10336], Loss: 0.3139\n",
      "Epoch [2/5], Step [2320/10336], Loss: 4.0208\n",
      "Epoch [2/5], Step [2322/10336], Loss: 0.1932\n",
      "Epoch [2/5], Step [2324/10336], Loss: 0.0243\n",
      "Epoch [2/5], Step [2326/10336], Loss: 0.1323\n",
      "Epoch [2/5], Step [2328/10336], Loss: 1.6521\n",
      "Epoch [2/5], Step [2330/10336], Loss: 0.3021\n",
      "Epoch [2/5], Step [2332/10336], Loss: 0.1302\n",
      "Epoch [2/5], Step [2334/10336], Loss: 0.0425\n",
      "Epoch [2/5], Step [2336/10336], Loss: 3.9048\n",
      "Epoch [2/5], Step [2338/10336], Loss: 1.4203\n",
      "Epoch [2/5], Step [2340/10336], Loss: 0.0915\n",
      "Epoch [2/5], Step [2342/10336], Loss: 0.1806\n",
      "Epoch [2/5], Step [2344/10336], Loss: 0.1357\n",
      "Epoch [2/5], Step [2346/10336], Loss: 2.3404\n",
      "Epoch [2/5], Step [2348/10336], Loss: 0.0016\n",
      "Epoch [2/5], Step [2350/10336], Loss: 0.4297\n",
      "Epoch [2/5], Step [2352/10336], Loss: 0.0146\n",
      "Epoch [2/5], Step [2354/10336], Loss: 1.1827\n",
      "Epoch [2/5], Step [2356/10336], Loss: 0.0079\n",
      "Epoch [2/5], Step [2358/10336], Loss: 0.0427\n",
      "Epoch [2/5], Step [2360/10336], Loss: 1.1393\n",
      "Epoch [2/5], Step [2362/10336], Loss: 3.2697\n",
      "Epoch [2/5], Step [2364/10336], Loss: 0.5205\n",
      "Epoch [2/5], Step [2366/10336], Loss: 0.5635\n",
      "Epoch [2/5], Step [2368/10336], Loss: 4.7136\n",
      "Epoch [2/5], Step [2370/10336], Loss: 0.2701\n",
      "Epoch [2/5], Step [2372/10336], Loss: 0.3660\n",
      "Epoch [2/5], Step [2374/10336], Loss: 1.2112\n",
      "Epoch [2/5], Step [2376/10336], Loss: 0.0032\n",
      "Epoch [2/5], Step [2378/10336], Loss: 0.0065\n",
      "Epoch [2/5], Step [2380/10336], Loss: 0.1556\n",
      "Epoch [2/5], Step [2382/10336], Loss: 3.2987\n",
      "Epoch [2/5], Step [2384/10336], Loss: 0.8279\n",
      "Epoch [2/5], Step [2386/10336], Loss: 0.5040\n",
      "Epoch [2/5], Step [2388/10336], Loss: 0.2779\n",
      "Epoch [2/5], Step [2390/10336], Loss: 0.1407\n",
      "Epoch [2/5], Step [2392/10336], Loss: 1.1880\n",
      "Epoch [2/5], Step [2394/10336], Loss: 0.3802\n",
      "Epoch [2/5], Step [2396/10336], Loss: 0.1687\n",
      "Epoch [2/5], Step [2398/10336], Loss: 3.4392\n",
      "Epoch [2/5], Step [2400/10336], Loss: 1.7430\n",
      "Epoch [2/5], Step [2402/10336], Loss: 1.0856\n",
      "Epoch [2/5], Step [2404/10336], Loss: 0.7849\n",
      "Epoch [2/5], Step [2406/10336], Loss: 0.1927\n",
      "Epoch [2/5], Step [2408/10336], Loss: 0.0228\n",
      "Epoch [2/5], Step [2410/10336], Loss: 0.4973\n",
      "Epoch [2/5], Step [2412/10336], Loss: 0.2659\n",
      "Epoch [2/5], Step [2414/10336], Loss: 0.2964\n",
      "Epoch [2/5], Step [2416/10336], Loss: 0.2152\n",
      "Epoch [2/5], Step [2418/10336], Loss: 2.5613\n",
      "Epoch [2/5], Step [2420/10336], Loss: 4.0029\n",
      "Epoch [2/5], Step [2422/10336], Loss: 1.7314\n",
      "Epoch [2/5], Step [2424/10336], Loss: 0.1197\n",
      "Epoch [2/5], Step [2426/10336], Loss: 2.4875\n",
      "Epoch [2/5], Step [2428/10336], Loss: 0.2536\n",
      "Epoch [2/5], Step [2430/10336], Loss: 0.4209\n",
      "Epoch [2/5], Step [2432/10336], Loss: 0.2731\n",
      "Epoch [2/5], Step [2434/10336], Loss: 0.2163\n",
      "Epoch [2/5], Step [2436/10336], Loss: 1.1263\n",
      "Epoch [2/5], Step [2438/10336], Loss: 3.5222\n",
      "Epoch [2/5], Step [2440/10336], Loss: 1.9243\n",
      "Epoch [2/5], Step [2442/10336], Loss: 0.4858\n",
      "Epoch [2/5], Step [2444/10336], Loss: 0.1077\n",
      "Epoch [2/5], Step [2446/10336], Loss: 3.4727\n",
      "Epoch [2/5], Step [2448/10336], Loss: 5.1260\n",
      "Epoch [2/5], Step [2450/10336], Loss: 0.2793\n",
      "Epoch [2/5], Step [2452/10336], Loss: 0.2155\n",
      "Epoch [2/5], Step [2454/10336], Loss: 1.1919\n",
      "Epoch [2/5], Step [2456/10336], Loss: 0.7771\n",
      "Epoch [2/5], Step [2458/10336], Loss: 0.1476\n",
      "Epoch [2/5], Step [2460/10336], Loss: 0.2662\n",
      "Epoch [2/5], Step [2462/10336], Loss: 5.7668\n",
      "Epoch [2/5], Step [2464/10336], Loss: 0.2960\n",
      "Epoch [2/5], Step [2466/10336], Loss: 0.7258\n",
      "Epoch [2/5], Step [2468/10336], Loss: 0.0109\n",
      "Epoch [2/5], Step [2470/10336], Loss: 0.0794\n",
      "Epoch [2/5], Step [2472/10336], Loss: 0.9132\n",
      "Epoch [2/5], Step [2474/10336], Loss: 0.0183\n",
      "Epoch [2/5], Step [2476/10336], Loss: 0.1173\n",
      "Epoch [2/5], Step [2478/10336], Loss: 4.0664\n",
      "Epoch [2/5], Step [2480/10336], Loss: 0.5304\n",
      "Epoch [2/5], Step [2482/10336], Loss: 0.8333\n",
      "Epoch [2/5], Step [2484/10336], Loss: 0.5607\n",
      "Epoch [2/5], Step [2486/10336], Loss: 0.5737\n",
      "Epoch [2/5], Step [2488/10336], Loss: 1.7382\n",
      "Epoch [2/5], Step [2490/10336], Loss: 0.2819\n",
      "Epoch [2/5], Step [2492/10336], Loss: 0.1506\n",
      "Epoch [2/5], Step [2494/10336], Loss: 1.1220\n",
      "Epoch [2/5], Step [2496/10336], Loss: 2.3213\n",
      "Epoch [2/5], Step [2498/10336], Loss: 0.7093\n",
      "Epoch [2/5], Step [2500/10336], Loss: 0.2955\n",
      "Epoch [2/5], Step [2502/10336], Loss: 0.5178\n",
      "Epoch [2/5], Step [2504/10336], Loss: 0.7576\n",
      "Epoch [2/5], Step [2506/10336], Loss: 4.3031\n",
      "Epoch [2/5], Step [2508/10336], Loss: 0.0351\n",
      "Epoch [2/5], Step [2510/10336], Loss: 0.0238\n",
      "Epoch [2/5], Step [2512/10336], Loss: 1.0934\n",
      "Epoch [2/5], Step [2514/10336], Loss: 0.4557\n",
      "Epoch [2/5], Step [2516/10336], Loss: 0.5288\n",
      "Epoch [2/5], Step [2518/10336], Loss: 0.5243\n",
      "Epoch [2/5], Step [2520/10336], Loss: 0.2089\n",
      "Epoch [2/5], Step [2522/10336], Loss: 2.6398\n",
      "Epoch [2/5], Step [2524/10336], Loss: 0.6585\n",
      "Epoch [2/5], Step [2526/10336], Loss: 0.6879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [2528/10336], Loss: 1.2429\n",
      "Epoch [2/5], Step [2530/10336], Loss: 0.1189\n",
      "Epoch [2/5], Step [2532/10336], Loss: 0.8933\n",
      "Epoch [2/5], Step [2534/10336], Loss: 1.8932\n",
      "Epoch [2/5], Step [2536/10336], Loss: 0.0357\n",
      "Epoch [2/5], Step [2538/10336], Loss: 0.6932\n",
      "Epoch [2/5], Step [2540/10336], Loss: 0.4590\n",
      "Epoch [2/5], Step [2542/10336], Loss: 0.4704\n",
      "Epoch [2/5], Step [2544/10336], Loss: 1.2037\n",
      "Epoch [2/5], Step [2546/10336], Loss: 2.1338\n",
      "Epoch [2/5], Step [2548/10336], Loss: 0.2215\n",
      "Epoch [2/5], Step [2550/10336], Loss: 0.2287\n",
      "Epoch [2/5], Step [2552/10336], Loss: 0.1857\n",
      "Epoch [2/5], Step [2554/10336], Loss: 0.2461\n",
      "Epoch [2/5], Step [2556/10336], Loss: 1.1860\n",
      "Epoch [2/5], Step [2558/10336], Loss: 0.7432\n",
      "Epoch [2/5], Step [2560/10336], Loss: 0.3248\n",
      "Epoch [2/5], Step [2562/10336], Loss: 2.1728\n",
      "Epoch [2/5], Step [2564/10336], Loss: 0.0144\n",
      "Epoch [2/5], Step [2566/10336], Loss: 6.9811\n",
      "Epoch [2/5], Step [2568/10336], Loss: 0.2201\n",
      "Epoch [2/5], Step [2570/10336], Loss: 0.2968\n",
      "Epoch [2/5], Step [2572/10336], Loss: 0.0247\n",
      "Epoch [2/5], Step [2574/10336], Loss: 3.2776\n",
      "Epoch [2/5], Step [2576/10336], Loss: 0.2099\n",
      "Epoch [2/5], Step [2578/10336], Loss: 0.2197\n",
      "Epoch [2/5], Step [2580/10336], Loss: 0.2726\n",
      "Epoch [2/5], Step [2582/10336], Loss: 0.2686\n",
      "Epoch [2/5], Step [2584/10336], Loss: 3.5961\n",
      "Epoch [2/5], Step [2586/10336], Loss: 3.4871\n",
      "Epoch [2/5], Step [2588/10336], Loss: 0.4029\n",
      "Epoch [2/5], Step [2590/10336], Loss: 0.0838\n",
      "Epoch [2/5], Step [2592/10336], Loss: 0.2085\n",
      "Epoch [2/5], Step [2594/10336], Loss: 0.2861\n",
      "Epoch [2/5], Step [2596/10336], Loss: 0.3348\n",
      "Epoch [2/5], Step [2598/10336], Loss: 0.6284\n",
      "Epoch [2/5], Step [2600/10336], Loss: 0.1852\n",
      "Epoch [2/5], Step [2602/10336], Loss: 3.5858\n",
      "Epoch [2/5], Step [2604/10336], Loss: 2.2676\n",
      "Epoch [2/5], Step [2606/10336], Loss: 1.3819\n",
      "Epoch [2/5], Step [2608/10336], Loss: 0.1386\n",
      "Epoch [2/5], Step [2610/10336], Loss: 0.1545\n",
      "Epoch [2/5], Step [2612/10336], Loss: 0.0482\n",
      "Epoch [2/5], Step [2614/10336], Loss: 0.2334\n",
      "Epoch [2/5], Step [2616/10336], Loss: 2.6116\n",
      "Epoch [2/5], Step [2618/10336], Loss: 1.5866\n",
      "Epoch [2/5], Step [2620/10336], Loss: 0.3531\n",
      "Epoch [2/5], Step [2622/10336], Loss: 0.2189\n",
      "Epoch [2/5], Step [2624/10336], Loss: 2.5442\n",
      "Epoch [2/5], Step [2626/10336], Loss: 2.1648\n",
      "Epoch [2/5], Step [2628/10336], Loss: 0.2222\n",
      "Epoch [2/5], Step [2630/10336], Loss: 0.6442\n",
      "Epoch [2/5], Step [2632/10336], Loss: 0.0461\n",
      "Epoch [2/5], Step [2634/10336], Loss: 0.8702\n",
      "Epoch [2/5], Step [2636/10336], Loss: 0.5136\n",
      "Epoch [2/5], Step [2638/10336], Loss: 0.0878\n",
      "Epoch [2/5], Step [2640/10336], Loss: 0.0426\n",
      "Epoch [2/5], Step [2642/10336], Loss: 3.6282\n",
      "Epoch [2/5], Step [2644/10336], Loss: 0.3147\n",
      "Epoch [2/5], Step [2646/10336], Loss: 0.3453\n",
      "Epoch [2/5], Step [2648/10336], Loss: 0.1108\n",
      "Epoch [2/5], Step [2650/10336], Loss: 0.3321\n",
      "Epoch [2/5], Step [2652/10336], Loss: 3.5267\n",
      "Epoch [2/5], Step [2654/10336], Loss: 0.1323\n",
      "Epoch [2/5], Step [2656/10336], Loss: 0.1855\n",
      "Epoch [2/5], Step [2658/10336], Loss: 0.2818\n",
      "Epoch [2/5], Step [2660/10336], Loss: 0.2120\n",
      "Epoch [2/5], Step [2662/10336], Loss: 3.0023\n",
      "Epoch [2/5], Step [2664/10336], Loss: 0.5880\n",
      "Epoch [2/5], Step [2666/10336], Loss: 1.2540\n",
      "Epoch [2/5], Step [2668/10336], Loss: 0.7941\n",
      "Epoch [2/5], Step [2670/10336], Loss: 0.0765\n",
      "Epoch [2/5], Step [2672/10336], Loss: 0.4860\n",
      "Epoch [2/5], Step [2674/10336], Loss: 3.3323\n",
      "Epoch [2/5], Step [2676/10336], Loss: 0.0260\n",
      "Epoch [2/5], Step [2678/10336], Loss: 0.1114\n",
      "Epoch [2/5], Step [2680/10336], Loss: 0.0214\n",
      "Epoch [2/5], Step [2682/10336], Loss: 3.1367\n",
      "Epoch [2/5], Step [2684/10336], Loss: 1.6382\n",
      "Epoch [2/5], Step [2686/10336], Loss: 0.0682\n",
      "Epoch [2/5], Step [2688/10336], Loss: 0.0232\n",
      "Epoch [2/5], Step [2690/10336], Loss: 0.3640\n",
      "Epoch [2/5], Step [2692/10336], Loss: 0.2075\n",
      "Epoch [2/5], Step [2694/10336], Loss: 0.0162\n",
      "Epoch [2/5], Step [2696/10336], Loss: 0.9056\n",
      "Epoch [2/5], Step [2698/10336], Loss: 6.6668\n",
      "Epoch [2/5], Step [2700/10336], Loss: 0.5361\n",
      "Epoch [2/5], Step [2702/10336], Loss: 0.3625\n",
      "Epoch [2/5], Step [2704/10336], Loss: 0.2573\n",
      "Epoch [2/5], Step [2706/10336], Loss: 0.1509\n",
      "Epoch [2/5], Step [2708/10336], Loss: 0.0183\n",
      "Epoch [2/5], Step [2710/10336], Loss: 0.3511\n",
      "Epoch [2/5], Step [2712/10336], Loss: 0.4784\n",
      "Epoch [2/5], Step [2714/10336], Loss: 0.1141\n",
      "Epoch [2/5], Step [2716/10336], Loss: 0.0052\n",
      "Epoch [2/5], Step [2718/10336], Loss: 0.6050\n",
      "Epoch [2/5], Step [2720/10336], Loss: 0.8319\n",
      "Epoch [2/5], Step [2722/10336], Loss: 0.4538\n",
      "Epoch [2/5], Step [2724/10336], Loss: 0.3202\n",
      "Epoch [2/5], Step [2726/10336], Loss: 0.1778\n",
      "Epoch [2/5], Step [2728/10336], Loss: 0.4891\n",
      "Epoch [2/5], Step [2730/10336], Loss: 0.3880\n",
      "Epoch [2/5], Step [2732/10336], Loss: 0.3161\n",
      "Epoch [2/5], Step [2734/10336], Loss: 0.2948\n",
      "Epoch [2/5], Step [2736/10336], Loss: 0.1414\n",
      "Epoch [2/5], Step [2738/10336], Loss: 0.1936\n",
      "Epoch [2/5], Step [2740/10336], Loss: 0.2940\n",
      "Epoch [2/5], Step [2742/10336], Loss: 3.5630\n",
      "Epoch [2/5], Step [2744/10336], Loss: 0.7102\n",
      "Epoch [2/5], Step [2746/10336], Loss: 1.0838\n",
      "Epoch [2/5], Step [2748/10336], Loss: 0.8269\n",
      "Epoch [2/5], Step [2750/10336], Loss: 0.7720\n",
      "Epoch [2/5], Step [2752/10336], Loss: 0.3365\n",
      "Epoch [2/5], Step [2754/10336], Loss: 1.2910\n",
      "Epoch [2/5], Step [2756/10336], Loss: 0.1854\n",
      "Epoch [2/5], Step [2758/10336], Loss: 0.1602\n",
      "Epoch [2/5], Step [2760/10336], Loss: 0.3003\n",
      "Epoch [2/5], Step [2762/10336], Loss: 1.3247\n",
      "Epoch [2/5], Step [2764/10336], Loss: 0.2078\n",
      "Epoch [2/5], Step [2766/10336], Loss: 0.2118\n",
      "Epoch [2/5], Step [2768/10336], Loss: 3.8495\n",
      "Epoch [2/5], Step [2770/10336], Loss: 0.6476\n",
      "Epoch [2/5], Step [2772/10336], Loss: 0.3204\n",
      "Epoch [2/5], Step [2774/10336], Loss: 0.0078\n",
      "Epoch [2/5], Step [2776/10336], Loss: 0.2855\n",
      "Epoch [2/5], Step [2778/10336], Loss: 0.4080\n",
      "Epoch [2/5], Step [2780/10336], Loss: 0.1858\n",
      "Epoch [2/5], Step [2782/10336], Loss: 0.3581\n",
      "Epoch [2/5], Step [2784/10336], Loss: 0.2203\n",
      "Epoch [2/5], Step [2786/10336], Loss: 0.2322\n",
      "Epoch [2/5], Step [2788/10336], Loss: 4.0456\n",
      "Epoch [2/5], Step [2790/10336], Loss: 0.0782\n",
      "Epoch [2/5], Step [2792/10336], Loss: 0.0376\n",
      "Epoch [2/5], Step [2794/10336], Loss: 0.1006\n",
      "Epoch [2/5], Step [2796/10336], Loss: 0.0829\n",
      "Epoch [2/5], Step [2798/10336], Loss: 0.3126\n",
      "Epoch [2/5], Step [2800/10336], Loss: 2.0158\n",
      "Epoch [2/5], Step [2802/10336], Loss: 0.5000\n",
      "Epoch [2/5], Step [2804/10336], Loss: 0.1252\n",
      "Epoch [2/5], Step [2806/10336], Loss: 0.6719\n",
      "Epoch [2/5], Step [2808/10336], Loss: 0.0290\n",
      "Epoch [2/5], Step [2810/10336], Loss: 0.1199\n",
      "Epoch [2/5], Step [2812/10336], Loss: 0.0419\n",
      "Epoch [2/5], Step [2814/10336], Loss: 2.5211\n",
      "Epoch [2/5], Step [2816/10336], Loss: 1.7768\n",
      "Epoch [2/5], Step [2818/10336], Loss: 0.0470\n",
      "Epoch [2/5], Step [2820/10336], Loss: 0.1358\n",
      "Epoch [2/5], Step [2822/10336], Loss: 0.6256\n",
      "Epoch [2/5], Step [2824/10336], Loss: 0.0826\n",
      "Epoch [2/5], Step [2826/10336], Loss: 0.1365\n",
      "Epoch [2/5], Step [2828/10336], Loss: 1.8285\n",
      "Epoch [2/5], Step [2830/10336], Loss: 3.2053\n",
      "Epoch [2/5], Step [2832/10336], Loss: 3.7339\n",
      "Epoch [2/5], Step [2834/10336], Loss: 2.0485\n",
      "Epoch [2/5], Step [2836/10336], Loss: 0.1064\n",
      "Epoch [2/5], Step [2838/10336], Loss: 0.1654\n",
      "Epoch [2/5], Step [2840/10336], Loss: 2.6800\n",
      "Epoch [2/5], Step [2842/10336], Loss: 0.1994\n",
      "Epoch [2/5], Step [2844/10336], Loss: 0.2174\n",
      "Epoch [2/5], Step [2846/10336], Loss: 5.5555\n",
      "Epoch [2/5], Step [2848/10336], Loss: 2.4338\n",
      "Epoch [2/5], Step [2850/10336], Loss: 0.2141\n",
      "Epoch [2/5], Step [2852/10336], Loss: 1.9782\n",
      "Epoch [2/5], Step [2854/10336], Loss: 1.5763\n",
      "Epoch [2/5], Step [2856/10336], Loss: 4.9109\n",
      "Epoch [2/5], Step [2858/10336], Loss: 1.0949\n",
      "Epoch [2/5], Step [2860/10336], Loss: 2.2604\n",
      "Epoch [2/5], Step [2862/10336], Loss: 0.3551\n",
      "Epoch [2/5], Step [2864/10336], Loss: 0.2967\n",
      "Epoch [2/5], Step [2866/10336], Loss: 0.3346\n",
      "Epoch [2/5], Step [2868/10336], Loss: 0.1903\n",
      "Epoch [2/5], Step [2870/10336], Loss: 0.4356\n",
      "Epoch [2/5], Step [2872/10336], Loss: 1.6719\n",
      "Epoch [2/5], Step [2874/10336], Loss: 1.9357\n",
      "Epoch [2/5], Step [2876/10336], Loss: 1.6507\n",
      "Epoch [2/5], Step [2878/10336], Loss: 1.6665\n",
      "Epoch [2/5], Step [2880/10336], Loss: 0.3045\n",
      "Epoch [2/5], Step [2882/10336], Loss: 0.0068\n",
      "Epoch [2/5], Step [2884/10336], Loss: 0.1260\n",
      "Epoch [2/5], Step [2886/10336], Loss: 0.3230\n",
      "Epoch [2/5], Step [2888/10336], Loss: 0.0346\n",
      "Epoch [2/5], Step [2890/10336], Loss: 0.0081\n",
      "Epoch [2/5], Step [2892/10336], Loss: 4.5156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [2894/10336], Loss: 0.5031\n",
      "Epoch [2/5], Step [2896/10336], Loss: 5.5038\n",
      "Epoch [2/5], Step [2898/10336], Loss: 1.2487\n",
      "Epoch [2/5], Step [2900/10336], Loss: 3.3041\n",
      "Epoch [2/5], Step [2902/10336], Loss: 2.0811\n",
      "Epoch [2/5], Step [2904/10336], Loss: 0.1099\n",
      "Epoch [2/5], Step [2906/10336], Loss: 0.7631\n",
      "Epoch [2/5], Step [2908/10336], Loss: 0.8121\n",
      "Epoch [2/5], Step [2910/10336], Loss: 0.6392\n",
      "Epoch [2/5], Step [2912/10336], Loss: 0.4158\n",
      "Epoch [2/5], Step [2914/10336], Loss: 4.0184\n",
      "Epoch [2/5], Step [2916/10336], Loss: 1.0095\n",
      "Epoch [2/5], Step [2918/10336], Loss: 0.0491\n",
      "Epoch [2/5], Step [2920/10336], Loss: 0.3827\n",
      "Epoch [2/5], Step [2922/10336], Loss: 0.1483\n",
      "Epoch [2/5], Step [2924/10336], Loss: 0.0386\n",
      "Epoch [2/5], Step [2926/10336], Loss: 0.2934\n",
      "Epoch [2/5], Step [2928/10336], Loss: 0.0159\n",
      "Epoch [2/5], Step [2930/10336], Loss: 0.0624\n",
      "Epoch [2/5], Step [2932/10336], Loss: 0.0033\n",
      "Epoch [2/5], Step [2934/10336], Loss: 1.5864\n",
      "Epoch [2/5], Step [2936/10336], Loss: 0.3341\n",
      "Epoch [2/5], Step [2938/10336], Loss: 0.4190\n",
      "Epoch [2/5], Step [2940/10336], Loss: 1.0474\n",
      "Epoch [2/5], Step [2942/10336], Loss: 0.9731\n",
      "Epoch [2/5], Step [2944/10336], Loss: 0.4100\n",
      "Epoch [2/5], Step [2946/10336], Loss: 0.4921\n",
      "Epoch [2/5], Step [2948/10336], Loss: 0.4992\n",
      "Epoch [2/5], Step [2950/10336], Loss: 0.4923\n",
      "Epoch [2/5], Step [2952/10336], Loss: 0.3666\n",
      "Epoch [2/5], Step [2954/10336], Loss: 0.9945\n",
      "Epoch [2/5], Step [2956/10336], Loss: 0.0596\n",
      "Epoch [2/5], Step [2958/10336], Loss: 2.8202\n",
      "Epoch [2/5], Step [2960/10336], Loss: 0.5399\n",
      "Epoch [2/5], Step [2962/10336], Loss: 0.2333\n",
      "Epoch [2/5], Step [2964/10336], Loss: 0.1250\n",
      "Epoch [2/5], Step [2966/10336], Loss: 3.7753\n",
      "Epoch [2/5], Step [2968/10336], Loss: 1.1543\n",
      "Epoch [2/5], Step [2970/10336], Loss: 3.2273\n",
      "Epoch [2/5], Step [2972/10336], Loss: 0.9318\n",
      "Epoch [2/5], Step [2974/10336], Loss: 0.4305\n",
      "Epoch [2/5], Step [2976/10336], Loss: 0.3873\n",
      "Epoch [2/5], Step [2978/10336], Loss: 0.0303\n",
      "Epoch [2/5], Step [2980/10336], Loss: 1.1116\n",
      "Epoch [2/5], Step [2982/10336], Loss: 2.9953\n",
      "Epoch [2/5], Step [2984/10336], Loss: 4.4128\n",
      "Epoch [2/5], Step [2986/10336], Loss: 0.2026\n",
      "Epoch [2/5], Step [2988/10336], Loss: 2.3694\n",
      "Epoch [2/5], Step [2990/10336], Loss: 0.3745\n",
      "Epoch [2/5], Step [2992/10336], Loss: 0.4673\n",
      "Epoch [2/5], Step [2994/10336], Loss: 0.2516\n",
      "Epoch [2/5], Step [2996/10336], Loss: 1.4560\n",
      "Epoch [2/5], Step [2998/10336], Loss: 0.1099\n",
      "Epoch [2/5], Step [3000/10336], Loss: 0.1218\n",
      "Epoch [2/5], Step [3002/10336], Loss: 0.9696\n",
      "Epoch [2/5], Step [3004/10336], Loss: 0.0930\n",
      "Epoch [2/5], Step [3006/10336], Loss: 0.0040\n",
      "Epoch [2/5], Step [3008/10336], Loss: 0.0728\n",
      "Epoch [2/5], Step [3010/10336], Loss: 0.0687\n",
      "Epoch [2/5], Step [3012/10336], Loss: 1.3355\n",
      "Epoch [2/5], Step [3014/10336], Loss: 0.2158\n",
      "Epoch [2/5], Step [3016/10336], Loss: 2.5812\n",
      "Epoch [2/5], Step [3018/10336], Loss: 0.0781\n",
      "Epoch [2/5], Step [3020/10336], Loss: 0.3402\n",
      "Epoch [2/5], Step [3022/10336], Loss: 0.1411\n",
      "Epoch [2/5], Step [3024/10336], Loss: 0.3883\n",
      "Epoch [2/5], Step [3026/10336], Loss: 1.3178\n",
      "Epoch [2/5], Step [3028/10336], Loss: 0.0167\n",
      "Epoch [2/5], Step [3030/10336], Loss: 0.0755\n",
      "Epoch [2/5], Step [3032/10336], Loss: 2.5796\n",
      "Epoch [2/5], Step [3034/10336], Loss: 0.0694\n",
      "Epoch [2/5], Step [3036/10336], Loss: 1.3151\n",
      "Epoch [2/5], Step [3038/10336], Loss: 2.4747\n",
      "Epoch [2/5], Step [3040/10336], Loss: 0.0107\n",
      "Epoch [2/5], Step [3042/10336], Loss: 0.3110\n",
      "Epoch [2/5], Step [3044/10336], Loss: 0.3919\n",
      "Epoch [2/5], Step [3046/10336], Loss: 0.5760\n",
      "Epoch [2/5], Step [3048/10336], Loss: 0.4134\n",
      "Epoch [2/5], Step [3050/10336], Loss: 0.0629\n",
      "Epoch [2/5], Step [3052/10336], Loss: 0.3836\n",
      "Epoch [2/5], Step [3054/10336], Loss: 0.0079\n",
      "Epoch [2/5], Step [3056/10336], Loss: 0.0041\n",
      "Epoch [2/5], Step [3058/10336], Loss: 0.6550\n",
      "Epoch [2/5], Step [3060/10336], Loss: 0.2068\n",
      "Epoch [2/5], Step [3062/10336], Loss: 3.3990\n",
      "Epoch [2/5], Step [3064/10336], Loss: 0.0263\n",
      "Epoch [2/5], Step [3066/10336], Loss: 0.0417\n",
      "Epoch [2/5], Step [3068/10336], Loss: 0.4912\n",
      "Epoch [2/5], Step [3070/10336], Loss: 0.2151\n",
      "Epoch [2/5], Step [3072/10336], Loss: 3.5729\n",
      "Epoch [2/5], Step [3074/10336], Loss: 0.3102\n",
      "Epoch [2/5], Step [3076/10336], Loss: 0.0600\n",
      "Epoch [2/5], Step [3078/10336], Loss: 0.4509\n",
      "Epoch [2/5], Step [3080/10336], Loss: 0.7085\n",
      "Epoch [2/5], Step [3082/10336], Loss: 0.1700\n",
      "Epoch [2/5], Step [3084/10336], Loss: 0.1061\n",
      "Epoch [2/5], Step [3086/10336], Loss: 0.2605\n",
      "Epoch [2/5], Step [3088/10336], Loss: 0.5572\n",
      "Epoch [2/5], Step [3090/10336], Loss: 0.0115\n",
      "Epoch [2/5], Step [3092/10336], Loss: 0.1185\n",
      "Epoch [2/5], Step [3094/10336], Loss: 0.2379\n",
      "Epoch [2/5], Step [3096/10336], Loss: 0.3556\n",
      "Epoch [2/5], Step [3098/10336], Loss: 0.7630\n",
      "Epoch [2/5], Step [3100/10336], Loss: 0.1784\n",
      "Epoch [2/5], Step [3102/10336], Loss: 0.0281\n",
      "Epoch [2/5], Step [3104/10336], Loss: 3.5751\n",
      "Epoch [2/5], Step [3106/10336], Loss: 0.2288\n",
      "Epoch [2/5], Step [3108/10336], Loss: 0.3643\n",
      "Epoch [2/5], Step [3110/10336], Loss: 0.8147\n",
      "Epoch [2/5], Step [3112/10336], Loss: 1.6892\n",
      "Epoch [2/5], Step [3114/10336], Loss: 0.2326\n",
      "Epoch [2/5], Step [3116/10336], Loss: 2.1575\n",
      "Epoch [2/5], Step [3118/10336], Loss: 0.9299\n",
      "Epoch [2/5], Step [3120/10336], Loss: 0.7854\n",
      "Epoch [2/5], Step [3122/10336], Loss: 2.6642\n",
      "Epoch [2/5], Step [3124/10336], Loss: 0.1663\n",
      "Epoch [2/5], Step [3126/10336], Loss: 0.6594\n",
      "Epoch [2/5], Step [3128/10336], Loss: 1.0938\n",
      "Epoch [2/5], Step [3130/10336], Loss: 0.7805\n",
      "Epoch [2/5], Step [3132/10336], Loss: 0.8563\n",
      "Epoch [2/5], Step [3134/10336], Loss: 3.3676\n",
      "Epoch [2/5], Step [3136/10336], Loss: 1.2441\n",
      "Epoch [2/5], Step [3138/10336], Loss: 0.1684\n",
      "Epoch [2/5], Step [3140/10336], Loss: 0.9708\n",
      "Epoch [2/5], Step [3142/10336], Loss: 0.0132\n",
      "Epoch [2/5], Step [3144/10336], Loss: 2.0899\n",
      "Epoch [2/5], Step [3146/10336], Loss: 1.2506\n",
      "Epoch [2/5], Step [3148/10336], Loss: 0.1342\n",
      "Epoch [2/5], Step [3150/10336], Loss: 0.4241\n",
      "Epoch [2/5], Step [3152/10336], Loss: 0.8117\n",
      "Epoch [2/5], Step [3154/10336], Loss: 0.2407\n",
      "Epoch [2/5], Step [3156/10336], Loss: 0.1016\n",
      "Epoch [2/5], Step [3158/10336], Loss: 0.1005\n",
      "Epoch [2/5], Step [3160/10336], Loss: 0.0140\n",
      "Epoch [2/5], Step [3162/10336], Loss: 4.8316\n",
      "Epoch [2/5], Step [3164/10336], Loss: 0.3961\n",
      "Epoch [2/5], Step [3166/10336], Loss: 1.3779\n",
      "Epoch [2/5], Step [3168/10336], Loss: 0.5653\n",
      "Epoch [2/5], Step [3170/10336], Loss: 3.4755\n",
      "Epoch [2/5], Step [3172/10336], Loss: 0.6205\n",
      "Epoch [2/5], Step [3174/10336], Loss: 0.2536\n",
      "Epoch [2/5], Step [3176/10336], Loss: 0.6107\n",
      "Epoch [2/5], Step [3178/10336], Loss: 0.1535\n",
      "Epoch [2/5], Step [3180/10336], Loss: 1.0644\n",
      "Epoch [2/5], Step [3182/10336], Loss: 1.5006\n",
      "Epoch [2/5], Step [3184/10336], Loss: 0.2595\n",
      "Epoch [2/5], Step [3186/10336], Loss: 4.2172\n",
      "Epoch [2/5], Step [3188/10336], Loss: 0.0701\n",
      "Epoch [2/5], Step [3190/10336], Loss: 0.0797\n",
      "Epoch [2/5], Step [3192/10336], Loss: 0.4109\n",
      "Epoch [2/5], Step [3194/10336], Loss: 2.9645\n",
      "Epoch [2/5], Step [3196/10336], Loss: 0.1769\n",
      "Epoch [2/5], Step [3198/10336], Loss: 0.3375\n",
      "Epoch [2/5], Step [3200/10336], Loss: 0.3942\n",
      "Epoch [2/5], Step [3202/10336], Loss: 1.2993\n",
      "Epoch [2/5], Step [3204/10336], Loss: 0.1380\n",
      "Epoch [2/5], Step [3206/10336], Loss: 1.4962\n",
      "Epoch [2/5], Step [3208/10336], Loss: 1.6617\n",
      "Epoch [2/5], Step [3210/10336], Loss: 0.0433\n",
      "Epoch [2/5], Step [3212/10336], Loss: 0.0301\n",
      "Epoch [2/5], Step [3214/10336], Loss: 0.7967\n",
      "Epoch [2/5], Step [3216/10336], Loss: 0.0592\n",
      "Epoch [2/5], Step [3218/10336], Loss: 0.0713\n",
      "Epoch [2/5], Step [3220/10336], Loss: 0.1237\n",
      "Epoch [2/5], Step [3222/10336], Loss: 4.1134\n",
      "Epoch [2/5], Step [3224/10336], Loss: 0.2260\n",
      "Epoch [2/5], Step [3226/10336], Loss: 5.2233\n",
      "Epoch [2/5], Step [3228/10336], Loss: 0.4628\n",
      "Epoch [2/5], Step [3230/10336], Loss: 4.1684\n",
      "Epoch [2/5], Step [3232/10336], Loss: 0.0374\n",
      "Epoch [2/5], Step [3234/10336], Loss: 0.3541\n",
      "Epoch [2/5], Step [3236/10336], Loss: 0.0414\n",
      "Epoch [2/5], Step [3238/10336], Loss: 0.2291\n",
      "Epoch [2/5], Step [3240/10336], Loss: 4.4780\n",
      "Epoch [2/5], Step [3242/10336], Loss: 0.4331\n",
      "Epoch [2/5], Step [3244/10336], Loss: 4.0382\n",
      "Epoch [2/5], Step [3246/10336], Loss: 0.3228\n",
      "Epoch [2/5], Step [3248/10336], Loss: 0.9538\n",
      "Epoch [2/5], Step [3250/10336], Loss: 0.1670\n",
      "Epoch [2/5], Step [3252/10336], Loss: 0.4873\n",
      "Epoch [2/5], Step [3254/10336], Loss: 0.4499\n",
      "Epoch [2/5], Step [3256/10336], Loss: 2.3585\n",
      "Epoch [2/5], Step [3258/10336], Loss: 0.8746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [3260/10336], Loss: 0.6177\n",
      "Epoch [2/5], Step [3262/10336], Loss: 1.5399\n",
      "Epoch [2/5], Step [3264/10336], Loss: 2.8927\n",
      "Epoch [2/5], Step [3266/10336], Loss: 0.1586\n",
      "Epoch [2/5], Step [3268/10336], Loss: 3.0723\n",
      "Epoch [2/5], Step [3270/10336], Loss: 2.7364\n",
      "Epoch [2/5], Step [3272/10336], Loss: 0.1118\n",
      "Epoch [2/5], Step [3274/10336], Loss: 0.0380\n",
      "Epoch [2/5], Step [3276/10336], Loss: 3.0876\n",
      "Epoch [2/5], Step [3278/10336], Loss: 2.8606\n",
      "Epoch [2/5], Step [3280/10336], Loss: 0.6567\n",
      "Epoch [2/5], Step [3282/10336], Loss: 0.2811\n",
      "Epoch [2/5], Step [3284/10336], Loss: 4.3016\n",
      "Epoch [2/5], Step [3286/10336], Loss: 0.0838\n",
      "Epoch [2/5], Step [3288/10336], Loss: 1.1764\n",
      "Epoch [2/5], Step [3290/10336], Loss: 0.4904\n",
      "Epoch [2/5], Step [3292/10336], Loss: 0.1824\n",
      "Epoch [2/5], Step [3294/10336], Loss: 2.1049\n",
      "Epoch [2/5], Step [3296/10336], Loss: 2.4682\n",
      "Epoch [2/5], Step [3298/10336], Loss: 0.4789\n",
      "Epoch [2/5], Step [3300/10336], Loss: 0.0828\n",
      "Epoch [2/5], Step [3302/10336], Loss: 0.0181\n",
      "Epoch [2/5], Step [3304/10336], Loss: 0.3162\n",
      "Epoch [2/5], Step [3306/10336], Loss: 1.5312\n",
      "Epoch [2/5], Step [3308/10336], Loss: 1.3117\n",
      "Epoch [2/5], Step [3310/10336], Loss: 0.0740\n",
      "Epoch [2/5], Step [3312/10336], Loss: 6.1200\n",
      "Epoch [2/5], Step [3314/10336], Loss: 1.6188\n",
      "Epoch [2/5], Step [3316/10336], Loss: 0.0899\n",
      "Epoch [2/5], Step [3318/10336], Loss: 0.2088\n",
      "Epoch [2/5], Step [3320/10336], Loss: 0.3551\n",
      "Epoch [2/5], Step [3322/10336], Loss: 0.2065\n",
      "Epoch [2/5], Step [3324/10336], Loss: 0.3799\n",
      "Epoch [2/5], Step [3326/10336], Loss: 0.3349\n",
      "Epoch [2/5], Step [3328/10336], Loss: 0.0357\n",
      "Epoch [2/5], Step [3330/10336], Loss: 0.5961\n",
      "Epoch [2/5], Step [3332/10336], Loss: 0.1079\n",
      "Epoch [2/5], Step [3334/10336], Loss: 0.0157\n",
      "Epoch [2/5], Step [3336/10336], Loss: 0.1876\n",
      "Epoch [2/5], Step [3338/10336], Loss: 0.1892\n",
      "Epoch [2/5], Step [3340/10336], Loss: 6.3017\n",
      "Epoch [2/5], Step [3342/10336], Loss: 0.3903\n",
      "Epoch [2/5], Step [3344/10336], Loss: 2.4354\n",
      "Epoch [2/5], Step [3346/10336], Loss: 0.1831\n",
      "Epoch [2/5], Step [3348/10336], Loss: 0.2375\n",
      "Epoch [2/5], Step [3350/10336], Loss: 0.4615\n",
      "Epoch [2/5], Step [3352/10336], Loss: 0.9586\n",
      "Epoch [2/5], Step [3354/10336], Loss: 0.2539\n",
      "Epoch [2/5], Step [3356/10336], Loss: 0.2027\n",
      "Epoch [2/5], Step [3358/10336], Loss: 0.2345\n",
      "Epoch [2/5], Step [3360/10336], Loss: 0.1070\n",
      "Epoch [2/5], Step [3362/10336], Loss: 0.2220\n",
      "Epoch [2/5], Step [3364/10336], Loss: 0.0231\n",
      "Epoch [2/5], Step [3366/10336], Loss: 0.1292\n",
      "Epoch [2/5], Step [3368/10336], Loss: 0.2978\n",
      "Epoch [2/5], Step [3370/10336], Loss: 2.9706\n",
      "Epoch [2/5], Step [3372/10336], Loss: 0.1422\n",
      "Epoch [2/5], Step [3374/10336], Loss: 0.2962\n",
      "Epoch [2/5], Step [3376/10336], Loss: 0.7055\n",
      "Epoch [2/5], Step [3378/10336], Loss: 0.0084\n",
      "Epoch [2/5], Step [3380/10336], Loss: 0.2143\n",
      "Epoch [2/5], Step [3382/10336], Loss: 0.4950\n",
      "Epoch [2/5], Step [3384/10336], Loss: 0.1876\n",
      "Epoch [2/5], Step [3386/10336], Loss: 1.7158\n",
      "Epoch [2/5], Step [3388/10336], Loss: 0.2792\n",
      "Epoch [2/5], Step [3390/10336], Loss: 0.0026\n",
      "Epoch [2/5], Step [3392/10336], Loss: 1.1504\n",
      "Epoch [2/5], Step [3394/10336], Loss: 1.4981\n",
      "Epoch [2/5], Step [3396/10336], Loss: 3.7213\n",
      "Epoch [2/5], Step [3398/10336], Loss: 0.2451\n",
      "Epoch [2/5], Step [3400/10336], Loss: 0.0557\n",
      "Epoch [2/5], Step [3402/10336], Loss: 0.1100\n",
      "Epoch [2/5], Step [3404/10336], Loss: 0.1650\n",
      "Epoch [2/5], Step [3406/10336], Loss: 0.2733\n",
      "Epoch [2/5], Step [3408/10336], Loss: 0.3830\n",
      "Epoch [2/5], Step [3410/10336], Loss: 0.7126\n",
      "Epoch [2/5], Step [3412/10336], Loss: 0.1356\n",
      "Epoch [2/5], Step [3414/10336], Loss: 0.5600\n",
      "Epoch [2/5], Step [3416/10336], Loss: 1.6123\n",
      "Epoch [2/5], Step [3418/10336], Loss: 2.0950\n",
      "Epoch [2/5], Step [3420/10336], Loss: 0.8817\n",
      "Epoch [2/5], Step [3422/10336], Loss: 5.9760\n",
      "Epoch [2/5], Step [3424/10336], Loss: 0.5861\n",
      "Epoch [2/5], Step [3426/10336], Loss: 0.1022\n",
      "Epoch [2/5], Step [3428/10336], Loss: 0.0472\n",
      "Epoch [2/5], Step [3430/10336], Loss: 2.6005\n",
      "Epoch [2/5], Step [3432/10336], Loss: 0.8610\n",
      "Epoch [2/5], Step [3434/10336], Loss: 0.3298\n",
      "Epoch [2/5], Step [3436/10336], Loss: 3.1500\n",
      "Epoch [2/5], Step [3438/10336], Loss: 0.2576\n",
      "Epoch [2/5], Step [3440/10336], Loss: 0.4922\n",
      "Epoch [2/5], Step [3442/10336], Loss: 1.8040\n",
      "Epoch [2/5], Step [3444/10336], Loss: 2.4721\n",
      "Epoch [2/5], Step [3446/10336], Loss: 0.5851\n",
      "Epoch [2/5], Step [3448/10336], Loss: 0.1882\n",
      "Epoch [2/5], Step [3450/10336], Loss: 0.7304\n",
      "Epoch [2/5], Step [3452/10336], Loss: 1.9237\n",
      "Epoch [2/5], Step [3454/10336], Loss: 0.1374\n",
      "Epoch [2/5], Step [3456/10336], Loss: 0.0679\n",
      "Epoch [2/5], Step [3458/10336], Loss: 0.2387\n",
      "Epoch [2/5], Step [3460/10336], Loss: 0.0681\n",
      "Epoch [2/5], Step [3462/10336], Loss: 5.4816\n",
      "Epoch [2/5], Step [3464/10336], Loss: 0.3508\n",
      "Epoch [2/5], Step [3466/10336], Loss: 0.0152\n",
      "Epoch [2/5], Step [3468/10336], Loss: 1.4332\n",
      "Epoch [2/5], Step [3470/10336], Loss: 0.4367\n",
      "Epoch [2/5], Step [3472/10336], Loss: 1.8883\n",
      "Epoch [2/5], Step [3474/10336], Loss: 1.6312\n",
      "Epoch [2/5], Step [3476/10336], Loss: 5.8277\n",
      "Epoch [2/5], Step [3478/10336], Loss: 2.8543\n",
      "Epoch [2/5], Step [3480/10336], Loss: 0.2340\n",
      "Epoch [2/5], Step [3482/10336], Loss: 0.4480\n",
      "Epoch [2/5], Step [3484/10336], Loss: 0.1151\n",
      "Epoch [2/5], Step [3486/10336], Loss: 0.1796\n",
      "Epoch [2/5], Step [3488/10336], Loss: 3.7355\n",
      "Epoch [2/5], Step [3490/10336], Loss: 1.2204\n",
      "Epoch [2/5], Step [3492/10336], Loss: 1.1186\n",
      "Epoch [2/5], Step [3494/10336], Loss: 0.2781\n",
      "Epoch [2/5], Step [3496/10336], Loss: 0.1902\n",
      "Epoch [2/5], Step [3498/10336], Loss: 1.7883\n",
      "Epoch [2/5], Step [3500/10336], Loss: 0.2398\n",
      "Epoch [2/5], Step [3502/10336], Loss: 0.3109\n",
      "Epoch [2/5], Step [3504/10336], Loss: 0.5654\n",
      "Epoch [2/5], Step [3506/10336], Loss: 0.9637\n",
      "Epoch [2/5], Step [3508/10336], Loss: 0.5534\n",
      "Epoch [2/5], Step [3510/10336], Loss: 0.8153\n",
      "Epoch [2/5], Step [3512/10336], Loss: 0.0044\n",
      "Epoch [2/5], Step [3514/10336], Loss: 1.2756\n",
      "Epoch [2/5], Step [3516/10336], Loss: 0.2804\n",
      "Epoch [2/5], Step [3518/10336], Loss: 0.2977\n",
      "Epoch [2/5], Step [3520/10336], Loss: 0.2212\n",
      "Epoch [2/5], Step [3522/10336], Loss: 1.4744\n",
      "Epoch [2/5], Step [3524/10336], Loss: 3.4056\n",
      "Epoch [2/5], Step [3526/10336], Loss: 2.7544\n",
      "Epoch [2/5], Step [3528/10336], Loss: 0.5165\n",
      "Epoch [2/5], Step [3530/10336], Loss: 0.3099\n",
      "Epoch [2/5], Step [3532/10336], Loss: 0.2363\n",
      "Epoch [2/5], Step [3534/10336], Loss: 0.1060\n",
      "Epoch [2/5], Step [3536/10336], Loss: 0.7195\n",
      "Epoch [2/5], Step [3538/10336], Loss: 0.0438\n",
      "Epoch [2/5], Step [3540/10336], Loss: 1.3363\n",
      "Epoch [2/5], Step [3542/10336], Loss: 0.0038\n",
      "Epoch [2/5], Step [3544/10336], Loss: 0.7798\n",
      "Epoch [2/5], Step [3546/10336], Loss: 0.9756\n",
      "Epoch [2/5], Step [3548/10336], Loss: 1.5413\n",
      "Epoch [2/5], Step [3550/10336], Loss: 0.0069\n",
      "Epoch [2/5], Step [3552/10336], Loss: 0.5030\n",
      "Epoch [2/5], Step [3554/10336], Loss: 4.5489\n",
      "Epoch [2/5], Step [3556/10336], Loss: 0.4475\n",
      "Epoch [2/5], Step [3558/10336], Loss: 3.4053\n",
      "Epoch [2/5], Step [3560/10336], Loss: 0.0009\n",
      "Epoch [2/5], Step [3562/10336], Loss: 0.6863\n",
      "Epoch [2/5], Step [3564/10336], Loss: 0.7761\n",
      "Epoch [2/5], Step [3566/10336], Loss: 0.2822\n",
      "Epoch [2/5], Step [3568/10336], Loss: 0.1157\n",
      "Epoch [2/5], Step [3570/10336], Loss: 0.0828\n",
      "Epoch [2/5], Step [3572/10336], Loss: 1.3648\n",
      "Epoch [2/5], Step [3574/10336], Loss: 1.5946\n",
      "Epoch [2/5], Step [3576/10336], Loss: 0.9846\n",
      "Epoch [2/5], Step [3578/10336], Loss: 4.4470\n",
      "Epoch [2/5], Step [3580/10336], Loss: 0.1491\n",
      "Epoch [2/5], Step [3582/10336], Loss: 2.0396\n",
      "Epoch [2/5], Step [3584/10336], Loss: 0.2921\n",
      "Epoch [2/5], Step [3586/10336], Loss: 0.0427\n",
      "Epoch [2/5], Step [3588/10336], Loss: 0.2538\n",
      "Epoch [2/5], Step [3590/10336], Loss: 0.2733\n",
      "Epoch [2/5], Step [3592/10336], Loss: 0.1017\n",
      "Epoch [2/5], Step [3594/10336], Loss: 3.4064\n",
      "Epoch [2/5], Step [3596/10336], Loss: 0.2588\n",
      "Epoch [2/5], Step [3598/10336], Loss: 0.2011\n",
      "Epoch [2/5], Step [3600/10336], Loss: 0.3316\n",
      "Epoch [2/5], Step [3602/10336], Loss: 1.7678\n",
      "Epoch [2/5], Step [3604/10336], Loss: 0.1439\n",
      "Epoch [2/5], Step [3606/10336], Loss: 0.3210\n",
      "Epoch [2/5], Step [3608/10336], Loss: 0.2992\n",
      "Epoch [2/5], Step [3610/10336], Loss: 0.0500\n",
      "Epoch [2/5], Step [3612/10336], Loss: 0.6023\n",
      "Epoch [2/5], Step [3614/10336], Loss: 0.0038\n",
      "Epoch [2/5], Step [3616/10336], Loss: 0.4281\n",
      "Epoch [2/5], Step [3618/10336], Loss: 1.2735\n",
      "Epoch [2/5], Step [3620/10336], Loss: 0.2822\n",
      "Epoch [2/5], Step [3622/10336], Loss: 0.0069\n",
      "Epoch [2/5], Step [3624/10336], Loss: 0.3445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [3626/10336], Loss: 3.6524\n",
      "Epoch [2/5], Step [3628/10336], Loss: 0.1814\n",
      "Epoch [2/5], Step [3630/10336], Loss: 1.3300\n",
      "Epoch [2/5], Step [3632/10336], Loss: 0.1781\n",
      "Epoch [2/5], Step [3634/10336], Loss: 0.1677\n",
      "Epoch [2/5], Step [3636/10336], Loss: 1.7686\n",
      "Epoch [2/5], Step [3638/10336], Loss: 0.1814\n",
      "Epoch [2/5], Step [3640/10336], Loss: 1.6731\n",
      "Epoch [2/5], Step [3642/10336], Loss: 0.4264\n",
      "Epoch [2/5], Step [3644/10336], Loss: 0.3813\n",
      "Epoch [2/5], Step [3646/10336], Loss: 0.0223\n",
      "Epoch [2/5], Step [3648/10336], Loss: 0.9702\n",
      "Epoch [2/5], Step [3650/10336], Loss: 0.6842\n",
      "Epoch [2/5], Step [3652/10336], Loss: 0.7295\n",
      "Epoch [2/5], Step [3654/10336], Loss: 0.8928\n",
      "Epoch [2/5], Step [3656/10336], Loss: 0.9928\n",
      "Epoch [2/5], Step [3658/10336], Loss: 0.0787\n",
      "Epoch [2/5], Step [3660/10336], Loss: 2.9778\n",
      "Epoch [2/5], Step [3662/10336], Loss: 0.0884\n",
      "Epoch [2/5], Step [3664/10336], Loss: 0.4459\n",
      "Epoch [2/5], Step [3666/10336], Loss: 2.6085\n",
      "Epoch [2/5], Step [3668/10336], Loss: 0.8402\n",
      "Epoch [2/5], Step [3670/10336], Loss: 4.7366\n",
      "Epoch [2/5], Step [3672/10336], Loss: 1.7873\n",
      "Epoch [2/5], Step [3674/10336], Loss: 0.2719\n",
      "Epoch [2/5], Step [3676/10336], Loss: 0.1777\n",
      "Epoch [2/5], Step [3678/10336], Loss: 0.0139\n",
      "Epoch [2/5], Step [3680/10336], Loss: 0.1735\n",
      "Epoch [2/5], Step [3682/10336], Loss: 1.3432\n",
      "Epoch [2/5], Step [3684/10336], Loss: 1.6581\n",
      "Epoch [2/5], Step [3686/10336], Loss: 2.8317\n",
      "Epoch [2/5], Step [3688/10336], Loss: 0.5240\n",
      "Epoch [2/5], Step [3690/10336], Loss: 0.3785\n",
      "Epoch [2/5], Step [3692/10336], Loss: 0.4970\n",
      "Epoch [2/5], Step [3694/10336], Loss: 0.0426\n",
      "Epoch [2/5], Step [3696/10336], Loss: 0.2772\n",
      "Epoch [2/5], Step [3698/10336], Loss: 3.0779\n",
      "Epoch [2/5], Step [3700/10336], Loss: 1.3648\n",
      "Epoch [2/5], Step [3702/10336], Loss: 0.3133\n",
      "Epoch [2/5], Step [3704/10336], Loss: 0.1272\n",
      "Epoch [2/5], Step [3706/10336], Loss: 0.2516\n",
      "Epoch [2/5], Step [3708/10336], Loss: 3.1093\n",
      "Epoch [2/5], Step [3710/10336], Loss: 0.1430\n",
      "Epoch [2/5], Step [3712/10336], Loss: 0.0132\n",
      "Epoch [2/5], Step [3714/10336], Loss: 1.0933\n",
      "Epoch [2/5], Step [3716/10336], Loss: 4.3263\n",
      "Epoch [2/5], Step [3718/10336], Loss: 0.0142\n",
      "Epoch [2/5], Step [3720/10336], Loss: 2.3654\n",
      "Epoch [2/5], Step [3722/10336], Loss: 0.4476\n",
      "Epoch [2/5], Step [3724/10336], Loss: 0.1917\n",
      "Epoch [2/5], Step [3726/10336], Loss: 1.1837\n",
      "Epoch [2/5], Step [3728/10336], Loss: 0.2060\n",
      "Epoch [2/5], Step [3730/10336], Loss: 1.1935\n",
      "Epoch [2/5], Step [3732/10336], Loss: 0.2302\n",
      "Epoch [2/5], Step [3734/10336], Loss: 0.0527\n",
      "Epoch [2/5], Step [3736/10336], Loss: 0.2506\n",
      "Epoch [2/5], Step [3738/10336], Loss: 1.1929\n",
      "Epoch [2/5], Step [3740/10336], Loss: 0.1293\n",
      "Epoch [2/5], Step [3742/10336], Loss: 0.1964\n",
      "Epoch [2/5], Step [3744/10336], Loss: 0.4258\n",
      "Epoch [2/5], Step [3746/10336], Loss: 0.1842\n",
      "Epoch [2/5], Step [3748/10336], Loss: 0.4082\n",
      "Epoch [2/5], Step [3750/10336], Loss: 0.0393\n",
      "Epoch [2/5], Step [3752/10336], Loss: 0.3242\n",
      "Epoch [2/5], Step [3754/10336], Loss: 0.3489\n",
      "Epoch [2/5], Step [3756/10336], Loss: 0.0977\n",
      "Epoch [2/5], Step [3758/10336], Loss: 0.2181\n",
      "Epoch [2/5], Step [3760/10336], Loss: 1.6795\n",
      "Epoch [2/5], Step [3762/10336], Loss: 0.2108\n",
      "Epoch [2/5], Step [3764/10336], Loss: 0.1548\n",
      "Epoch [2/5], Step [3766/10336], Loss: 3.6076\n",
      "Epoch [2/5], Step [3768/10336], Loss: 0.4316\n",
      "Epoch [2/5], Step [3770/10336], Loss: 0.5431\n",
      "Epoch [2/5], Step [3772/10336], Loss: 0.0591\n",
      "Epoch [2/5], Step [3774/10336], Loss: 0.0469\n",
      "Epoch [2/5], Step [3776/10336], Loss: 3.1264\n",
      "Epoch [2/5], Step [3778/10336], Loss: 1.0116\n",
      "Epoch [2/5], Step [3780/10336], Loss: 0.4404\n",
      "Epoch [2/5], Step [3782/10336], Loss: 1.2281\n",
      "Epoch [2/5], Step [3784/10336], Loss: 0.0037\n",
      "Epoch [2/5], Step [3786/10336], Loss: 0.1965\n",
      "Epoch [2/5], Step [3788/10336], Loss: 0.0621\n",
      "Epoch [2/5], Step [3790/10336], Loss: 0.4039\n",
      "Epoch [2/5], Step [3792/10336], Loss: 1.8745\n",
      "Epoch [2/5], Step [3794/10336], Loss: 0.3235\n",
      "Epoch [2/5], Step [3796/10336], Loss: 0.2709\n",
      "Epoch [2/5], Step [3798/10336], Loss: 3.2506\n",
      "Epoch [2/5], Step [3800/10336], Loss: 0.0172\n",
      "Epoch [2/5], Step [3802/10336], Loss: 0.2339\n",
      "Epoch [2/5], Step [3804/10336], Loss: 6.2506\n",
      "Epoch [2/5], Step [3806/10336], Loss: 0.6915\n",
      "Epoch [2/5], Step [3808/10336], Loss: 0.7972\n",
      "Epoch [2/5], Step [3810/10336], Loss: 0.2287\n",
      "Epoch [2/5], Step [3812/10336], Loss: 0.4457\n",
      "Epoch [2/5], Step [3814/10336], Loss: 0.1043\n",
      "Epoch [2/5], Step [3816/10336], Loss: 0.2314\n",
      "Epoch [2/5], Step [3818/10336], Loss: 0.5610\n",
      "Epoch [2/5], Step [3820/10336], Loss: 0.3450\n",
      "Epoch [2/5], Step [3822/10336], Loss: 0.1762\n",
      "Epoch [2/5], Step [3824/10336], Loss: 0.5515\n",
      "Epoch [2/5], Step [3826/10336], Loss: 0.0174\n",
      "Epoch [2/5], Step [3828/10336], Loss: 0.0028\n",
      "Epoch [2/5], Step [3830/10336], Loss: 1.9274\n",
      "Epoch [2/5], Step [3832/10336], Loss: 0.0858\n",
      "Epoch [2/5], Step [3834/10336], Loss: 4.5914\n",
      "Epoch [2/5], Step [3836/10336], Loss: 0.0547\n",
      "Epoch [2/5], Step [3838/10336], Loss: 0.3711\n",
      "Epoch [2/5], Step [3840/10336], Loss: 1.5524\n",
      "Epoch [2/5], Step [3842/10336], Loss: 3.9489\n",
      "Epoch [2/5], Step [3844/10336], Loss: 0.0800\n",
      "Epoch [2/5], Step [3846/10336], Loss: 1.9986\n",
      "Epoch [2/5], Step [3848/10336], Loss: 0.1783\n",
      "Epoch [2/5], Step [3850/10336], Loss: 0.3980\n",
      "Epoch [2/5], Step [3852/10336], Loss: 0.3284\n",
      "Epoch [2/5], Step [3854/10336], Loss: 2.0234\n",
      "Epoch [2/5], Step [3856/10336], Loss: 0.4109\n",
      "Epoch [2/5], Step [3858/10336], Loss: 0.3203\n",
      "Epoch [2/5], Step [3860/10336], Loss: 0.3786\n",
      "Epoch [2/5], Step [3862/10336], Loss: 0.3118\n",
      "Epoch [2/5], Step [3864/10336], Loss: 0.9439\n",
      "Epoch [2/5], Step [3866/10336], Loss: 3.3893\n",
      "Epoch [2/5], Step [3868/10336], Loss: 0.0967\n",
      "Epoch [2/5], Step [3870/10336], Loss: 0.9380\n",
      "Epoch [2/5], Step [3872/10336], Loss: 0.5625\n",
      "Epoch [2/5], Step [3874/10336], Loss: 3.2636\n",
      "Epoch [2/5], Step [3876/10336], Loss: 0.2416\n",
      "Epoch [2/5], Step [3878/10336], Loss: 1.1185\n",
      "Epoch [2/5], Step [3880/10336], Loss: 0.0142\n",
      "Epoch [2/5], Step [3882/10336], Loss: 0.9115\n",
      "Epoch [2/5], Step [3884/10336], Loss: 0.7253\n",
      "Epoch [2/5], Step [3886/10336], Loss: 0.1326\n",
      "Epoch [2/5], Step [3888/10336], Loss: 0.9440\n",
      "Epoch [2/5], Step [3890/10336], Loss: 0.3049\n",
      "Epoch [2/5], Step [3892/10336], Loss: 1.5921\n",
      "Epoch [2/5], Step [3894/10336], Loss: 0.5553\n",
      "Epoch [2/5], Step [3896/10336], Loss: 1.8034\n",
      "Epoch [2/5], Step [3898/10336], Loss: 4.0756\n",
      "Epoch [2/5], Step [3900/10336], Loss: 0.2081\n",
      "Epoch [2/5], Step [3902/10336], Loss: 2.5239\n",
      "Epoch [2/5], Step [3904/10336], Loss: 0.3118\n",
      "Epoch [2/5], Step [3906/10336], Loss: 0.1633\n",
      "Epoch [2/5], Step [3908/10336], Loss: 0.2447\n",
      "Epoch [2/5], Step [3910/10336], Loss: 0.1245\n",
      "Epoch [2/5], Step [3912/10336], Loss: 0.6276\n",
      "Epoch [2/5], Step [3914/10336], Loss: 0.0580\n",
      "Epoch [2/5], Step [3916/10336], Loss: 3.0483\n",
      "Epoch [2/5], Step [3918/10336], Loss: 0.0866\n",
      "Epoch [2/5], Step [3920/10336], Loss: 4.3230\n",
      "Epoch [2/5], Step [3922/10336], Loss: 0.1199\n",
      "Epoch [2/5], Step [3924/10336], Loss: 0.1643\n",
      "Epoch [2/5], Step [3926/10336], Loss: 0.2203\n",
      "Epoch [2/5], Step [3928/10336], Loss: 1.9632\n",
      "Epoch [2/5], Step [3930/10336], Loss: 0.5586\n",
      "Epoch [2/5], Step [3932/10336], Loss: 0.2394\n",
      "Epoch [2/5], Step [3934/10336], Loss: 0.2576\n",
      "Epoch [2/5], Step [3936/10336], Loss: 1.1658\n",
      "Epoch [2/5], Step [3938/10336], Loss: 2.9757\n",
      "Epoch [2/5], Step [3940/10336], Loss: 0.4302\n",
      "Epoch [2/5], Step [3942/10336], Loss: 0.5625\n",
      "Epoch [2/5], Step [3944/10336], Loss: 0.5909\n",
      "Epoch [2/5], Step [3946/10336], Loss: 0.2064\n",
      "Epoch [2/5], Step [3948/10336], Loss: 1.5074\n",
      "Epoch [2/5], Step [3950/10336], Loss: 4.7851\n",
      "Epoch [2/5], Step [3952/10336], Loss: 0.9103\n",
      "Epoch [2/5], Step [3954/10336], Loss: 0.2582\n",
      "Epoch [2/5], Step [3956/10336], Loss: 1.4224\n",
      "Epoch [2/5], Step [3958/10336], Loss: 0.0141\n",
      "Epoch [2/5], Step [3960/10336], Loss: 1.1902\n",
      "Epoch [2/5], Step [3962/10336], Loss: 0.8341\n",
      "Epoch [2/5], Step [3964/10336], Loss: 3.9075\n",
      "Epoch [2/5], Step [3966/10336], Loss: 2.3355\n",
      "Epoch [2/5], Step [3968/10336], Loss: 0.0043\n",
      "Epoch [2/5], Step [3970/10336], Loss: 2.5770\n",
      "Epoch [2/5], Step [3972/10336], Loss: 0.3059\n",
      "Epoch [2/5], Step [3974/10336], Loss: 0.1367\n",
      "Epoch [2/5], Step [3976/10336], Loss: 0.6313\n",
      "Epoch [2/5], Step [3978/10336], Loss: 1.5068\n",
      "Epoch [2/5], Step [3980/10336], Loss: 1.1050\n",
      "Epoch [2/5], Step [3982/10336], Loss: 0.3811\n",
      "Epoch [2/5], Step [3984/10336], Loss: 0.2144\n",
      "Epoch [2/5], Step [3986/10336], Loss: 0.1841\n",
      "Epoch [2/5], Step [3988/10336], Loss: 0.0214\n",
      "Epoch [2/5], Step [3990/10336], Loss: 5.3829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [3992/10336], Loss: 0.0044\n",
      "Epoch [2/5], Step [3994/10336], Loss: 6.0987\n",
      "Epoch [2/5], Step [3996/10336], Loss: 0.1171\n",
      "Epoch [2/5], Step [3998/10336], Loss: 0.3109\n",
      "Epoch [2/5], Step [4000/10336], Loss: 0.5035\n",
      "Epoch [2/5], Step [4002/10336], Loss: 1.1674\n",
      "Epoch [2/5], Step [4004/10336], Loss: 0.8383\n",
      "Epoch [2/5], Step [4006/10336], Loss: 0.0237\n",
      "Epoch [2/5], Step [4008/10336], Loss: 0.2569\n",
      "Epoch [2/5], Step [4010/10336], Loss: 0.2293\n",
      "Epoch [2/5], Step [4012/10336], Loss: 1.7451\n",
      "Epoch [2/5], Step [4014/10336], Loss: 0.1038\n",
      "Epoch [2/5], Step [4016/10336], Loss: 0.0929\n",
      "Epoch [2/5], Step [4018/10336], Loss: 0.2629\n",
      "Epoch [2/5], Step [4020/10336], Loss: 1.0784\n",
      "Epoch [2/5], Step [4022/10336], Loss: 1.5414\n",
      "Epoch [2/5], Step [4024/10336], Loss: 0.5867\n",
      "Epoch [2/5], Step [4026/10336], Loss: 1.0424\n",
      "Epoch [2/5], Step [4028/10336], Loss: 0.5163\n",
      "Epoch [2/5], Step [4030/10336], Loss: 0.5387\n",
      "Epoch [2/5], Step [4032/10336], Loss: 0.2061\n",
      "Epoch [2/5], Step [4034/10336], Loss: 0.2214\n",
      "Epoch [2/5], Step [4036/10336], Loss: 0.3195\n",
      "Epoch [2/5], Step [4038/10336], Loss: 0.0215\n",
      "Epoch [2/5], Step [4040/10336], Loss: 0.1461\n",
      "Epoch [2/5], Step [4042/10336], Loss: 4.0041\n",
      "Epoch [2/5], Step [4044/10336], Loss: 1.9802\n",
      "Epoch [2/5], Step [4046/10336], Loss: 0.0235\n",
      "Epoch [2/5], Step [4048/10336], Loss: 3.2788\n",
      "Epoch [2/5], Step [4050/10336], Loss: 0.5911\n",
      "Epoch [2/5], Step [4052/10336], Loss: 0.0623\n",
      "Epoch [2/5], Step [4054/10336], Loss: 0.1066\n",
      "Epoch [2/5], Step [4056/10336], Loss: 2.6349\n",
      "Epoch [2/5], Step [4058/10336], Loss: 0.4457\n",
      "Epoch [2/5], Step [4060/10336], Loss: 0.7143\n",
      "Epoch [2/5], Step [4062/10336], Loss: 0.0114\n",
      "Epoch [2/5], Step [4064/10336], Loss: 0.0653\n",
      "Epoch [2/5], Step [4066/10336], Loss: 0.2993\n",
      "Epoch [2/5], Step [4068/10336], Loss: 0.0649\n",
      "Epoch [2/5], Step [4070/10336], Loss: 4.0327\n",
      "Epoch [2/5], Step [4072/10336], Loss: 0.0186\n",
      "Epoch [2/5], Step [4074/10336], Loss: 0.3698\n",
      "Epoch [2/5], Step [4076/10336], Loss: 0.3250\n",
      "Epoch [2/5], Step [4078/10336], Loss: 0.0222\n",
      "Epoch [2/5], Step [4080/10336], Loss: 1.5623\n",
      "Epoch [2/5], Step [4082/10336], Loss: 2.9583\n",
      "Epoch [2/5], Step [4084/10336], Loss: 0.3660\n",
      "Epoch [2/5], Step [4086/10336], Loss: 0.4243\n",
      "Epoch [2/5], Step [4088/10336], Loss: 0.0877\n",
      "Epoch [2/5], Step [4090/10336], Loss: 0.0477\n",
      "Epoch [2/5], Step [4092/10336], Loss: 0.5274\n",
      "Epoch [2/5], Step [4094/10336], Loss: 1.2244\n",
      "Epoch [2/5], Step [4096/10336], Loss: 0.1892\n",
      "Epoch [2/5], Step [4098/10336], Loss: 3.4235\n",
      "Epoch [2/5], Step [4100/10336], Loss: 0.1366\n",
      "Epoch [2/5], Step [4102/10336], Loss: 4.1107\n",
      "Epoch [2/5], Step [4104/10336], Loss: 0.8577\n",
      "Epoch [2/5], Step [4106/10336], Loss: 0.0480\n",
      "Epoch [2/5], Step [4108/10336], Loss: 0.0622\n",
      "Epoch [2/5], Step [4110/10336], Loss: 4.2024\n",
      "Epoch [2/5], Step [4112/10336], Loss: 2.6796\n",
      "Epoch [2/5], Step [4114/10336], Loss: 0.4726\n",
      "Epoch [2/5], Step [4116/10336], Loss: 0.5114\n",
      "Epoch [2/5], Step [4118/10336], Loss: 0.7951\n",
      "Epoch [2/5], Step [4120/10336], Loss: 1.3808\n",
      "Epoch [2/5], Step [4122/10336], Loss: 0.3593\n",
      "Epoch [2/5], Step [4124/10336], Loss: 1.0785\n",
      "Epoch [2/5], Step [4126/10336], Loss: 0.1908\n",
      "Epoch [2/5], Step [4128/10336], Loss: 2.0961\n",
      "Epoch [2/5], Step [4130/10336], Loss: 0.0205\n",
      "Epoch [2/5], Step [4132/10336], Loss: 1.1034\n",
      "Epoch [2/5], Step [4134/10336], Loss: 0.2670\n",
      "Epoch [2/5], Step [4136/10336], Loss: 3.9666\n",
      "Epoch [2/5], Step [4138/10336], Loss: 0.0397\n",
      "Epoch [2/5], Step [4140/10336], Loss: 1.6612\n",
      "Epoch [2/5], Step [4142/10336], Loss: 0.2062\n",
      "Epoch [2/5], Step [4144/10336], Loss: 4.5856\n",
      "Epoch [2/5], Step [4146/10336], Loss: 0.7955\n",
      "Epoch [2/5], Step [4148/10336], Loss: 0.5204\n",
      "Epoch [2/5], Step [4150/10336], Loss: 4.2562\n",
      "Epoch [2/5], Step [4152/10336], Loss: 0.3740\n",
      "Epoch [2/5], Step [4154/10336], Loss: 2.1956\n",
      "Epoch [2/5], Step [4156/10336], Loss: 0.1934\n",
      "Epoch [2/5], Step [4158/10336], Loss: 0.4086\n",
      "Epoch [2/5], Step [4160/10336], Loss: 0.1052\n",
      "Epoch [2/5], Step [4162/10336], Loss: 0.2091\n",
      "Epoch [2/5], Step [4164/10336], Loss: 3.6521\n",
      "Epoch [2/5], Step [4166/10336], Loss: 0.7174\n",
      "Epoch [2/5], Step [4168/10336], Loss: 0.0290\n",
      "Epoch [2/5], Step [4170/10336], Loss: 1.1906\n",
      "Epoch [2/5], Step [4172/10336], Loss: 0.4516\n",
      "Epoch [2/5], Step [4174/10336], Loss: 3.1635\n",
      "Epoch [2/5], Step [4176/10336], Loss: 0.0195\n",
      "Epoch [2/5], Step [4178/10336], Loss: 0.2556\n",
      "Epoch [2/5], Step [4180/10336], Loss: 1.6361\n",
      "Epoch [2/5], Step [4182/10336], Loss: 0.1260\n",
      "Epoch [2/5], Step [4184/10336], Loss: 0.0044\n",
      "Epoch [2/5], Step [4186/10336], Loss: 1.2674\n",
      "Epoch [2/5], Step [4188/10336], Loss: 0.3098\n",
      "Epoch [2/5], Step [4190/10336], Loss: 0.7990\n",
      "Epoch [2/5], Step [4192/10336], Loss: 0.0058\n",
      "Epoch [2/5], Step [4194/10336], Loss: 0.1733\n",
      "Epoch [2/5], Step [4196/10336], Loss: 0.1549\n",
      "Epoch [2/5], Step [4198/10336], Loss: 0.0909\n",
      "Epoch [2/5], Step [4200/10336], Loss: 0.0005\n",
      "Epoch [2/5], Step [4202/10336], Loss: 3.1196\n",
      "Epoch [2/5], Step [4204/10336], Loss: 5.4184\n",
      "Epoch [2/5], Step [4206/10336], Loss: 2.2080\n",
      "Epoch [2/5], Step [4208/10336], Loss: 4.4962\n",
      "Epoch [2/5], Step [4210/10336], Loss: 0.3034\n",
      "Epoch [2/5], Step [4212/10336], Loss: 0.0316\n",
      "Epoch [2/5], Step [4214/10336], Loss: 1.6326\n",
      "Epoch [2/5], Step [4216/10336], Loss: 0.0595\n",
      "Epoch [2/5], Step [4218/10336], Loss: 1.0085\n",
      "Epoch [2/5], Step [4220/10336], Loss: 0.0621\n",
      "Epoch [2/5], Step [4222/10336], Loss: 2.4772\n",
      "Epoch [2/5], Step [4224/10336], Loss: 0.1694\n",
      "Epoch [2/5], Step [4226/10336], Loss: 0.1052\n",
      "Epoch [2/5], Step [4228/10336], Loss: 0.3897\n",
      "Epoch [2/5], Step [4230/10336], Loss: 0.0228\n",
      "Epoch [2/5], Step [4232/10336], Loss: 0.5991\n",
      "Epoch [2/5], Step [4234/10336], Loss: 0.3558\n",
      "Epoch [2/5], Step [4236/10336], Loss: 1.2846\n",
      "Epoch [2/5], Step [4238/10336], Loss: 0.0381\n",
      "Epoch [2/5], Step [4240/10336], Loss: 0.2502\n",
      "Epoch [2/5], Step [4242/10336], Loss: 3.8295\n",
      "Epoch [2/5], Step [4244/10336], Loss: 0.8629\n",
      "Epoch [2/5], Step [4246/10336], Loss: 0.3761\n",
      "Epoch [2/5], Step [4248/10336], Loss: 0.0396\n",
      "Epoch [2/5], Step [4250/10336], Loss: 0.3376\n",
      "Epoch [2/5], Step [4252/10336], Loss: 0.2450\n",
      "Epoch [2/5], Step [4254/10336], Loss: 3.1403\n",
      "Epoch [2/5], Step [4256/10336], Loss: 2.7972\n",
      "Epoch [2/5], Step [4258/10336], Loss: 1.2350\n",
      "Epoch [2/5], Step [4260/10336], Loss: 0.5896\n",
      "Epoch [2/5], Step [4262/10336], Loss: 0.2395\n",
      "Epoch [2/5], Step [4264/10336], Loss: 1.6572\n",
      "Epoch [2/5], Step [4266/10336], Loss: 0.3606\n",
      "Epoch [2/5], Step [4268/10336], Loss: 0.0957\n",
      "Epoch [2/5], Step [4270/10336], Loss: 1.2010\n",
      "Epoch [2/5], Step [4272/10336], Loss: 1.7470\n",
      "Epoch [2/5], Step [4274/10336], Loss: 2.5762\n",
      "Epoch [2/5], Step [4276/10336], Loss: 0.8774\n",
      "Epoch [2/5], Step [4278/10336], Loss: 0.9076\n",
      "Epoch [2/5], Step [4280/10336], Loss: 0.8517\n",
      "Epoch [2/5], Step [4282/10336], Loss: 1.8116\n",
      "Epoch [2/5], Step [4284/10336], Loss: 0.2512\n",
      "Epoch [2/5], Step [4286/10336], Loss: 0.1238\n",
      "Epoch [2/5], Step [4288/10336], Loss: 0.2180\n",
      "Epoch [2/5], Step [4290/10336], Loss: 1.5809\n",
      "Epoch [2/5], Step [4292/10336], Loss: 2.7491\n",
      "Epoch [2/5], Step [4294/10336], Loss: 2.5210\n",
      "Epoch [2/5], Step [4296/10336], Loss: 0.5536\n",
      "Epoch [2/5], Step [4298/10336], Loss: 0.9910\n",
      "Epoch [2/5], Step [4300/10336], Loss: 0.3779\n",
      "Epoch [2/5], Step [4302/10336], Loss: 5.0732\n",
      "Epoch [2/5], Step [4304/10336], Loss: 4.6467\n",
      "Epoch [2/5], Step [4306/10336], Loss: 0.1587\n",
      "Epoch [2/5], Step [4308/10336], Loss: 0.0617\n",
      "Epoch [2/5], Step [4310/10336], Loss: 0.1103\n",
      "Epoch [2/5], Step [4312/10336], Loss: 0.7180\n",
      "Epoch [2/5], Step [4314/10336], Loss: 0.4757\n",
      "Epoch [2/5], Step [4316/10336], Loss: 0.3974\n",
      "Epoch [2/5], Step [4318/10336], Loss: 1.6250\n",
      "Epoch [2/5], Step [4320/10336], Loss: 3.0914\n",
      "Epoch [2/5], Step [4322/10336], Loss: 6.1261\n",
      "Epoch [2/5], Step [4324/10336], Loss: 0.0105\n",
      "Epoch [2/5], Step [4326/10336], Loss: 2.7826\n",
      "Epoch [2/5], Step [4328/10336], Loss: 0.6969\n",
      "Epoch [2/5], Step [4330/10336], Loss: 1.0822\n",
      "Epoch [2/5], Step [4332/10336], Loss: 1.6086\n",
      "Epoch [2/5], Step [4334/10336], Loss: 0.1968\n",
      "Epoch [2/5], Step [4336/10336], Loss: 0.2168\n",
      "Epoch [2/5], Step [4338/10336], Loss: 0.6068\n",
      "Epoch [2/5], Step [4340/10336], Loss: 3.0746\n",
      "Epoch [2/5], Step [4342/10336], Loss: 1.6847\n",
      "Epoch [2/5], Step [4344/10336], Loss: 1.6649\n",
      "Epoch [2/5], Step [4346/10336], Loss: 1.0526\n",
      "Epoch [2/5], Step [4348/10336], Loss: 0.3004\n",
      "Epoch [2/5], Step [4350/10336], Loss: 1.4629\n",
      "Epoch [2/5], Step [4352/10336], Loss: 0.5322\n",
      "Epoch [2/5], Step [4354/10336], Loss: 0.7201\n",
      "Epoch [2/5], Step [4356/10336], Loss: 3.9829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [4358/10336], Loss: 0.4924\n",
      "Epoch [2/5], Step [4360/10336], Loss: 0.2173\n",
      "Epoch [2/5], Step [4362/10336], Loss: 1.8169\n",
      "Epoch [2/5], Step [4364/10336], Loss: 0.8714\n",
      "Epoch [2/5], Step [4366/10336], Loss: 2.3178\n",
      "Epoch [2/5], Step [4368/10336], Loss: 1.2338\n",
      "Epoch [2/5], Step [4370/10336], Loss: 4.9088\n",
      "Epoch [2/5], Step [4372/10336], Loss: 2.4070\n",
      "Epoch [2/5], Step [4374/10336], Loss: 0.0460\n",
      "Epoch [2/5], Step [4376/10336], Loss: 3.3062\n",
      "Epoch [2/5], Step [4378/10336], Loss: 0.0474\n",
      "Epoch [2/5], Step [4380/10336], Loss: 2.6204\n",
      "Epoch [2/5], Step [4382/10336], Loss: 2.2626\n",
      "Epoch [2/5], Step [4384/10336], Loss: 3.3679\n",
      "Epoch [2/5], Step [4386/10336], Loss: 0.2083\n",
      "Epoch [2/5], Step [4388/10336], Loss: 0.2317\n",
      "Epoch [2/5], Step [4390/10336], Loss: 3.0823\n",
      "Epoch [2/5], Step [4392/10336], Loss: 0.1445\n",
      "Epoch [2/5], Step [4394/10336], Loss: 0.3147\n",
      "Epoch [2/5], Step [4396/10336], Loss: 0.3082\n",
      "Epoch [2/5], Step [4398/10336], Loss: 2.7000\n",
      "Epoch [2/5], Step [4400/10336], Loss: 0.1725\n",
      "Epoch [2/5], Step [4402/10336], Loss: 0.0428\n",
      "Epoch [2/5], Step [4404/10336], Loss: 2.8608\n",
      "Epoch [2/5], Step [4406/10336], Loss: 0.5926\n",
      "Epoch [2/5], Step [4408/10336], Loss: 1.1965\n",
      "Epoch [2/5], Step [4410/10336], Loss: 0.2875\n",
      "Epoch [2/5], Step [4412/10336], Loss: 0.0355\n",
      "Epoch [2/5], Step [4414/10336], Loss: 0.0121\n",
      "Epoch [2/5], Step [4416/10336], Loss: 0.2191\n",
      "Epoch [2/5], Step [4418/10336], Loss: 0.4531\n",
      "Epoch [2/5], Step [4420/10336], Loss: 4.2564\n",
      "Epoch [2/5], Step [4422/10336], Loss: 0.1470\n",
      "Epoch [2/5], Step [4424/10336], Loss: 1.7954\n",
      "Epoch [2/5], Step [4426/10336], Loss: 0.0095\n",
      "Epoch [2/5], Step [4428/10336], Loss: 0.3373\n",
      "Epoch [2/5], Step [4430/10336], Loss: 0.0174\n",
      "Epoch [2/5], Step [4432/10336], Loss: 0.8579\n",
      "Epoch [2/5], Step [4434/10336], Loss: 0.5431\n",
      "Epoch [2/5], Step [4436/10336], Loss: 0.5424\n",
      "Epoch [2/5], Step [4438/10336], Loss: 2.6933\n",
      "Epoch [2/5], Step [4440/10336], Loss: 4.1114\n",
      "Epoch [2/5], Step [4442/10336], Loss: 2.4168\n",
      "Epoch [2/5], Step [4444/10336], Loss: 1.1134\n",
      "Epoch [2/5], Step [4446/10336], Loss: 0.2802\n",
      "Epoch [2/5], Step [4448/10336], Loss: 0.0837\n",
      "Epoch [2/5], Step [4450/10336], Loss: 0.0100\n",
      "Epoch [2/5], Step [4452/10336], Loss: 3.1586\n",
      "Epoch [2/5], Step [4454/10336], Loss: 0.1382\n",
      "Epoch [2/5], Step [4456/10336], Loss: 0.0243\n",
      "Epoch [2/5], Step [4458/10336], Loss: 0.1813\n",
      "Epoch [2/5], Step [4460/10336], Loss: 0.8295\n",
      "Epoch [2/5], Step [4462/10336], Loss: 0.0333\n",
      "Epoch [2/5], Step [4464/10336], Loss: 0.3736\n",
      "Epoch [2/5], Step [4466/10336], Loss: 0.0043\n",
      "Epoch [2/5], Step [4468/10336], Loss: 0.2889\n",
      "Epoch [2/5], Step [4470/10336], Loss: 2.4894\n",
      "Epoch [2/5], Step [4472/10336], Loss: 2.3061\n",
      "Epoch [2/5], Step [4474/10336], Loss: 2.7342\n",
      "Epoch [2/5], Step [4476/10336], Loss: 0.0082\n",
      "Epoch [2/5], Step [4478/10336], Loss: 1.6300\n",
      "Epoch [2/5], Step [4480/10336], Loss: 4.9123\n",
      "Epoch [2/5], Step [4482/10336], Loss: 0.5596\n",
      "Epoch [2/5], Step [4484/10336], Loss: 0.1834\n",
      "Epoch [2/5], Step [4486/10336], Loss: 0.0200\n",
      "Epoch [2/5], Step [4488/10336], Loss: 0.8059\n",
      "Epoch [2/5], Step [4490/10336], Loss: 1.1753\n",
      "Epoch [2/5], Step [4492/10336], Loss: 0.4833\n",
      "Epoch [2/5], Step [4494/10336], Loss: 0.3485\n",
      "Epoch [2/5], Step [4496/10336], Loss: 2.6399\n",
      "Epoch [2/5], Step [4498/10336], Loss: 0.7843\n",
      "Epoch [2/5], Step [4500/10336], Loss: 1.4552\n",
      "Epoch [2/5], Step [4502/10336], Loss: 2.1065\n",
      "Epoch [2/5], Step [4504/10336], Loss: 0.3671\n",
      "Epoch [2/5], Step [4506/10336], Loss: 0.3172\n",
      "Epoch [2/5], Step [4508/10336], Loss: 0.0160\n",
      "Epoch [2/5], Step [4510/10336], Loss: 0.0069\n",
      "Epoch [2/5], Step [4512/10336], Loss: 0.0260\n",
      "Epoch [2/5], Step [4514/10336], Loss: 3.0398\n",
      "Epoch [2/5], Step [4516/10336], Loss: 0.1561\n",
      "Epoch [2/5], Step [4518/10336], Loss: 5.2160\n",
      "Epoch [2/5], Step [4520/10336], Loss: 1.1822\n",
      "Epoch [2/5], Step [4522/10336], Loss: 0.1349\n",
      "Epoch [2/5], Step [4524/10336], Loss: 0.3071\n",
      "Epoch [2/5], Step [4526/10336], Loss: 0.3655\n",
      "Epoch [2/5], Step [4528/10336], Loss: 0.3428\n",
      "Epoch [2/5], Step [4530/10336], Loss: 1.4869\n",
      "Epoch [2/5], Step [4532/10336], Loss: 1.0405\n",
      "Epoch [2/5], Step [4534/10336], Loss: 0.6765\n",
      "Epoch [2/5], Step [4536/10336], Loss: 1.7307\n",
      "Epoch [2/5], Step [4538/10336], Loss: 0.3656\n",
      "Epoch [2/5], Step [4540/10336], Loss: 1.2618\n",
      "Epoch [2/5], Step [4542/10336], Loss: 0.2928\n",
      "Epoch [2/5], Step [4544/10336], Loss: 1.2195\n",
      "Epoch [2/5], Step [4546/10336], Loss: 2.6487\n",
      "Epoch [2/5], Step [4548/10336], Loss: 1.0310\n",
      "Epoch [2/5], Step [4550/10336], Loss: 0.4809\n",
      "Epoch [2/5], Step [4552/10336], Loss: 0.0031\n",
      "Epoch [2/5], Step [4554/10336], Loss: 0.7021\n",
      "Epoch [2/5], Step [4556/10336], Loss: 0.1933\n",
      "Epoch [2/5], Step [4558/10336], Loss: 0.4171\n",
      "Epoch [2/5], Step [4560/10336], Loss: 0.5389\n",
      "Epoch [2/5], Step [4562/10336], Loss: 0.0228\n",
      "Epoch [2/5], Step [4564/10336], Loss: 0.0041\n",
      "Epoch [2/5], Step [4566/10336], Loss: 4.1310\n",
      "Epoch [2/5], Step [4568/10336], Loss: 0.8607\n",
      "Epoch [2/5], Step [4570/10336], Loss: 0.6412\n",
      "Epoch [2/5], Step [4572/10336], Loss: 0.0015\n",
      "Epoch [2/5], Step [4574/10336], Loss: 2.2645\n",
      "Epoch [2/5], Step [4576/10336], Loss: 4.4951\n",
      "Epoch [2/5], Step [4578/10336], Loss: 0.3372\n",
      "Epoch [2/5], Step [4580/10336], Loss: 0.1805\n",
      "Epoch [2/5], Step [4582/10336], Loss: 0.2652\n",
      "Epoch [2/5], Step [4584/10336], Loss: 0.4498\n",
      "Epoch [2/5], Step [4586/10336], Loss: 0.1149\n",
      "Epoch [2/5], Step [4588/10336], Loss: 0.2867\n",
      "Epoch [2/5], Step [4590/10336], Loss: 0.1193\n",
      "Epoch [2/5], Step [4592/10336], Loss: 2.6652\n",
      "Epoch [2/5], Step [4594/10336], Loss: 0.1724\n",
      "Epoch [2/5], Step [4596/10336], Loss: 0.4124\n",
      "Epoch [2/5], Step [4598/10336], Loss: 2.5829\n",
      "Epoch [2/5], Step [4600/10336], Loss: 0.3525\n",
      "Epoch [2/5], Step [4602/10336], Loss: 1.1451\n",
      "Epoch [2/5], Step [4604/10336], Loss: 4.9877\n",
      "Epoch [2/5], Step [4606/10336], Loss: 1.2683\n",
      "Epoch [2/5], Step [4608/10336], Loss: 0.9571\n",
      "Epoch [2/5], Step [4610/10336], Loss: 0.0590\n",
      "Epoch [2/5], Step [4612/10336], Loss: 2.2399\n",
      "Epoch [2/5], Step [4614/10336], Loss: 1.4084\n",
      "Epoch [2/5], Step [4616/10336], Loss: 0.0248\n",
      "Epoch [2/5], Step [4618/10336], Loss: 4.7152\n",
      "Epoch [2/5], Step [4620/10336], Loss: 2.9133\n",
      "Epoch [2/5], Step [4622/10336], Loss: 3.3058\n",
      "Epoch [2/5], Step [4624/10336], Loss: 0.2928\n",
      "Epoch [2/5], Step [4626/10336], Loss: 0.0683\n",
      "Epoch [2/5], Step [4628/10336], Loss: 1.5708\n",
      "Epoch [2/5], Step [4630/10336], Loss: 0.6108\n",
      "Epoch [2/5], Step [4632/10336], Loss: 0.4669\n",
      "Epoch [2/5], Step [4634/10336], Loss: 0.5001\n",
      "Epoch [2/5], Step [4636/10336], Loss: 0.1150\n",
      "Epoch [2/5], Step [4638/10336], Loss: 0.1514\n",
      "Epoch [2/5], Step [4640/10336], Loss: 0.2790\n",
      "Epoch [2/5], Step [4642/10336], Loss: 1.6451\n",
      "Epoch [2/5], Step [4644/10336], Loss: 0.8212\n",
      "Epoch [2/5], Step [4646/10336], Loss: 0.8614\n",
      "Epoch [2/5], Step [4648/10336], Loss: 2.1783\n",
      "Epoch [2/5], Step [4650/10336], Loss: 2.9260\n",
      "Epoch [2/5], Step [4652/10336], Loss: 1.6066\n",
      "Epoch [2/5], Step [4654/10336], Loss: 4.5332\n",
      "Epoch [2/5], Step [4656/10336], Loss: 0.1076\n",
      "Epoch [2/5], Step [4658/10336], Loss: 4.0233\n",
      "Epoch [2/5], Step [4660/10336], Loss: 0.0080\n",
      "Epoch [2/5], Step [4662/10336], Loss: 0.0463\n",
      "Epoch [2/5], Step [4664/10336], Loss: 0.0215\n",
      "Epoch [2/5], Step [4666/10336], Loss: 0.5455\n",
      "Epoch [2/5], Step [4668/10336], Loss: 1.3274\n",
      "Epoch [2/5], Step [4670/10336], Loss: 1.1460\n",
      "Epoch [2/5], Step [4672/10336], Loss: 0.6091\n",
      "Epoch [2/5], Step [4674/10336], Loss: 0.8483\n",
      "Epoch [2/5], Step [4676/10336], Loss: 0.5304\n",
      "Epoch [2/5], Step [4678/10336], Loss: 2.7031\n",
      "Epoch [2/5], Step [4680/10336], Loss: 0.1053\n",
      "Epoch [2/5], Step [4682/10336], Loss: 1.1206\n",
      "Epoch [2/5], Step [4684/10336], Loss: 0.0321\n",
      "Epoch [2/5], Step [4686/10336], Loss: 0.0092\n",
      "Epoch [2/5], Step [4688/10336], Loss: 0.0498\n",
      "Epoch [2/5], Step [4690/10336], Loss: 0.1069\n",
      "Epoch [2/5], Step [4692/10336], Loss: 0.5803\n",
      "Epoch [2/5], Step [4694/10336], Loss: 0.0676\n",
      "Epoch [2/5], Step [4696/10336], Loss: 2.5189\n",
      "Epoch [2/5], Step [4698/10336], Loss: 1.0038\n",
      "Epoch [2/5], Step [4700/10336], Loss: 1.9123\n",
      "Epoch [2/5], Step [4702/10336], Loss: 0.3439\n",
      "Epoch [2/5], Step [4704/10336], Loss: 3.0127\n",
      "Epoch [2/5], Step [4706/10336], Loss: 0.3878\n",
      "Epoch [2/5], Step [4708/10336], Loss: 1.1699\n",
      "Epoch [2/5], Step [4710/10336], Loss: 0.0116\n",
      "Epoch [2/5], Step [4712/10336], Loss: 0.3176\n",
      "Epoch [2/5], Step [4714/10336], Loss: 0.3464\n",
      "Epoch [2/5], Step [4716/10336], Loss: 0.4684\n",
      "Epoch [2/5], Step [4718/10336], Loss: 0.2946\n",
      "Epoch [2/5], Step [4720/10336], Loss: 1.7701\n",
      "Epoch [2/5], Step [4722/10336], Loss: 0.2449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [4724/10336], Loss: 0.3976\n",
      "Epoch [2/5], Step [4726/10336], Loss: 2.5441\n",
      "Epoch [2/5], Step [4728/10336], Loss: 0.1289\n",
      "Epoch [2/5], Step [4730/10336], Loss: 0.1323\n",
      "Epoch [2/5], Step [4732/10336], Loss: 0.0940\n",
      "Epoch [2/5], Step [4734/10336], Loss: 0.0139\n",
      "Epoch [2/5], Step [4736/10336], Loss: 0.2906\n",
      "Epoch [2/5], Step [4738/10336], Loss: 0.4061\n",
      "Epoch [2/5], Step [4740/10336], Loss: 0.4684\n",
      "Epoch [2/5], Step [4742/10336], Loss: 0.2419\n",
      "Epoch [2/5], Step [4744/10336], Loss: 0.8938\n",
      "Epoch [2/5], Step [4746/10336], Loss: 0.0333\n",
      "Epoch [2/5], Step [4748/10336], Loss: 0.0029\n",
      "Epoch [2/5], Step [4750/10336], Loss: 0.0335\n",
      "Epoch [2/5], Step [4752/10336], Loss: 0.3284\n",
      "Epoch [2/5], Step [4754/10336], Loss: 0.3540\n",
      "Epoch [2/5], Step [4756/10336], Loss: 3.4040\n",
      "Epoch [2/5], Step [4758/10336], Loss: 0.4325\n",
      "Epoch [2/5], Step [4760/10336], Loss: 0.0793\n",
      "Epoch [2/5], Step [4762/10336], Loss: 0.0806\n",
      "Epoch [2/5], Step [4764/10336], Loss: 0.0671\n",
      "Epoch [2/5], Step [4766/10336], Loss: 0.1344\n",
      "Epoch [2/5], Step [4768/10336], Loss: 5.4540\n",
      "Epoch [2/5], Step [4770/10336], Loss: 1.3248\n",
      "Epoch [2/5], Step [4772/10336], Loss: 0.2794\n",
      "Epoch [2/5], Step [4774/10336], Loss: 0.0299\n",
      "Epoch [2/5], Step [4776/10336], Loss: 0.0793\n",
      "Epoch [2/5], Step [4778/10336], Loss: 0.0291\n",
      "Epoch [2/5], Step [4780/10336], Loss: 3.3024\n",
      "Epoch [2/5], Step [4782/10336], Loss: 0.2706\n",
      "Epoch [2/5], Step [4784/10336], Loss: 0.3627\n",
      "Epoch [2/5], Step [4786/10336], Loss: 0.0805\n",
      "Epoch [2/5], Step [4788/10336], Loss: 0.1021\n",
      "Epoch [2/5], Step [4790/10336], Loss: 1.1492\n",
      "Epoch [2/5], Step [4792/10336], Loss: 0.5055\n",
      "Epoch [2/5], Step [4794/10336], Loss: 0.4348\n",
      "Epoch [2/5], Step [4796/10336], Loss: 1.6026\n",
      "Epoch [2/5], Step [4798/10336], Loss: 0.1875\n",
      "Epoch [2/5], Step [4800/10336], Loss: 0.4640\n",
      "Epoch [2/5], Step [4802/10336], Loss: 0.1010\n",
      "Epoch [2/5], Step [4804/10336], Loss: 0.5755\n",
      "Epoch [2/5], Step [4806/10336], Loss: 0.0587\n",
      "Epoch [2/5], Step [4808/10336], Loss: 1.1900\n",
      "Epoch [2/5], Step [4810/10336], Loss: 0.2808\n",
      "Epoch [2/5], Step [4812/10336], Loss: 0.0243\n",
      "Epoch [2/5], Step [4814/10336], Loss: 0.2355\n",
      "Epoch [2/5], Step [4816/10336], Loss: 0.0040\n",
      "Epoch [2/5], Step [4818/10336], Loss: 3.3251\n",
      "Epoch [2/5], Step [4820/10336], Loss: 4.7564\n",
      "Epoch [2/5], Step [4822/10336], Loss: 2.1934\n",
      "Epoch [2/5], Step [4824/10336], Loss: 0.3749\n",
      "Epoch [2/5], Step [4826/10336], Loss: 0.3452\n",
      "Epoch [2/5], Step [4828/10336], Loss: 0.2970\n",
      "Epoch [2/5], Step [4830/10336], Loss: 0.1073\n",
      "Epoch [2/5], Step [4832/10336], Loss: 3.3335\n",
      "Epoch [2/5], Step [4834/10336], Loss: 0.3840\n",
      "Epoch [2/5], Step [4836/10336], Loss: 0.8964\n",
      "Epoch [2/5], Step [4838/10336], Loss: 0.0651\n",
      "Epoch [2/5], Step [4840/10336], Loss: 0.2663\n",
      "Epoch [2/5], Step [4842/10336], Loss: 0.2541\n",
      "Epoch [2/5], Step [4844/10336], Loss: 1.0878\n",
      "Epoch [2/5], Step [4846/10336], Loss: 0.2133\n",
      "Epoch [2/5], Step [4848/10336], Loss: 2.8021\n",
      "Epoch [2/5], Step [4850/10336], Loss: 0.1015\n",
      "Epoch [2/5], Step [4852/10336], Loss: 0.3256\n",
      "Epoch [2/5], Step [4854/10336], Loss: 0.0169\n",
      "Epoch [2/5], Step [4856/10336], Loss: 1.0115\n",
      "Epoch [2/5], Step [4858/10336], Loss: 1.0456\n",
      "Epoch [2/5], Step [4860/10336], Loss: 0.1188\n",
      "Epoch [2/5], Step [4862/10336], Loss: 1.9021\n",
      "Epoch [2/5], Step [4864/10336], Loss: 0.0299\n",
      "Epoch [2/5], Step [4866/10336], Loss: 0.4099\n",
      "Epoch [2/5], Step [4868/10336], Loss: 0.4448\n",
      "Epoch [2/5], Step [4870/10336], Loss: 0.0701\n",
      "Epoch [2/5], Step [4872/10336], Loss: 0.2281\n",
      "Epoch [2/5], Step [4874/10336], Loss: 2.5417\n",
      "Epoch [2/5], Step [4876/10336], Loss: 0.4099\n",
      "Epoch [2/5], Step [4878/10336], Loss: 1.0957\n",
      "Epoch [2/5], Step [4880/10336], Loss: 0.0081\n",
      "Epoch [2/5], Step [4882/10336], Loss: 2.8052\n",
      "Epoch [2/5], Step [4884/10336], Loss: 0.0121\n",
      "Epoch [2/5], Step [4886/10336], Loss: 0.7656\n",
      "Epoch [2/5], Step [4888/10336], Loss: 0.3882\n",
      "Epoch [2/5], Step [4890/10336], Loss: 0.2028\n",
      "Epoch [2/5], Step [4892/10336], Loss: 1.3027\n",
      "Epoch [2/5], Step [4894/10336], Loss: 0.8502\n",
      "Epoch [2/5], Step [4896/10336], Loss: 0.0331\n",
      "Epoch [2/5], Step [4898/10336], Loss: 0.0210\n",
      "Epoch [2/5], Step [4900/10336], Loss: 0.1199\n",
      "Epoch [2/5], Step [4902/10336], Loss: 0.3393\n",
      "Epoch [2/5], Step [4904/10336], Loss: 3.9109\n",
      "Epoch [2/5], Step [4906/10336], Loss: 0.0148\n",
      "Epoch [2/5], Step [4908/10336], Loss: 0.1775\n",
      "Epoch [2/5], Step [4910/10336], Loss: 4.9041\n",
      "Epoch [2/5], Step [4912/10336], Loss: 0.8775\n",
      "Epoch [2/5], Step [4914/10336], Loss: 0.0120\n",
      "Epoch [2/5], Step [4916/10336], Loss: 0.0067\n",
      "Epoch [2/5], Step [4918/10336], Loss: 0.6245\n",
      "Epoch [2/5], Step [4920/10336], Loss: 0.0847\n",
      "Epoch [2/5], Step [4922/10336], Loss: 0.2968\n",
      "Epoch [2/5], Step [4924/10336], Loss: 0.3400\n",
      "Epoch [2/5], Step [4926/10336], Loss: 2.8506\n",
      "Epoch [2/5], Step [4928/10336], Loss: 0.2995\n",
      "Epoch [2/5], Step [4930/10336], Loss: 1.0895\n",
      "Epoch [2/5], Step [4932/10336], Loss: 0.0111\n",
      "Epoch [2/5], Step [4934/10336], Loss: 0.0161\n",
      "Epoch [2/5], Step [4936/10336], Loss: 3.7937\n",
      "Epoch [2/5], Step [4938/10336], Loss: 0.4690\n",
      "Epoch [2/5], Step [4940/10336], Loss: 4.4772\n",
      "Epoch [2/5], Step [4942/10336], Loss: 1.2690\n",
      "Epoch [2/5], Step [4944/10336], Loss: 0.0287\n",
      "Epoch [2/5], Step [4946/10336], Loss: 0.3508\n",
      "Epoch [2/5], Step [4948/10336], Loss: 0.5413\n",
      "Epoch [2/5], Step [4950/10336], Loss: 0.0798\n",
      "Epoch [2/5], Step [4952/10336], Loss: 1.0248\n",
      "Epoch [2/5], Step [4954/10336], Loss: 0.0758\n",
      "Epoch [2/5], Step [4956/10336], Loss: 1.7570\n",
      "Epoch [2/5], Step [4958/10336], Loss: 2.4433\n",
      "Epoch [2/5], Step [4960/10336], Loss: 0.2857\n",
      "Epoch [2/5], Step [4962/10336], Loss: 0.4416\n",
      "Epoch [2/5], Step [4964/10336], Loss: 0.7766\n",
      "Epoch [2/5], Step [4966/10336], Loss: 0.0131\n",
      "Epoch [2/5], Step [4968/10336], Loss: 3.8771\n",
      "Epoch [2/5], Step [4970/10336], Loss: 0.3252\n",
      "Epoch [2/5], Step [4972/10336], Loss: 0.2233\n",
      "Epoch [2/5], Step [4974/10336], Loss: 0.2660\n",
      "Epoch [2/5], Step [4976/10336], Loss: 0.7150\n",
      "Epoch [2/5], Step [4978/10336], Loss: 3.2181\n",
      "Epoch [2/5], Step [4980/10336], Loss: 0.0091\n",
      "Epoch [2/5], Step [4982/10336], Loss: 1.8904\n",
      "Epoch [2/5], Step [4984/10336], Loss: 0.0105\n",
      "Epoch [2/5], Step [4986/10336], Loss: 0.0278\n",
      "Epoch [2/5], Step [4988/10336], Loss: 0.1524\n",
      "Epoch [2/5], Step [4990/10336], Loss: 0.3554\n",
      "Epoch [2/5], Step [4992/10336], Loss: 2.1650\n",
      "Epoch [2/5], Step [4994/10336], Loss: 0.0678\n",
      "Epoch [2/5], Step [4996/10336], Loss: 0.3502\n",
      "Epoch [2/5], Step [4998/10336], Loss: 0.0106\n",
      "Epoch [2/5], Step [5000/10336], Loss: 4.3860\n",
      "Epoch [2/5], Step [5002/10336], Loss: 0.6381\n",
      "Epoch [2/5], Step [5004/10336], Loss: 1.3659\n",
      "Epoch [2/5], Step [5006/10336], Loss: 1.3039\n",
      "Epoch [2/5], Step [5008/10336], Loss: 0.2763\n",
      "Epoch [2/5], Step [5010/10336], Loss: 0.2228\n",
      "Epoch [2/5], Step [5012/10336], Loss: 0.3851\n",
      "Epoch [2/5], Step [5014/10336], Loss: 0.4844\n",
      "Epoch [2/5], Step [5016/10336], Loss: 0.3621\n",
      "Epoch [2/5], Step [5018/10336], Loss: 0.7088\n",
      "Epoch [2/5], Step [5020/10336], Loss: 0.1453\n",
      "Epoch [2/5], Step [5022/10336], Loss: 0.2147\n",
      "Epoch [2/5], Step [5024/10336], Loss: 0.0174\n",
      "Epoch [2/5], Step [5026/10336], Loss: 0.1599\n",
      "Epoch [2/5], Step [5028/10336], Loss: 0.0423\n",
      "Epoch [2/5], Step [5030/10336], Loss: 0.1969\n",
      "Epoch [2/5], Step [5032/10336], Loss: 2.3792\n",
      "Epoch [2/5], Step [5034/10336], Loss: 0.2206\n",
      "Epoch [2/5], Step [5036/10336], Loss: 0.1982\n",
      "Epoch [2/5], Step [5038/10336], Loss: 0.1958\n",
      "Epoch [2/5], Step [5040/10336], Loss: 1.7270\n",
      "Epoch [2/5], Step [5042/10336], Loss: 0.0797\n",
      "Epoch [2/5], Step [5044/10336], Loss: 0.0363\n",
      "Epoch [2/5], Step [5046/10336], Loss: 0.5819\n",
      "Epoch [2/5], Step [5048/10336], Loss: 0.2818\n",
      "Epoch [2/5], Step [5050/10336], Loss: 0.1167\n",
      "Epoch [2/5], Step [5052/10336], Loss: 0.5711\n",
      "Epoch [2/5], Step [5054/10336], Loss: 0.0475\n",
      "Epoch [2/5], Step [5056/10336], Loss: 0.6786\n",
      "Epoch [2/5], Step [5058/10336], Loss: 0.5449\n",
      "Epoch [2/5], Step [5060/10336], Loss: 1.8523\n",
      "Epoch [2/5], Step [5062/10336], Loss: 1.6639\n",
      "Epoch [2/5], Step [5064/10336], Loss: 0.2881\n",
      "Epoch [2/5], Step [5066/10336], Loss: 0.5808\n",
      "Epoch [2/5], Step [5068/10336], Loss: 1.3177\n",
      "Epoch [2/5], Step [5070/10336], Loss: 4.3816\n",
      "Epoch [2/5], Step [5072/10336], Loss: 0.1157\n",
      "Epoch [2/5], Step [5074/10336], Loss: 4.1405\n",
      "Epoch [2/5], Step [5076/10336], Loss: 0.2243\n",
      "Epoch [2/5], Step [5078/10336], Loss: 0.0028\n",
      "Epoch [2/5], Step [5080/10336], Loss: 0.3435\n",
      "Epoch [2/5], Step [5082/10336], Loss: 0.3592\n",
      "Epoch [2/5], Step [5084/10336], Loss: 0.2059\n",
      "Epoch [2/5], Step [5086/10336], Loss: 0.0976\n",
      "Epoch [2/5], Step [5088/10336], Loss: 0.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [5090/10336], Loss: 0.1707\n",
      "Epoch [2/5], Step [5092/10336], Loss: 4.5929\n",
      "Epoch [2/5], Step [5094/10336], Loss: 1.0633\n",
      "Epoch [2/5], Step [5096/10336], Loss: 0.3966\n",
      "Epoch [2/5], Step [5098/10336], Loss: 1.9900\n",
      "Epoch [2/5], Step [5100/10336], Loss: 2.2319\n",
      "Epoch [2/5], Step [5102/10336], Loss: 3.5022\n",
      "Epoch [2/5], Step [5104/10336], Loss: 2.5276\n",
      "Epoch [2/5], Step [5106/10336], Loss: 0.3972\n",
      "Epoch [2/5], Step [5108/10336], Loss: 2.5921\n",
      "Epoch [2/5], Step [5110/10336], Loss: 0.3299\n",
      "Epoch [2/5], Step [5112/10336], Loss: 0.6340\n",
      "Epoch [2/5], Step [5114/10336], Loss: 0.2183\n",
      "Epoch [2/5], Step [5116/10336], Loss: 0.7401\n",
      "Epoch [2/5], Step [5118/10336], Loss: 1.5569\n",
      "Epoch [2/5], Step [5120/10336], Loss: 0.2048\n",
      "Epoch [2/5], Step [5122/10336], Loss: 1.1705\n",
      "Epoch [2/5], Step [5124/10336], Loss: 1.5784\n",
      "Epoch [2/5], Step [5126/10336], Loss: 4.0894\n",
      "Epoch [2/5], Step [5128/10336], Loss: 1.1218\n",
      "Epoch [2/5], Step [5130/10336], Loss: 0.0264\n",
      "Epoch [2/5], Step [5132/10336], Loss: 0.2553\n",
      "Epoch [2/5], Step [5134/10336], Loss: 1.1654\n",
      "Epoch [2/5], Step [5136/10336], Loss: 1.9093\n",
      "Epoch [2/5], Step [5138/10336], Loss: 0.4402\n",
      "Epoch [2/5], Step [5140/10336], Loss: 0.0112\n",
      "Epoch [2/5], Step [5142/10336], Loss: 0.3803\n",
      "Epoch [2/5], Step [5144/10336], Loss: 0.3614\n",
      "Epoch [2/5], Step [5146/10336], Loss: 0.3378\n",
      "Epoch [2/5], Step [5148/10336], Loss: 0.2061\n",
      "Epoch [2/5], Step [5150/10336], Loss: 0.0502\n",
      "Epoch [2/5], Step [5152/10336], Loss: 0.0454\n",
      "Epoch [2/5], Step [5154/10336], Loss: 0.1048\n",
      "Epoch [2/5], Step [5156/10336], Loss: 0.0770\n",
      "Epoch [2/5], Step [5158/10336], Loss: 1.4387\n",
      "Epoch [2/5], Step [5160/10336], Loss: 0.3120\n",
      "Epoch [2/5], Step [5162/10336], Loss: 0.3479\n",
      "Epoch [2/5], Step [5164/10336], Loss: 0.3014\n",
      "Epoch [2/5], Step [5166/10336], Loss: 0.3577\n",
      "Epoch [2/5], Step [5168/10336], Loss: 0.2070\n",
      "Epoch [2/5], Step [5170/10336], Loss: 3.4360\n",
      "Epoch [2/5], Step [5172/10336], Loss: 0.6595\n",
      "Epoch [2/5], Step [5174/10336], Loss: 1.5060\n",
      "Epoch [2/5], Step [5176/10336], Loss: 0.1565\n",
      "Epoch [2/5], Step [5178/10336], Loss: 0.6767\n",
      "Epoch [2/5], Step [5180/10336], Loss: 1.1148\n",
      "Epoch [2/5], Step [5182/10336], Loss: 3.4601\n",
      "Epoch [2/5], Step [5184/10336], Loss: 0.5584\n",
      "Epoch [2/5], Step [5186/10336], Loss: 0.0723\n",
      "Epoch [2/5], Step [5188/10336], Loss: 3.9035\n",
      "Epoch [2/5], Step [5190/10336], Loss: 2.6963\n",
      "Epoch [2/5], Step [5192/10336], Loss: 0.8501\n",
      "Epoch [2/5], Step [5194/10336], Loss: 2.0831\n",
      "Epoch [2/5], Step [5196/10336], Loss: 0.3298\n",
      "Epoch [2/5], Step [5198/10336], Loss: 0.0066\n",
      "Epoch [2/5], Step [5200/10336], Loss: 0.2353\n",
      "Epoch [2/5], Step [5202/10336], Loss: 0.7403\n",
      "Epoch [2/5], Step [5204/10336], Loss: 1.2007\n",
      "Epoch [2/5], Step [5206/10336], Loss: 0.3519\n",
      "Epoch [2/5], Step [5208/10336], Loss: 0.9354\n",
      "Epoch [2/5], Step [5210/10336], Loss: 0.4177\n",
      "Epoch [2/5], Step [5212/10336], Loss: 0.1610\n",
      "Epoch [2/5], Step [5214/10336], Loss: 0.2089\n",
      "Epoch [2/5], Step [5216/10336], Loss: 4.3782\n",
      "Epoch [2/5], Step [5218/10336], Loss: 1.1725\n",
      "Epoch [2/5], Step [5220/10336], Loss: 0.6255\n",
      "Epoch [2/5], Step [5222/10336], Loss: 0.3461\n",
      "Epoch [2/5], Step [5224/10336], Loss: 3.4877\n",
      "Epoch [2/5], Step [5226/10336], Loss: 0.4432\n",
      "Epoch [2/5], Step [5228/10336], Loss: 0.6530\n",
      "Epoch [2/5], Step [5230/10336], Loss: 0.0933\n",
      "Epoch [2/5], Step [5232/10336], Loss: 0.2218\n",
      "Epoch [2/5], Step [5234/10336], Loss: 0.8372\n",
      "Epoch [2/5], Step [5236/10336], Loss: 0.2951\n",
      "Epoch [2/5], Step [5238/10336], Loss: 1.4597\n",
      "Epoch [2/5], Step [5240/10336], Loss: 0.0098\n",
      "Epoch [2/5], Step [5242/10336], Loss: 0.0417\n",
      "Epoch [2/5], Step [5244/10336], Loss: 1.4301\n",
      "Epoch [2/5], Step [5246/10336], Loss: 0.3687\n",
      "Epoch [2/5], Step [5248/10336], Loss: 0.4036\n",
      "Epoch [2/5], Step [5250/10336], Loss: 0.0184\n",
      "Epoch [2/5], Step [5252/10336], Loss: 0.0217\n",
      "Epoch [2/5], Step [5254/10336], Loss: 0.0265\n",
      "Epoch [2/5], Step [5256/10336], Loss: 0.0626\n",
      "Epoch [2/5], Step [5258/10336], Loss: 4.7942\n",
      "Epoch [2/5], Step [5260/10336], Loss: 0.0284\n",
      "Epoch [2/5], Step [5262/10336], Loss: 0.2570\n",
      "Epoch [2/5], Step [5264/10336], Loss: 0.2294\n",
      "Epoch [2/5], Step [5266/10336], Loss: 0.2789\n",
      "Epoch [2/5], Step [5268/10336], Loss: 0.2888\n",
      "Epoch [2/5], Step [5270/10336], Loss: 0.7173\n",
      "Epoch [2/5], Step [5272/10336], Loss: 0.0950\n",
      "Epoch [2/5], Step [5274/10336], Loss: 0.8378\n",
      "Epoch [2/5], Step [5276/10336], Loss: 0.0002\n",
      "Epoch [2/5], Step [5278/10336], Loss: 4.3152\n",
      "Epoch [2/5], Step [5280/10336], Loss: 1.5406\n",
      "Epoch [2/5], Step [5282/10336], Loss: 0.7745\n",
      "Epoch [2/5], Step [5284/10336], Loss: 0.1523\n",
      "Epoch [2/5], Step [5286/10336], Loss: 0.2660\n",
      "Epoch [2/5], Step [5288/10336], Loss: 0.0119\n",
      "Epoch [2/5], Step [5290/10336], Loss: 4.1544\n",
      "Epoch [2/5], Step [5292/10336], Loss: 1.9035\n",
      "Epoch [2/5], Step [5294/10336], Loss: 0.3742\n",
      "Epoch [2/5], Step [5296/10336], Loss: 4.0320\n",
      "Epoch [2/5], Step [5298/10336], Loss: 0.0084\n",
      "Epoch [2/5], Step [5300/10336], Loss: 0.6091\n",
      "Epoch [2/5], Step [5302/10336], Loss: 0.0550\n",
      "Epoch [2/5], Step [5304/10336], Loss: 0.7553\n",
      "Epoch [2/5], Step [5306/10336], Loss: 0.6484\n",
      "Epoch [2/5], Step [5308/10336], Loss: 0.6414\n",
      "Epoch [2/5], Step [5310/10336], Loss: 4.2815\n",
      "Epoch [2/5], Step [5312/10336], Loss: 0.2441\n",
      "Epoch [2/5], Step [5314/10336], Loss: 3.6195\n",
      "Epoch [2/5], Step [5316/10336], Loss: 2.8044\n",
      "Epoch [2/5], Step [5318/10336], Loss: 0.2015\n",
      "Epoch [2/5], Step [5320/10336], Loss: 0.2500\n",
      "Epoch [2/5], Step [5322/10336], Loss: 0.0905\n",
      "Epoch [2/5], Step [5324/10336], Loss: 1.3841\n",
      "Epoch [2/5], Step [5326/10336], Loss: 1.9040\n",
      "Epoch [2/5], Step [5328/10336], Loss: 0.0359\n",
      "Epoch [2/5], Step [5330/10336], Loss: 0.6729\n",
      "Epoch [2/5], Step [5332/10336], Loss: 0.1341\n",
      "Epoch [2/5], Step [5334/10336], Loss: 3.4678\n",
      "Epoch [2/5], Step [5336/10336], Loss: 0.0659\n",
      "Epoch [2/5], Step [5338/10336], Loss: 0.9711\n",
      "Epoch [2/5], Step [5340/10336], Loss: 0.9092\n",
      "Epoch [2/5], Step [5342/10336], Loss: 2.7875\n",
      "Epoch [2/5], Step [5344/10336], Loss: 1.2898\n",
      "Epoch [2/5], Step [5346/10336], Loss: 3.6120\n",
      "Epoch [2/5], Step [5348/10336], Loss: 0.0969\n",
      "Epoch [2/5], Step [5350/10336], Loss: 0.4215\n",
      "Epoch [2/5], Step [5352/10336], Loss: 0.3860\n",
      "Epoch [2/5], Step [5354/10336], Loss: 0.3234\n",
      "Epoch [2/5], Step [5356/10336], Loss: 1.7175\n",
      "Epoch [2/5], Step [5358/10336], Loss: 0.9346\n",
      "Epoch [2/5], Step [5360/10336], Loss: 0.2858\n",
      "Epoch [2/5], Step [5362/10336], Loss: 0.3609\n",
      "Epoch [2/5], Step [5364/10336], Loss: 0.0129\n",
      "Epoch [2/5], Step [5366/10336], Loss: 0.1053\n",
      "Epoch [2/5], Step [5368/10336], Loss: 1.5274\n",
      "Epoch [2/5], Step [5370/10336], Loss: 0.0025\n",
      "Epoch [2/5], Step [5372/10336], Loss: 1.3658\n",
      "Epoch [2/5], Step [5374/10336], Loss: 0.0659\n",
      "Epoch [2/5], Step [5376/10336], Loss: 1.0749\n",
      "Epoch [2/5], Step [5378/10336], Loss: 0.0016\n",
      "Epoch [2/5], Step [5380/10336], Loss: 3.1687\n",
      "Epoch [2/5], Step [5382/10336], Loss: 0.2338\n",
      "Epoch [2/5], Step [5384/10336], Loss: 1.5441\n",
      "Epoch [2/5], Step [5386/10336], Loss: 2.4220\n",
      "Epoch [2/5], Step [5388/10336], Loss: 0.0013\n",
      "Epoch [2/5], Step [5390/10336], Loss: 0.1470\n",
      "Epoch [2/5], Step [5392/10336], Loss: 1.9344\n",
      "Epoch [2/5], Step [5394/10336], Loss: 0.6350\n",
      "Epoch [2/5], Step [5396/10336], Loss: 0.3335\n",
      "Epoch [2/5], Step [5398/10336], Loss: 0.9690\n",
      "Epoch [2/5], Step [5400/10336], Loss: 0.3219\n",
      "Epoch [2/5], Step [5402/10336], Loss: 0.0017\n",
      "Epoch [2/5], Step [5404/10336], Loss: 2.7735\n",
      "Epoch [2/5], Step [5406/10336], Loss: 0.1375\n",
      "Epoch [2/5], Step [5408/10336], Loss: 0.3591\n",
      "Epoch [2/5], Step [5410/10336], Loss: 0.1651\n",
      "Epoch [2/5], Step [5412/10336], Loss: 0.0472\n",
      "Epoch [2/5], Step [5414/10336], Loss: 1.3288\n",
      "Epoch [2/5], Step [5416/10336], Loss: 2.2716\n",
      "Epoch [2/5], Step [5418/10336], Loss: 1.9684\n",
      "Epoch [2/5], Step [5420/10336], Loss: 0.9517\n",
      "Epoch [2/5], Step [5422/10336], Loss: 0.1520\n",
      "Epoch [2/5], Step [5424/10336], Loss: 0.5780\n",
      "Epoch [2/5], Step [5426/10336], Loss: 0.8361\n",
      "Epoch [2/5], Step [5428/10336], Loss: 0.2972\n",
      "Epoch [2/5], Step [5430/10336], Loss: 2.9412\n",
      "Epoch [2/5], Step [5432/10336], Loss: 0.1545\n",
      "Epoch [2/5], Step [5434/10336], Loss: 0.0113\n",
      "Epoch [2/5], Step [5436/10336], Loss: 0.1383\n",
      "Epoch [2/5], Step [5438/10336], Loss: 0.0995\n",
      "Epoch [2/5], Step [5440/10336], Loss: 0.2879\n",
      "Epoch [2/5], Step [5442/10336], Loss: 1.9731\n",
      "Epoch [2/5], Step [5444/10336], Loss: 0.3653\n",
      "Epoch [2/5], Step [5446/10336], Loss: 0.0232\n",
      "Epoch [2/5], Step [5448/10336], Loss: 0.0594\n",
      "Epoch [2/5], Step [5450/10336], Loss: 1.1404\n",
      "Epoch [2/5], Step [5452/10336], Loss: 2.9495\n",
      "Epoch [2/5], Step [5454/10336], Loss: 0.8592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [5456/10336], Loss: 0.0294\n",
      "Epoch [2/5], Step [5458/10336], Loss: 0.5288\n",
      "Epoch [2/5], Step [5460/10336], Loss: 0.0399\n",
      "Epoch [2/5], Step [5462/10336], Loss: 0.0578\n",
      "Epoch [2/5], Step [5464/10336], Loss: 5.5519\n",
      "Epoch [2/5], Step [5466/10336], Loss: 0.2341\n",
      "Epoch [2/5], Step [5468/10336], Loss: 0.0031\n",
      "Epoch [2/5], Step [5470/10336], Loss: 0.0118\n",
      "Epoch [2/5], Step [5472/10336], Loss: 1.6714\n",
      "Epoch [2/5], Step [5474/10336], Loss: 0.0759\n",
      "Epoch [2/5], Step [5476/10336], Loss: 0.2549\n",
      "Epoch [2/5], Step [5478/10336], Loss: 0.2637\n",
      "Epoch [2/5], Step [5480/10336], Loss: 1.3897\n",
      "Epoch [2/5], Step [5482/10336], Loss: 0.6107\n",
      "Epoch [2/5], Step [5484/10336], Loss: 2.0073\n",
      "Epoch [2/5], Step [5486/10336], Loss: 0.1319\n",
      "Epoch [2/5], Step [5488/10336], Loss: 0.0075\n",
      "Epoch [2/5], Step [5490/10336], Loss: 0.1208\n",
      "Epoch [2/5], Step [5492/10336], Loss: 0.8652\n",
      "Epoch [2/5], Step [5494/10336], Loss: 3.9614\n",
      "Epoch [2/5], Step [5496/10336], Loss: 0.2891\n",
      "Epoch [2/5], Step [5498/10336], Loss: 0.4890\n",
      "Epoch [2/5], Step [5500/10336], Loss: 0.1008\n",
      "Epoch [2/5], Step [5502/10336], Loss: 0.0038\n",
      "Epoch [2/5], Step [5504/10336], Loss: 0.2425\n",
      "Epoch [2/5], Step [5506/10336], Loss: 1.6227\n",
      "Epoch [2/5], Step [5508/10336], Loss: 0.2731\n",
      "Epoch [2/5], Step [5510/10336], Loss: 1.9973\n",
      "Epoch [2/5], Step [5512/10336], Loss: 0.1167\n",
      "Epoch [2/5], Step [5514/10336], Loss: 0.0007\n",
      "Epoch [2/5], Step [5516/10336], Loss: 0.0109\n",
      "Epoch [2/5], Step [5518/10336], Loss: 0.0636\n",
      "Epoch [2/5], Step [5520/10336], Loss: 0.4507\n",
      "Epoch [2/5], Step [5522/10336], Loss: 0.3237\n",
      "Epoch [2/5], Step [5524/10336], Loss: 0.2178\n",
      "Epoch [2/5], Step [5526/10336], Loss: 0.2411\n",
      "Epoch [2/5], Step [5528/10336], Loss: 0.6439\n",
      "Epoch [2/5], Step [5530/10336], Loss: 0.0078\n",
      "Epoch [2/5], Step [5532/10336], Loss: 0.3544\n",
      "Epoch [2/5], Step [5534/10336], Loss: 2.8256\n",
      "Epoch [2/5], Step [5536/10336], Loss: 0.2470\n",
      "Epoch [2/5], Step [5538/10336], Loss: 3.5915\n",
      "Epoch [2/5], Step [5540/10336], Loss: 0.2410\n",
      "Epoch [2/5], Step [5542/10336], Loss: 0.8588\n",
      "Epoch [2/5], Step [5544/10336], Loss: 0.2384\n",
      "Epoch [2/5], Step [5546/10336], Loss: 0.1603\n",
      "Epoch [2/5], Step [5548/10336], Loss: 0.4961\n",
      "Epoch [2/5], Step [5550/10336], Loss: 1.3140\n",
      "Epoch [2/5], Step [5552/10336], Loss: 0.3380\n",
      "Epoch [2/5], Step [5554/10336], Loss: 0.9122\n",
      "Epoch [2/5], Step [5556/10336], Loss: 0.0435\n",
      "Epoch [2/5], Step [5558/10336], Loss: 0.0073\n",
      "Epoch [2/5], Step [5560/10336], Loss: 0.0895\n",
      "Epoch [2/5], Step [5562/10336], Loss: 0.2858\n",
      "Epoch [2/5], Step [5564/10336], Loss: 0.3779\n",
      "Epoch [2/5], Step [5566/10336], Loss: 2.2786\n",
      "Epoch [2/5], Step [5568/10336], Loss: 3.0759\n",
      "Epoch [2/5], Step [5570/10336], Loss: 3.2328\n",
      "Epoch [2/5], Step [5572/10336], Loss: 0.3950\n",
      "Epoch [2/5], Step [5574/10336], Loss: 0.7909\n",
      "Epoch [2/5], Step [5576/10336], Loss: 0.3210\n",
      "Epoch [2/5], Step [5578/10336], Loss: 5.2824\n",
      "Epoch [2/5], Step [5580/10336], Loss: 0.1888\n",
      "Epoch [2/5], Step [5582/10336], Loss: 0.6725\n",
      "Epoch [2/5], Step [5584/10336], Loss: 2.1758\n",
      "Epoch [2/5], Step [5586/10336], Loss: 0.9568\n",
      "Epoch [2/5], Step [5588/10336], Loss: 0.0084\n",
      "Epoch [2/5], Step [5590/10336], Loss: 3.3365\n",
      "Epoch [2/5], Step [5592/10336], Loss: 2.4153\n",
      "Epoch [2/5], Step [5594/10336], Loss: 0.3891\n",
      "Epoch [2/5], Step [5596/10336], Loss: 0.5902\n",
      "Epoch [2/5], Step [5598/10336], Loss: 3.0786\n",
      "Epoch [2/5], Step [5600/10336], Loss: 1.0173\n",
      "Epoch [2/5], Step [5602/10336], Loss: 2.1166\n",
      "Epoch [2/5], Step [5604/10336], Loss: 0.9389\n",
      "Epoch [2/5], Step [5606/10336], Loss: 0.0456\n",
      "Epoch [2/5], Step [5608/10336], Loss: 0.0487\n",
      "Epoch [2/5], Step [5610/10336], Loss: 0.0430\n",
      "Epoch [2/5], Step [5612/10336], Loss: 0.3381\n",
      "Epoch [2/5], Step [5614/10336], Loss: 0.1662\n",
      "Epoch [2/5], Step [5616/10336], Loss: 0.3941\n",
      "Epoch [2/5], Step [5618/10336], Loss: 0.2660\n",
      "Epoch [2/5], Step [5620/10336], Loss: 1.3423\n",
      "Epoch [2/5], Step [5622/10336], Loss: 0.5845\n",
      "Epoch [2/5], Step [5624/10336], Loss: 0.0183\n",
      "Epoch [2/5], Step [5626/10336], Loss: 0.3997\n",
      "Epoch [2/5], Step [5628/10336], Loss: 0.4767\n",
      "Epoch [2/5], Step [5630/10336], Loss: 0.5884\n",
      "Epoch [2/5], Step [5632/10336], Loss: 0.7509\n",
      "Epoch [2/5], Step [5634/10336], Loss: 4.6156\n",
      "Epoch [2/5], Step [5636/10336], Loss: 0.1200\n",
      "Epoch [2/5], Step [5638/10336], Loss: 0.1703\n",
      "Epoch [2/5], Step [5640/10336], Loss: 0.1820\n",
      "Epoch [2/5], Step [5642/10336], Loss: 1.7287\n",
      "Epoch [2/5], Step [5644/10336], Loss: 4.3022\n",
      "Epoch [2/5], Step [5646/10336], Loss: 1.5386\n",
      "Epoch [2/5], Step [5648/10336], Loss: 0.5686\n",
      "Epoch [2/5], Step [5650/10336], Loss: 0.3368\n",
      "Epoch [2/5], Step [5652/10336], Loss: 0.0973\n",
      "Epoch [2/5], Step [5654/10336], Loss: 0.9348\n",
      "Epoch [2/5], Step [5656/10336], Loss: 0.2600\n",
      "Epoch [2/5], Step [5658/10336], Loss: 0.4513\n",
      "Epoch [2/5], Step [5660/10336], Loss: 0.5218\n",
      "Epoch [2/5], Step [5662/10336], Loss: 0.7594\n",
      "Epoch [2/5], Step [5664/10336], Loss: 3.1502\n",
      "Epoch [2/5], Step [5666/10336], Loss: 0.0975\n",
      "Epoch [2/5], Step [5668/10336], Loss: 0.9591\n",
      "Epoch [2/5], Step [5670/10336], Loss: 1.8416\n",
      "Epoch [2/5], Step [5672/10336], Loss: 1.2909\n",
      "Epoch [2/5], Step [5674/10336], Loss: 0.2575\n",
      "Epoch [2/5], Step [5676/10336], Loss: 3.0001\n",
      "Epoch [2/5], Step [5678/10336], Loss: 0.0617\n",
      "Epoch [2/5], Step [5680/10336], Loss: 0.7619\n",
      "Epoch [2/5], Step [5682/10336], Loss: 0.0345\n",
      "Epoch [2/5], Step [5684/10336], Loss: 0.1252\n",
      "Epoch [2/5], Step [5686/10336], Loss: 0.0915\n",
      "Epoch [2/5], Step [5688/10336], Loss: 0.9948\n",
      "Epoch [2/5], Step [5690/10336], Loss: 3.6854\n",
      "Epoch [2/5], Step [5692/10336], Loss: 0.5980\n",
      "Epoch [2/5], Step [5694/10336], Loss: 0.1275\n",
      "Epoch [2/5], Step [5696/10336], Loss: 3.6053\n",
      "Epoch [2/5], Step [5698/10336], Loss: 0.0006\n",
      "Epoch [2/5], Step [5700/10336], Loss: 0.2081\n",
      "Epoch [2/5], Step [5702/10336], Loss: 3.0216\n",
      "Epoch [2/5], Step [5704/10336], Loss: 1.2260\n",
      "Epoch [2/5], Step [5706/10336], Loss: 0.1523\n",
      "Epoch [2/5], Step [5708/10336], Loss: 0.1366\n",
      "Epoch [2/5], Step [5710/10336], Loss: 0.0317\n",
      "Epoch [2/5], Step [5712/10336], Loss: 0.0740\n",
      "Epoch [2/5], Step [5714/10336], Loss: 0.0022\n",
      "Epoch [2/5], Step [5716/10336], Loss: 3.3103\n",
      "Epoch [2/5], Step [5718/10336], Loss: 0.4016\n",
      "Epoch [2/5], Step [5720/10336], Loss: 2.3310\n",
      "Epoch [2/5], Step [5722/10336], Loss: 1.0930\n",
      "Epoch [2/5], Step [5724/10336], Loss: 4.6923\n",
      "Epoch [2/5], Step [5726/10336], Loss: 0.6758\n",
      "Epoch [2/5], Step [5728/10336], Loss: 1.1924\n",
      "Epoch [2/5], Step [5730/10336], Loss: 4.0236\n",
      "Epoch [2/5], Step [5732/10336], Loss: 0.4065\n",
      "Epoch [2/5], Step [5734/10336], Loss: 4.1414\n",
      "Epoch [2/5], Step [5736/10336], Loss: 2.0960\n",
      "Epoch [2/5], Step [5738/10336], Loss: 0.3890\n",
      "Epoch [2/5], Step [5740/10336], Loss: 1.0408\n",
      "Epoch [2/5], Step [5742/10336], Loss: 0.3323\n",
      "Epoch [2/5], Step [5744/10336], Loss: 3.0289\n",
      "Epoch [2/5], Step [5746/10336], Loss: 1.2713\n",
      "Epoch [2/5], Step [5748/10336], Loss: 0.2323\n",
      "Epoch [2/5], Step [5750/10336], Loss: 2.9622\n",
      "Epoch [2/5], Step [5752/10336], Loss: 0.2879\n",
      "Epoch [2/5], Step [5754/10336], Loss: 0.1313\n",
      "Epoch [2/5], Step [5756/10336], Loss: 0.2964\n",
      "Epoch [2/5], Step [5758/10336], Loss: 0.2889\n",
      "Epoch [2/5], Step [5760/10336], Loss: 4.2377\n",
      "Epoch [2/5], Step [5762/10336], Loss: 4.1309\n",
      "Epoch [2/5], Step [5764/10336], Loss: 0.3143\n",
      "Epoch [2/5], Step [5766/10336], Loss: 0.3352\n",
      "Epoch [2/5], Step [5768/10336], Loss: 4.7188\n",
      "Epoch [2/5], Step [5770/10336], Loss: 0.0200\n",
      "Epoch [2/5], Step [5772/10336], Loss: 0.8594\n",
      "Epoch [2/5], Step [5774/10336], Loss: 2.0946\n",
      "Epoch [2/5], Step [5776/10336], Loss: 1.4218\n",
      "Epoch [2/5], Step [5778/10336], Loss: 0.1370\n",
      "Epoch [2/5], Step [5780/10336], Loss: 0.5107\n",
      "Epoch [2/5], Step [5782/10336], Loss: 1.0538\n",
      "Epoch [2/5], Step [5784/10336], Loss: 0.5513\n",
      "Epoch [2/5], Step [5786/10336], Loss: 0.2817\n",
      "Epoch [2/5], Step [5788/10336], Loss: 0.3468\n",
      "Epoch [2/5], Step [5790/10336], Loss: 0.0943\n",
      "Epoch [2/5], Step [5792/10336], Loss: 2.7916\n",
      "Epoch [2/5], Step [5794/10336], Loss: 0.0738\n",
      "Epoch [2/5], Step [5796/10336], Loss: 0.0534\n",
      "Epoch [2/5], Step [5798/10336], Loss: 0.5512\n",
      "Epoch [2/5], Step [5800/10336], Loss: 0.2420\n",
      "Epoch [2/5], Step [5802/10336], Loss: 0.0969\n",
      "Epoch [2/5], Step [5804/10336], Loss: 0.1827\n",
      "Epoch [2/5], Step [5806/10336], Loss: 0.0438\n",
      "Epoch [2/5], Step [5808/10336], Loss: 0.3417\n",
      "Epoch [2/5], Step [5810/10336], Loss: 0.9776\n",
      "Epoch [2/5], Step [5812/10336], Loss: 6.3182\n",
      "Epoch [2/5], Step [5814/10336], Loss: 0.0033\n",
      "Epoch [2/5], Step [5816/10336], Loss: 0.1185\n",
      "Epoch [2/5], Step [5818/10336], Loss: 0.3633\n",
      "Epoch [2/5], Step [5820/10336], Loss: 0.3692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [5822/10336], Loss: 5.3373\n",
      "Epoch [2/5], Step [5824/10336], Loss: 0.1514\n",
      "Epoch [2/5], Step [5826/10336], Loss: 3.2858\n",
      "Epoch [2/5], Step [5828/10336], Loss: 0.4742\n",
      "Epoch [2/5], Step [5830/10336], Loss: 0.0240\n",
      "Epoch [2/5], Step [5832/10336], Loss: 0.6942\n",
      "Epoch [2/5], Step [5834/10336], Loss: 0.0608\n",
      "Epoch [2/5], Step [5836/10336], Loss: 3.5981\n",
      "Epoch [2/5], Step [5838/10336], Loss: 0.0792\n",
      "Epoch [2/5], Step [5840/10336], Loss: 0.2942\n",
      "Epoch [2/5], Step [5842/10336], Loss: 0.3942\n",
      "Epoch [2/5], Step [5844/10336], Loss: 1.2100\n",
      "Epoch [2/5], Step [5846/10336], Loss: 0.6374\n",
      "Epoch [2/5], Step [5848/10336], Loss: 3.3787\n",
      "Epoch [2/5], Step [5850/10336], Loss: 3.0368\n",
      "Epoch [2/5], Step [5852/10336], Loss: 0.3414\n",
      "Epoch [2/5], Step [5854/10336], Loss: 3.4339\n",
      "Epoch [2/5], Step [5856/10336], Loss: 0.3383\n",
      "Epoch [2/5], Step [5858/10336], Loss: 0.0794\n",
      "Epoch [2/5], Step [5860/10336], Loss: 4.6068\n",
      "Epoch [2/5], Step [5862/10336], Loss: 3.7293\n",
      "Epoch [2/5], Step [5864/10336], Loss: 0.0081\n",
      "Epoch [2/5], Step [5866/10336], Loss: 0.6162\n",
      "Epoch [2/5], Step [5868/10336], Loss: 0.9053\n",
      "Epoch [2/5], Step [5870/10336], Loss: 3.8431\n",
      "Epoch [2/5], Step [5872/10336], Loss: 0.8786\n",
      "Epoch [2/5], Step [5874/10336], Loss: 0.1977\n",
      "Epoch [2/5], Step [5876/10336], Loss: 1.2067\n",
      "Epoch [2/5], Step [5878/10336], Loss: 0.0283\n",
      "Epoch [2/5], Step [5880/10336], Loss: 0.2912\n",
      "Epoch [2/5], Step [5882/10336], Loss: 0.0222\n",
      "Epoch [2/5], Step [5884/10336], Loss: 4.1385\n",
      "Epoch [2/5], Step [5886/10336], Loss: 6.9547\n",
      "Epoch [2/5], Step [5888/10336], Loss: 2.7883\n",
      "Epoch [2/5], Step [5890/10336], Loss: 0.7075\n",
      "Epoch [2/5], Step [5892/10336], Loss: 0.2899\n",
      "Epoch [2/5], Step [5894/10336], Loss: 0.1313\n",
      "Epoch [2/5], Step [5896/10336], Loss: 0.8262\n",
      "Epoch [2/5], Step [5898/10336], Loss: 2.7419\n",
      "Epoch [2/5], Step [5900/10336], Loss: 0.8722\n",
      "Epoch [2/5], Step [5902/10336], Loss: 0.2955\n",
      "Epoch [2/5], Step [5904/10336], Loss: 0.2407\n",
      "Epoch [2/5], Step [5906/10336], Loss: 1.2992\n",
      "Epoch [2/5], Step [5908/10336], Loss: 0.4171\n",
      "Epoch [2/5], Step [5910/10336], Loss: 0.2266\n",
      "Epoch [2/5], Step [5912/10336], Loss: 0.2382\n",
      "Epoch [2/5], Step [5914/10336], Loss: 1.2539\n",
      "Epoch [2/5], Step [5916/10336], Loss: 0.2212\n",
      "Epoch [2/5], Step [5918/10336], Loss: 0.2438\n",
      "Epoch [2/5], Step [5920/10336], Loss: 0.9519\n",
      "Epoch [2/5], Step [5922/10336], Loss: 0.4205\n",
      "Epoch [2/5], Step [5924/10336], Loss: 1.5145\n",
      "Epoch [2/5], Step [5926/10336], Loss: 0.0736\n",
      "Epoch [2/5], Step [5928/10336], Loss: 0.2783\n",
      "Epoch [2/5], Step [5930/10336], Loss: 0.1416\n",
      "Epoch [2/5], Step [5932/10336], Loss: 0.2092\n",
      "Epoch [2/5], Step [5934/10336], Loss: 0.1978\n",
      "Epoch [2/5], Step [5936/10336], Loss: 0.0136\n",
      "Epoch [2/5], Step [5938/10336], Loss: 3.1340\n",
      "Epoch [2/5], Step [5940/10336], Loss: 3.0247\n",
      "Epoch [2/5], Step [5942/10336], Loss: 0.0322\n",
      "Epoch [2/5], Step [5944/10336], Loss: 1.5124\n",
      "Epoch [2/5], Step [5946/10336], Loss: 0.1111\n",
      "Epoch [2/5], Step [5948/10336], Loss: 0.4359\n",
      "Epoch [2/5], Step [5950/10336], Loss: 0.4655\n",
      "Epoch [2/5], Step [5952/10336], Loss: 0.0743\n",
      "Epoch [2/5], Step [5954/10336], Loss: 0.6775\n",
      "Epoch [2/5], Step [5956/10336], Loss: 0.8460\n",
      "Epoch [2/5], Step [5958/10336], Loss: 0.2020\n",
      "Epoch [2/5], Step [5960/10336], Loss: 0.0246\n",
      "Epoch [2/5], Step [5962/10336], Loss: 0.0979\n",
      "Epoch [2/5], Step [5964/10336], Loss: 0.5884\n",
      "Epoch [2/5], Step [5966/10336], Loss: 2.1693\n",
      "Epoch [2/5], Step [5968/10336], Loss: 1.4566\n",
      "Epoch [2/5], Step [5970/10336], Loss: 0.1221\n",
      "Epoch [2/5], Step [5972/10336], Loss: 0.1516\n",
      "Epoch [2/5], Step [5974/10336], Loss: 0.0988\n",
      "Epoch [2/5], Step [5976/10336], Loss: 0.2385\n",
      "Epoch [2/5], Step [5978/10336], Loss: 1.0094\n",
      "Epoch [2/5], Step [5980/10336], Loss: 4.5948\n",
      "Epoch [2/5], Step [5982/10336], Loss: 0.7317\n",
      "Epoch [2/5], Step [5984/10336], Loss: 0.2958\n",
      "Epoch [2/5], Step [5986/10336], Loss: 0.0127\n",
      "Epoch [2/5], Step [5988/10336], Loss: 0.0023\n",
      "Epoch [2/5], Step [5990/10336], Loss: 0.3142\n",
      "Epoch [2/5], Step [5992/10336], Loss: 0.0133\n",
      "Epoch [2/5], Step [5994/10336], Loss: 0.3138\n",
      "Epoch [2/5], Step [5996/10336], Loss: 2.8717\n",
      "Epoch [2/5], Step [5998/10336], Loss: 0.2752\n",
      "Epoch [2/5], Step [6000/10336], Loss: 1.4920\n",
      "Epoch [2/5], Step [6002/10336], Loss: 1.5069\n",
      "Epoch [2/5], Step [6004/10336], Loss: 0.0238\n",
      "Epoch [2/5], Step [6006/10336], Loss: 0.6575\n",
      "Epoch [2/5], Step [6008/10336], Loss: 0.3691\n",
      "Epoch [2/5], Step [6010/10336], Loss: 0.4896\n",
      "Epoch [2/5], Step [6012/10336], Loss: 2.5679\n",
      "Epoch [2/5], Step [6014/10336], Loss: 0.2359\n",
      "Epoch [2/5], Step [6016/10336], Loss: 0.1468\n",
      "Epoch [2/5], Step [6018/10336], Loss: 0.6157\n",
      "Epoch [2/5], Step [6020/10336], Loss: 0.1205\n",
      "Epoch [2/5], Step [6022/10336], Loss: 0.1805\n",
      "Epoch [2/5], Step [6024/10336], Loss: 4.4372\n",
      "Epoch [2/5], Step [6026/10336], Loss: 0.0330\n",
      "Epoch [2/5], Step [6028/10336], Loss: 0.1344\n",
      "Epoch [2/5], Step [6030/10336], Loss: 0.1368\n",
      "Epoch [2/5], Step [6032/10336], Loss: 1.2945\n",
      "Epoch [2/5], Step [6034/10336], Loss: 1.9157\n",
      "Epoch [2/5], Step [6036/10336], Loss: 0.2312\n",
      "Epoch [2/5], Step [6038/10336], Loss: 0.1877\n",
      "Epoch [2/5], Step [6040/10336], Loss: 1.3397\n",
      "Epoch [2/5], Step [6042/10336], Loss: 0.1464\n",
      "Epoch [2/5], Step [6044/10336], Loss: 0.4411\n",
      "Epoch [2/5], Step [6046/10336], Loss: 0.7932\n",
      "Epoch [2/5], Step [6048/10336], Loss: 1.8143\n",
      "Epoch [2/5], Step [6050/10336], Loss: 0.3938\n",
      "Epoch [2/5], Step [6052/10336], Loss: 0.3422\n",
      "Epoch [2/5], Step [6054/10336], Loss: 1.4307\n",
      "Epoch [2/5], Step [6056/10336], Loss: 1.7317\n",
      "Epoch [2/5], Step [6058/10336], Loss: 3.2831\n",
      "Epoch [2/5], Step [6060/10336], Loss: 0.0530\n",
      "Epoch [2/5], Step [6062/10336], Loss: 2.5786\n",
      "Epoch [2/5], Step [6064/10336], Loss: 0.2246\n",
      "Epoch [2/5], Step [6066/10336], Loss: 0.0251\n",
      "Epoch [2/5], Step [6068/10336], Loss: 1.3877\n",
      "Epoch [2/5], Step [6070/10336], Loss: 0.0540\n",
      "Epoch [2/5], Step [6072/10336], Loss: 2.6258\n",
      "Epoch [2/5], Step [6074/10336], Loss: 0.2262\n",
      "Epoch [2/5], Step [6076/10336], Loss: 0.0702\n",
      "Epoch [2/5], Step [6078/10336], Loss: 0.5325\n",
      "Epoch [2/5], Step [6080/10336], Loss: 0.2516\n",
      "Epoch [2/5], Step [6082/10336], Loss: 0.2729\n",
      "Epoch [2/5], Step [6084/10336], Loss: 0.4545\n",
      "Epoch [2/5], Step [6086/10336], Loss: 0.2561\n",
      "Epoch [2/5], Step [6088/10336], Loss: 0.1692\n",
      "Epoch [2/5], Step [6090/10336], Loss: 1.5112\n",
      "Epoch [2/5], Step [6092/10336], Loss: 3.3599\n",
      "Epoch [2/5], Step [6094/10336], Loss: 0.0464\n",
      "Epoch [2/5], Step [6096/10336], Loss: 2.0219\n",
      "Epoch [2/5], Step [6098/10336], Loss: 0.4607\n",
      "Epoch [2/5], Step [6100/10336], Loss: 0.0856\n",
      "Epoch [2/5], Step [6102/10336], Loss: 0.0076\n",
      "Epoch [2/5], Step [6104/10336], Loss: 0.4015\n",
      "Epoch [2/5], Step [6106/10336], Loss: 0.1580\n",
      "Epoch [2/5], Step [6108/10336], Loss: 0.4382\n",
      "Epoch [2/5], Step [6110/10336], Loss: 0.1053\n",
      "Epoch [2/5], Step [6112/10336], Loss: 5.9226\n",
      "Epoch [2/5], Step [6114/10336], Loss: 1.0072\n",
      "Epoch [2/5], Step [6116/10336], Loss: 3.7599\n",
      "Epoch [2/5], Step [6118/10336], Loss: 1.0844\n",
      "Epoch [2/5], Step [6120/10336], Loss: 0.0091\n",
      "Epoch [2/5], Step [6122/10336], Loss: 0.2820\n",
      "Epoch [2/5], Step [6124/10336], Loss: 2.1722\n",
      "Epoch [2/5], Step [6126/10336], Loss: 0.8513\n",
      "Epoch [2/5], Step [6128/10336], Loss: 0.0282\n",
      "Epoch [2/5], Step [6130/10336], Loss: 0.2259\n",
      "Epoch [2/5], Step [6132/10336], Loss: 4.1255\n",
      "Epoch [2/5], Step [6134/10336], Loss: 0.3379\n",
      "Epoch [2/5], Step [6136/10336], Loss: 0.5245\n",
      "Epoch [2/5], Step [6138/10336], Loss: 0.7095\n",
      "Epoch [2/5], Step [6140/10336], Loss: 0.1569\n",
      "Epoch [2/5], Step [6142/10336], Loss: 0.4965\n",
      "Epoch [2/5], Step [6144/10336], Loss: 0.3395\n",
      "Epoch [2/5], Step [6146/10336], Loss: 0.5766\n",
      "Epoch [2/5], Step [6148/10336], Loss: 0.5504\n",
      "Epoch [2/5], Step [6150/10336], Loss: 0.4194\n",
      "Epoch [2/5], Step [6152/10336], Loss: 0.4007\n",
      "Epoch [2/5], Step [6154/10336], Loss: 1.0464\n",
      "Epoch [2/5], Step [6156/10336], Loss: 0.7811\n",
      "Epoch [2/5], Step [6158/10336], Loss: 0.1620\n",
      "Epoch [2/5], Step [6160/10336], Loss: 0.6704\n",
      "Epoch [2/5], Step [6162/10336], Loss: 0.6732\n",
      "Epoch [2/5], Step [6164/10336], Loss: 0.0194\n",
      "Epoch [2/5], Step [6166/10336], Loss: 0.9057\n",
      "Epoch [2/5], Step [6168/10336], Loss: 2.8817\n",
      "Epoch [2/5], Step [6170/10336], Loss: 2.9555\n",
      "Epoch [2/5], Step [6172/10336], Loss: 0.3877\n",
      "Epoch [2/5], Step [6174/10336], Loss: 0.3749\n",
      "Epoch [2/5], Step [6176/10336], Loss: 0.2992\n",
      "Epoch [2/5], Step [6178/10336], Loss: 0.1517\n",
      "Epoch [2/5], Step [6180/10336], Loss: 3.0507\n",
      "Epoch [2/5], Step [6182/10336], Loss: 0.0037\n",
      "Epoch [2/5], Step [6184/10336], Loss: 0.7211\n",
      "Epoch [2/5], Step [6186/10336], Loss: 0.0240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [6188/10336], Loss: 0.0181\n",
      "Epoch [2/5], Step [6190/10336], Loss: 0.0550\n",
      "Epoch [2/5], Step [6192/10336], Loss: 0.1961\n",
      "Epoch [2/5], Step [6194/10336], Loss: 3.1053\n",
      "Epoch [2/5], Step [6196/10336], Loss: 0.0155\n",
      "Epoch [2/5], Step [6198/10336], Loss: 0.5429\n",
      "Epoch [2/5], Step [6200/10336], Loss: 0.1250\n",
      "Epoch [2/5], Step [6202/10336], Loss: 0.5728\n",
      "Epoch [2/5], Step [6204/10336], Loss: 1.2804\n",
      "Epoch [2/5], Step [6206/10336], Loss: 0.2910\n",
      "Epoch [2/5], Step [6208/10336], Loss: 0.1533\n",
      "Epoch [2/5], Step [6210/10336], Loss: 0.2303\n",
      "Epoch [2/5], Step [6212/10336], Loss: 0.3426\n",
      "Epoch [2/5], Step [6214/10336], Loss: 0.0428\n",
      "Epoch [2/5], Step [6216/10336], Loss: 3.8418\n",
      "Epoch [2/5], Step [6218/10336], Loss: 0.2415\n",
      "Epoch [2/5], Step [6220/10336], Loss: 2.6242\n",
      "Epoch [2/5], Step [6222/10336], Loss: 2.6867\n",
      "Epoch [2/5], Step [6224/10336], Loss: 1.9040\n",
      "Epoch [2/5], Step [6226/10336], Loss: 0.7089\n",
      "Epoch [2/5], Step [6228/10336], Loss: 0.4986\n",
      "Epoch [2/5], Step [6230/10336], Loss: 0.3818\n",
      "Epoch [2/5], Step [6232/10336], Loss: 1.6818\n",
      "Epoch [2/5], Step [6234/10336], Loss: 0.7291\n",
      "Epoch [2/5], Step [6236/10336], Loss: 0.9849\n",
      "Epoch [2/5], Step [6238/10336], Loss: 2.9871\n",
      "Epoch [2/5], Step [6240/10336], Loss: 0.0322\n",
      "Epoch [2/5], Step [6242/10336], Loss: 0.0036\n",
      "Epoch [2/5], Step [6244/10336], Loss: 0.4809\n",
      "Epoch [2/5], Step [6246/10336], Loss: 0.8347\n",
      "Epoch [2/5], Step [6248/10336], Loss: 0.0058\n",
      "Epoch [2/5], Step [6250/10336], Loss: 0.2097\n",
      "Epoch [2/5], Step [6252/10336], Loss: 4.6267\n",
      "Epoch [2/5], Step [6254/10336], Loss: 2.1643\n",
      "Epoch [2/5], Step [6256/10336], Loss: 0.2314\n",
      "Epoch [2/5], Step [6258/10336], Loss: 0.0031\n",
      "Epoch [2/5], Step [6260/10336], Loss: 0.6142\n",
      "Epoch [2/5], Step [6262/10336], Loss: 0.1794\n",
      "Epoch [2/5], Step [6264/10336], Loss: 0.1184\n",
      "Epoch [2/5], Step [6266/10336], Loss: 0.2583\n",
      "Epoch [2/5], Step [6268/10336], Loss: 0.1643\n",
      "Epoch [2/5], Step [6270/10336], Loss: 0.4507\n",
      "Epoch [2/5], Step [6272/10336], Loss: 0.8267\n",
      "Epoch [2/5], Step [6274/10336], Loss: 2.5370\n",
      "Epoch [2/5], Step [6276/10336], Loss: 1.9763\n",
      "Epoch [2/5], Step [6278/10336], Loss: 0.3322\n",
      "Epoch [2/5], Step [6280/10336], Loss: 0.2753\n",
      "Epoch [2/5], Step [6282/10336], Loss: 0.0097\n",
      "Epoch [2/5], Step [6284/10336], Loss: 0.0425\n",
      "Epoch [2/5], Step [6286/10336], Loss: 2.3230\n",
      "Epoch [2/5], Step [6288/10336], Loss: 0.5679\n",
      "Epoch [2/5], Step [6290/10336], Loss: 3.3131\n",
      "Epoch [2/5], Step [6292/10336], Loss: 0.2207\n",
      "Epoch [2/5], Step [6294/10336], Loss: 0.2688\n",
      "Epoch [2/5], Step [6296/10336], Loss: 0.2997\n",
      "Epoch [2/5], Step [6298/10336], Loss: 0.5423\n",
      "Epoch [2/5], Step [6300/10336], Loss: 0.2550\n",
      "Epoch [2/5], Step [6302/10336], Loss: 3.7052\n",
      "Epoch [2/5], Step [6304/10336], Loss: 1.5148\n",
      "Epoch [2/5], Step [6306/10336], Loss: 0.4015\n",
      "Epoch [2/5], Step [6308/10336], Loss: 0.0231\n",
      "Epoch [2/5], Step [6310/10336], Loss: 0.0895\n",
      "Epoch [2/5], Step [6312/10336], Loss: 1.3381\n",
      "Epoch [2/5], Step [6314/10336], Loss: 3.5737\n",
      "Epoch [2/5], Step [6316/10336], Loss: 0.4215\n",
      "Epoch [2/5], Step [6318/10336], Loss: 1.0677\n",
      "Epoch [2/5], Step [6320/10336], Loss: 0.2184\n",
      "Epoch [2/5], Step [6322/10336], Loss: 2.9731\n",
      "Epoch [2/5], Step [6324/10336], Loss: 0.2157\n",
      "Epoch [2/5], Step [6326/10336], Loss: 0.4289\n",
      "Epoch [2/5], Step [6328/10336], Loss: 2.4289\n",
      "Epoch [2/5], Step [6330/10336], Loss: 0.2525\n",
      "Epoch [2/5], Step [6332/10336], Loss: 0.0980\n",
      "Epoch [2/5], Step [6334/10336], Loss: 0.0065\n",
      "Epoch [2/5], Step [6336/10336], Loss: 0.4037\n",
      "Epoch [2/5], Step [6338/10336], Loss: 0.7738\n",
      "Epoch [2/5], Step [6340/10336], Loss: 0.4476\n",
      "Epoch [2/5], Step [6342/10336], Loss: 0.3320\n",
      "Epoch [2/5], Step [6344/10336], Loss: 0.8349\n",
      "Epoch [2/5], Step [6346/10336], Loss: 0.7114\n",
      "Epoch [2/5], Step [6348/10336], Loss: 2.5891\n",
      "Epoch [2/5], Step [6350/10336], Loss: 0.3242\n",
      "Epoch [2/5], Step [6352/10336], Loss: 1.8758\n",
      "Epoch [2/5], Step [6354/10336], Loss: 0.0031\n",
      "Epoch [2/5], Step [6356/10336], Loss: 0.3057\n",
      "Epoch [2/5], Step [6358/10336], Loss: 0.2910\n",
      "Epoch [2/5], Step [6360/10336], Loss: 0.3315\n",
      "Epoch [2/5], Step [6362/10336], Loss: 0.3164\n",
      "Epoch [2/5], Step [6364/10336], Loss: 0.0480\n",
      "Epoch [2/5], Step [6366/10336], Loss: 0.1938\n",
      "Epoch [2/5], Step [6368/10336], Loss: 0.2880\n",
      "Epoch [2/5], Step [6370/10336], Loss: 0.0313\n",
      "Epoch [2/5], Step [6372/10336], Loss: 3.2194\n",
      "Epoch [2/5], Step [6374/10336], Loss: 0.8034\n",
      "Epoch [2/5], Step [6376/10336], Loss: 0.9494\n",
      "Epoch [2/5], Step [6378/10336], Loss: 0.2330\n",
      "Epoch [2/5], Step [6380/10336], Loss: 0.0423\n",
      "Epoch [2/5], Step [6382/10336], Loss: 0.3154\n",
      "Epoch [2/5], Step [6384/10336], Loss: 0.0647\n",
      "Epoch [2/5], Step [6386/10336], Loss: 0.0132\n",
      "Epoch [2/5], Step [6388/10336], Loss: 0.9147\n",
      "Epoch [2/5], Step [6390/10336], Loss: 3.9543\n",
      "Epoch [2/5], Step [6392/10336], Loss: 0.0185\n",
      "Epoch [2/5], Step [6394/10336], Loss: 0.1176\n",
      "Epoch [2/5], Step [6396/10336], Loss: 0.0229\n",
      "Epoch [2/5], Step [6398/10336], Loss: 0.1191\n",
      "Epoch [2/5], Step [6400/10336], Loss: 0.4846\n",
      "Epoch [2/5], Step [6402/10336], Loss: 0.4091\n",
      "Epoch [2/5], Step [6404/10336], Loss: 0.7436\n",
      "Epoch [2/5], Step [6406/10336], Loss: 0.1866\n",
      "Epoch [2/5], Step [6408/10336], Loss: 0.4929\n",
      "Epoch [2/5], Step [6410/10336], Loss: 0.9633\n",
      "Epoch [2/5], Step [6412/10336], Loss: 0.0430\n",
      "Epoch [2/5], Step [6414/10336], Loss: 1.3222\n",
      "Epoch [2/5], Step [6416/10336], Loss: 0.0399\n",
      "Epoch [2/5], Step [6418/10336], Loss: 0.2147\n",
      "Epoch [2/5], Step [6420/10336], Loss: 0.2944\n",
      "Epoch [2/5], Step [6422/10336], Loss: 0.3998\n",
      "Epoch [2/5], Step [6424/10336], Loss: 0.0018\n",
      "Epoch [2/5], Step [6426/10336], Loss: 2.1556\n",
      "Epoch [2/5], Step [6428/10336], Loss: 0.0385\n",
      "Epoch [2/5], Step [6430/10336], Loss: 0.1610\n",
      "Epoch [2/5], Step [6432/10336], Loss: 1.1476\n",
      "Epoch [2/5], Step [6434/10336], Loss: 1.4686\n",
      "Epoch [2/5], Step [6436/10336], Loss: 0.7454\n",
      "Epoch [2/5], Step [6438/10336], Loss: 6.0523\n",
      "Epoch [2/5], Step [6440/10336], Loss: 1.6712\n",
      "Epoch [2/5], Step [6442/10336], Loss: 0.4667\n",
      "Epoch [2/5], Step [6444/10336], Loss: 0.0476\n",
      "Epoch [2/5], Step [6446/10336], Loss: 0.6927\n",
      "Epoch [2/5], Step [6448/10336], Loss: 1.6745\n",
      "Epoch [2/5], Step [6450/10336], Loss: 0.9957\n",
      "Epoch [2/5], Step [6452/10336], Loss: 0.7873\n",
      "Epoch [2/5], Step [6454/10336], Loss: 1.0632\n",
      "Epoch [2/5], Step [6456/10336], Loss: 0.9557\n",
      "Epoch [2/5], Step [6458/10336], Loss: 0.4728\n",
      "Epoch [2/5], Step [6460/10336], Loss: 0.3250\n",
      "Epoch [2/5], Step [6462/10336], Loss: 0.2665\n",
      "Epoch [2/5], Step [6464/10336], Loss: 0.3625\n",
      "Epoch [2/5], Step [6466/10336], Loss: 2.7244\n",
      "Epoch [2/5], Step [6468/10336], Loss: 1.0823\n",
      "Epoch [2/5], Step [6470/10336], Loss: 0.6881\n",
      "Epoch [2/5], Step [6472/10336], Loss: 0.0301\n",
      "Epoch [2/5], Step [6474/10336], Loss: 0.6291\n",
      "Epoch [2/5], Step [6476/10336], Loss: 0.7460\n",
      "Epoch [2/5], Step [6478/10336], Loss: 0.5263\n",
      "Epoch [2/5], Step [6480/10336], Loss: 4.7173\n",
      "Epoch [2/5], Step [6482/10336], Loss: 0.2053\n",
      "Epoch [2/5], Step [6484/10336], Loss: 0.4377\n",
      "Epoch [2/5], Step [6486/10336], Loss: 0.0995\n",
      "Epoch [2/5], Step [6488/10336], Loss: 1.0470\n",
      "Epoch [2/5], Step [6490/10336], Loss: 0.0531\n",
      "Epoch [2/5], Step [6492/10336], Loss: 1.1106\n",
      "Epoch [2/5], Step [6494/10336], Loss: 0.1481\n",
      "Epoch [2/5], Step [6496/10336], Loss: 0.4041\n",
      "Epoch [2/5], Step [6498/10336], Loss: 4.2811\n",
      "Epoch [2/5], Step [6500/10336], Loss: 4.0443\n",
      "Epoch [2/5], Step [6502/10336], Loss: 2.2142\n",
      "Epoch [2/5], Step [6504/10336], Loss: 0.2155\n",
      "Epoch [2/5], Step [6506/10336], Loss: 0.3088\n",
      "Epoch [2/5], Step [6508/10336], Loss: 0.0334\n",
      "Epoch [2/5], Step [6510/10336], Loss: 0.4452\n",
      "Epoch [2/5], Step [6512/10336], Loss: 1.1943\n",
      "Epoch [2/5], Step [6514/10336], Loss: 0.8220\n",
      "Epoch [2/5], Step [6516/10336], Loss: 1.2619\n",
      "Epoch [2/5], Step [6518/10336], Loss: 2.0267\n",
      "Epoch [2/5], Step [6520/10336], Loss: 1.0390\n",
      "Epoch [2/5], Step [6522/10336], Loss: 0.0972\n",
      "Epoch [2/5], Step [6524/10336], Loss: 0.0900\n",
      "Epoch [2/5], Step [6526/10336], Loss: 1.1922\n",
      "Epoch [2/5], Step [6528/10336], Loss: 0.0920\n",
      "Epoch [2/5], Step [6530/10336], Loss: 0.2202\n",
      "Epoch [2/5], Step [6532/10336], Loss: 0.1472\n",
      "Epoch [2/5], Step [6534/10336], Loss: 0.2508\n",
      "Epoch [2/5], Step [6536/10336], Loss: 0.0944\n",
      "Epoch [2/5], Step [6538/10336], Loss: 0.0930\n",
      "Epoch [2/5], Step [6540/10336], Loss: 6.4634\n",
      "Epoch [2/5], Step [6542/10336], Loss: 0.1969\n",
      "Epoch [2/5], Step [6544/10336], Loss: 0.2422\n",
      "Epoch [2/5], Step [6546/10336], Loss: 0.2673\n",
      "Epoch [2/5], Step [6548/10336], Loss: 0.1070\n",
      "Epoch [2/5], Step [6550/10336], Loss: 0.4263\n",
      "Epoch [2/5], Step [6552/10336], Loss: 0.1156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [6554/10336], Loss: 2.0869\n",
      "Epoch [2/5], Step [6556/10336], Loss: 0.0643\n",
      "Epoch [2/5], Step [6558/10336], Loss: 0.1907\n",
      "Epoch [2/5], Step [6560/10336], Loss: 0.1094\n",
      "Epoch [2/5], Step [6562/10336], Loss: 1.5689\n",
      "Epoch [2/5], Step [6564/10336], Loss: 0.3631\n",
      "Epoch [2/5], Step [6566/10336], Loss: 0.1986\n",
      "Epoch [2/5], Step [6568/10336], Loss: 0.3304\n",
      "Epoch [2/5], Step [6570/10336], Loss: 0.1698\n",
      "Epoch [2/5], Step [6572/10336], Loss: 0.6559\n",
      "Epoch [2/5], Step [6574/10336], Loss: 2.3708\n",
      "Epoch [2/5], Step [6576/10336], Loss: 0.2093\n",
      "Epoch [2/5], Step [6578/10336], Loss: 4.6459\n",
      "Epoch [2/5], Step [6580/10336], Loss: 0.3148\n",
      "Epoch [2/5], Step [6582/10336], Loss: 0.4442\n",
      "Epoch [2/5], Step [6584/10336], Loss: 0.0179\n",
      "Epoch [2/5], Step [6586/10336], Loss: 0.0027\n",
      "Epoch [2/5], Step [6588/10336], Loss: 0.1795\n",
      "Epoch [2/5], Step [6590/10336], Loss: 0.2031\n",
      "Epoch [2/5], Step [6592/10336], Loss: 0.0682\n",
      "Epoch [2/5], Step [6594/10336], Loss: 3.3186\n",
      "Epoch [2/5], Step [6596/10336], Loss: 0.0229\n",
      "Epoch [2/5], Step [6598/10336], Loss: 0.4477\n",
      "Epoch [2/5], Step [6600/10336], Loss: 0.1131\n",
      "Epoch [2/5], Step [6602/10336], Loss: 0.3170\n",
      "Epoch [2/5], Step [6604/10336], Loss: 1.9657\n",
      "Epoch [2/5], Step [6606/10336], Loss: 0.4284\n",
      "Epoch [2/5], Step [6608/10336], Loss: 0.5806\n",
      "Epoch [2/5], Step [6610/10336], Loss: 0.0084\n",
      "Epoch [2/5], Step [6612/10336], Loss: 1.4783\n",
      "Epoch [2/5], Step [6614/10336], Loss: 1.6405\n",
      "Epoch [2/5], Step [6616/10336], Loss: 0.3101\n",
      "Epoch [2/5], Step [6618/10336], Loss: 0.6358\n",
      "Epoch [2/5], Step [6620/10336], Loss: 0.3733\n",
      "Epoch [2/5], Step [6622/10336], Loss: 0.1386\n",
      "Epoch [2/5], Step [6624/10336], Loss: 0.1438\n",
      "Epoch [2/5], Step [6626/10336], Loss: 0.1291\n",
      "Epoch [2/5], Step [6628/10336], Loss: 0.2955\n",
      "Epoch [2/5], Step [6630/10336], Loss: 0.9616\n",
      "Epoch [2/5], Step [6632/10336], Loss: 1.6574\n",
      "Epoch [2/5], Step [6634/10336], Loss: 2.4823\n",
      "Epoch [2/5], Step [6636/10336], Loss: 0.1261\n",
      "Epoch [2/5], Step [6638/10336], Loss: 0.2547\n",
      "Epoch [2/5], Step [6640/10336], Loss: 0.0591\n",
      "Epoch [2/5], Step [6642/10336], Loss: 0.2619\n",
      "Epoch [2/5], Step [6644/10336], Loss: 0.1745\n",
      "Epoch [2/5], Step [6646/10336], Loss: 0.0710\n",
      "Epoch [2/5], Step [6648/10336], Loss: 4.0709\n",
      "Epoch [2/5], Step [6650/10336], Loss: 0.1121\n",
      "Epoch [2/5], Step [6652/10336], Loss: 0.1744\n",
      "Epoch [2/5], Step [6654/10336], Loss: 1.9405\n",
      "Epoch [2/5], Step [6656/10336], Loss: 0.1644\n",
      "Epoch [2/5], Step [6658/10336], Loss: 4.2835\n",
      "Epoch [2/5], Step [6660/10336], Loss: 0.4229\n",
      "Epoch [2/5], Step [6662/10336], Loss: 0.6227\n",
      "Epoch [2/5], Step [6664/10336], Loss: 0.0577\n",
      "Epoch [2/5], Step [6666/10336], Loss: 0.0104\n",
      "Epoch [2/5], Step [6668/10336], Loss: 0.0027\n",
      "Epoch [2/5], Step [6670/10336], Loss: 2.3704\n",
      "Epoch [2/5], Step [6672/10336], Loss: 0.0195\n",
      "Epoch [2/5], Step [6674/10336], Loss: 0.2769\n",
      "Epoch [2/5], Step [6676/10336], Loss: 0.8301\n",
      "Epoch [2/5], Step [6678/10336], Loss: 0.2441\n",
      "Epoch [2/5], Step [6680/10336], Loss: 0.7741\n",
      "Epoch [2/5], Step [6682/10336], Loss: 0.2873\n",
      "Epoch [2/5], Step [6684/10336], Loss: 0.2676\n",
      "Epoch [2/5], Step [6686/10336], Loss: 0.6749\n",
      "Epoch [2/5], Step [6688/10336], Loss: 0.3437\n",
      "Epoch [2/5], Step [6690/10336], Loss: 1.0607\n",
      "Epoch [2/5], Step [6692/10336], Loss: 0.2862\n",
      "Epoch [2/5], Step [6694/10336], Loss: 0.6051\n",
      "Epoch [2/5], Step [6696/10336], Loss: 0.5639\n",
      "Epoch [2/5], Step [6698/10336], Loss: 0.6023\n",
      "Epoch [2/5], Step [6700/10336], Loss: 0.0273\n",
      "Epoch [2/5], Step [6702/10336], Loss: 1.3900\n",
      "Epoch [2/5], Step [6704/10336], Loss: 0.0747\n",
      "Epoch [2/5], Step [6706/10336], Loss: 0.2938\n",
      "Epoch [2/5], Step [6708/10336], Loss: 0.4661\n",
      "Epoch [2/5], Step [6710/10336], Loss: 0.0619\n",
      "Epoch [2/5], Step [6712/10336], Loss: 0.0058\n",
      "Epoch [2/5], Step [6714/10336], Loss: 0.3554\n",
      "Epoch [2/5], Step [6716/10336], Loss: 4.5483\n",
      "Epoch [2/5], Step [6718/10336], Loss: 0.1923\n",
      "Epoch [2/5], Step [6720/10336], Loss: 0.1859\n",
      "Epoch [2/5], Step [6722/10336], Loss: 1.2870\n",
      "Epoch [2/5], Step [6724/10336], Loss: 0.2062\n",
      "Epoch [2/5], Step [6726/10336], Loss: 0.1923\n",
      "Epoch [2/5], Step [6728/10336], Loss: 0.3136\n",
      "Epoch [2/5], Step [6730/10336], Loss: 0.2578\n",
      "Epoch [2/5], Step [6732/10336], Loss: 0.0818\n",
      "Epoch [2/5], Step [6734/10336], Loss: 0.1820\n",
      "Epoch [2/5], Step [6736/10336], Loss: 0.5747\n",
      "Epoch [2/5], Step [6738/10336], Loss: 0.2309\n",
      "Epoch [2/5], Step [6740/10336], Loss: 0.2308\n",
      "Epoch [2/5], Step [6742/10336], Loss: 2.7890\n",
      "Epoch [2/5], Step [6744/10336], Loss: 0.3810\n",
      "Epoch [2/5], Step [6746/10336], Loss: 0.0195\n",
      "Epoch [2/5], Step [6748/10336], Loss: 0.3564\n",
      "Epoch [2/5], Step [6750/10336], Loss: 1.5662\n",
      "Epoch [2/5], Step [6752/10336], Loss: 0.1386\n",
      "Epoch [2/5], Step [6754/10336], Loss: 2.9453\n",
      "Epoch [2/5], Step [6756/10336], Loss: 0.1171\n",
      "Epoch [2/5], Step [6758/10336], Loss: 0.1756\n",
      "Epoch [2/5], Step [6760/10336], Loss: 0.6610\n",
      "Epoch [2/5], Step [6762/10336], Loss: 0.0472\n",
      "Epoch [2/5], Step [6764/10336], Loss: 0.1246\n",
      "Epoch [2/5], Step [6766/10336], Loss: 2.6039\n",
      "Epoch [2/5], Step [6768/10336], Loss: 0.7587\n",
      "Epoch [2/5], Step [6770/10336], Loss: 0.0226\n",
      "Epoch [2/5], Step [6772/10336], Loss: 0.0460\n",
      "Epoch [2/5], Step [6774/10336], Loss: 1.2701\n",
      "Epoch [2/5], Step [6776/10336], Loss: 0.3503\n",
      "Epoch [2/5], Step [6778/10336], Loss: 0.8459\n",
      "Epoch [2/5], Step [6780/10336], Loss: 0.3745\n",
      "Epoch [2/5], Step [6782/10336], Loss: 0.0002\n",
      "Epoch [2/5], Step [6784/10336], Loss: 0.7142\n",
      "Epoch [2/5], Step [6786/10336], Loss: 0.3505\n",
      "Epoch [2/5], Step [6788/10336], Loss: 0.3066\n",
      "Epoch [2/5], Step [6790/10336], Loss: 0.1626\n",
      "Epoch [2/5], Step [6792/10336], Loss: 0.3389\n",
      "Epoch [2/5], Step [6794/10336], Loss: 0.3770\n",
      "Epoch [2/5], Step [6796/10336], Loss: 0.2113\n",
      "Epoch [2/5], Step [6798/10336], Loss: 0.2414\n",
      "Epoch [2/5], Step [6800/10336], Loss: 0.3483\n",
      "Epoch [2/5], Step [6802/10336], Loss: 0.1838\n",
      "Epoch [2/5], Step [6804/10336], Loss: 0.0602\n",
      "Epoch [2/5], Step [6806/10336], Loss: 0.3794\n",
      "Epoch [2/5], Step [6808/10336], Loss: 0.0019\n",
      "Epoch [2/5], Step [6810/10336], Loss: 0.4524\n",
      "Epoch [2/5], Step [6812/10336], Loss: 1.5955\n",
      "Epoch [2/5], Step [6814/10336], Loss: 1.3536\n",
      "Epoch [2/5], Step [6816/10336], Loss: 0.1263\n",
      "Epoch [2/5], Step [6818/10336], Loss: 0.0043\n",
      "Epoch [2/5], Step [6820/10336], Loss: 0.0317\n",
      "Epoch [2/5], Step [6822/10336], Loss: 0.2445\n",
      "Epoch [2/5], Step [6824/10336], Loss: 0.2260\n",
      "Epoch [2/5], Step [6826/10336], Loss: 0.1077\n",
      "Epoch [2/5], Step [6828/10336], Loss: 0.6806\n",
      "Epoch [2/5], Step [6830/10336], Loss: 0.2474\n",
      "Epoch [2/5], Step [6832/10336], Loss: 0.0240\n",
      "Epoch [2/5], Step [6834/10336], Loss: 0.1443\n",
      "Epoch [2/5], Step [6836/10336], Loss: 2.0827\n",
      "Epoch [2/5], Step [6838/10336], Loss: 0.1796\n",
      "Epoch [2/5], Step [6840/10336], Loss: 1.2963\n",
      "Epoch [2/5], Step [6842/10336], Loss: 0.0070\n",
      "Epoch [2/5], Step [6844/10336], Loss: 0.0838\n",
      "Epoch [2/5], Step [6846/10336], Loss: 0.0258\n",
      "Epoch [2/5], Step [6848/10336], Loss: 0.2502\n",
      "Epoch [2/5], Step [6850/10336], Loss: 0.0238\n",
      "Epoch [2/5], Step [6852/10336], Loss: 0.0220\n",
      "Epoch [2/5], Step [6854/10336], Loss: 4.4397\n",
      "Epoch [2/5], Step [6856/10336], Loss: 2.4416\n",
      "Epoch [2/5], Step [6858/10336], Loss: 1.6943\n",
      "Epoch [2/5], Step [6860/10336], Loss: 3.8123\n",
      "Epoch [2/5], Step [6862/10336], Loss: 0.1490\n",
      "Epoch [2/5], Step [6864/10336], Loss: 2.3971\n",
      "Epoch [2/5], Step [6866/10336], Loss: 1.2068\n",
      "Epoch [2/5], Step [6868/10336], Loss: 0.1010\n",
      "Epoch [2/5], Step [6870/10336], Loss: 0.0341\n",
      "Epoch [2/5], Step [6872/10336], Loss: 3.9229\n",
      "Epoch [2/5], Step [6874/10336], Loss: 0.0624\n",
      "Epoch [2/5], Step [6876/10336], Loss: 1.7582\n",
      "Epoch [2/5], Step [6878/10336], Loss: 2.9316\n",
      "Epoch [2/5], Step [6880/10336], Loss: 0.2910\n",
      "Epoch [2/5], Step [6882/10336], Loss: 0.1730\n",
      "Epoch [2/5], Step [6884/10336], Loss: 0.7390\n",
      "Epoch [2/5], Step [6886/10336], Loss: 0.0135\n",
      "Epoch [2/5], Step [6888/10336], Loss: 0.0272\n",
      "Epoch [2/5], Step [6890/10336], Loss: 1.3694\n",
      "Epoch [2/5], Step [6892/10336], Loss: 0.0601\n",
      "Epoch [2/5], Step [6894/10336], Loss: 1.7003\n",
      "Epoch [2/5], Step [6896/10336], Loss: 0.1691\n",
      "Epoch [2/5], Step [6898/10336], Loss: 0.0091\n",
      "Epoch [2/5], Step [6900/10336], Loss: 0.1107\n",
      "Epoch [2/5], Step [6902/10336], Loss: 0.2385\n",
      "Epoch [2/5], Step [6904/10336], Loss: 0.3887\n",
      "Epoch [2/5], Step [6906/10336], Loss: 0.5955\n",
      "Epoch [2/5], Step [6908/10336], Loss: 0.0464\n",
      "Epoch [2/5], Step [6910/10336], Loss: 0.1871\n",
      "Epoch [2/5], Step [6912/10336], Loss: 0.6530\n",
      "Epoch [2/5], Step [6914/10336], Loss: 3.4015\n",
      "Epoch [2/5], Step [6916/10336], Loss: 0.2528\n",
      "Epoch [2/5], Step [6918/10336], Loss: 0.0174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [6920/10336], Loss: 0.0116\n",
      "Epoch [2/5], Step [6922/10336], Loss: 0.0807\n",
      "Epoch [2/5], Step [6924/10336], Loss: 4.3231\n",
      "Epoch [2/5], Step [6926/10336], Loss: 1.0903\n",
      "Epoch [2/5], Step [6928/10336], Loss: 3.9727\n",
      "Epoch [2/5], Step [6930/10336], Loss: 0.4197\n",
      "Epoch [2/5], Step [6932/10336], Loss: 2.3222\n",
      "Epoch [2/5], Step [6934/10336], Loss: 0.3601\n",
      "Epoch [2/5], Step [6936/10336], Loss: 0.0157\n",
      "Epoch [2/5], Step [6938/10336], Loss: 0.7029\n",
      "Epoch [2/5], Step [6940/10336], Loss: 0.1465\n",
      "Epoch [2/5], Step [6942/10336], Loss: 0.2214\n",
      "Epoch [2/5], Step [6944/10336], Loss: 0.5284\n",
      "Epoch [2/5], Step [6946/10336], Loss: 0.2661\n",
      "Epoch [2/5], Step [6948/10336], Loss: 2.5900\n",
      "Epoch [2/5], Step [6950/10336], Loss: 0.3534\n",
      "Epoch [2/5], Step [6952/10336], Loss: 0.2339\n",
      "Epoch [2/5], Step [6954/10336], Loss: 0.0357\n",
      "Epoch [2/5], Step [6956/10336], Loss: 0.0762\n",
      "Epoch [2/5], Step [6958/10336], Loss: 0.5352\n",
      "Epoch [2/5], Step [6960/10336], Loss: 2.4345\n",
      "Epoch [2/5], Step [6962/10336], Loss: 0.4121\n",
      "Epoch [2/5], Step [6964/10336], Loss: 0.3127\n",
      "Epoch [2/5], Step [6966/10336], Loss: 0.4544\n",
      "Epoch [2/5], Step [6968/10336], Loss: 0.2670\n",
      "Epoch [2/5], Step [6970/10336], Loss: 0.5791\n",
      "Epoch [2/5], Step [6972/10336], Loss: 0.0128\n",
      "Epoch [2/5], Step [6974/10336], Loss: 0.6680\n",
      "Epoch [2/5], Step [6976/10336], Loss: 0.3634\n",
      "Epoch [2/5], Step [6978/10336], Loss: 0.0031\n",
      "Epoch [2/5], Step [6980/10336], Loss: 0.5757\n",
      "Epoch [2/5], Step [6982/10336], Loss: 0.2149\n",
      "Epoch [2/5], Step [6984/10336], Loss: 0.7965\n",
      "Epoch [2/5], Step [6986/10336], Loss: 0.1652\n",
      "Epoch [2/5], Step [6988/10336], Loss: 0.4483\n",
      "Epoch [2/5], Step [6990/10336], Loss: 0.7279\n",
      "Epoch [2/5], Step [6992/10336], Loss: 0.3111\n",
      "Epoch [2/5], Step [6994/10336], Loss: 2.9647\n",
      "Epoch [2/5], Step [6996/10336], Loss: 0.4787\n",
      "Epoch [2/5], Step [6998/10336], Loss: 0.2971\n",
      "Epoch [2/5], Step [7000/10336], Loss: 0.2336\n",
      "Epoch [2/5], Step [7002/10336], Loss: 0.2301\n",
      "Epoch [2/5], Step [7004/10336], Loss: 4.7665\n",
      "Epoch [2/5], Step [7006/10336], Loss: 0.3339\n",
      "Epoch [2/5], Step [7008/10336], Loss: 0.6221\n",
      "Epoch [2/5], Step [7010/10336], Loss: 0.0565\n",
      "Epoch [2/5], Step [7012/10336], Loss: 2.1161\n",
      "Epoch [2/5], Step [7014/10336], Loss: 3.3735\n",
      "Epoch [2/5], Step [7016/10336], Loss: 0.7768\n",
      "Epoch [2/5], Step [7018/10336], Loss: 0.0057\n",
      "Epoch [2/5], Step [7020/10336], Loss: 0.0622\n",
      "Epoch [2/5], Step [7022/10336], Loss: 0.0735\n",
      "Epoch [2/5], Step [7024/10336], Loss: 3.2525\n",
      "Epoch [2/5], Step [7026/10336], Loss: 0.0060\n",
      "Epoch [2/5], Step [7028/10336], Loss: 1.1114\n",
      "Epoch [2/5], Step [7030/10336], Loss: 1.3454\n",
      "Epoch [2/5], Step [7032/10336], Loss: 0.0461\n",
      "Epoch [2/5], Step [7034/10336], Loss: 0.4093\n",
      "Epoch [2/5], Step [7036/10336], Loss: 0.9315\n",
      "Epoch [2/5], Step [7038/10336], Loss: 2.1469\n",
      "Epoch [2/5], Step [7040/10336], Loss: 0.5508\n",
      "Epoch [2/5], Step [7042/10336], Loss: 3.4910\n",
      "Epoch [2/5], Step [7044/10336], Loss: 0.2552\n",
      "Epoch [2/5], Step [7046/10336], Loss: 1.4024\n",
      "Epoch [2/5], Step [7048/10336], Loss: 0.5674\n",
      "Epoch [2/5], Step [7050/10336], Loss: 0.3376\n",
      "Epoch [2/5], Step [7052/10336], Loss: 0.0213\n",
      "Epoch [2/5], Step [7054/10336], Loss: 0.3609\n",
      "Epoch [2/5], Step [7056/10336], Loss: 0.2420\n",
      "Epoch [2/5], Step [7058/10336], Loss: 2.0005\n",
      "Epoch [2/5], Step [7060/10336], Loss: 0.5192\n",
      "Epoch [2/5], Step [7062/10336], Loss: 0.2069\n",
      "Epoch [2/5], Step [7064/10336], Loss: 2.5963\n",
      "Epoch [2/5], Step [7066/10336], Loss: 1.0343\n",
      "Epoch [2/5], Step [7068/10336], Loss: 0.2916\n",
      "Epoch [2/5], Step [7070/10336], Loss: 0.1405\n",
      "Epoch [2/5], Step [7072/10336], Loss: 0.0179\n",
      "Epoch [2/5], Step [7074/10336], Loss: 2.4233\n",
      "Epoch [2/5], Step [7076/10336], Loss: 2.0171\n",
      "Epoch [2/5], Step [7078/10336], Loss: 0.6635\n",
      "Epoch [2/5], Step [7080/10336], Loss: 1.3530\n",
      "Epoch [2/5], Step [7082/10336], Loss: 0.1144\n",
      "Epoch [2/5], Step [7084/10336], Loss: 0.0085\n",
      "Epoch [2/5], Step [7086/10336], Loss: 2.2126\n",
      "Epoch [2/5], Step [7088/10336], Loss: 0.2033\n",
      "Epoch [2/5], Step [7090/10336], Loss: 0.3437\n",
      "Epoch [2/5], Step [7092/10336], Loss: 0.3350\n",
      "Epoch [2/5], Step [7094/10336], Loss: 0.0596\n",
      "Epoch [2/5], Step [7096/10336], Loss: 0.4092\n",
      "Epoch [2/5], Step [7098/10336], Loss: 4.0881\n",
      "Epoch [2/5], Step [7100/10336], Loss: 0.3951\n",
      "Epoch [2/5], Step [7102/10336], Loss: 1.6698\n",
      "Epoch [2/5], Step [7104/10336], Loss: 3.6525\n",
      "Epoch [2/5], Step [7106/10336], Loss: 1.2178\n",
      "Epoch [2/5], Step [7108/10336], Loss: 0.1590\n",
      "Epoch [2/5], Step [7110/10336], Loss: 0.9513\n",
      "Epoch [2/5], Step [7112/10336], Loss: 0.1193\n",
      "Epoch [2/5], Step [7114/10336], Loss: 1.6032\n",
      "Epoch [2/5], Step [7116/10336], Loss: 0.4240\n",
      "Epoch [2/5], Step [7118/10336], Loss: 3.1120\n",
      "Epoch [2/5], Step [7120/10336], Loss: 0.6185\n",
      "Epoch [2/5], Step [7122/10336], Loss: 0.4663\n",
      "Epoch [2/5], Step [7124/10336], Loss: 0.2862\n",
      "Epoch [2/5], Step [7126/10336], Loss: 0.7041\n",
      "Epoch [2/5], Step [7128/10336], Loss: 0.0313\n",
      "Epoch [2/5], Step [7130/10336], Loss: 4.4991\n",
      "Epoch [2/5], Step [7132/10336], Loss: 0.2789\n",
      "Epoch [2/5], Step [7134/10336], Loss: 0.2277\n",
      "Epoch [2/5], Step [7136/10336], Loss: 0.4645\n",
      "Epoch [2/5], Step [7138/10336], Loss: 0.3504\n",
      "Epoch [2/5], Step [7140/10336], Loss: 0.1544\n",
      "Epoch [2/5], Step [7142/10336], Loss: 2.0594\n",
      "Epoch [2/5], Step [7144/10336], Loss: 0.1274\n",
      "Epoch [2/5], Step [7146/10336], Loss: 1.3087\n",
      "Epoch [2/5], Step [7148/10336], Loss: 0.6054\n",
      "Epoch [2/5], Step [7150/10336], Loss: 1.6884\n",
      "Epoch [2/5], Step [7152/10336], Loss: 0.0139\n",
      "Epoch [2/5], Step [7154/10336], Loss: 0.2916\n",
      "Epoch [2/5], Step [7156/10336], Loss: 0.2885\n",
      "Epoch [2/5], Step [7158/10336], Loss: 0.3360\n",
      "Epoch [2/5], Step [7160/10336], Loss: 0.0047\n",
      "Epoch [2/5], Step [7162/10336], Loss: 0.4496\n",
      "Epoch [2/5], Step [7164/10336], Loss: 0.4512\n",
      "Epoch [2/5], Step [7166/10336], Loss: 0.4722\n",
      "Epoch [2/5], Step [7168/10336], Loss: 0.1310\n",
      "Epoch [2/5], Step [7170/10336], Loss: 0.0038\n",
      "Epoch [2/5], Step [7172/10336], Loss: 0.1028\n",
      "Epoch [2/5], Step [7174/10336], Loss: 4.9374\n",
      "Epoch [2/5], Step [7176/10336], Loss: 0.1876\n",
      "Epoch [2/5], Step [7178/10336], Loss: 0.2743\n",
      "Epoch [2/5], Step [7180/10336], Loss: 0.0066\n",
      "Epoch [2/5], Step [7182/10336], Loss: 2.8163\n",
      "Epoch [2/5], Step [7184/10336], Loss: 0.0144\n",
      "Epoch [2/5], Step [7186/10336], Loss: 0.3900\n",
      "Epoch [2/5], Step [7188/10336], Loss: 0.2389\n",
      "Epoch [2/5], Step [7190/10336], Loss: 3.9673\n",
      "Epoch [2/5], Step [7192/10336], Loss: 3.6756\n",
      "Epoch [2/5], Step [7194/10336], Loss: 0.3210\n",
      "Epoch [2/5], Step [7196/10336], Loss: 1.1333\n",
      "Epoch [2/5], Step [7198/10336], Loss: 0.1204\n",
      "Epoch [2/5], Step [7200/10336], Loss: 0.3149\n",
      "Epoch [2/5], Step [7202/10336], Loss: 1.4108\n",
      "Epoch [2/5], Step [7204/10336], Loss: 2.9347\n",
      "Epoch [2/5], Step [7206/10336], Loss: 1.8835\n",
      "Epoch [2/5], Step [7208/10336], Loss: 0.3191\n",
      "Epoch [2/5], Step [7210/10336], Loss: 1.2935\n",
      "Epoch [2/5], Step [7212/10336], Loss: 0.0092\n",
      "Epoch [2/5], Step [7214/10336], Loss: 0.0858\n",
      "Epoch [2/5], Step [7216/10336], Loss: 0.1659\n",
      "Epoch [2/5], Step [7218/10336], Loss: 0.8436\n",
      "Epoch [2/5], Step [7220/10336], Loss: 0.0020\n",
      "Epoch [2/5], Step [7222/10336], Loss: 0.8458\n",
      "Epoch [2/5], Step [7224/10336], Loss: 0.4233\n",
      "Epoch [2/5], Step [7226/10336], Loss: 0.1079\n",
      "Epoch [2/5], Step [7228/10336], Loss: 0.2009\n",
      "Epoch [2/5], Step [7230/10336], Loss: 0.2555\n",
      "Epoch [2/5], Step [7232/10336], Loss: 0.8134\n",
      "Epoch [2/5], Step [7234/10336], Loss: 0.1502\n",
      "Epoch [2/5], Step [7236/10336], Loss: 0.0902\n",
      "Epoch [2/5], Step [7238/10336], Loss: 2.7931\n",
      "Epoch [2/5], Step [7240/10336], Loss: 1.9629\n",
      "Epoch [2/5], Step [7242/10336], Loss: 0.2881\n",
      "Epoch [2/5], Step [7244/10336], Loss: 2.0135\n",
      "Epoch [2/5], Step [7246/10336], Loss: 0.4330\n",
      "Epoch [2/5], Step [7248/10336], Loss: 0.3781\n",
      "Epoch [2/5], Step [7250/10336], Loss: 3.2766\n",
      "Epoch [2/5], Step [7252/10336], Loss: 0.3166\n",
      "Epoch [2/5], Step [7254/10336], Loss: 2.3863\n",
      "Epoch [2/5], Step [7256/10336], Loss: 0.2225\n",
      "Epoch [2/5], Step [7258/10336], Loss: 1.0413\n",
      "Epoch [2/5], Step [7260/10336], Loss: 2.4338\n",
      "Epoch [2/5], Step [7262/10336], Loss: 0.6713\n",
      "Epoch [2/5], Step [7264/10336], Loss: 0.0871\n",
      "Epoch [2/5], Step [7266/10336], Loss: 0.0532\n",
      "Epoch [2/5], Step [7268/10336], Loss: 0.1546\n",
      "Epoch [2/5], Step [7270/10336], Loss: 0.7703\n",
      "Epoch [2/5], Step [7272/10336], Loss: 0.0846\n",
      "Epoch [2/5], Step [7274/10336], Loss: 0.0085\n",
      "Epoch [2/5], Step [7276/10336], Loss: 2.5876\n",
      "Epoch [2/5], Step [7278/10336], Loss: 0.2609\n",
      "Epoch [2/5], Step [7280/10336], Loss: 0.9287\n",
      "Epoch [2/5], Step [7282/10336], Loss: 0.0966\n",
      "Epoch [2/5], Step [7284/10336], Loss: 1.5325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [7286/10336], Loss: 1.1488\n",
      "Epoch [2/5], Step [7288/10336], Loss: 0.4657\n",
      "Epoch [2/5], Step [7290/10336], Loss: 0.8074\n",
      "Epoch [2/5], Step [7292/10336], Loss: 0.4307\n",
      "Epoch [2/5], Step [7294/10336], Loss: 0.4710\n",
      "Epoch [2/5], Step [7296/10336], Loss: 0.2243\n",
      "Epoch [2/5], Step [7298/10336], Loss: 1.0087\n",
      "Epoch [2/5], Step [7300/10336], Loss: 0.2321\n",
      "Epoch [2/5], Step [7302/10336], Loss: 1.5277\n",
      "Epoch [2/5], Step [7304/10336], Loss: 0.3462\n",
      "Epoch [2/5], Step [7306/10336], Loss: 0.0329\n",
      "Epoch [2/5], Step [7308/10336], Loss: 0.2165\n",
      "Epoch [2/5], Step [7310/10336], Loss: 0.1135\n",
      "Epoch [2/5], Step [7312/10336], Loss: 0.1602\n",
      "Epoch [2/5], Step [7314/10336], Loss: 1.3539\n",
      "Epoch [2/5], Step [7316/10336], Loss: 0.2260\n",
      "Epoch [2/5], Step [7318/10336], Loss: 0.8489\n",
      "Epoch [2/5], Step [7320/10336], Loss: 0.0004\n",
      "Epoch [2/5], Step [7322/10336], Loss: 0.9036\n",
      "Epoch [2/5], Step [7324/10336], Loss: 0.0088\n",
      "Epoch [2/5], Step [7326/10336], Loss: 0.4945\n",
      "Epoch [2/5], Step [7328/10336], Loss: 0.6708\n",
      "Epoch [2/5], Step [7330/10336], Loss: 0.2197\n",
      "Epoch [2/5], Step [7332/10336], Loss: 3.6379\n",
      "Epoch [2/5], Step [7334/10336], Loss: 4.3493\n",
      "Epoch [2/5], Step [7336/10336], Loss: 0.0898\n",
      "Epoch [2/5], Step [7338/10336], Loss: 0.3145\n",
      "Epoch [2/5], Step [7340/10336], Loss: 0.2200\n",
      "Epoch [2/5], Step [7342/10336], Loss: 0.2927\n",
      "Epoch [2/5], Step [7344/10336], Loss: 1.0812\n",
      "Epoch [2/5], Step [7346/10336], Loss: 0.5703\n",
      "Epoch [2/5], Step [7348/10336], Loss: 0.1736\n",
      "Epoch [2/5], Step [7350/10336], Loss: 0.1969\n",
      "Epoch [2/5], Step [7352/10336], Loss: 0.4220\n",
      "Epoch [2/5], Step [7354/10336], Loss: 0.0438\n",
      "Epoch [2/5], Step [7356/10336], Loss: 0.2106\n",
      "Epoch [2/5], Step [7358/10336], Loss: 0.1331\n",
      "Epoch [2/5], Step [7360/10336], Loss: 1.3915\n",
      "Epoch [2/5], Step [7362/10336], Loss: 0.0470\n",
      "Epoch [2/5], Step [7364/10336], Loss: 0.2480\n",
      "Epoch [2/5], Step [7366/10336], Loss: 1.4560\n",
      "Epoch [2/5], Step [7368/10336], Loss: 0.2110\n",
      "Epoch [2/5], Step [7370/10336], Loss: 0.1713\n",
      "Epoch [2/5], Step [7372/10336], Loss: 4.1163\n",
      "Epoch [2/5], Step [7374/10336], Loss: 1.3570\n",
      "Epoch [2/5], Step [7376/10336], Loss: 0.2871\n",
      "Epoch [2/5], Step [7378/10336], Loss: 0.0155\n",
      "Epoch [2/5], Step [7380/10336], Loss: 1.8765\n",
      "Epoch [2/5], Step [7382/10336], Loss: 0.0943\n",
      "Epoch [2/5], Step [7384/10336], Loss: 2.1237\n",
      "Epoch [2/5], Step [7386/10336], Loss: 0.1090\n",
      "Epoch [2/5], Step [7388/10336], Loss: 2.4832\n",
      "Epoch [2/5], Step [7390/10336], Loss: 2.1581\n",
      "Epoch [2/5], Step [7392/10336], Loss: 0.2794\n",
      "Epoch [2/5], Step [7394/10336], Loss: 0.5151\n",
      "Epoch [2/5], Step [7396/10336], Loss: 0.2287\n",
      "Epoch [2/5], Step [7398/10336], Loss: 0.8975\n",
      "Epoch [2/5], Step [7400/10336], Loss: 0.1601\n",
      "Epoch [2/5], Step [7402/10336], Loss: 0.4692\n",
      "Epoch [2/5], Step [7404/10336], Loss: 1.3437\n",
      "Epoch [2/5], Step [7406/10336], Loss: 0.2569\n",
      "Epoch [2/5], Step [7408/10336], Loss: 5.1583\n",
      "Epoch [2/5], Step [7410/10336], Loss: 0.1727\n",
      "Epoch [2/5], Step [7412/10336], Loss: 0.5694\n",
      "Epoch [2/5], Step [7414/10336], Loss: 0.1725\n",
      "Epoch [2/5], Step [7416/10336], Loss: 0.1309\n",
      "Epoch [2/5], Step [7418/10336], Loss: 2.2818\n",
      "Epoch [2/5], Step [7420/10336], Loss: 0.4748\n",
      "Epoch [2/5], Step [7422/10336], Loss: 0.2919\n",
      "Epoch [2/5], Step [7424/10336], Loss: 2.4439\n",
      "Epoch [2/5], Step [7426/10336], Loss: 0.4142\n",
      "Epoch [2/5], Step [7428/10336], Loss: 0.0140\n",
      "Epoch [2/5], Step [7430/10336], Loss: 0.0545\n",
      "Epoch [2/5], Step [7432/10336], Loss: 0.0554\n",
      "Epoch [2/5], Step [7434/10336], Loss: 0.0971\n",
      "Epoch [2/5], Step [7436/10336], Loss: 0.2693\n",
      "Epoch [2/5], Step [7438/10336], Loss: 0.1406\n",
      "Epoch [2/5], Step [7440/10336], Loss: 0.4353\n",
      "Epoch [2/5], Step [7442/10336], Loss: 1.7041\n",
      "Epoch [2/5], Step [7444/10336], Loss: 1.0847\n",
      "Epoch [2/5], Step [7446/10336], Loss: 0.2370\n",
      "Epoch [2/5], Step [7448/10336], Loss: 4.1422\n",
      "Epoch [2/5], Step [7450/10336], Loss: 3.0049\n",
      "Epoch [2/5], Step [7452/10336], Loss: 2.4540\n",
      "Epoch [2/5], Step [7454/10336], Loss: 0.0003\n",
      "Epoch [2/5], Step [7456/10336], Loss: 0.1587\n",
      "Epoch [2/5], Step [7458/10336], Loss: 2.1761\n",
      "Epoch [2/5], Step [7460/10336], Loss: 0.3727\n",
      "Epoch [2/5], Step [7462/10336], Loss: 0.2894\n",
      "Epoch [2/5], Step [7464/10336], Loss: 0.3476\n",
      "Epoch [2/5], Step [7466/10336], Loss: 0.1077\n",
      "Epoch [2/5], Step [7468/10336], Loss: 1.0362\n",
      "Epoch [2/5], Step [7470/10336], Loss: 1.0137\n",
      "Epoch [2/5], Step [7472/10336], Loss: 0.4673\n",
      "Epoch [2/5], Step [7474/10336], Loss: 2.1368\n",
      "Epoch [2/5], Step [7476/10336], Loss: 0.2025\n",
      "Epoch [2/5], Step [7478/10336], Loss: 0.6695\n",
      "Epoch [2/5], Step [7480/10336], Loss: 0.6275\n",
      "Epoch [2/5], Step [7482/10336], Loss: 0.0264\n",
      "Epoch [2/5], Step [7484/10336], Loss: 0.1078\n",
      "Epoch [2/5], Step [7486/10336], Loss: 0.2192\n",
      "Epoch [2/5], Step [7488/10336], Loss: 0.0282\n",
      "Epoch [2/5], Step [7490/10336], Loss: 0.3177\n",
      "Epoch [2/5], Step [7492/10336], Loss: 0.3625\n",
      "Epoch [2/5], Step [7494/10336], Loss: 3.1192\n",
      "Epoch [2/5], Step [7496/10336], Loss: 2.9171\n",
      "Epoch [2/5], Step [7498/10336], Loss: 0.0708\n",
      "Epoch [2/5], Step [7500/10336], Loss: 0.0143\n",
      "Epoch [2/5], Step [7502/10336], Loss: 0.2753\n",
      "Epoch [2/5], Step [7504/10336], Loss: 4.8831\n",
      "Epoch [2/5], Step [7506/10336], Loss: 0.2299\n",
      "Epoch [2/5], Step [7508/10336], Loss: 0.4093\n",
      "Epoch [2/5], Step [7510/10336], Loss: 0.3409\n",
      "Epoch [2/5], Step [7512/10336], Loss: 0.0980\n",
      "Epoch [2/5], Step [7514/10336], Loss: 4.5280\n",
      "Epoch [2/5], Step [7516/10336], Loss: 1.0726\n",
      "Epoch [2/5], Step [7518/10336], Loss: 0.7162\n",
      "Epoch [2/5], Step [7520/10336], Loss: 1.0465\n",
      "Epoch [2/5], Step [7522/10336], Loss: 2.1597\n",
      "Epoch [2/5], Step [7524/10336], Loss: 0.2439\n",
      "Epoch [2/5], Step [7526/10336], Loss: 0.6587\n",
      "Epoch [2/5], Step [7528/10336], Loss: 2.3805\n",
      "Epoch [2/5], Step [7530/10336], Loss: 3.4335\n",
      "Epoch [2/5], Step [7532/10336], Loss: 0.7484\n",
      "Epoch [2/5], Step [7534/10336], Loss: 0.1753\n",
      "Epoch [2/5], Step [7536/10336], Loss: 0.4659\n",
      "Epoch [2/5], Step [7538/10336], Loss: 0.0591\n",
      "Epoch [2/5], Step [7540/10336], Loss: 0.1509\n",
      "Epoch [2/5], Step [7542/10336], Loss: 4.0792\n",
      "Epoch [2/5], Step [7544/10336], Loss: 1.8765\n",
      "Epoch [2/5], Step [7546/10336], Loss: 0.3585\n",
      "Epoch [2/5], Step [7548/10336], Loss: 1.8953\n",
      "Epoch [2/5], Step [7550/10336], Loss: 0.5592\n",
      "Epoch [2/5], Step [7552/10336], Loss: 0.9134\n",
      "Epoch [2/5], Step [7554/10336], Loss: 0.2732\n",
      "Epoch [2/5], Step [7556/10336], Loss: 1.1384\n",
      "Epoch [2/5], Step [7558/10336], Loss: 1.6702\n",
      "Epoch [2/5], Step [7560/10336], Loss: 0.5734\n",
      "Epoch [2/5], Step [7562/10336], Loss: 0.0308\n",
      "Epoch [2/5], Step [7564/10336], Loss: 0.5637\n",
      "Epoch [2/5], Step [7566/10336], Loss: 0.1754\n",
      "Epoch [2/5], Step [7568/10336], Loss: 0.0722\n",
      "Epoch [2/5], Step [7570/10336], Loss: 0.2703\n",
      "Epoch [2/5], Step [7572/10336], Loss: 0.2927\n",
      "Epoch [2/5], Step [7574/10336], Loss: 2.7758\n",
      "Epoch [2/5], Step [7576/10336], Loss: 4.0311\n",
      "Epoch [2/5], Step [7578/10336], Loss: 1.0033\n",
      "Epoch [2/5], Step [7580/10336], Loss: 0.3402\n",
      "Epoch [2/5], Step [7582/10336], Loss: 1.9770\n",
      "Epoch [2/5], Step [7584/10336], Loss: 0.3581\n",
      "Epoch [2/5], Step [7586/10336], Loss: 1.3384\n",
      "Epoch [2/5], Step [7588/10336], Loss: 0.8033\n",
      "Epoch [2/5], Step [7590/10336], Loss: 0.1487\n",
      "Epoch [2/5], Step [7592/10336], Loss: 0.5446\n",
      "Epoch [2/5], Step [7594/10336], Loss: 0.6898\n",
      "Epoch [2/5], Step [7596/10336], Loss: 0.2139\n",
      "Epoch [2/5], Step [7598/10336], Loss: 0.3031\n",
      "Epoch [2/5], Step [7600/10336], Loss: 1.2476\n",
      "Epoch [2/5], Step [7602/10336], Loss: 1.5252\n",
      "Epoch [2/5], Step [7604/10336], Loss: 0.2005\n",
      "Epoch [2/5], Step [7606/10336], Loss: 0.4913\n",
      "Epoch [2/5], Step [7608/10336], Loss: 0.0490\n",
      "Epoch [2/5], Step [7610/10336], Loss: 0.4242\n",
      "Epoch [2/5], Step [7612/10336], Loss: 0.3579\n",
      "Epoch [2/5], Step [7614/10336], Loss: 0.6318\n",
      "Epoch [2/5], Step [7616/10336], Loss: 4.6864\n",
      "Epoch [2/5], Step [7618/10336], Loss: 0.2607\n",
      "Epoch [2/5], Step [7620/10336], Loss: 0.1744\n",
      "Epoch [2/5], Step [7622/10336], Loss: 4.0319\n",
      "Epoch [2/5], Step [7624/10336], Loss: 0.1857\n",
      "Epoch [2/5], Step [7626/10336], Loss: 4.5269\n",
      "Epoch [2/5], Step [7628/10336], Loss: 0.3882\n",
      "Epoch [2/5], Step [7630/10336], Loss: 0.0041\n",
      "Epoch [2/5], Step [7632/10336], Loss: 2.1914\n",
      "Epoch [2/5], Step [7634/10336], Loss: 0.0955\n",
      "Epoch [2/5], Step [7636/10336], Loss: 0.0219\n",
      "Epoch [2/5], Step [7638/10336], Loss: 0.0630\n",
      "Epoch [2/5], Step [7640/10336], Loss: 5.7400\n",
      "Epoch [2/5], Step [7642/10336], Loss: 0.8255\n",
      "Epoch [2/5], Step [7644/10336], Loss: 0.0613\n",
      "Epoch [2/5], Step [7646/10336], Loss: 0.0732\n",
      "Epoch [2/5], Step [7648/10336], Loss: 0.3691\n",
      "Epoch [2/5], Step [7650/10336], Loss: 0.1748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [7652/10336], Loss: 1.1113\n",
      "Epoch [2/5], Step [7654/10336], Loss: 1.0103\n",
      "Epoch [2/5], Step [7656/10336], Loss: 0.3096\n",
      "Epoch [2/5], Step [7658/10336], Loss: 0.0615\n",
      "Epoch [2/5], Step [7660/10336], Loss: 0.6842\n",
      "Epoch [2/5], Step [7662/10336], Loss: 0.3822\n",
      "Epoch [2/5], Step [7664/10336], Loss: 0.2377\n",
      "Epoch [2/5], Step [7666/10336], Loss: 6.8897\n",
      "Epoch [2/5], Step [7668/10336], Loss: 0.6189\n",
      "Epoch [2/5], Step [7670/10336], Loss: 0.0443\n",
      "Epoch [2/5], Step [7672/10336], Loss: 0.4794\n",
      "Epoch [2/5], Step [7674/10336], Loss: 2.1999\n",
      "Epoch [2/5], Step [7676/10336], Loss: 0.0840\n",
      "Epoch [2/5], Step [7678/10336], Loss: 0.0377\n",
      "Epoch [2/5], Step [7680/10336], Loss: 0.1735\n",
      "Epoch [2/5], Step [7682/10336], Loss: 1.4468\n",
      "Epoch [2/5], Step [7684/10336], Loss: 0.0021\n",
      "Epoch [2/5], Step [7686/10336], Loss: 0.0821\n",
      "Epoch [2/5], Step [7688/10336], Loss: 2.9640\n",
      "Epoch [2/5], Step [7690/10336], Loss: 0.6711\n",
      "Epoch [2/5], Step [7692/10336], Loss: 0.1257\n",
      "Epoch [2/5], Step [7694/10336], Loss: 0.9675\n",
      "Epoch [2/5], Step [7696/10336], Loss: 2.2888\n",
      "Epoch [2/5], Step [7698/10336], Loss: 1.1300\n",
      "Epoch [2/5], Step [7700/10336], Loss: 2.6227\n",
      "Epoch [2/5], Step [7702/10336], Loss: 0.1764\n",
      "Epoch [2/5], Step [7704/10336], Loss: 0.4027\n",
      "Epoch [2/5], Step [7706/10336], Loss: 0.0355\n",
      "Epoch [2/5], Step [7708/10336], Loss: 0.3234\n",
      "Epoch [2/5], Step [7710/10336], Loss: 0.0113\n",
      "Epoch [2/5], Step [7712/10336], Loss: 0.6691\n",
      "Epoch [2/5], Step [7714/10336], Loss: 0.4921\n",
      "Epoch [2/5], Step [7716/10336], Loss: 0.1873\n",
      "Epoch [2/5], Step [7718/10336], Loss: 2.4737\n",
      "Epoch [2/5], Step [7720/10336], Loss: 0.1742\n",
      "Epoch [2/5], Step [7722/10336], Loss: 1.5110\n",
      "Epoch [2/5], Step [7724/10336], Loss: 1.9147\n",
      "Epoch [2/5], Step [7726/10336], Loss: 0.0485\n",
      "Epoch [2/5], Step [7728/10336], Loss: 0.8380\n",
      "Epoch [2/5], Step [7730/10336], Loss: 0.1386\n",
      "Epoch [2/5], Step [7732/10336], Loss: 0.0177\n",
      "Epoch [2/5], Step [7734/10336], Loss: 0.7550\n",
      "Epoch [2/5], Step [7736/10336], Loss: 0.2596\n",
      "Epoch [2/5], Step [7738/10336], Loss: 0.4155\n",
      "Epoch [2/5], Step [7740/10336], Loss: 2.6115\n",
      "Epoch [2/5], Step [7742/10336], Loss: 3.9766\n",
      "Epoch [2/5], Step [7744/10336], Loss: 5.4868\n",
      "Epoch [2/5], Step [7746/10336], Loss: 0.0408\n",
      "Epoch [2/5], Step [7748/10336], Loss: 0.9316\n",
      "Epoch [2/5], Step [7750/10336], Loss: 0.2054\n",
      "Epoch [2/5], Step [7752/10336], Loss: 0.7653\n",
      "Epoch [2/5], Step [7754/10336], Loss: 2.1226\n",
      "Epoch [2/5], Step [7756/10336], Loss: 0.7006\n",
      "Epoch [2/5], Step [7758/10336], Loss: 1.6982\n",
      "Epoch [2/5], Step [7760/10336], Loss: 2.9204\n",
      "Epoch [2/5], Step [7762/10336], Loss: 0.0479\n",
      "Epoch [2/5], Step [7764/10336], Loss: 2.6149\n",
      "Epoch [2/5], Step [7766/10336], Loss: 1.8064\n",
      "Epoch [2/5], Step [7768/10336], Loss: 0.0080\n",
      "Epoch [2/5], Step [7770/10336], Loss: 1.0468\n",
      "Epoch [2/5], Step [7772/10336], Loss: 0.3517\n",
      "Epoch [2/5], Step [7774/10336], Loss: 0.8828\n",
      "Epoch [2/5], Step [7776/10336], Loss: 0.2546\n",
      "Epoch [2/5], Step [7778/10336], Loss: 0.3773\n",
      "Epoch [2/5], Step [7780/10336], Loss: 0.2705\n",
      "Epoch [2/5], Step [7782/10336], Loss: 1.6182\n",
      "Epoch [2/5], Step [7784/10336], Loss: 0.0201\n",
      "Epoch [2/5], Step [7786/10336], Loss: 0.7031\n",
      "Epoch [2/5], Step [7788/10336], Loss: 0.0118\n",
      "Epoch [2/5], Step [7790/10336], Loss: 3.6625\n",
      "Epoch [2/5], Step [7792/10336], Loss: 0.0852\n",
      "Epoch [2/5], Step [7794/10336], Loss: 0.0578\n",
      "Epoch [2/5], Step [7796/10336], Loss: 0.8788\n",
      "Epoch [2/5], Step [7798/10336], Loss: 1.7739\n",
      "Epoch [2/5], Step [7800/10336], Loss: 0.5023\n",
      "Epoch [2/5], Step [7802/10336], Loss: 0.0180\n",
      "Epoch [2/5], Step [7804/10336], Loss: 1.2487\n",
      "Epoch [2/5], Step [7806/10336], Loss: 0.0577\n",
      "Epoch [2/5], Step [7808/10336], Loss: 0.2317\n",
      "Epoch [2/5], Step [7810/10336], Loss: 0.0386\n",
      "Epoch [2/5], Step [7812/10336], Loss: 0.6357\n",
      "Epoch [2/5], Step [7814/10336], Loss: 0.2099\n",
      "Epoch [2/5], Step [7816/10336], Loss: 1.3266\n",
      "Epoch [2/5], Step [7818/10336], Loss: 3.8380\n",
      "Epoch [2/5], Step [7820/10336], Loss: 0.0895\n",
      "Epoch [2/5], Step [7822/10336], Loss: 2.9485\n",
      "Epoch [2/5], Step [7824/10336], Loss: 0.3107\n",
      "Epoch [2/5], Step [7826/10336], Loss: 2.5237\n",
      "Epoch [2/5], Step [7828/10336], Loss: 0.5020\n",
      "Epoch [2/5], Step [7830/10336], Loss: 3.5825\n",
      "Epoch [2/5], Step [7832/10336], Loss: 2.3477\n",
      "Epoch [2/5], Step [7834/10336], Loss: 0.7163\n",
      "Epoch [2/5], Step [7836/10336], Loss: 0.5867\n",
      "Epoch [2/5], Step [7838/10336], Loss: 0.1303\n",
      "Epoch [2/5], Step [7840/10336], Loss: 0.1569\n",
      "Epoch [2/5], Step [7842/10336], Loss: 3.4435\n",
      "Epoch [2/5], Step [7844/10336], Loss: 0.2058\n",
      "Epoch [2/5], Step [7846/10336], Loss: 0.2105\n",
      "Epoch [2/5], Step [7848/10336], Loss: 2.5990\n",
      "Epoch [2/5], Step [7850/10336], Loss: 0.4636\n",
      "Epoch [2/5], Step [7852/10336], Loss: 2.5129\n",
      "Epoch [2/5], Step [7854/10336], Loss: 2.6442\n",
      "Epoch [2/5], Step [7856/10336], Loss: 0.0682\n",
      "Epoch [2/5], Step [7858/10336], Loss: 0.3833\n",
      "Epoch [2/5], Step [7860/10336], Loss: 0.3388\n",
      "Epoch [2/5], Step [7862/10336], Loss: 0.3661\n",
      "Epoch [2/5], Step [7864/10336], Loss: 0.6638\n",
      "Epoch [2/5], Step [7866/10336], Loss: 0.1076\n",
      "Epoch [2/5], Step [7868/10336], Loss: 1.1622\n",
      "Epoch [2/5], Step [7870/10336], Loss: 0.5478\n",
      "Epoch [2/5], Step [7872/10336], Loss: 0.6809\n",
      "Epoch [2/5], Step [7874/10336], Loss: 0.3429\n",
      "Epoch [2/5], Step [7876/10336], Loss: 3.6985\n",
      "Epoch [2/5], Step [7878/10336], Loss: 1.2836\n",
      "Epoch [2/5], Step [7880/10336], Loss: 2.2501\n",
      "Epoch [2/5], Step [7882/10336], Loss: 3.4516\n",
      "Epoch [2/5], Step [7884/10336], Loss: 4.0326\n",
      "Epoch [2/5], Step [7886/10336], Loss: 4.4375\n",
      "Epoch [2/5], Step [7888/10336], Loss: 2.7772\n",
      "Epoch [2/5], Step [7890/10336], Loss: 0.5085\n",
      "Epoch [2/5], Step [7892/10336], Loss: 1.4354\n",
      "Epoch [2/5], Step [7894/10336], Loss: 0.4780\n",
      "Epoch [2/5], Step [7896/10336], Loss: 0.4114\n",
      "Epoch [2/5], Step [7898/10336], Loss: 2.1333\n",
      "Epoch [2/5], Step [7900/10336], Loss: 1.1012\n",
      "Epoch [2/5], Step [7902/10336], Loss: 0.8167\n",
      "Epoch [2/5], Step [7904/10336], Loss: 0.3483\n",
      "Epoch [2/5], Step [7906/10336], Loss: 0.3968\n",
      "Epoch [2/5], Step [7908/10336], Loss: 0.3095\n",
      "Epoch [2/5], Step [7910/10336], Loss: 0.3651\n",
      "Epoch [2/5], Step [7912/10336], Loss: 3.3239\n",
      "Epoch [2/5], Step [7914/10336], Loss: 0.0686\n",
      "Epoch [2/5], Step [7916/10336], Loss: 0.0841\n",
      "Epoch [2/5], Step [7918/10336], Loss: 0.3162\n",
      "Epoch [2/5], Step [7920/10336], Loss: 0.4415\n",
      "Epoch [2/5], Step [7922/10336], Loss: 0.2791\n",
      "Epoch [2/5], Step [7924/10336], Loss: 0.4165\n",
      "Epoch [2/5], Step [7926/10336], Loss: 0.0613\n",
      "Epoch [2/5], Step [7928/10336], Loss: 0.2703\n",
      "Epoch [2/5], Step [7930/10336], Loss: 0.0067\n",
      "Epoch [2/5], Step [7932/10336], Loss: 0.2869\n",
      "Epoch [2/5], Step [7934/10336], Loss: 0.2649\n",
      "Epoch [2/5], Step [7936/10336], Loss: 1.8254\n",
      "Epoch [2/5], Step [7938/10336], Loss: 0.9693\n",
      "Epoch [2/5], Step [7940/10336], Loss: 0.3067\n",
      "Epoch [2/5], Step [7942/10336], Loss: 0.3992\n",
      "Epoch [2/5], Step [7944/10336], Loss: 3.3121\n",
      "Epoch [2/5], Step [7946/10336], Loss: 1.6541\n",
      "Epoch [2/5], Step [7948/10336], Loss: 0.3757\n",
      "Epoch [2/5], Step [7950/10336], Loss: 0.0895\n",
      "Epoch [2/5], Step [7952/10336], Loss: 0.1738\n",
      "Epoch [2/5], Step [7954/10336], Loss: 2.2658\n",
      "Epoch [2/5], Step [7956/10336], Loss: 0.0724\n",
      "Epoch [2/5], Step [7958/10336], Loss: 3.9826\n",
      "Epoch [2/5], Step [7960/10336], Loss: 0.5976\n",
      "Epoch [2/5], Step [7962/10336], Loss: 3.0762\n",
      "Epoch [2/5], Step [7964/10336], Loss: 0.1093\n",
      "Epoch [2/5], Step [7966/10336], Loss: 0.1176\n",
      "Epoch [2/5], Step [7968/10336], Loss: 2.0047\n",
      "Epoch [2/5], Step [7970/10336], Loss: 1.5387\n",
      "Epoch [2/5], Step [7972/10336], Loss: 0.1400\n",
      "Epoch [2/5], Step [7974/10336], Loss: 0.4643\n",
      "Epoch [2/5], Step [7976/10336], Loss: 0.1649\n",
      "Epoch [2/5], Step [7978/10336], Loss: 0.5051\n",
      "Epoch [2/5], Step [7980/10336], Loss: 0.2410\n",
      "Epoch [2/5], Step [7982/10336], Loss: 0.2353\n",
      "Epoch [2/5], Step [7984/10336], Loss: 0.0956\n",
      "Epoch [2/5], Step [7986/10336], Loss: 0.0214\n",
      "Epoch [2/5], Step [7988/10336], Loss: 0.4950\n",
      "Epoch [2/5], Step [7990/10336], Loss: 3.0084\n",
      "Epoch [2/5], Step [7992/10336], Loss: 0.2148\n",
      "Epoch [2/5], Step [7994/10336], Loss: 0.1321\n",
      "Epoch [2/5], Step [7996/10336], Loss: 0.0118\n",
      "Epoch [2/5], Step [7998/10336], Loss: 1.5070\n",
      "Epoch [2/5], Step [8000/10336], Loss: 0.3274\n",
      "Epoch [2/5], Step [8002/10336], Loss: 0.2771\n",
      "Epoch [2/5], Step [8004/10336], Loss: 0.4196\n",
      "Epoch [2/5], Step [8006/10336], Loss: 0.1706\n",
      "Epoch [2/5], Step [8008/10336], Loss: 1.0066\n",
      "Epoch [2/5], Step [8010/10336], Loss: 0.3746\n",
      "Epoch [2/5], Step [8012/10336], Loss: 0.2159\n",
      "Epoch [2/5], Step [8014/10336], Loss: 0.0538\n",
      "Epoch [2/5], Step [8016/10336], Loss: 0.3405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [8018/10336], Loss: 1.4466\n",
      "Epoch [2/5], Step [8020/10336], Loss: 0.0250\n",
      "Epoch [2/5], Step [8022/10336], Loss: 0.2735\n",
      "Epoch [2/5], Step [8024/10336], Loss: 2.8155\n",
      "Epoch [2/5], Step [8026/10336], Loss: 0.1331\n",
      "Epoch [2/5], Step [8028/10336], Loss: 0.8117\n",
      "Epoch [2/5], Step [8030/10336], Loss: 0.1992\n",
      "Epoch [2/5], Step [8032/10336], Loss: 0.4323\n",
      "Epoch [2/5], Step [8034/10336], Loss: 5.2252\n",
      "Epoch [2/5], Step [8036/10336], Loss: 1.2478\n",
      "Epoch [2/5], Step [8038/10336], Loss: 0.0614\n",
      "Epoch [2/5], Step [8040/10336], Loss: 0.1506\n",
      "Epoch [2/5], Step [8042/10336], Loss: 0.1735\n",
      "Epoch [2/5], Step [8044/10336], Loss: 0.0190\n",
      "Epoch [2/5], Step [8046/10336], Loss: 0.0018\n",
      "Epoch [2/5], Step [8048/10336], Loss: 0.2557\n",
      "Epoch [2/5], Step [8050/10336], Loss: 0.1719\n",
      "Epoch [2/5], Step [8052/10336], Loss: 1.1765\n",
      "Epoch [2/5], Step [8054/10336], Loss: 0.0873\n",
      "Epoch [2/5], Step [8056/10336], Loss: 1.4157\n",
      "Epoch [2/5], Step [8058/10336], Loss: 0.3066\n",
      "Epoch [2/5], Step [8060/10336], Loss: 0.1089\n",
      "Epoch [2/5], Step [8062/10336], Loss: 0.0723\n",
      "Epoch [2/5], Step [8064/10336], Loss: 0.6350\n",
      "Epoch [2/5], Step [8066/10336], Loss: 0.2648\n",
      "Epoch [2/5], Step [8068/10336], Loss: 0.3870\n",
      "Epoch [2/5], Step [8070/10336], Loss: 2.9265\n",
      "Epoch [2/5], Step [8072/10336], Loss: 0.2652\n",
      "Epoch [2/5], Step [8074/10336], Loss: 1.9388\n",
      "Epoch [2/5], Step [8076/10336], Loss: 0.0293\n",
      "Epoch [2/5], Step [8078/10336], Loss: 1.7044\n",
      "Epoch [2/5], Step [8080/10336], Loss: 0.2089\n",
      "Epoch [2/5], Step [8082/10336], Loss: 0.7346\n",
      "Epoch [2/5], Step [8084/10336], Loss: 0.2508\n",
      "Epoch [2/5], Step [8086/10336], Loss: 1.3870\n",
      "Epoch [2/5], Step [8088/10336], Loss: 0.1748\n",
      "Epoch [2/5], Step [8090/10336], Loss: 0.0083\n",
      "Epoch [2/5], Step [8092/10336], Loss: 0.0920\n",
      "Epoch [2/5], Step [8094/10336], Loss: 0.6290\n",
      "Epoch [2/5], Step [8096/10336], Loss: 0.0354\n",
      "Epoch [2/5], Step [8098/10336], Loss: 0.2717\n",
      "Epoch [2/5], Step [8100/10336], Loss: 3.1588\n",
      "Epoch [2/5], Step [8102/10336], Loss: 1.6759\n",
      "Epoch [2/5], Step [8104/10336], Loss: 0.0114\n",
      "Epoch [2/5], Step [8106/10336], Loss: 0.0618\n",
      "Epoch [2/5], Step [8108/10336], Loss: 0.2271\n",
      "Epoch [2/5], Step [8110/10336], Loss: 0.0020\n",
      "Epoch [2/5], Step [8112/10336], Loss: 0.0174\n",
      "Epoch [2/5], Step [8114/10336], Loss: 0.4656\n",
      "Epoch [2/5], Step [8116/10336], Loss: 0.1476\n",
      "Epoch [2/5], Step [8118/10336], Loss: 0.4394\n",
      "Epoch [2/5], Step [8120/10336], Loss: 0.1363\n",
      "Epoch [2/5], Step [8122/10336], Loss: 0.4200\n",
      "Epoch [2/5], Step [8124/10336], Loss: 1.8997\n",
      "Epoch [2/5], Step [8126/10336], Loss: 0.1400\n",
      "Epoch [2/5], Step [8128/10336], Loss: 0.2324\n",
      "Epoch [2/5], Step [8130/10336], Loss: 0.0845\n",
      "Epoch [2/5], Step [8132/10336], Loss: 0.9170\n",
      "Epoch [2/5], Step [8134/10336], Loss: 0.4061\n",
      "Epoch [2/5], Step [8136/10336], Loss: 0.8044\n",
      "Epoch [2/5], Step [8138/10336], Loss: 0.1951\n",
      "Epoch [2/5], Step [8140/10336], Loss: 0.6022\n",
      "Epoch [2/5], Step [8142/10336], Loss: 4.4737\n",
      "Epoch [2/5], Step [8144/10336], Loss: 0.0286\n",
      "Epoch [2/5], Step [8146/10336], Loss: 0.1226\n",
      "Epoch [2/5], Step [8148/10336], Loss: 0.3336\n",
      "Epoch [2/5], Step [8150/10336], Loss: 0.1103\n",
      "Epoch [2/5], Step [8152/10336], Loss: 2.8419\n",
      "Epoch [2/5], Step [8154/10336], Loss: 0.4212\n",
      "Epoch [2/5], Step [8156/10336], Loss: 0.0174\n",
      "Epoch [2/5], Step [8158/10336], Loss: 0.0357\n",
      "Epoch [2/5], Step [8160/10336], Loss: 0.1765\n",
      "Epoch [2/5], Step [8162/10336], Loss: 0.2179\n",
      "Epoch [2/5], Step [8164/10336], Loss: 0.4999\n",
      "Epoch [2/5], Step [8166/10336], Loss: 0.1551\n",
      "Epoch [2/5], Step [8168/10336], Loss: 0.5749\n",
      "Epoch [2/5], Step [8170/10336], Loss: 0.6546\n",
      "Epoch [2/5], Step [8172/10336], Loss: 0.1876\n",
      "Epoch [2/5], Step [8174/10336], Loss: 0.2028\n",
      "Epoch [2/5], Step [8176/10336], Loss: 4.4123\n",
      "Epoch [2/5], Step [8178/10336], Loss: 0.2712\n",
      "Epoch [2/5], Step [8180/10336], Loss: 1.8954\n",
      "Epoch [2/5], Step [8182/10336], Loss: 0.3351\n",
      "Epoch [2/5], Step [8184/10336], Loss: 0.1004\n",
      "Epoch [2/5], Step [8186/10336], Loss: 2.0968\n",
      "Epoch [2/5], Step [8188/10336], Loss: 0.0079\n",
      "Epoch [2/5], Step [8190/10336], Loss: 0.0141\n",
      "Epoch [2/5], Step [8192/10336], Loss: 4.7346\n",
      "Epoch [2/5], Step [8194/10336], Loss: 0.2616\n",
      "Epoch [2/5], Step [8196/10336], Loss: 0.3134\n",
      "Epoch [2/5], Step [8198/10336], Loss: 0.3892\n",
      "Epoch [2/5], Step [8200/10336], Loss: 0.2675\n",
      "Epoch [2/5], Step [8202/10336], Loss: 0.0355\n",
      "Epoch [2/5], Step [8204/10336], Loss: 0.1700\n",
      "Epoch [2/5], Step [8206/10336], Loss: 0.3551\n",
      "Epoch [2/5], Step [8208/10336], Loss: 1.1916\n",
      "Epoch [2/5], Step [8210/10336], Loss: 0.3419\n",
      "Epoch [2/5], Step [8212/10336], Loss: 0.0050\n",
      "Epoch [2/5], Step [8214/10336], Loss: 0.2370\n",
      "Epoch [2/5], Step [8216/10336], Loss: 0.2207\n",
      "Epoch [2/5], Step [8218/10336], Loss: 4.2204\n",
      "Epoch [2/5], Step [8220/10336], Loss: 1.9727\n",
      "Epoch [2/5], Step [8222/10336], Loss: 0.4194\n",
      "Epoch [2/5], Step [8224/10336], Loss: 0.1126\n",
      "Epoch [2/5], Step [8226/10336], Loss: 0.0918\n",
      "Epoch [2/5], Step [8228/10336], Loss: 1.2650\n",
      "Epoch [2/5], Step [8230/10336], Loss: 1.4144\n",
      "Epoch [2/5], Step [8232/10336], Loss: 1.0897\n",
      "Epoch [2/5], Step [8234/10336], Loss: 0.2965\n",
      "Epoch [2/5], Step [8236/10336], Loss: 1.0222\n",
      "Epoch [2/5], Step [8238/10336], Loss: 0.1670\n",
      "Epoch [2/5], Step [8240/10336], Loss: 0.6561\n",
      "Epoch [2/5], Step [8242/10336], Loss: 0.2319\n",
      "Epoch [2/5], Step [8244/10336], Loss: 3.1787\n",
      "Epoch [2/5], Step [8246/10336], Loss: 5.9409\n",
      "Epoch [2/5], Step [8248/10336], Loss: 0.0757\n",
      "Epoch [2/5], Step [8250/10336], Loss: 0.1253\n",
      "Epoch [2/5], Step [8252/10336], Loss: 0.7001\n",
      "Epoch [2/5], Step [8254/10336], Loss: 3.1737\n",
      "Epoch [2/5], Step [8256/10336], Loss: 0.1848\n",
      "Epoch [2/5], Step [8258/10336], Loss: 0.1748\n",
      "Epoch [2/5], Step [8260/10336], Loss: 0.0626\n",
      "Epoch [2/5], Step [8262/10336], Loss: 0.3628\n",
      "Epoch [2/5], Step [8264/10336], Loss: 2.0156\n",
      "Epoch [2/5], Step [8266/10336], Loss: 1.2573\n",
      "Epoch [2/5], Step [8268/10336], Loss: 0.2123\n",
      "Epoch [2/5], Step [8270/10336], Loss: 0.0942\n",
      "Epoch [2/5], Step [8272/10336], Loss: 0.3246\n",
      "Epoch [2/5], Step [8274/10336], Loss: 1.4901\n",
      "Epoch [2/5], Step [8276/10336], Loss: 3.3502\n",
      "Epoch [2/5], Step [8278/10336], Loss: 0.9450\n",
      "Epoch [2/5], Step [8280/10336], Loss: 2.3228\n",
      "Epoch [2/5], Step [8282/10336], Loss: 0.7153\n",
      "Epoch [2/5], Step [8284/10336], Loss: 1.5536\n",
      "Epoch [2/5], Step [8286/10336], Loss: 2.5044\n",
      "Epoch [2/5], Step [8288/10336], Loss: 0.2978\n",
      "Epoch [2/5], Step [8290/10336], Loss: 0.0332\n",
      "Epoch [2/5], Step [8292/10336], Loss: 3.8648\n",
      "Epoch [2/5], Step [8294/10336], Loss: 1.8601\n",
      "Epoch [2/5], Step [8296/10336], Loss: 0.6518\n",
      "Epoch [2/5], Step [8298/10336], Loss: 4.9714\n",
      "Epoch [2/5], Step [8300/10336], Loss: 0.6273\n",
      "Epoch [2/5], Step [8302/10336], Loss: 3.2819\n",
      "Epoch [2/5], Step [8304/10336], Loss: 0.0987\n",
      "Epoch [2/5], Step [8306/10336], Loss: 0.2926\n",
      "Epoch [2/5], Step [8308/10336], Loss: 0.0173\n",
      "Epoch [2/5], Step [8310/10336], Loss: 0.2095\n",
      "Epoch [2/5], Step [8312/10336], Loss: 3.5935\n",
      "Epoch [2/5], Step [8314/10336], Loss: 0.0191\n",
      "Epoch [2/5], Step [8316/10336], Loss: 0.0016\n",
      "Epoch [2/5], Step [8318/10336], Loss: 0.8122\n",
      "Epoch [2/5], Step [8320/10336], Loss: 0.0085\n",
      "Epoch [2/5], Step [8322/10336], Loss: 2.8633\n",
      "Epoch [2/5], Step [8324/10336], Loss: 4.9581\n",
      "Epoch [2/5], Step [8326/10336], Loss: 0.0838\n",
      "Epoch [2/5], Step [8328/10336], Loss: 0.1204\n",
      "Epoch [2/5], Step [8330/10336], Loss: 0.2520\n",
      "Epoch [2/5], Step [8332/10336], Loss: 0.5332\n",
      "Epoch [2/5], Step [8334/10336], Loss: 0.0438\n",
      "Epoch [2/5], Step [8336/10336], Loss: 1.7967\n",
      "Epoch [2/5], Step [8338/10336], Loss: 0.2641\n",
      "Epoch [2/5], Step [8340/10336], Loss: 0.4780\n",
      "Epoch [2/5], Step [8342/10336], Loss: 2.9113\n",
      "Epoch [2/5], Step [8344/10336], Loss: 0.2792\n",
      "Epoch [2/5], Step [8346/10336], Loss: 1.7064\n",
      "Epoch [2/5], Step [8348/10336], Loss: 0.1447\n",
      "Epoch [2/5], Step [8350/10336], Loss: 0.0584\n",
      "Epoch [2/5], Step [8352/10336], Loss: 0.7734\n",
      "Epoch [2/5], Step [8354/10336], Loss: 0.8679\n",
      "Epoch [2/5], Step [8356/10336], Loss: 1.4084\n",
      "Epoch [2/5], Step [8358/10336], Loss: 2.2285\n",
      "Epoch [2/5], Step [8360/10336], Loss: 0.4441\n",
      "Epoch [2/5], Step [8362/10336], Loss: 0.3209\n",
      "Epoch [2/5], Step [8364/10336], Loss: 0.5742\n",
      "Epoch [2/5], Step [8366/10336], Loss: 0.3591\n",
      "Epoch [2/5], Step [8368/10336], Loss: 0.0032\n",
      "Epoch [2/5], Step [8370/10336], Loss: 0.1567\n",
      "Epoch [2/5], Step [8372/10336], Loss: 1.1315\n",
      "Epoch [2/5], Step [8374/10336], Loss: 0.2742\n",
      "Epoch [2/5], Step [8376/10336], Loss: 0.3104\n",
      "Epoch [2/5], Step [8378/10336], Loss: 1.0451\n",
      "Epoch [2/5], Step [8380/10336], Loss: 0.3142\n",
      "Epoch [2/5], Step [8382/10336], Loss: 3.5712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [8384/10336], Loss: 0.2472\n",
      "Epoch [2/5], Step [8386/10336], Loss: 0.1894\n",
      "Epoch [2/5], Step [8388/10336], Loss: 1.3019\n",
      "Epoch [2/5], Step [8390/10336], Loss: 3.0391\n",
      "Epoch [2/5], Step [8392/10336], Loss: 0.6653\n",
      "Epoch [2/5], Step [8394/10336], Loss: 0.0205\n",
      "Epoch [2/5], Step [8396/10336], Loss: 0.2006\n",
      "Epoch [2/5], Step [8398/10336], Loss: 5.3247\n",
      "Epoch [2/5], Step [8400/10336], Loss: 0.0748\n",
      "Epoch [2/5], Step [8402/10336], Loss: 0.0233\n",
      "Epoch [2/5], Step [8404/10336], Loss: 0.2659\n",
      "Epoch [2/5], Step [8406/10336], Loss: 2.3577\n",
      "Epoch [2/5], Step [8408/10336], Loss: 0.1876\n",
      "Epoch [2/5], Step [8410/10336], Loss: 0.1395\n",
      "Epoch [2/5], Step [8412/10336], Loss: 1.9828\n",
      "Epoch [2/5], Step [8414/10336], Loss: 0.0063\n",
      "Epoch [2/5], Step [8416/10336], Loss: 0.4646\n",
      "Epoch [2/5], Step [8418/10336], Loss: 0.0208\n",
      "Epoch [2/5], Step [8420/10336], Loss: 4.0640\n",
      "Epoch [2/5], Step [8422/10336], Loss: 0.5062\n",
      "Epoch [2/5], Step [8424/10336], Loss: 0.0181\n",
      "Epoch [2/5], Step [8426/10336], Loss: 0.2116\n",
      "Epoch [2/5], Step [8428/10336], Loss: 0.1157\n",
      "Epoch [2/5], Step [8430/10336], Loss: 0.3913\n",
      "Epoch [2/5], Step [8432/10336], Loss: 0.0514\n",
      "Epoch [2/5], Step [8434/10336], Loss: 0.7362\n",
      "Epoch [2/5], Step [8436/10336], Loss: 0.0992\n",
      "Epoch [2/5], Step [8438/10336], Loss: 0.1187\n",
      "Epoch [2/5], Step [8440/10336], Loss: 0.0433\n",
      "Epoch [2/5], Step [8442/10336], Loss: 0.0076\n",
      "Epoch [2/5], Step [8444/10336], Loss: 0.7057\n",
      "Epoch [2/5], Step [8446/10336], Loss: 0.1984\n",
      "Epoch [2/5], Step [8448/10336], Loss: 0.0747\n",
      "Epoch [2/5], Step [8450/10336], Loss: 0.2498\n",
      "Epoch [2/5], Step [8452/10336], Loss: 0.3686\n",
      "Epoch [2/5], Step [8454/10336], Loss: 1.9611\n",
      "Epoch [2/5], Step [8456/10336], Loss: 0.1668\n",
      "Epoch [2/5], Step [8458/10336], Loss: 0.1052\n",
      "Epoch [2/5], Step [8460/10336], Loss: 0.0159\n",
      "Epoch [2/5], Step [8462/10336], Loss: 2.1632\n",
      "Epoch [2/5], Step [8464/10336], Loss: 0.2811\n",
      "Epoch [2/5], Step [8466/10336], Loss: 0.2378\n",
      "Epoch [2/5], Step [8468/10336], Loss: 0.1511\n",
      "Epoch [2/5], Step [8470/10336], Loss: 0.2359\n",
      "Epoch [2/5], Step [8472/10336], Loss: 0.1449\n",
      "Epoch [2/5], Step [8474/10336], Loss: 0.0613\n",
      "Epoch [2/5], Step [8476/10336], Loss: 0.4393\n",
      "Epoch [2/5], Step [8478/10336], Loss: 0.2436\n",
      "Epoch [2/5], Step [8480/10336], Loss: 1.0332\n",
      "Epoch [2/5], Step [8482/10336], Loss: 0.2979\n",
      "Epoch [2/5], Step [8484/10336], Loss: 0.0983\n",
      "Epoch [2/5], Step [8486/10336], Loss: 0.2431\n",
      "Epoch [2/5], Step [8488/10336], Loss: 0.5489\n",
      "Epoch [2/5], Step [8490/10336], Loss: 0.1814\n",
      "Epoch [2/5], Step [8492/10336], Loss: 0.0950\n",
      "Epoch [2/5], Step [8494/10336], Loss: 1.7486\n",
      "Epoch [2/5], Step [8496/10336], Loss: 0.4414\n",
      "Epoch [2/5], Step [8498/10336], Loss: 3.4238\n",
      "Epoch [2/5], Step [8500/10336], Loss: 1.4123\n",
      "Epoch [2/5], Step [8502/10336], Loss: 1.9374\n",
      "Epoch [2/5], Step [8504/10336], Loss: 0.7457\n",
      "Epoch [2/5], Step [8506/10336], Loss: 0.0874\n",
      "Epoch [2/5], Step [8508/10336], Loss: 0.2314\n",
      "Epoch [2/5], Step [8510/10336], Loss: 0.2306\n",
      "Epoch [2/5], Step [8512/10336], Loss: 1.6168\n",
      "Epoch [2/5], Step [8514/10336], Loss: 0.1741\n",
      "Epoch [2/5], Step [8516/10336], Loss: 2.1956\n",
      "Epoch [2/5], Step [8518/10336], Loss: 0.3748\n",
      "Epoch [2/5], Step [8520/10336], Loss: 0.0883\n",
      "Epoch [2/5], Step [8522/10336], Loss: 0.1539\n",
      "Epoch [2/5], Step [8524/10336], Loss: 0.0199\n",
      "Epoch [2/5], Step [8526/10336], Loss: 0.4489\n",
      "Epoch [2/5], Step [8528/10336], Loss: 0.0121\n",
      "Epoch [2/5], Step [8530/10336], Loss: 0.1122\n",
      "Epoch [2/5], Step [8532/10336], Loss: 0.4215\n",
      "Epoch [2/5], Step [8534/10336], Loss: 0.1317\n",
      "Epoch [2/5], Step [8536/10336], Loss: 0.2790\n",
      "Epoch [2/5], Step [8538/10336], Loss: 0.9041\n",
      "Epoch [2/5], Step [8540/10336], Loss: 3.8597\n",
      "Epoch [2/5], Step [8542/10336], Loss: 0.1636\n",
      "Epoch [2/5], Step [8544/10336], Loss: 0.4450\n",
      "Epoch [2/5], Step [8546/10336], Loss: 0.7834\n",
      "Epoch [2/5], Step [8548/10336], Loss: 0.5528\n",
      "Epoch [2/5], Step [8550/10336], Loss: 0.0904\n",
      "Epoch [2/5], Step [8552/10336], Loss: 0.9242\n",
      "Epoch [2/5], Step [8554/10336], Loss: 0.0231\n",
      "Epoch [2/5], Step [8556/10336], Loss: 4.9274\n",
      "Epoch [2/5], Step [8558/10336], Loss: 1.5208\n",
      "Epoch [2/5], Step [8560/10336], Loss: 0.1307\n",
      "Epoch [2/5], Step [8562/10336], Loss: 0.1449\n",
      "Epoch [2/5], Step [8564/10336], Loss: 0.0310\n",
      "Epoch [2/5], Step [8566/10336], Loss: 0.0595\n",
      "Epoch [2/5], Step [8568/10336], Loss: 0.0644\n",
      "Epoch [2/5], Step [8570/10336], Loss: 2.2558\n",
      "Epoch [2/5], Step [8572/10336], Loss: 0.0700\n",
      "Epoch [2/5], Step [8574/10336], Loss: 0.0380\n",
      "Epoch [2/5], Step [8576/10336], Loss: 0.0243\n",
      "Epoch [2/5], Step [8578/10336], Loss: 2.1052\n",
      "Epoch [2/5], Step [8580/10336], Loss: 0.0674\n",
      "Epoch [2/5], Step [8582/10336], Loss: 0.4013\n",
      "Epoch [2/5], Step [8584/10336], Loss: 0.3229\n",
      "Epoch [2/5], Step [8586/10336], Loss: 1.9661\n",
      "Epoch [2/5], Step [8588/10336], Loss: 0.1036\n",
      "Epoch [2/5], Step [8590/10336], Loss: 1.1859\n",
      "Epoch [2/5], Step [8592/10336], Loss: 0.0674\n",
      "Epoch [2/5], Step [8594/10336], Loss: 0.9312\n",
      "Epoch [2/5], Step [8596/10336], Loss: 0.2780\n",
      "Epoch [2/5], Step [8598/10336], Loss: 0.1377\n",
      "Epoch [2/5], Step [8600/10336], Loss: 3.0244\n",
      "Epoch [2/5], Step [8602/10336], Loss: 0.1685\n",
      "Epoch [2/5], Step [8604/10336], Loss: 0.0027\n",
      "Epoch [2/5], Step [8606/10336], Loss: 0.3201\n",
      "Epoch [2/5], Step [8608/10336], Loss: 0.0699\n",
      "Epoch [2/5], Step [8610/10336], Loss: 0.2871\n",
      "Epoch [2/5], Step [8612/10336], Loss: 0.6631\n",
      "Epoch [2/5], Step [8614/10336], Loss: 0.0501\n",
      "Epoch [2/5], Step [8616/10336], Loss: 0.1124\n",
      "Epoch [2/5], Step [8618/10336], Loss: 0.0293\n",
      "Epoch [2/5], Step [8620/10336], Loss: 0.3687\n",
      "Epoch [2/5], Step [8622/10336], Loss: 0.1694\n",
      "Epoch [2/5], Step [8624/10336], Loss: 0.0648\n",
      "Epoch [2/5], Step [8626/10336], Loss: 0.1843\n",
      "Epoch [2/5], Step [8628/10336], Loss: 0.0319\n",
      "Epoch [2/5], Step [8630/10336], Loss: 0.3428\n",
      "Epoch [2/5], Step [8632/10336], Loss: 1.5415\n",
      "Epoch [2/5], Step [8634/10336], Loss: 1.2527\n",
      "Epoch [2/5], Step [8636/10336], Loss: 0.0063\n",
      "Epoch [2/5], Step [8638/10336], Loss: 0.3704\n",
      "Epoch [2/5], Step [8640/10336], Loss: 0.4747\n",
      "Epoch [2/5], Step [8642/10336], Loss: 0.0376\n",
      "Epoch [2/5], Step [8644/10336], Loss: 0.0043\n",
      "Epoch [2/5], Step [8646/10336], Loss: 0.0079\n",
      "Epoch [2/5], Step [8648/10336], Loss: 0.1122\n",
      "Epoch [2/5], Step [8650/10336], Loss: 6.4543\n",
      "Epoch [2/5], Step [8652/10336], Loss: 1.5936\n",
      "Epoch [2/5], Step [8654/10336], Loss: 2.4361\n",
      "Epoch [2/5], Step [8656/10336], Loss: 0.2214\n",
      "Epoch [2/5], Step [8658/10336], Loss: 0.6950\n",
      "Epoch [2/5], Step [8660/10336], Loss: 0.0021\n",
      "Epoch [2/5], Step [8662/10336], Loss: 3.2986\n",
      "Epoch [2/5], Step [8664/10336], Loss: 0.1527\n",
      "Epoch [2/5], Step [8666/10336], Loss: 0.5645\n",
      "Epoch [2/5], Step [8668/10336], Loss: 0.2688\n",
      "Epoch [2/5], Step [8670/10336], Loss: 1.8374\n",
      "Epoch [2/5], Step [8672/10336], Loss: 0.1608\n",
      "Epoch [2/5], Step [8674/10336], Loss: 0.2266\n",
      "Epoch [2/5], Step [8676/10336], Loss: 0.3712\n",
      "Epoch [2/5], Step [8678/10336], Loss: 0.0259\n",
      "Epoch [2/5], Step [8680/10336], Loss: 3.5388\n",
      "Epoch [2/5], Step [8682/10336], Loss: 6.3046\n",
      "Epoch [2/5], Step [8684/10336], Loss: 0.8948\n",
      "Epoch [2/5], Step [8686/10336], Loss: 0.0901\n",
      "Epoch [2/5], Step [8688/10336], Loss: 0.0075\n",
      "Epoch [2/5], Step [8690/10336], Loss: 0.0332\n",
      "Epoch [2/5], Step [8692/10336], Loss: 0.2677\n",
      "Epoch [2/5], Step [8694/10336], Loss: 0.3573\n",
      "Epoch [2/5], Step [8696/10336], Loss: 0.2377\n",
      "Epoch [2/5], Step [8698/10336], Loss: 0.5176\n",
      "Epoch [2/5], Step [8700/10336], Loss: 0.0736\n",
      "Epoch [2/5], Step [8702/10336], Loss: 0.1735\n",
      "Epoch [2/5], Step [8704/10336], Loss: 4.2262\n",
      "Epoch [2/5], Step [8706/10336], Loss: 3.0556\n",
      "Epoch [2/5], Step [8708/10336], Loss: 1.0777\n",
      "Epoch [2/5], Step [8710/10336], Loss: 0.1082\n",
      "Epoch [2/5], Step [8712/10336], Loss: 0.9772\n",
      "Epoch [2/5], Step [8714/10336], Loss: 0.4198\n",
      "Epoch [2/5], Step [8716/10336], Loss: 1.2130\n",
      "Epoch [2/5], Step [8718/10336], Loss: 0.3861\n",
      "Epoch [2/5], Step [8720/10336], Loss: 0.5669\n",
      "Epoch [2/5], Step [8722/10336], Loss: 1.7399\n",
      "Epoch [2/5], Step [8724/10336], Loss: 0.5020\n",
      "Epoch [2/5], Step [8726/10336], Loss: 0.4005\n",
      "Epoch [2/5], Step [8728/10336], Loss: 0.0075\n",
      "Epoch [2/5], Step [8730/10336], Loss: 0.3169\n",
      "Epoch [2/5], Step [8732/10336], Loss: 0.3165\n",
      "Epoch [2/5], Step [8734/10336], Loss: 0.0078\n",
      "Epoch [2/5], Step [8736/10336], Loss: 0.0677\n",
      "Epoch [2/5], Step [8738/10336], Loss: 0.0915\n",
      "Epoch [2/5], Step [8740/10336], Loss: 0.2719\n",
      "Epoch [2/5], Step [8742/10336], Loss: 1.1223\n",
      "Epoch [2/5], Step [8744/10336], Loss: 0.2710\n",
      "Epoch [2/5], Step [8746/10336], Loss: 0.0601\n",
      "Epoch [2/5], Step [8748/10336], Loss: 0.0061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [8750/10336], Loss: 3.7271\n",
      "Epoch [2/5], Step [8752/10336], Loss: 0.0396\n",
      "Epoch [2/5], Step [8754/10336], Loss: 1.8561\n",
      "Epoch [2/5], Step [8756/10336], Loss: 1.2963\n",
      "Epoch [2/5], Step [8758/10336], Loss: 0.4872\n",
      "Epoch [2/5], Step [8760/10336], Loss: 0.3617\n",
      "Epoch [2/5], Step [8762/10336], Loss: 0.4309\n",
      "Epoch [2/5], Step [8764/10336], Loss: 3.2831\n",
      "Epoch [2/5], Step [8766/10336], Loss: 3.7768\n",
      "Epoch [2/5], Step [8768/10336], Loss: 0.0830\n",
      "Epoch [2/5], Step [8770/10336], Loss: 0.0546\n",
      "Epoch [2/5], Step [8772/10336], Loss: 1.4191\n",
      "Epoch [2/5], Step [8774/10336], Loss: 1.2825\n",
      "Epoch [2/5], Step [8776/10336], Loss: 1.2073\n",
      "Epoch [2/5], Step [8778/10336], Loss: 2.3597\n",
      "Epoch [2/5], Step [8780/10336], Loss: 4.3958\n",
      "Epoch [2/5], Step [8782/10336], Loss: 0.1371\n",
      "Epoch [2/5], Step [8784/10336], Loss: 1.2973\n",
      "Epoch [2/5], Step [8786/10336], Loss: 0.8402\n",
      "Epoch [2/5], Step [8788/10336], Loss: 0.5036\n",
      "Epoch [2/5], Step [8790/10336], Loss: 0.1342\n",
      "Epoch [2/5], Step [8792/10336], Loss: 1.2035\n",
      "Epoch [2/5], Step [8794/10336], Loss: 0.0479\n",
      "Epoch [2/5], Step [8796/10336], Loss: 0.8810\n",
      "Epoch [2/5], Step [8798/10336], Loss: 0.0179\n",
      "Epoch [2/5], Step [8800/10336], Loss: 0.0245\n",
      "Epoch [2/5], Step [8802/10336], Loss: 0.2226\n",
      "Epoch [2/5], Step [8804/10336], Loss: 5.1224\n",
      "Epoch [2/5], Step [8806/10336], Loss: 0.2727\n",
      "Epoch [2/5], Step [8808/10336], Loss: 0.0202\n",
      "Epoch [2/5], Step [8810/10336], Loss: 0.5591\n",
      "Epoch [2/5], Step [8812/10336], Loss: 3.0421\n",
      "Epoch [2/5], Step [8814/10336], Loss: 2.2806\n",
      "Epoch [2/5], Step [8816/10336], Loss: 0.0301\n",
      "Epoch [2/5], Step [8818/10336], Loss: 0.2978\n",
      "Epoch [2/5], Step [8820/10336], Loss: 0.3934\n",
      "Epoch [2/5], Step [8822/10336], Loss: 1.6282\n",
      "Epoch [2/5], Step [8824/10336], Loss: 2.2386\n",
      "Epoch [2/5], Step [8826/10336], Loss: 0.2741\n",
      "Epoch [2/5], Step [8828/10336], Loss: 0.1335\n",
      "Epoch [2/5], Step [8830/10336], Loss: 0.5123\n",
      "Epoch [2/5], Step [8832/10336], Loss: 0.6380\n",
      "Epoch [2/5], Step [8834/10336], Loss: 0.6447\n",
      "Epoch [2/5], Step [8836/10336], Loss: 0.5393\n",
      "Epoch [2/5], Step [8838/10336], Loss: 0.3403\n",
      "Epoch [2/5], Step [8840/10336], Loss: 1.5937\n",
      "Epoch [2/5], Step [8842/10336], Loss: 0.0457\n",
      "Epoch [2/5], Step [8844/10336], Loss: 0.1167\n",
      "Epoch [2/5], Step [8846/10336], Loss: 2.3671\n",
      "Epoch [2/5], Step [8848/10336], Loss: 0.2511\n",
      "Epoch [2/5], Step [8850/10336], Loss: 0.2145\n",
      "Epoch [2/5], Step [8852/10336], Loss: 0.3532\n",
      "Epoch [2/5], Step [8854/10336], Loss: 0.9910\n",
      "Epoch [2/5], Step [8856/10336], Loss: 1.9377\n",
      "Epoch [2/5], Step [8858/10336], Loss: 1.1252\n",
      "Epoch [2/5], Step [8860/10336], Loss: 0.2071\n",
      "Epoch [2/5], Step [8862/10336], Loss: 2.8477\n",
      "Epoch [2/5], Step [8864/10336], Loss: 0.2600\n",
      "Epoch [2/5], Step [8866/10336], Loss: 2.0805\n",
      "Epoch [2/5], Step [8868/10336], Loss: 0.6789\n",
      "Epoch [2/5], Step [8870/10336], Loss: 0.2464\n",
      "Epoch [2/5], Step [8872/10336], Loss: 0.4986\n",
      "Epoch [2/5], Step [8874/10336], Loss: 2.8536\n",
      "Epoch [2/5], Step [8876/10336], Loss: 0.2828\n",
      "Epoch [2/5], Step [8878/10336], Loss: 0.9051\n",
      "Epoch [2/5], Step [8880/10336], Loss: 5.1071\n",
      "Epoch [2/5], Step [8882/10336], Loss: 0.4294\n",
      "Epoch [2/5], Step [8884/10336], Loss: 0.0126\n",
      "Epoch [2/5], Step [8886/10336], Loss: 0.0457\n",
      "Epoch [2/5], Step [8888/10336], Loss: 0.2341\n",
      "Epoch [2/5], Step [8890/10336], Loss: 0.0617\n",
      "Epoch [2/5], Step [8892/10336], Loss: 3.8262\n",
      "Epoch [2/5], Step [8894/10336], Loss: 0.1796\n",
      "Epoch [2/5], Step [8896/10336], Loss: 0.1027\n",
      "Epoch [2/5], Step [8898/10336], Loss: 0.7306\n",
      "Epoch [2/5], Step [8900/10336], Loss: 1.8250\n",
      "Epoch [2/5], Step [8902/10336], Loss: 0.2165\n",
      "Epoch [2/5], Step [8904/10336], Loss: 2.9841\n",
      "Epoch [2/5], Step [8906/10336], Loss: 1.4647\n",
      "Epoch [2/5], Step [8908/10336], Loss: 0.0410\n",
      "Epoch [2/5], Step [8910/10336], Loss: 0.1000\n",
      "Epoch [2/5], Step [8912/10336], Loss: 0.0072\n",
      "Epoch [2/5], Step [8914/10336], Loss: 0.8740\n",
      "Epoch [2/5], Step [8916/10336], Loss: 0.0640\n",
      "Epoch [2/5], Step [8918/10336], Loss: 0.1630\n",
      "Epoch [2/5], Step [8920/10336], Loss: 0.0808\n",
      "Epoch [2/5], Step [8922/10336], Loss: 0.4020\n",
      "Epoch [2/5], Step [8924/10336], Loss: 0.1164\n",
      "Epoch [2/5], Step [8926/10336], Loss: 3.6946\n",
      "Epoch [2/5], Step [8928/10336], Loss: 0.5655\n",
      "Epoch [2/5], Step [8930/10336], Loss: 0.2076\n",
      "Epoch [2/5], Step [8932/10336], Loss: 4.5139\n",
      "Epoch [2/5], Step [8934/10336], Loss: 1.5565\n",
      "Epoch [2/5], Step [8936/10336], Loss: 0.0181\n",
      "Epoch [2/5], Step [8938/10336], Loss: 2.3039\n",
      "Epoch [2/5], Step [8940/10336], Loss: 4.1582\n",
      "Epoch [2/5], Step [8942/10336], Loss: 0.8270\n",
      "Epoch [2/5], Step [8944/10336], Loss: 2.4877\n",
      "Epoch [2/5], Step [8946/10336], Loss: 1.6441\n",
      "Epoch [2/5], Step [8948/10336], Loss: 1.6221\n",
      "Epoch [2/5], Step [8950/10336], Loss: 0.2776\n",
      "Epoch [2/5], Step [8952/10336], Loss: 0.3270\n",
      "Epoch [2/5], Step [8954/10336], Loss: 0.8687\n",
      "Epoch [2/5], Step [8956/10336], Loss: 0.0263\n",
      "Epoch [2/5], Step [8958/10336], Loss: 0.8281\n",
      "Epoch [2/5], Step [8960/10336], Loss: 3.0297\n",
      "Epoch [2/5], Step [8962/10336], Loss: 1.0984\n",
      "Epoch [2/5], Step [8964/10336], Loss: 0.1878\n",
      "Epoch [2/5], Step [8966/10336], Loss: 0.4108\n",
      "Epoch [2/5], Step [8968/10336], Loss: 0.0485\n",
      "Epoch [2/5], Step [8970/10336], Loss: 0.2858\n",
      "Epoch [2/5], Step [8972/10336], Loss: 0.2878\n",
      "Epoch [2/5], Step [8974/10336], Loss: 2.6884\n",
      "Epoch [2/5], Step [8976/10336], Loss: 2.4871\n",
      "Epoch [2/5], Step [8978/10336], Loss: 0.1327\n",
      "Epoch [2/5], Step [8980/10336], Loss: 0.3995\n",
      "Epoch [2/5], Step [8982/10336], Loss: 0.0507\n",
      "Epoch [2/5], Step [8984/10336], Loss: 0.2129\n",
      "Epoch [2/5], Step [8986/10336], Loss: 0.0199\n",
      "Epoch [2/5], Step [8988/10336], Loss: 0.4353\n",
      "Epoch [2/5], Step [8990/10336], Loss: 0.0452\n",
      "Epoch [2/5], Step [8992/10336], Loss: 0.0231\n",
      "Epoch [2/5], Step [8994/10336], Loss: 1.8374\n",
      "Epoch [2/5], Step [8996/10336], Loss: 0.1600\n",
      "Epoch [2/5], Step [8998/10336], Loss: 2.4488\n",
      "Epoch [2/5], Step [9000/10336], Loss: 0.4918\n",
      "Epoch [2/5], Step [9002/10336], Loss: 1.7975\n",
      "Epoch [2/5], Step [9004/10336], Loss: 0.0033\n",
      "Epoch [2/5], Step [9006/10336], Loss: 0.0813\n",
      "Epoch [2/5], Step [9008/10336], Loss: 0.0265\n",
      "Epoch [2/5], Step [9010/10336], Loss: 0.3421\n",
      "Epoch [2/5], Step [9012/10336], Loss: 0.1274\n",
      "Epoch [2/5], Step [9014/10336], Loss: 5.2943\n",
      "Epoch [2/5], Step [9016/10336], Loss: 0.3767\n",
      "Epoch [2/5], Step [9018/10336], Loss: 0.1707\n",
      "Epoch [2/5], Step [9020/10336], Loss: 0.1824\n",
      "Epoch [2/5], Step [9022/10336], Loss: 0.0470\n",
      "Epoch [2/5], Step [9024/10336], Loss: 0.2094\n",
      "Epoch [2/5], Step [9026/10336], Loss: 0.5010\n",
      "Epoch [2/5], Step [9028/10336], Loss: 0.2261\n",
      "Epoch [2/5], Step [9030/10336], Loss: 0.1673\n",
      "Epoch [2/5], Step [9032/10336], Loss: 2.5730\n",
      "Epoch [2/5], Step [9034/10336], Loss: 0.3598\n",
      "Epoch [2/5], Step [9036/10336], Loss: 0.1718\n",
      "Epoch [2/5], Step [9038/10336], Loss: 1.1359\n",
      "Epoch [2/5], Step [9040/10336], Loss: 3.8537\n",
      "Epoch [2/5], Step [9042/10336], Loss: 0.3516\n",
      "Epoch [2/5], Step [9044/10336], Loss: 0.0025\n",
      "Epoch [2/5], Step [9046/10336], Loss: 0.3278\n",
      "Epoch [2/5], Step [9048/10336], Loss: 1.6577\n",
      "Epoch [2/5], Step [9050/10336], Loss: 0.6987\n",
      "Epoch [2/5], Step [9052/10336], Loss: 5.1201\n",
      "Epoch [2/5], Step [9054/10336], Loss: 0.3453\n",
      "Epoch [2/5], Step [9056/10336], Loss: 0.3525\n",
      "Epoch [2/5], Step [9058/10336], Loss: 0.4488\n",
      "Epoch [2/5], Step [9060/10336], Loss: 2.5503\n",
      "Epoch [2/5], Step [9062/10336], Loss: 0.3617\n",
      "Epoch [2/5], Step [9064/10336], Loss: 0.1993\n",
      "Epoch [2/5], Step [9066/10336], Loss: 0.2671\n",
      "Epoch [2/5], Step [9068/10336], Loss: 1.7754\n",
      "Epoch [2/5], Step [9070/10336], Loss: 0.3462\n",
      "Epoch [2/5], Step [9072/10336], Loss: 0.0128\n",
      "Epoch [2/5], Step [9074/10336], Loss: 0.0998\n",
      "Epoch [2/5], Step [9076/10336], Loss: 0.0070\n",
      "Epoch [2/5], Step [9078/10336], Loss: 0.0173\n",
      "Epoch [2/5], Step [9080/10336], Loss: 0.0002\n",
      "Epoch [2/5], Step [9082/10336], Loss: 0.0083\n",
      "Epoch [2/5], Step [9084/10336], Loss: 0.0006\n",
      "Epoch [2/5], Step [9086/10336], Loss: 3.0062\n",
      "Epoch [2/5], Step [9088/10336], Loss: 3.0419\n",
      "Epoch [2/5], Step [9090/10336], Loss: 0.3159\n",
      "Epoch [2/5], Step [9092/10336], Loss: 0.9264\n",
      "Epoch [2/5], Step [9094/10336], Loss: 4.7927\n",
      "Epoch [2/5], Step [9096/10336], Loss: 0.0206\n",
      "Epoch [2/5], Step [9098/10336], Loss: 0.0036\n",
      "Epoch [2/5], Step [9100/10336], Loss: 0.4816\n",
      "Epoch [2/5], Step [9102/10336], Loss: 0.6061\n",
      "Epoch [2/5], Step [9104/10336], Loss: 3.3136\n",
      "Epoch [2/5], Step [9106/10336], Loss: 1.6805\n",
      "Epoch [2/5], Step [9108/10336], Loss: 0.1686\n",
      "Epoch [2/5], Step [9110/10336], Loss: 1.4766\n",
      "Epoch [2/5], Step [9112/10336], Loss: 0.2110\n",
      "Epoch [2/5], Step [9114/10336], Loss: 0.8057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [9116/10336], Loss: 1.7543\n",
      "Epoch [2/5], Step [9118/10336], Loss: 2.9133\n",
      "Epoch [2/5], Step [9120/10336], Loss: 0.3042\n",
      "Epoch [2/5], Step [9122/10336], Loss: 2.8572\n",
      "Epoch [2/5], Step [9124/10336], Loss: 0.0952\n",
      "Epoch [2/5], Step [9126/10336], Loss: 0.8949\n",
      "Epoch [2/5], Step [9128/10336], Loss: 0.1134\n",
      "Epoch [2/5], Step [9130/10336], Loss: 5.8190\n",
      "Epoch [2/5], Step [9132/10336], Loss: 0.4419\n",
      "Epoch [2/5], Step [9134/10336], Loss: 0.3696\n",
      "Epoch [2/5], Step [9136/10336], Loss: 0.1398\n",
      "Epoch [2/5], Step [9138/10336], Loss: 1.4587\n",
      "Epoch [2/5], Step [9140/10336], Loss: 2.3693\n",
      "Epoch [2/5], Step [9142/10336], Loss: 1.2804\n",
      "Epoch [2/5], Step [9144/10336], Loss: 0.0022\n",
      "Epoch [2/5], Step [9146/10336], Loss: 0.1543\n",
      "Epoch [2/5], Step [9148/10336], Loss: 0.2216\n",
      "Epoch [2/5], Step [9150/10336], Loss: 0.2863\n",
      "Epoch [2/5], Step [9152/10336], Loss: 0.0610\n",
      "Epoch [2/5], Step [9154/10336], Loss: 0.5022\n",
      "Epoch [2/5], Step [9156/10336], Loss: 0.7095\n",
      "Epoch [2/5], Step [9158/10336], Loss: 0.0666\n",
      "Epoch [2/5], Step [9160/10336], Loss: 0.0033\n",
      "Epoch [2/5], Step [9162/10336], Loss: 0.2575\n",
      "Epoch [2/5], Step [9164/10336], Loss: 2.6933\n",
      "Epoch [2/5], Step [9166/10336], Loss: 0.0306\n",
      "Epoch [2/5], Step [9168/10336], Loss: 0.8114\n",
      "Epoch [2/5], Step [9170/10336], Loss: 0.7481\n",
      "Epoch [2/5], Step [9172/10336], Loss: 0.1367\n",
      "Epoch [2/5], Step [9174/10336], Loss: 0.0268\n",
      "Epoch [2/5], Step [9176/10336], Loss: 0.9549\n",
      "Epoch [2/5], Step [9178/10336], Loss: 0.9482\n",
      "Epoch [2/5], Step [9180/10336], Loss: 4.3604\n",
      "Epoch [2/5], Step [9182/10336], Loss: 0.1874\n",
      "Epoch [2/5], Step [9184/10336], Loss: 0.6667\n",
      "Epoch [2/5], Step [9186/10336], Loss: 0.0023\n",
      "Epoch [2/5], Step [9188/10336], Loss: 0.8074\n",
      "Epoch [2/5], Step [9190/10336], Loss: 2.1993\n",
      "Epoch [2/5], Step [9192/10336], Loss: 6.2416\n",
      "Epoch [2/5], Step [9194/10336], Loss: 0.5264\n",
      "Epoch [2/5], Step [9196/10336], Loss: 2.6371\n",
      "Epoch [2/5], Step [9198/10336], Loss: 0.0350\n",
      "Epoch [2/5], Step [9200/10336], Loss: 0.4097\n",
      "Epoch [2/5], Step [9202/10336], Loss: 1.6501\n",
      "Epoch [2/5], Step [9204/10336], Loss: 2.5640\n",
      "Epoch [2/5], Step [9206/10336], Loss: 0.0538\n",
      "Epoch [2/5], Step [9208/10336], Loss: 0.1582\n",
      "Epoch [2/5], Step [9210/10336], Loss: 0.1289\n",
      "Epoch [2/5], Step [9212/10336], Loss: 0.0580\n",
      "Epoch [2/5], Step [9214/10336], Loss: 0.0936\n",
      "Epoch [2/5], Step [9216/10336], Loss: 0.8961\n",
      "Epoch [2/5], Step [9218/10336], Loss: 0.2326\n",
      "Epoch [2/5], Step [9220/10336], Loss: 1.5069\n",
      "Epoch [2/5], Step [9222/10336], Loss: 0.1649\n",
      "Epoch [2/5], Step [9224/10336], Loss: 1.8681\n",
      "Epoch [2/5], Step [9226/10336], Loss: 0.5562\n",
      "Epoch [2/5], Step [9228/10336], Loss: 0.0282\n",
      "Epoch [2/5], Step [9230/10336], Loss: 1.0023\n",
      "Epoch [2/5], Step [9232/10336], Loss: 2.6730\n",
      "Epoch [2/5], Step [9234/10336], Loss: 1.2117\n",
      "Epoch [2/5], Step [9236/10336], Loss: 2.7689\n",
      "Epoch [2/5], Step [9238/10336], Loss: 0.2340\n",
      "Epoch [2/5], Step [9240/10336], Loss: 0.0379\n",
      "Epoch [2/5], Step [9242/10336], Loss: 0.2976\n",
      "Epoch [2/5], Step [9244/10336], Loss: 0.2925\n",
      "Epoch [2/5], Step [9246/10336], Loss: 0.3926\n",
      "Epoch [2/5], Step [9248/10336], Loss: 0.0115\n",
      "Epoch [2/5], Step [9250/10336], Loss: 0.2424\n",
      "Epoch [2/5], Step [9252/10336], Loss: 0.1850\n",
      "Epoch [2/5], Step [9254/10336], Loss: 0.2166\n",
      "Epoch [2/5], Step [9256/10336], Loss: 0.2704\n",
      "Epoch [2/5], Step [9258/10336], Loss: 0.0386\n",
      "Epoch [2/5], Step [9260/10336], Loss: 0.0756\n",
      "Epoch [2/5], Step [9262/10336], Loss: 0.9223\n",
      "Epoch [2/5], Step [9264/10336], Loss: 0.3054\n",
      "Epoch [2/5], Step [9266/10336], Loss: 0.4652\n",
      "Epoch [2/5], Step [9268/10336], Loss: 0.0146\n",
      "Epoch [2/5], Step [9270/10336], Loss: 4.4185\n",
      "Epoch [2/5], Step [9272/10336], Loss: 0.2716\n",
      "Epoch [2/5], Step [9274/10336], Loss: 0.6216\n",
      "Epoch [2/5], Step [9276/10336], Loss: 0.0670\n",
      "Epoch [2/5], Step [9278/10336], Loss: 2.8466\n",
      "Epoch [2/5], Step [9280/10336], Loss: 2.2491\n",
      "Epoch [2/5], Step [9282/10336], Loss: 3.0466\n",
      "Epoch [2/5], Step [9284/10336], Loss: 0.3014\n",
      "Epoch [2/5], Step [9286/10336], Loss: 0.8947\n",
      "Epoch [2/5], Step [9288/10336], Loss: 0.1720\n",
      "Epoch [2/5], Step [9290/10336], Loss: 0.3859\n",
      "Epoch [2/5], Step [9292/10336], Loss: 2.6651\n",
      "Epoch [2/5], Step [9294/10336], Loss: 0.2366\n",
      "Epoch [2/5], Step [9296/10336], Loss: 0.0319\n",
      "Epoch [2/5], Step [9298/10336], Loss: 0.3611\n",
      "Epoch [2/5], Step [9300/10336], Loss: 0.6931\n",
      "Epoch [2/5], Step [9302/10336], Loss: 0.4987\n",
      "Epoch [2/5], Step [9304/10336], Loss: 0.3507\n",
      "Epoch [2/5], Step [9306/10336], Loss: 0.0311\n",
      "Epoch [2/5], Step [9308/10336], Loss: 0.0133\n",
      "Epoch [2/5], Step [9310/10336], Loss: 0.1829\n",
      "Epoch [2/5], Step [9312/10336], Loss: 1.4993\n",
      "Epoch [2/5], Step [9314/10336], Loss: 2.1887\n",
      "Epoch [2/5], Step [9316/10336], Loss: 0.2297\n",
      "Epoch [2/5], Step [9318/10336], Loss: 0.4926\n",
      "Epoch [2/5], Step [9320/10336], Loss: 3.6313\n",
      "Epoch [2/5], Step [9322/10336], Loss: 1.9239\n",
      "Epoch [2/5], Step [9324/10336], Loss: 2.4057\n",
      "Epoch [2/5], Step [9326/10336], Loss: 1.5564\n",
      "Epoch [2/5], Step [9328/10336], Loss: 0.3850\n",
      "Epoch [2/5], Step [9330/10336], Loss: 2.4213\n",
      "Epoch [2/5], Step [9332/10336], Loss: 0.2970\n",
      "Epoch [2/5], Step [9334/10336], Loss: 1.9081\n",
      "Epoch [2/5], Step [9336/10336], Loss: 0.4112\n",
      "Epoch [2/5], Step [9338/10336], Loss: 0.0628\n",
      "Epoch [2/5], Step [9340/10336], Loss: 3.6901\n",
      "Epoch [2/5], Step [9342/10336], Loss: 0.0039\n",
      "Epoch [2/5], Step [9344/10336], Loss: 0.1924\n",
      "Epoch [2/5], Step [9346/10336], Loss: 5.0003\n",
      "Epoch [2/5], Step [9348/10336], Loss: 0.0604\n",
      "Epoch [2/5], Step [9350/10336], Loss: 1.2117\n",
      "Epoch [2/5], Step [9352/10336], Loss: 1.0287\n",
      "Epoch [2/5], Step [9354/10336], Loss: 0.1268\n",
      "Epoch [2/5], Step [9356/10336], Loss: 2.5370\n",
      "Epoch [2/5], Step [9358/10336], Loss: 0.2367\n",
      "Epoch [2/5], Step [9360/10336], Loss: 0.6434\n",
      "Epoch [2/5], Step [9362/10336], Loss: 0.0778\n",
      "Epoch [2/5], Step [9364/10336], Loss: 0.4148\n",
      "Epoch [2/5], Step [9366/10336], Loss: 1.4635\n",
      "Epoch [2/5], Step [9368/10336], Loss: 2.7376\n",
      "Epoch [2/5], Step [9370/10336], Loss: 0.2478\n",
      "Epoch [2/5], Step [9372/10336], Loss: 0.0278\n",
      "Epoch [2/5], Step [9374/10336], Loss: 0.0206\n",
      "Epoch [2/5], Step [9376/10336], Loss: 1.2752\n",
      "Epoch [2/5], Step [9378/10336], Loss: 2.0219\n",
      "Epoch [2/5], Step [9380/10336], Loss: 0.3114\n",
      "Epoch [2/5], Step [9382/10336], Loss: 0.2222\n",
      "Epoch [2/5], Step [9384/10336], Loss: 0.0904\n",
      "Epoch [2/5], Step [9386/10336], Loss: 0.6598\n",
      "Epoch [2/5], Step [9388/10336], Loss: 0.4659\n",
      "Epoch [2/5], Step [9390/10336], Loss: 0.6088\n",
      "Epoch [2/5], Step [9392/10336], Loss: 0.2700\n",
      "Epoch [2/5], Step [9394/10336], Loss: 0.0996\n",
      "Epoch [2/5], Step [9396/10336], Loss: 0.3569\n",
      "Epoch [2/5], Step [9398/10336], Loss: 0.0071\n",
      "Epoch [2/5], Step [9400/10336], Loss: 0.2077\n",
      "Epoch [2/5], Step [9402/10336], Loss: 0.5996\n",
      "Epoch [2/5], Step [9404/10336], Loss: 0.9856\n",
      "Epoch [2/5], Step [9406/10336], Loss: 2.2051\n",
      "Epoch [2/5], Step [9408/10336], Loss: 0.2636\n",
      "Epoch [2/5], Step [9410/10336], Loss: 0.7669\n",
      "Epoch [2/5], Step [9412/10336], Loss: 0.0096\n",
      "Epoch [2/5], Step [9414/10336], Loss: 1.1834\n",
      "Epoch [2/5], Step [9416/10336], Loss: 0.0241\n",
      "Epoch [2/5], Step [9418/10336], Loss: 0.5574\n",
      "Epoch [2/5], Step [9420/10336], Loss: 0.0030\n",
      "Epoch [2/5], Step [9422/10336], Loss: 0.7228\n",
      "Epoch [2/5], Step [9424/10336], Loss: 0.3364\n",
      "Epoch [2/5], Step [9426/10336], Loss: 0.2988\n",
      "Epoch [2/5], Step [9428/10336], Loss: 0.1179\n",
      "Epoch [2/5], Step [9430/10336], Loss: 0.1449\n",
      "Epoch [2/5], Step [9432/10336], Loss: 0.0636\n",
      "Epoch [2/5], Step [9434/10336], Loss: 0.1341\n",
      "Epoch [2/5], Step [9436/10336], Loss: 0.8590\n",
      "Epoch [2/5], Step [9438/10336], Loss: 0.1353\n",
      "Epoch [2/5], Step [9440/10336], Loss: 0.0409\n",
      "Epoch [2/5], Step [9442/10336], Loss: 0.5350\n",
      "Epoch [2/5], Step [9444/10336], Loss: 4.1450\n",
      "Epoch [2/5], Step [9446/10336], Loss: 0.0174\n",
      "Epoch [2/5], Step [9448/10336], Loss: 0.9494\n",
      "Epoch [2/5], Step [9450/10336], Loss: 0.0733\n",
      "Epoch [2/5], Step [9452/10336], Loss: 0.2513\n",
      "Epoch [2/5], Step [9454/10336], Loss: 0.3011\n",
      "Epoch [2/5], Step [9456/10336], Loss: 0.8660\n",
      "Epoch [2/5], Step [9458/10336], Loss: 1.5043\n",
      "Epoch [2/5], Step [9460/10336], Loss: 0.9497\n",
      "Epoch [2/5], Step [9462/10336], Loss: 0.0068\n",
      "Epoch [2/5], Step [9464/10336], Loss: 2.4941\n",
      "Epoch [2/5], Step [9466/10336], Loss: 0.0025\n",
      "Epoch [2/5], Step [9468/10336], Loss: 0.0200\n",
      "Epoch [2/5], Step [9470/10336], Loss: 0.0651\n",
      "Epoch [2/5], Step [9472/10336], Loss: 1.3046\n",
      "Epoch [2/5], Step [9474/10336], Loss: 0.1701\n",
      "Epoch [2/5], Step [9476/10336], Loss: 3.6138\n",
      "Epoch [2/5], Step [9478/10336], Loss: 0.3291\n",
      "Epoch [2/5], Step [9480/10336], Loss: 3.1041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [9482/10336], Loss: 0.0413\n",
      "Epoch [2/5], Step [9484/10336], Loss: 0.6380\n",
      "Epoch [2/5], Step [9486/10336], Loss: 0.5199\n",
      "Epoch [2/5], Step [9488/10336], Loss: 0.3123\n",
      "Epoch [2/5], Step [9490/10336], Loss: 2.4774\n",
      "Epoch [2/5], Step [9492/10336], Loss: 0.2162\n",
      "Epoch [2/5], Step [9494/10336], Loss: 2.6670\n",
      "Epoch [2/5], Step [9496/10336], Loss: 2.3501\n",
      "Epoch [2/5], Step [9498/10336], Loss: 0.0092\n",
      "Epoch [2/5], Step [9500/10336], Loss: 0.6404\n",
      "Epoch [2/5], Step [9502/10336], Loss: 0.0241\n",
      "Epoch [2/5], Step [9504/10336], Loss: 0.3373\n",
      "Epoch [2/5], Step [9506/10336], Loss: 2.2016\n",
      "Epoch [2/5], Step [9508/10336], Loss: 0.3603\n",
      "Epoch [2/5], Step [9510/10336], Loss: 0.0438\n",
      "Epoch [2/5], Step [9512/10336], Loss: 0.2029\n",
      "Epoch [2/5], Step [9514/10336], Loss: 0.0054\n",
      "Epoch [2/5], Step [9516/10336], Loss: 1.4301\n",
      "Epoch [2/5], Step [9518/10336], Loss: 0.1826\n",
      "Epoch [2/5], Step [9520/10336], Loss: 0.0854\n",
      "Epoch [2/5], Step [9522/10336], Loss: 0.2452\n",
      "Epoch [2/5], Step [9524/10336], Loss: 0.7847\n",
      "Epoch [2/5], Step [9526/10336], Loss: 0.3973\n",
      "Epoch [2/5], Step [9528/10336], Loss: 0.0946\n",
      "Epoch [2/5], Step [9530/10336], Loss: 0.2736\n",
      "Epoch [2/5], Step [9532/10336], Loss: 4.8427\n",
      "Epoch [2/5], Step [9534/10336], Loss: 0.3949\n",
      "Epoch [2/5], Step [9536/10336], Loss: 0.0863\n",
      "Epoch [2/5], Step [9538/10336], Loss: 4.5522\n",
      "Epoch [2/5], Step [9540/10336], Loss: 0.9813\n",
      "Epoch [2/5], Step [9542/10336], Loss: 1.7972\n",
      "Epoch [2/5], Step [9544/10336], Loss: 0.2180\n",
      "Epoch [2/5], Step [9546/10336], Loss: 1.2555\n",
      "Epoch [2/5], Step [9548/10336], Loss: 0.3226\n",
      "Epoch [2/5], Step [9550/10336], Loss: 0.1015\n",
      "Epoch [2/5], Step [9552/10336], Loss: 1.0126\n",
      "Epoch [2/5], Step [9554/10336], Loss: 0.1939\n",
      "Epoch [2/5], Step [9556/10336], Loss: 0.7981\n",
      "Epoch [2/5], Step [9558/10336], Loss: 3.6076\n",
      "Epoch [2/5], Step [9560/10336], Loss: 1.3052\n",
      "Epoch [2/5], Step [9562/10336], Loss: 0.1952\n",
      "Epoch [2/5], Step [9564/10336], Loss: 0.0078\n",
      "Epoch [2/5], Step [9566/10336], Loss: 2.4331\n",
      "Epoch [2/5], Step [9568/10336], Loss: 2.2113\n",
      "Epoch [2/5], Step [9570/10336], Loss: 0.5714\n",
      "Epoch [2/5], Step [9572/10336], Loss: 0.3869\n",
      "Epoch [2/5], Step [9574/10336], Loss: 1.4615\n",
      "Epoch [2/5], Step [9576/10336], Loss: 1.1762\n",
      "Epoch [2/5], Step [9578/10336], Loss: 3.6422\n",
      "Epoch [2/5], Step [9580/10336], Loss: 1.2732\n",
      "Epoch [2/5], Step [9582/10336], Loss: 0.0850\n",
      "Epoch [2/5], Step [9584/10336], Loss: 0.0746\n",
      "Epoch [2/5], Step [9586/10336], Loss: 1.5786\n",
      "Epoch [2/5], Step [9588/10336], Loss: 0.2059\n",
      "Epoch [2/5], Step [9590/10336], Loss: 0.1835\n",
      "Epoch [2/5], Step [9592/10336], Loss: 0.2108\n",
      "Epoch [2/5], Step [9594/10336], Loss: 3.1657\n",
      "Epoch [2/5], Step [9596/10336], Loss: 0.5166\n",
      "Epoch [2/5], Step [9598/10336], Loss: 1.0682\n",
      "Epoch [2/5], Step [9600/10336], Loss: 0.1394\n",
      "Epoch [2/5], Step [9602/10336], Loss: 0.1630\n",
      "Epoch [2/5], Step [9604/10336], Loss: 0.0654\n",
      "Epoch [2/5], Step [9606/10336], Loss: 1.0492\n",
      "Epoch [2/5], Step [9608/10336], Loss: 0.5151\n",
      "Epoch [2/5], Step [9610/10336], Loss: 1.8181\n",
      "Epoch [2/5], Step [9612/10336], Loss: 0.2398\n",
      "Epoch [2/5], Step [9614/10336], Loss: 2.6111\n",
      "Epoch [2/5], Step [9616/10336], Loss: 0.5140\n",
      "Epoch [2/5], Step [9618/10336], Loss: 3.5457\n",
      "Epoch [2/5], Step [9620/10336], Loss: 1.6137\n",
      "Epoch [2/5], Step [9622/10336], Loss: 0.1353\n",
      "Epoch [2/5], Step [9624/10336], Loss: 0.8772\n",
      "Epoch [2/5], Step [9626/10336], Loss: 0.3046\n",
      "Epoch [2/5], Step [9628/10336], Loss: 0.2998\n",
      "Epoch [2/5], Step [9630/10336], Loss: 2.5225\n",
      "Epoch [2/5], Step [9632/10336], Loss: 0.9653\n",
      "Epoch [2/5], Step [9634/10336], Loss: 0.0196\n",
      "Epoch [2/5], Step [9636/10336], Loss: 0.2406\n",
      "Epoch [2/5], Step [9638/10336], Loss: 0.3152\n",
      "Epoch [2/5], Step [9640/10336], Loss: 4.8320\n",
      "Epoch [2/5], Step [9642/10336], Loss: 0.2982\n",
      "Epoch [2/5], Step [9644/10336], Loss: 2.7658\n",
      "Epoch [2/5], Step [9646/10336], Loss: 0.2662\n",
      "Epoch [2/5], Step [9648/10336], Loss: 0.3702\n",
      "Epoch [2/5], Step [9650/10336], Loss: 0.4672\n",
      "Epoch [2/5], Step [9652/10336], Loss: 0.0530\n",
      "Epoch [2/5], Step [9654/10336], Loss: 0.0145\n",
      "Epoch [2/5], Step [9656/10336], Loss: 0.2065\n",
      "Epoch [2/5], Step [9658/10336], Loss: 1.8774\n",
      "Epoch [2/5], Step [9660/10336], Loss: 0.0026\n",
      "Epoch [2/5], Step [9662/10336], Loss: 0.9771\n",
      "Epoch [2/5], Step [9664/10336], Loss: 0.7973\n",
      "Epoch [2/5], Step [9666/10336], Loss: 0.1035\n",
      "Epoch [2/5], Step [9668/10336], Loss: 0.3430\n",
      "Epoch [2/5], Step [9670/10336], Loss: 7.4672\n",
      "Epoch [2/5], Step [9672/10336], Loss: 0.3433\n",
      "Epoch [2/5], Step [9674/10336], Loss: 0.0122\n",
      "Epoch [2/5], Step [9676/10336], Loss: 0.0264\n",
      "Epoch [2/5], Step [9678/10336], Loss: 0.1051\n",
      "Epoch [2/5], Step [9680/10336], Loss: 0.0064\n",
      "Epoch [2/5], Step [9682/10336], Loss: 4.8801\n",
      "Epoch [2/5], Step [9684/10336], Loss: 0.2628\n",
      "Epoch [2/5], Step [9686/10336], Loss: 0.3643\n",
      "Epoch [2/5], Step [9688/10336], Loss: 0.2890\n",
      "Epoch [2/5], Step [9690/10336], Loss: 0.0050\n",
      "Epoch [2/5], Step [9692/10336], Loss: 1.5025\n",
      "Epoch [2/5], Step [9694/10336], Loss: 0.0397\n",
      "Epoch [2/5], Step [9696/10336], Loss: 0.1755\n",
      "Epoch [2/5], Step [9698/10336], Loss: 0.0349\n",
      "Epoch [2/5], Step [9700/10336], Loss: 0.0129\n",
      "Epoch [2/5], Step [9702/10336], Loss: 0.0557\n",
      "Epoch [2/5], Step [9704/10336], Loss: 0.5758\n",
      "Epoch [2/5], Step [9706/10336], Loss: 0.1160\n",
      "Epoch [2/5], Step [9708/10336], Loss: 1.9851\n",
      "Epoch [2/5], Step [9710/10336], Loss: 0.1388\n",
      "Epoch [2/5], Step [9712/10336], Loss: 0.3718\n",
      "Epoch [2/5], Step [9714/10336], Loss: 1.4671\n",
      "Epoch [2/5], Step [9716/10336], Loss: 0.5733\n",
      "Epoch [2/5], Step [9718/10336], Loss: 0.5613\n",
      "Epoch [2/5], Step [9720/10336], Loss: 0.0176\n",
      "Epoch [2/5], Step [9722/10336], Loss: 1.7550\n",
      "Epoch [2/5], Step [9724/10336], Loss: 1.2327\n",
      "Epoch [2/5], Step [9726/10336], Loss: 0.5907\n",
      "Epoch [2/5], Step [9728/10336], Loss: 0.0464\n",
      "Epoch [2/5], Step [9730/10336], Loss: 0.3755\n",
      "Epoch [2/5], Step [9732/10336], Loss: 0.0631\n",
      "Epoch [2/5], Step [9734/10336], Loss: 0.1145\n",
      "Epoch [2/5], Step [9736/10336], Loss: 0.8433\n",
      "Epoch [2/5], Step [9738/10336], Loss: 0.0368\n",
      "Epoch [2/5], Step [9740/10336], Loss: 0.0772\n",
      "Epoch [2/5], Step [9742/10336], Loss: 0.1074\n",
      "Epoch [2/5], Step [9744/10336], Loss: 1.0009\n",
      "Epoch [2/5], Step [9746/10336], Loss: 0.2941\n",
      "Epoch [2/5], Step [9748/10336], Loss: 0.3887\n",
      "Epoch [2/5], Step [9750/10336], Loss: 0.2022\n",
      "Epoch [2/5], Step [9752/10336], Loss: 0.0790\n",
      "Epoch [2/5], Step [9754/10336], Loss: 0.5770\n",
      "Epoch [2/5], Step [9756/10336], Loss: 0.0025\n",
      "Epoch [2/5], Step [9758/10336], Loss: 0.5269\n",
      "Epoch [2/5], Step [9760/10336], Loss: 0.0076\n",
      "Epoch [2/5], Step [9762/10336], Loss: 2.8414\n",
      "Epoch [2/5], Step [9764/10336], Loss: 0.1519\n",
      "Epoch [2/5], Step [9766/10336], Loss: 0.2764\n",
      "Epoch [2/5], Step [9768/10336], Loss: 0.0446\n",
      "Epoch [2/5], Step [9770/10336], Loss: 0.6390\n",
      "Epoch [2/5], Step [9772/10336], Loss: 0.6120\n",
      "Epoch [2/5], Step [9774/10336], Loss: 0.3472\n",
      "Epoch [2/5], Step [9776/10336], Loss: 0.0772\n",
      "Epoch [2/5], Step [9778/10336], Loss: 0.2521\n",
      "Epoch [2/5], Step [9780/10336], Loss: 0.9666\n",
      "Epoch [2/5], Step [9782/10336], Loss: 0.1725\n",
      "Epoch [2/5], Step [9784/10336], Loss: 0.9837\n",
      "Epoch [2/5], Step [9786/10336], Loss: 0.3460\n",
      "Epoch [2/5], Step [9788/10336], Loss: 0.4218\n",
      "Epoch [2/5], Step [9790/10336], Loss: 5.4289\n",
      "Epoch [2/5], Step [9792/10336], Loss: 0.2530\n",
      "Epoch [2/5], Step [9794/10336], Loss: 0.0939\n",
      "Epoch [2/5], Step [9796/10336], Loss: 0.7981\n",
      "Epoch [2/5], Step [9798/10336], Loss: 0.2642\n",
      "Epoch [2/5], Step [9800/10336], Loss: 1.0220\n",
      "Epoch [2/5], Step [9802/10336], Loss: 0.1102\n",
      "Epoch [2/5], Step [9804/10336], Loss: 1.3404\n",
      "Epoch [2/5], Step [9806/10336], Loss: 0.1188\n",
      "Epoch [2/5], Step [9808/10336], Loss: 0.0046\n",
      "Epoch [2/5], Step [9810/10336], Loss: 3.4131\n",
      "Epoch [2/5], Step [9812/10336], Loss: 0.0025\n",
      "Epoch [2/5], Step [9814/10336], Loss: 3.0103\n",
      "Epoch [2/5], Step [9816/10336], Loss: 3.7072\n",
      "Epoch [2/5], Step [9818/10336], Loss: 0.2110\n",
      "Epoch [2/5], Step [9820/10336], Loss: 0.2213\n",
      "Epoch [2/5], Step [9822/10336], Loss: 0.1986\n",
      "Epoch [2/5], Step [9824/10336], Loss: 0.8475\n",
      "Epoch [2/5], Step [9826/10336], Loss: 0.0482\n",
      "Epoch [2/5], Step [9828/10336], Loss: 0.5288\n",
      "Epoch [2/5], Step [9830/10336], Loss: 0.3276\n",
      "Epoch [2/5], Step [9832/10336], Loss: 0.0159\n",
      "Epoch [2/5], Step [9834/10336], Loss: 1.0416\n",
      "Epoch [2/5], Step [9836/10336], Loss: 0.4299\n",
      "Epoch [2/5], Step [9838/10336], Loss: 2.3975\n",
      "Epoch [2/5], Step [9840/10336], Loss: 0.0070\n",
      "Epoch [2/5], Step [9842/10336], Loss: 2.2324\n",
      "Epoch [2/5], Step [9844/10336], Loss: 0.0107\n",
      "Epoch [2/5], Step [9846/10336], Loss: 0.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [9848/10336], Loss: 0.8403\n",
      "Epoch [2/5], Step [9850/10336], Loss: 0.0574\n",
      "Epoch [2/5], Step [9852/10336], Loss: 0.0059\n",
      "Epoch [2/5], Step [9854/10336], Loss: 2.3138\n",
      "Epoch [2/5], Step [9856/10336], Loss: 0.0302\n",
      "Epoch [2/5], Step [9858/10336], Loss: 0.8504\n",
      "Epoch [2/5], Step [9860/10336], Loss: 4.2698\n",
      "Epoch [2/5], Step [9862/10336], Loss: 0.2067\n",
      "Epoch [2/5], Step [9864/10336], Loss: 0.1154\n",
      "Epoch [2/5], Step [9866/10336], Loss: 0.0683\n",
      "Epoch [2/5], Step [9868/10336], Loss: 0.1898\n",
      "Epoch [2/5], Step [9870/10336], Loss: 0.2038\n",
      "Epoch [2/5], Step [9872/10336], Loss: 2.4731\n",
      "Epoch [2/5], Step [9874/10336], Loss: 0.1753\n",
      "Epoch [2/5], Step [9876/10336], Loss: 0.0813\n",
      "Epoch [2/5], Step [9878/10336], Loss: 0.5880\n",
      "Epoch [2/5], Step [9880/10336], Loss: 0.9251\n",
      "Epoch [2/5], Step [9882/10336], Loss: 2.1866\n",
      "Epoch [2/5], Step [9884/10336], Loss: 0.6333\n",
      "Epoch [2/5], Step [9886/10336], Loss: 0.0845\n",
      "Epoch [2/5], Step [9888/10336], Loss: 0.3983\n",
      "Epoch [2/5], Step [9890/10336], Loss: 0.2910\n",
      "Epoch [2/5], Step [9892/10336], Loss: 0.1611\n",
      "Epoch [2/5], Step [9894/10336], Loss: 0.0025\n",
      "Epoch [2/5], Step [9896/10336], Loss: 0.1727\n",
      "Epoch [2/5], Step [9898/10336], Loss: 1.6188\n",
      "Epoch [2/5], Step [9900/10336], Loss: 4.1124\n",
      "Epoch [2/5], Step [9902/10336], Loss: 1.0445\n",
      "Epoch [2/5], Step [9904/10336], Loss: 0.6463\n",
      "Epoch [2/5], Step [9906/10336], Loss: 0.1185\n",
      "Epoch [2/5], Step [9908/10336], Loss: 0.0033\n",
      "Epoch [2/5], Step [9910/10336], Loss: 0.0522\n",
      "Epoch [2/5], Step [9912/10336], Loss: 0.8041\n",
      "Epoch [2/5], Step [9914/10336], Loss: 0.2453\n",
      "Epoch [2/5], Step [9916/10336], Loss: 4.8104\n",
      "Epoch [2/5], Step [9918/10336], Loss: 0.3276\n",
      "Epoch [2/5], Step [9920/10336], Loss: 2.0867\n",
      "Epoch [2/5], Step [9922/10336], Loss: 3.3196\n",
      "Epoch [2/5], Step [9924/10336], Loss: 0.7195\n",
      "Epoch [2/5], Step [9926/10336], Loss: 0.5976\n",
      "Epoch [2/5], Step [9928/10336], Loss: 0.0401\n",
      "Epoch [2/5], Step [9930/10336], Loss: 0.7537\n",
      "Epoch [2/5], Step [9932/10336], Loss: 0.5699\n",
      "Epoch [2/5], Step [9934/10336], Loss: 0.0153\n",
      "Epoch [2/5], Step [9936/10336], Loss: 2.2196\n",
      "Epoch [2/5], Step [9938/10336], Loss: 2.1079\n",
      "Epoch [2/5], Step [9940/10336], Loss: 3.5544\n",
      "Epoch [2/5], Step [9942/10336], Loss: 0.0986\n",
      "Epoch [2/5], Step [9944/10336], Loss: 0.4948\n",
      "Epoch [2/5], Step [9946/10336], Loss: 0.1089\n",
      "Epoch [2/5], Step [9948/10336], Loss: 0.6689\n",
      "Epoch [2/5], Step [9950/10336], Loss: 0.1032\n",
      "Epoch [2/5], Step [9952/10336], Loss: 0.5942\n",
      "Epoch [2/5], Step [9954/10336], Loss: 0.1272\n",
      "Epoch [2/5], Step [9956/10336], Loss: 0.3484\n",
      "Epoch [2/5], Step [9958/10336], Loss: 0.2195\n",
      "Epoch [2/5], Step [9960/10336], Loss: 1.2672\n",
      "Epoch [2/5], Step [9962/10336], Loss: 0.0761\n",
      "Epoch [2/5], Step [9964/10336], Loss: 2.0675\n",
      "Epoch [2/5], Step [9966/10336], Loss: 0.1983\n",
      "Epoch [2/5], Step [9968/10336], Loss: 0.0081\n",
      "Epoch [2/5], Step [9970/10336], Loss: 0.8326\n",
      "Epoch [2/5], Step [9972/10336], Loss: 0.3125\n",
      "Epoch [2/5], Step [9974/10336], Loss: 2.7984\n",
      "Epoch [2/5], Step [9976/10336], Loss: 0.0413\n",
      "Epoch [2/5], Step [9978/10336], Loss: 4.0349\n",
      "Epoch [2/5], Step [9980/10336], Loss: 0.3612\n",
      "Epoch [2/5], Step [9982/10336], Loss: 2.4243\n",
      "Epoch [2/5], Step [9984/10336], Loss: 1.2179\n",
      "Epoch [2/5], Step [9986/10336], Loss: 0.2403\n",
      "Epoch [2/5], Step [9988/10336], Loss: 0.0317\n",
      "Epoch [2/5], Step [9990/10336], Loss: 0.4260\n",
      "Epoch [2/5], Step [9992/10336], Loss: 0.3944\n",
      "Epoch [2/5], Step [9994/10336], Loss: 0.0148\n",
      "Epoch [2/5], Step [9996/10336], Loss: 0.0394\n",
      "Epoch [2/5], Step [9998/10336], Loss: 0.0882\n",
      "Epoch [2/5], Step [10000/10336], Loss: 2.4598\n",
      "Epoch [2/5], Step [10002/10336], Loss: 6.3561\n",
      "Epoch [2/5], Step [10004/10336], Loss: 4.1519\n",
      "Epoch [2/5], Step [10006/10336], Loss: 1.1188\n",
      "Epoch [2/5], Step [10008/10336], Loss: 0.1146\n",
      "Epoch [2/5], Step [10010/10336], Loss: 0.2564\n",
      "Epoch [2/5], Step [10012/10336], Loss: 1.0753\n",
      "Epoch [2/5], Step [10014/10336], Loss: 0.6675\n",
      "Epoch [2/5], Step [10016/10336], Loss: 0.2113\n",
      "Epoch [2/5], Step [10018/10336], Loss: 2.2212\n",
      "Epoch [2/5], Step [10020/10336], Loss: 2.4667\n",
      "Epoch [2/5], Step [10022/10336], Loss: 1.9736\n",
      "Epoch [2/5], Step [10024/10336], Loss: 1.6037\n",
      "Epoch [2/5], Step [10026/10336], Loss: 0.0185\n",
      "Epoch [2/5], Step [10028/10336], Loss: 0.0343\n",
      "Epoch [2/5], Step [10030/10336], Loss: 0.2255\n",
      "Epoch [2/5], Step [10032/10336], Loss: 0.1382\n",
      "Epoch [2/5], Step [10034/10336], Loss: 0.2929\n",
      "Epoch [2/5], Step [10036/10336], Loss: 0.3702\n",
      "Epoch [2/5], Step [10038/10336], Loss: 0.0276\n",
      "Epoch [2/5], Step [10040/10336], Loss: 0.3713\n",
      "Epoch [2/5], Step [10042/10336], Loss: 0.2153\n",
      "Epoch [2/5], Step [10044/10336], Loss: 0.0324\n",
      "Epoch [2/5], Step [10046/10336], Loss: 0.3179\n",
      "Epoch [2/5], Step [10048/10336], Loss: 2.0342\n",
      "Epoch [2/5], Step [10050/10336], Loss: 0.1020\n",
      "Epoch [2/5], Step [10052/10336], Loss: 0.1469\n",
      "Epoch [2/5], Step [10054/10336], Loss: 2.3341\n",
      "Epoch [2/5], Step [10056/10336], Loss: 2.5164\n",
      "Epoch [2/5], Step [10058/10336], Loss: 0.0028\n",
      "Epoch [2/5], Step [10060/10336], Loss: 0.0237\n",
      "Epoch [2/5], Step [10062/10336], Loss: 2.7507\n",
      "Epoch [2/5], Step [10064/10336], Loss: 0.3283\n",
      "Epoch [2/5], Step [10066/10336], Loss: 0.0044\n",
      "Epoch [2/5], Step [10068/10336], Loss: 0.3415\n",
      "Epoch [2/5], Step [10070/10336], Loss: 0.0436\n",
      "Epoch [2/5], Step [10072/10336], Loss: 2.8897\n",
      "Epoch [2/5], Step [10074/10336], Loss: 0.0022\n",
      "Epoch [2/5], Step [10076/10336], Loss: 0.0687\n",
      "Epoch [2/5], Step [10078/10336], Loss: 0.2756\n",
      "Epoch [2/5], Step [10080/10336], Loss: 4.0902\n",
      "Epoch [2/5], Step [10082/10336], Loss: 0.0240\n",
      "Epoch [2/5], Step [10084/10336], Loss: 0.0140\n",
      "Epoch [2/5], Step [10086/10336], Loss: 1.2060\n",
      "Epoch [2/5], Step [10088/10336], Loss: 1.0651\n",
      "Epoch [2/5], Step [10090/10336], Loss: 1.5984\n",
      "Epoch [2/5], Step [10092/10336], Loss: 1.5636\n",
      "Epoch [2/5], Step [10094/10336], Loss: 0.1328\n",
      "Epoch [2/5], Step [10096/10336], Loss: 0.2089\n",
      "Epoch [2/5], Step [10098/10336], Loss: 2.9775\n",
      "Epoch [2/5], Step [10100/10336], Loss: 0.2728\n",
      "Epoch [2/5], Step [10102/10336], Loss: 0.1801\n",
      "Epoch [2/5], Step [10104/10336], Loss: 0.3935\n",
      "Epoch [2/5], Step [10106/10336], Loss: 0.7598\n",
      "Epoch [2/5], Step [10108/10336], Loss: 0.0672\n",
      "Epoch [2/5], Step [10110/10336], Loss: 0.3657\n",
      "Epoch [2/5], Step [10112/10336], Loss: 0.2546\n",
      "Epoch [2/5], Step [10114/10336], Loss: 0.4051\n",
      "Epoch [2/5], Step [10116/10336], Loss: 0.2852\n",
      "Epoch [2/5], Step [10118/10336], Loss: 0.3683\n",
      "Epoch [2/5], Step [10120/10336], Loss: 2.0711\n",
      "Epoch [2/5], Step [10122/10336], Loss: 0.0385\n",
      "Epoch [2/5], Step [10124/10336], Loss: 1.4829\n",
      "Epoch [2/5], Step [10126/10336], Loss: 3.9543\n",
      "Epoch [2/5], Step [10128/10336], Loss: 7.1103\n",
      "Epoch [2/5], Step [10130/10336], Loss: 0.0002\n",
      "Epoch [2/5], Step [10132/10336], Loss: 0.3926\n",
      "Epoch [2/5], Step [10134/10336], Loss: 0.0442\n",
      "Epoch [2/5], Step [10136/10336], Loss: 0.8065\n",
      "Epoch [2/5], Step [10138/10336], Loss: 3.3186\n",
      "Epoch [2/5], Step [10140/10336], Loss: 3.4757\n",
      "Epoch [2/5], Step [10142/10336], Loss: 0.0173\n",
      "Epoch [2/5], Step [10144/10336], Loss: 0.0189\n",
      "Epoch [2/5], Step [10146/10336], Loss: 0.1033\n",
      "Epoch [2/5], Step [10148/10336], Loss: 2.7361\n",
      "Epoch [2/5], Step [10150/10336], Loss: 0.1875\n",
      "Epoch [2/5], Step [10152/10336], Loss: 0.0034\n",
      "Epoch [2/5], Step [10154/10336], Loss: 0.0418\n",
      "Epoch [2/5], Step [10156/10336], Loss: 0.0200\n",
      "Epoch [2/5], Step [10158/10336], Loss: 0.4552\n",
      "Epoch [2/5], Step [10160/10336], Loss: 3.1782\n",
      "Epoch [2/5], Step [10162/10336], Loss: 0.0198\n",
      "Epoch [2/5], Step [10164/10336], Loss: 0.0695\n",
      "Epoch [2/5], Step [10166/10336], Loss: 2.2599\n",
      "Epoch [2/5], Step [10168/10336], Loss: 3.4673\n",
      "Epoch [2/5], Step [10170/10336], Loss: 0.8606\n",
      "Epoch [2/5], Step [10172/10336], Loss: 0.4918\n",
      "Epoch [2/5], Step [10174/10336], Loss: 2.1955\n",
      "Epoch [2/5], Step [10176/10336], Loss: 1.5082\n",
      "Epoch [2/5], Step [10178/10336], Loss: 0.1791\n",
      "Epoch [2/5], Step [10180/10336], Loss: 4.0862\n",
      "Epoch [2/5], Step [10182/10336], Loss: 0.2967\n",
      "Epoch [2/5], Step [10184/10336], Loss: 3.9276\n",
      "Epoch [2/5], Step [10186/10336], Loss: 0.0568\n",
      "Epoch [2/5], Step [10188/10336], Loss: 0.1200\n",
      "Epoch [2/5], Step [10190/10336], Loss: 0.0635\n",
      "Epoch [2/5], Step [10192/10336], Loss: 0.1537\n",
      "Epoch [2/5], Step [10194/10336], Loss: 0.1086\n",
      "Epoch [2/5], Step [10196/10336], Loss: 0.5413\n",
      "Epoch [2/5], Step [10198/10336], Loss: 1.3286\n",
      "Epoch [2/5], Step [10200/10336], Loss: 0.2492\n",
      "Epoch [2/5], Step [10202/10336], Loss: 0.1895\n",
      "Epoch [2/5], Step [10204/10336], Loss: 1.9259\n",
      "Epoch [2/5], Step [10206/10336], Loss: 0.8868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [10208/10336], Loss: 0.0720\n",
      "Epoch [2/5], Step [10210/10336], Loss: 2.8703\n",
      "Epoch [2/5], Step [10212/10336], Loss: 1.1498\n",
      "Epoch [2/5], Step [10214/10336], Loss: 1.8965\n",
      "Epoch [2/5], Step [10216/10336], Loss: 0.1925\n",
      "Epoch [2/5], Step [10218/10336], Loss: 0.3154\n",
      "Epoch [2/5], Step [10220/10336], Loss: 0.6390\n",
      "Epoch [2/5], Step [10222/10336], Loss: 0.4651\n",
      "Epoch [2/5], Step [10224/10336], Loss: 0.3091\n",
      "Epoch [2/5], Step [10226/10336], Loss: 0.1359\n",
      "Epoch [2/5], Step [10228/10336], Loss: 0.1653\n",
      "Epoch [2/5], Step [10230/10336], Loss: 1.0345\n",
      "Epoch [2/5], Step [10232/10336], Loss: 0.5787\n",
      "Epoch [2/5], Step [10234/10336], Loss: 0.1742\n",
      "Epoch [2/5], Step [10236/10336], Loss: 0.3626\n",
      "Epoch [2/5], Step [10238/10336], Loss: 0.0579\n",
      "Epoch [2/5], Step [10240/10336], Loss: 0.0557\n",
      "Epoch [2/5], Step [10242/10336], Loss: 0.2542\n",
      "Epoch [2/5], Step [10244/10336], Loss: 0.0098\n",
      "Epoch [2/5], Step [10246/10336], Loss: 0.2389\n",
      "Epoch [2/5], Step [10248/10336], Loss: 0.0121\n",
      "Epoch [2/5], Step [10250/10336], Loss: 0.3305\n",
      "Epoch [2/5], Step [10252/10336], Loss: 0.1699\n",
      "Epoch [2/5], Step [10254/10336], Loss: 1.2221\n",
      "Epoch [2/5], Step [10256/10336], Loss: 0.3990\n",
      "Epoch [2/5], Step [10258/10336], Loss: 1.5438\n",
      "Epoch [2/5], Step [10260/10336], Loss: 0.9983\n",
      "Epoch [2/5], Step [10262/10336], Loss: 0.7281\n",
      "Epoch [2/5], Step [10264/10336], Loss: 0.6054\n",
      "Epoch [2/5], Step [10266/10336], Loss: 0.1101\n",
      "Epoch [2/5], Step [10268/10336], Loss: 0.3348\n",
      "Epoch [2/5], Step [10270/10336], Loss: 4.6777\n",
      "Epoch [2/5], Step [10272/10336], Loss: 2.0844\n",
      "Epoch [2/5], Step [10274/10336], Loss: 0.2364\n",
      "Epoch [2/5], Step [10276/10336], Loss: 1.8506\n",
      "Epoch [2/5], Step [10278/10336], Loss: 1.5415\n",
      "Epoch [2/5], Step [10280/10336], Loss: 0.1856\n",
      "Epoch [2/5], Step [10282/10336], Loss: 1.2160\n",
      "Epoch [2/5], Step [10284/10336], Loss: 5.7383\n",
      "Epoch [2/5], Step [10286/10336], Loss: 0.3583\n",
      "Epoch [2/5], Step [10288/10336], Loss: 0.0166\n",
      "Epoch [2/5], Step [10290/10336], Loss: 4.1169\n",
      "Epoch [2/5], Step [10292/10336], Loss: 3.8143\n",
      "Epoch [2/5], Step [10294/10336], Loss: 0.1112\n",
      "Epoch [2/5], Step [10296/10336], Loss: 4.2999\n",
      "Epoch [2/5], Step [10298/10336], Loss: 0.8779\n",
      "Epoch [2/5], Step [10300/10336], Loss: 0.8654\n",
      "Epoch [2/5], Step [10302/10336], Loss: 0.6372\n",
      "Epoch [2/5], Step [10304/10336], Loss: 0.6146\n",
      "Epoch [2/5], Step [10306/10336], Loss: 1.0204\n",
      "Epoch [2/5], Step [10308/10336], Loss: 0.7155\n",
      "Epoch [2/5], Step [10310/10336], Loss: 0.3914\n",
      "Epoch [2/5], Step [10312/10336], Loss: 0.0524\n",
      "Epoch [2/5], Step [10314/10336], Loss: 0.1098\n",
      "Epoch [2/5], Step [10316/10336], Loss: 0.0125\n",
      "Epoch [2/5], Step [10318/10336], Loss: 0.0166\n",
      "Epoch [2/5], Step [10320/10336], Loss: 0.1084\n",
      "Epoch [2/5], Step [10322/10336], Loss: 3.6480\n",
      "Epoch [2/5], Step [10324/10336], Loss: 0.3140\n",
      "Epoch [2/5], Step [10326/10336], Loss: 1.8126\n",
      "Epoch [2/5], Step [10328/10336], Loss: 0.2379\n",
      "Epoch [2/5], Step [10330/10336], Loss: 0.3871\n",
      "Epoch [2/5], Step [10332/10336], Loss: 0.1498\n",
      "Epoch [2/5], Step [10334/10336], Loss: 0.0268\n",
      "Epoch [2/5], Step [10336/10336], Loss: 0.5699\n",
      "Epoch [3/5], Step [2/10336], Loss: 0.3099\n",
      "Epoch [3/5], Step [4/10336], Loss: 2.7667\n",
      "Epoch [3/5], Step [6/10336], Loss: 0.4411\n",
      "Epoch [3/5], Step [8/10336], Loss: 0.3835\n",
      "Epoch [3/5], Step [10/10336], Loss: 1.3452\n",
      "Epoch [3/5], Step [12/10336], Loss: 0.0380\n",
      "Epoch [3/5], Step [14/10336], Loss: 0.0074\n",
      "Epoch [3/5], Step [16/10336], Loss: 0.1428\n",
      "Epoch [3/5], Step [18/10336], Loss: 1.1027\n",
      "Epoch [3/5], Step [20/10336], Loss: 0.6992\n",
      "Epoch [3/5], Step [22/10336], Loss: 0.1557\n",
      "Epoch [3/5], Step [24/10336], Loss: 0.0472\n",
      "Epoch [3/5], Step [26/10336], Loss: 2.7908\n",
      "Epoch [3/5], Step [28/10336], Loss: 0.0319\n",
      "Epoch [3/5], Step [30/10336], Loss: 0.2420\n",
      "Epoch [3/5], Step [32/10336], Loss: 0.1636\n",
      "Epoch [3/5], Step [34/10336], Loss: 3.1328\n",
      "Epoch [3/5], Step [36/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [38/10336], Loss: 0.0654\n",
      "Epoch [3/5], Step [40/10336], Loss: 3.8616\n",
      "Epoch [3/5], Step [42/10336], Loss: 0.2298\n",
      "Epoch [3/5], Step [44/10336], Loss: 4.9735\n",
      "Epoch [3/5], Step [46/10336], Loss: 3.2940\n",
      "Epoch [3/5], Step [48/10336], Loss: 0.0941\n",
      "Epoch [3/5], Step [50/10336], Loss: 0.2823\n",
      "Epoch [3/5], Step [52/10336], Loss: 0.1466\n",
      "Epoch [3/5], Step [54/10336], Loss: 0.5415\n",
      "Epoch [3/5], Step [56/10336], Loss: 0.5075\n",
      "Epoch [3/5], Step [58/10336], Loss: 0.1461\n",
      "Epoch [3/5], Step [60/10336], Loss: 0.0054\n",
      "Epoch [3/5], Step [62/10336], Loss: 0.1153\n",
      "Epoch [3/5], Step [64/10336], Loss: 0.8843\n",
      "Epoch [3/5], Step [66/10336], Loss: 0.3155\n",
      "Epoch [3/5], Step [68/10336], Loss: 0.2714\n",
      "Epoch [3/5], Step [70/10336], Loss: 4.1113\n",
      "Epoch [3/5], Step [72/10336], Loss: 0.7974\n",
      "Epoch [3/5], Step [74/10336], Loss: 0.8288\n",
      "Epoch [3/5], Step [76/10336], Loss: 0.8026\n",
      "Epoch [3/5], Step [78/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [80/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [82/10336], Loss: 0.3570\n",
      "Epoch [3/5], Step [84/10336], Loss: 0.2343\n",
      "Epoch [3/5], Step [86/10336], Loss: 0.1023\n",
      "Epoch [3/5], Step [88/10336], Loss: 0.4908\n",
      "Epoch [3/5], Step [90/10336], Loss: 0.3430\n",
      "Epoch [3/5], Step [92/10336], Loss: 3.4681\n",
      "Epoch [3/5], Step [94/10336], Loss: 0.0351\n",
      "Epoch [3/5], Step [96/10336], Loss: 2.6888\n",
      "Epoch [3/5], Step [98/10336], Loss: 0.0183\n",
      "Epoch [3/5], Step [100/10336], Loss: 0.0616\n",
      "Epoch [3/5], Step [102/10336], Loss: 0.0598\n",
      "Epoch [3/5], Step [104/10336], Loss: 0.2433\n",
      "Epoch [3/5], Step [106/10336], Loss: 0.0248\n",
      "Epoch [3/5], Step [108/10336], Loss: 0.3720\n",
      "Epoch [3/5], Step [110/10336], Loss: 1.1344\n",
      "Epoch [3/5], Step [112/10336], Loss: 0.3585\n",
      "Epoch [3/5], Step [114/10336], Loss: 0.3004\n",
      "Epoch [3/5], Step [116/10336], Loss: 1.5189\n",
      "Epoch [3/5], Step [118/10336], Loss: 0.3349\n",
      "Epoch [3/5], Step [120/10336], Loss: 0.1678\n",
      "Epoch [3/5], Step [122/10336], Loss: 0.1404\n",
      "Epoch [3/5], Step [124/10336], Loss: 0.7717\n",
      "Epoch [3/5], Step [126/10336], Loss: 3.5220\n",
      "Epoch [3/5], Step [128/10336], Loss: 0.6864\n",
      "Epoch [3/5], Step [130/10336], Loss: 0.1607\n",
      "Epoch [3/5], Step [132/10336], Loss: 0.0963\n",
      "Epoch [3/5], Step [134/10336], Loss: 0.1183\n",
      "Epoch [3/5], Step [136/10336], Loss: 0.0511\n",
      "Epoch [3/5], Step [138/10336], Loss: 0.2588\n",
      "Epoch [3/5], Step [140/10336], Loss: 0.1276\n",
      "Epoch [3/5], Step [142/10336], Loss: 1.4800\n",
      "Epoch [3/5], Step [144/10336], Loss: 2.4174\n",
      "Epoch [3/5], Step [146/10336], Loss: 0.2371\n",
      "Epoch [3/5], Step [148/10336], Loss: 0.1242\n",
      "Epoch [3/5], Step [150/10336], Loss: 0.0051\n",
      "Epoch [3/5], Step [152/10336], Loss: 0.3067\n",
      "Epoch [3/5], Step [154/10336], Loss: 0.0870\n",
      "Epoch [3/5], Step [156/10336], Loss: 0.8984\n",
      "Epoch [3/5], Step [158/10336], Loss: 2.8388\n",
      "Epoch [3/5], Step [160/10336], Loss: 0.0506\n",
      "Epoch [3/5], Step [162/10336], Loss: 0.8655\n",
      "Epoch [3/5], Step [164/10336], Loss: 0.4044\n",
      "Epoch [3/5], Step [166/10336], Loss: 0.0710\n",
      "Epoch [3/5], Step [168/10336], Loss: 0.1966\n",
      "Epoch [3/5], Step [170/10336], Loss: 0.2141\n",
      "Epoch [3/5], Step [172/10336], Loss: 4.4332\n",
      "Epoch [3/5], Step [174/10336], Loss: 0.5108\n",
      "Epoch [3/5], Step [176/10336], Loss: 0.0230\n",
      "Epoch [3/5], Step [178/10336], Loss: 0.1039\n",
      "Epoch [3/5], Step [180/10336], Loss: 2.5412\n",
      "Epoch [3/5], Step [182/10336], Loss: 0.1682\n",
      "Epoch [3/5], Step [184/10336], Loss: 0.0235\n",
      "Epoch [3/5], Step [186/10336], Loss: 3.9302\n",
      "Epoch [3/5], Step [188/10336], Loss: 1.4414\n",
      "Epoch [3/5], Step [190/10336], Loss: 1.3669\n",
      "Epoch [3/5], Step [192/10336], Loss: 0.0012\n",
      "Epoch [3/5], Step [194/10336], Loss: 0.1429\n",
      "Epoch [3/5], Step [196/10336], Loss: 0.2915\n",
      "Epoch [3/5], Step [198/10336], Loss: 2.7393\n",
      "Epoch [3/5], Step [200/10336], Loss: 0.1066\n",
      "Epoch [3/5], Step [202/10336], Loss: 0.0247\n",
      "Epoch [3/5], Step [204/10336], Loss: 0.0819\n",
      "Epoch [3/5], Step [206/10336], Loss: 0.1744\n",
      "Epoch [3/5], Step [208/10336], Loss: 3.5301\n",
      "Epoch [3/5], Step [210/10336], Loss: 3.7384\n",
      "Epoch [3/5], Step [212/10336], Loss: 1.2172\n",
      "Epoch [3/5], Step [214/10336], Loss: 0.2841\n",
      "Epoch [3/5], Step [216/10336], Loss: 0.7925\n",
      "Epoch [3/5], Step [218/10336], Loss: 0.4102\n",
      "Epoch [3/5], Step [220/10336], Loss: 0.2166\n",
      "Epoch [3/5], Step [222/10336], Loss: 1.7190\n",
      "Epoch [3/5], Step [224/10336], Loss: 0.2724\n",
      "Epoch [3/5], Step [226/10336], Loss: 0.3145\n",
      "Epoch [3/5], Step [228/10336], Loss: 2.3210\n",
      "Epoch [3/5], Step [230/10336], Loss: 1.0039\n",
      "Epoch [3/5], Step [232/10336], Loss: 0.0363\n",
      "Epoch [3/5], Step [234/10336], Loss: 0.2511\n",
      "Epoch [3/5], Step [236/10336], Loss: 0.0283\n",
      "Epoch [3/5], Step [238/10336], Loss: 0.1417\n",
      "Epoch [3/5], Step [240/10336], Loss: 3.0159\n",
      "Epoch [3/5], Step [242/10336], Loss: 0.9732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [244/10336], Loss: 0.4211\n",
      "Epoch [3/5], Step [246/10336], Loss: 0.0650\n",
      "Epoch [3/5], Step [248/10336], Loss: 0.2795\n",
      "Epoch [3/5], Step [250/10336], Loss: 0.0305\n",
      "Epoch [3/5], Step [252/10336], Loss: 3.5593\n",
      "Epoch [3/5], Step [254/10336], Loss: 0.3590\n",
      "Epoch [3/5], Step [256/10336], Loss: 0.1035\n",
      "Epoch [3/5], Step [258/10336], Loss: 0.0259\n",
      "Epoch [3/5], Step [260/10336], Loss: 0.2111\n",
      "Epoch [3/5], Step [262/10336], Loss: 0.1963\n",
      "Epoch [3/5], Step [264/10336], Loss: 0.2799\n",
      "Epoch [3/5], Step [266/10336], Loss: 0.1571\n",
      "Epoch [3/5], Step [268/10336], Loss: 0.2498\n",
      "Epoch [3/5], Step [270/10336], Loss: 0.2807\n",
      "Epoch [3/5], Step [272/10336], Loss: 5.2106\n",
      "Epoch [3/5], Step [274/10336], Loss: 0.2452\n",
      "Epoch [3/5], Step [276/10336], Loss: 0.3215\n",
      "Epoch [3/5], Step [278/10336], Loss: 0.1687\n",
      "Epoch [3/5], Step [280/10336], Loss: 0.2802\n",
      "Epoch [3/5], Step [282/10336], Loss: 0.0450\n",
      "Epoch [3/5], Step [284/10336], Loss: 0.5782\n",
      "Epoch [3/5], Step [286/10336], Loss: 0.0801\n",
      "Epoch [3/5], Step [288/10336], Loss: 0.8453\n",
      "Epoch [3/5], Step [290/10336], Loss: 1.2780\n",
      "Epoch [3/5], Step [292/10336], Loss: 3.1379\n",
      "Epoch [3/5], Step [294/10336], Loss: 0.3395\n",
      "Epoch [3/5], Step [296/10336], Loss: 0.0212\n",
      "Epoch [3/5], Step [298/10336], Loss: 0.1665\n",
      "Epoch [3/5], Step [300/10336], Loss: 2.7232\n",
      "Epoch [3/5], Step [302/10336], Loss: 1.5683\n",
      "Epoch [3/5], Step [304/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [306/10336], Loss: 0.4043\n",
      "Epoch [3/5], Step [308/10336], Loss: 0.9192\n",
      "Epoch [3/5], Step [310/10336], Loss: 0.0468\n",
      "Epoch [3/5], Step [312/10336], Loss: 0.1524\n",
      "Epoch [3/5], Step [314/10336], Loss: 0.4115\n",
      "Epoch [3/5], Step [316/10336], Loss: 0.1760\n",
      "Epoch [3/5], Step [318/10336], Loss: 1.6059\n",
      "Epoch [3/5], Step [320/10336], Loss: 2.7512\n",
      "Epoch [3/5], Step [322/10336], Loss: 0.3227\n",
      "Epoch [3/5], Step [324/10336], Loss: 0.0029\n",
      "Epoch [3/5], Step [326/10336], Loss: 1.1622\n",
      "Epoch [3/5], Step [328/10336], Loss: 0.0198\n",
      "Epoch [3/5], Step [330/10336], Loss: 1.7215\n",
      "Epoch [3/5], Step [332/10336], Loss: 1.0936\n",
      "Epoch [3/5], Step [334/10336], Loss: 0.0216\n",
      "Epoch [3/5], Step [336/10336], Loss: 0.5082\n",
      "Epoch [3/5], Step [338/10336], Loss: 0.0314\n",
      "Epoch [3/5], Step [340/10336], Loss: 3.0685\n",
      "Epoch [3/5], Step [342/10336], Loss: 0.7884\n",
      "Epoch [3/5], Step [344/10336], Loss: 0.0628\n",
      "Epoch [3/5], Step [346/10336], Loss: 0.3187\n",
      "Epoch [3/5], Step [348/10336], Loss: 0.0207\n",
      "Epoch [3/5], Step [350/10336], Loss: 0.1976\n",
      "Epoch [3/5], Step [352/10336], Loss: 6.0527\n",
      "Epoch [3/5], Step [354/10336], Loss: 2.3612\n",
      "Epoch [3/5], Step [356/10336], Loss: 1.0859\n",
      "Epoch [3/5], Step [358/10336], Loss: 0.0572\n",
      "Epoch [3/5], Step [360/10336], Loss: 2.7422\n",
      "Epoch [3/5], Step [362/10336], Loss: 0.0385\n",
      "Epoch [3/5], Step [364/10336], Loss: 0.8999\n",
      "Epoch [3/5], Step [366/10336], Loss: 0.9155\n",
      "Epoch [3/5], Step [368/10336], Loss: 3.5175\n",
      "Epoch [3/5], Step [370/10336], Loss: 0.3043\n",
      "Epoch [3/5], Step [372/10336], Loss: 2.5165\n",
      "Epoch [3/5], Step [374/10336], Loss: 4.1694\n",
      "Epoch [3/5], Step [376/10336], Loss: 0.0964\n",
      "Epoch [3/5], Step [378/10336], Loss: 0.1652\n",
      "Epoch [3/5], Step [380/10336], Loss: 0.3221\n",
      "Epoch [3/5], Step [382/10336], Loss: 1.4976\n",
      "Epoch [3/5], Step [384/10336], Loss: 1.2379\n",
      "Epoch [3/5], Step [386/10336], Loss: 1.9931\n",
      "Epoch [3/5], Step [388/10336], Loss: 0.9338\n",
      "Epoch [3/5], Step [390/10336], Loss: 0.1276\n",
      "Epoch [3/5], Step [392/10336], Loss: 0.3263\n",
      "Epoch [3/5], Step [394/10336], Loss: 0.3572\n",
      "Epoch [3/5], Step [396/10336], Loss: 2.6090\n",
      "Epoch [3/5], Step [398/10336], Loss: 0.4570\n",
      "Epoch [3/5], Step [400/10336], Loss: 0.3884\n",
      "Epoch [3/5], Step [402/10336], Loss: 0.3351\n",
      "Epoch [3/5], Step [404/10336], Loss: 3.9415\n",
      "Epoch [3/5], Step [406/10336], Loss: 0.0725\n",
      "Epoch [3/5], Step [408/10336], Loss: 0.0739\n",
      "Epoch [3/5], Step [410/10336], Loss: 2.7301\n",
      "Epoch [3/5], Step [412/10336], Loss: 4.0633\n",
      "Epoch [3/5], Step [414/10336], Loss: 0.1966\n",
      "Epoch [3/5], Step [416/10336], Loss: 0.0195\n",
      "Epoch [3/5], Step [418/10336], Loss: 0.4970\n",
      "Epoch [3/5], Step [420/10336], Loss: 3.2993\n",
      "Epoch [3/5], Step [422/10336], Loss: 0.3409\n",
      "Epoch [3/5], Step [424/10336], Loss: 4.7977\n",
      "Epoch [3/5], Step [426/10336], Loss: 1.1785\n",
      "Epoch [3/5], Step [428/10336], Loss: 1.6134\n",
      "Epoch [3/5], Step [430/10336], Loss: 1.4578\n",
      "Epoch [3/5], Step [432/10336], Loss: 0.2606\n",
      "Epoch [3/5], Step [434/10336], Loss: 0.1901\n",
      "Epoch [3/5], Step [436/10336], Loss: 0.0200\n",
      "Epoch [3/5], Step [438/10336], Loss: 1.2621\n",
      "Epoch [3/5], Step [440/10336], Loss: 0.2992\n",
      "Epoch [3/5], Step [442/10336], Loss: 0.3329\n",
      "Epoch [3/5], Step [444/10336], Loss: 1.0017\n",
      "Epoch [3/5], Step [446/10336], Loss: 0.0582\n",
      "Epoch [3/5], Step [448/10336], Loss: 0.1260\n",
      "Epoch [3/5], Step [450/10336], Loss: 0.0071\n",
      "Epoch [3/5], Step [452/10336], Loss: 0.6242\n",
      "Epoch [3/5], Step [454/10336], Loss: 0.7306\n",
      "Epoch [3/5], Step [456/10336], Loss: 0.2224\n",
      "Epoch [3/5], Step [458/10336], Loss: 0.7308\n",
      "Epoch [3/5], Step [460/10336], Loss: 0.0870\n",
      "Epoch [3/5], Step [462/10336], Loss: 0.1657\n",
      "Epoch [3/5], Step [464/10336], Loss: 0.0365\n",
      "Epoch [3/5], Step [466/10336], Loss: 0.5935\n",
      "Epoch [3/5], Step [468/10336], Loss: 0.2311\n",
      "Epoch [3/5], Step [470/10336], Loss: 0.3508\n",
      "Epoch [3/5], Step [472/10336], Loss: 0.4207\n",
      "Epoch [3/5], Step [474/10336], Loss: 2.3096\n",
      "Epoch [3/5], Step [476/10336], Loss: 0.8523\n",
      "Epoch [3/5], Step [478/10336], Loss: 0.2318\n",
      "Epoch [3/5], Step [480/10336], Loss: 0.1652\n",
      "Epoch [3/5], Step [482/10336], Loss: 0.0834\n",
      "Epoch [3/5], Step [484/10336], Loss: 0.2485\n",
      "Epoch [3/5], Step [486/10336], Loss: 0.8494\n",
      "Epoch [3/5], Step [488/10336], Loss: 0.8026\n",
      "Epoch [3/5], Step [490/10336], Loss: 0.0038\n",
      "Epoch [3/5], Step [492/10336], Loss: 0.0787\n",
      "Epoch [3/5], Step [494/10336], Loss: 0.0221\n",
      "Epoch [3/5], Step [496/10336], Loss: 0.0457\n",
      "Epoch [3/5], Step [498/10336], Loss: 0.2363\n",
      "Epoch [3/5], Step [500/10336], Loss: 3.7492\n",
      "Epoch [3/5], Step [502/10336], Loss: 0.0143\n",
      "Epoch [3/5], Step [504/10336], Loss: 4.4519\n",
      "Epoch [3/5], Step [506/10336], Loss: 0.1783\n",
      "Epoch [3/5], Step [508/10336], Loss: 0.0675\n",
      "Epoch [3/5], Step [510/10336], Loss: 0.0614\n",
      "Epoch [3/5], Step [512/10336], Loss: 0.1540\n",
      "Epoch [3/5], Step [514/10336], Loss: 0.0125\n",
      "Epoch [3/5], Step [516/10336], Loss: 1.9377\n",
      "Epoch [3/5], Step [518/10336], Loss: 2.2886\n",
      "Epoch [3/5], Step [520/10336], Loss: 0.5031\n",
      "Epoch [3/5], Step [522/10336], Loss: 0.3597\n",
      "Epoch [3/5], Step [524/10336], Loss: 0.0160\n",
      "Epoch [3/5], Step [526/10336], Loss: 0.8218\n",
      "Epoch [3/5], Step [528/10336], Loss: 0.3600\n",
      "Epoch [3/5], Step [530/10336], Loss: 1.1614\n",
      "Epoch [3/5], Step [532/10336], Loss: 0.3017\n",
      "Epoch [3/5], Step [534/10336], Loss: 1.2471\n",
      "Epoch [3/5], Step [536/10336], Loss: 0.5556\n",
      "Epoch [3/5], Step [538/10336], Loss: 0.2466\n",
      "Epoch [3/5], Step [540/10336], Loss: 0.2390\n",
      "Epoch [3/5], Step [542/10336], Loss: 0.3369\n",
      "Epoch [3/5], Step [544/10336], Loss: 5.0869\n",
      "Epoch [3/5], Step [546/10336], Loss: 0.0360\n",
      "Epoch [3/5], Step [548/10336], Loss: 0.5219\n",
      "Epoch [3/5], Step [550/10336], Loss: 2.4669\n",
      "Epoch [3/5], Step [552/10336], Loss: 0.5303\n",
      "Epoch [3/5], Step [554/10336], Loss: 0.4137\n",
      "Epoch [3/5], Step [556/10336], Loss: 0.7143\n",
      "Epoch [3/5], Step [558/10336], Loss: 0.2877\n",
      "Epoch [3/5], Step [560/10336], Loss: 0.0240\n",
      "Epoch [3/5], Step [562/10336], Loss: 4.4126\n",
      "Epoch [3/5], Step [564/10336], Loss: 0.8382\n",
      "Epoch [3/5], Step [566/10336], Loss: 0.0873\n",
      "Epoch [3/5], Step [568/10336], Loss: 0.9921\n",
      "Epoch [3/5], Step [570/10336], Loss: 0.0189\n",
      "Epoch [3/5], Step [572/10336], Loss: 2.0619\n",
      "Epoch [3/5], Step [574/10336], Loss: 0.6837\n",
      "Epoch [3/5], Step [576/10336], Loss: 0.1543\n",
      "Epoch [3/5], Step [578/10336], Loss: 0.0346\n",
      "Epoch [3/5], Step [580/10336], Loss: 0.0227\n",
      "Epoch [3/5], Step [582/10336], Loss: 0.1033\n",
      "Epoch [3/5], Step [584/10336], Loss: 0.7695\n",
      "Epoch [3/5], Step [586/10336], Loss: 0.4778\n",
      "Epoch [3/5], Step [588/10336], Loss: 0.0468\n",
      "Epoch [3/5], Step [590/10336], Loss: 0.9606\n",
      "Epoch [3/5], Step [592/10336], Loss: 0.2922\n",
      "Epoch [3/5], Step [594/10336], Loss: 0.2809\n",
      "Epoch [3/5], Step [596/10336], Loss: 0.3925\n",
      "Epoch [3/5], Step [598/10336], Loss: 0.7188\n",
      "Epoch [3/5], Step [600/10336], Loss: 0.2586\n",
      "Epoch [3/5], Step [602/10336], Loss: 0.4032\n",
      "Epoch [3/5], Step [604/10336], Loss: 1.1643\n",
      "Epoch [3/5], Step [606/10336], Loss: 0.2901\n",
      "Epoch [3/5], Step [608/10336], Loss: 1.0011\n",
      "Epoch [3/5], Step [610/10336], Loss: 0.7357\n",
      "Epoch [3/5], Step [612/10336], Loss: 1.4700\n",
      "Epoch [3/5], Step [614/10336], Loss: 0.0117\n",
      "Epoch [3/5], Step [616/10336], Loss: 1.9748\n",
      "Epoch [3/5], Step [618/10336], Loss: 0.1869\n",
      "Epoch [3/5], Step [620/10336], Loss: 0.1291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [622/10336], Loss: 0.1656\n",
      "Epoch [3/5], Step [624/10336], Loss: 4.7380\n",
      "Epoch [3/5], Step [626/10336], Loss: 0.0532\n",
      "Epoch [3/5], Step [628/10336], Loss: 0.3047\n",
      "Epoch [3/5], Step [630/10336], Loss: 0.1013\n",
      "Epoch [3/5], Step [632/10336], Loss: 0.8407\n",
      "Epoch [3/5], Step [634/10336], Loss: 0.2620\n",
      "Epoch [3/5], Step [636/10336], Loss: 0.8683\n",
      "Epoch [3/5], Step [638/10336], Loss: 0.2864\n",
      "Epoch [3/5], Step [640/10336], Loss: 0.4886\n",
      "Epoch [3/5], Step [642/10336], Loss: 0.0207\n",
      "Epoch [3/5], Step [644/10336], Loss: 0.0486\n",
      "Epoch [3/5], Step [646/10336], Loss: 4.5937\n",
      "Epoch [3/5], Step [648/10336], Loss: 0.0831\n",
      "Epoch [3/5], Step [650/10336], Loss: 1.3400\n",
      "Epoch [3/5], Step [652/10336], Loss: 0.1210\n",
      "Epoch [3/5], Step [654/10336], Loss: 0.1447\n",
      "Epoch [3/5], Step [656/10336], Loss: 0.5650\n",
      "Epoch [3/5], Step [658/10336], Loss: 0.1347\n",
      "Epoch [3/5], Step [660/10336], Loss: 0.0334\n",
      "Epoch [3/5], Step [662/10336], Loss: 0.3292\n",
      "Epoch [3/5], Step [664/10336], Loss: 0.6427\n",
      "Epoch [3/5], Step [666/10336], Loss: 0.7214\n",
      "Epoch [3/5], Step [668/10336], Loss: 0.5697\n",
      "Epoch [3/5], Step [670/10336], Loss: 1.6798\n",
      "Epoch [3/5], Step [672/10336], Loss: 0.2752\n",
      "Epoch [3/5], Step [674/10336], Loss: 0.2379\n",
      "Epoch [3/5], Step [676/10336], Loss: 0.6068\n",
      "Epoch [3/5], Step [678/10336], Loss: 1.7896\n",
      "Epoch [3/5], Step [680/10336], Loss: 1.2820\n",
      "Epoch [3/5], Step [682/10336], Loss: 0.2944\n",
      "Epoch [3/5], Step [684/10336], Loss: 0.0457\n",
      "Epoch [3/5], Step [686/10336], Loss: 0.0116\n",
      "Epoch [3/5], Step [688/10336], Loss: 0.0717\n",
      "Epoch [3/5], Step [690/10336], Loss: 3.1391\n",
      "Epoch [3/5], Step [692/10336], Loss: 0.0266\n",
      "Epoch [3/5], Step [694/10336], Loss: 0.0662\n",
      "Epoch [3/5], Step [696/10336], Loss: 2.0704\n",
      "Epoch [3/5], Step [698/10336], Loss: 2.5606\n",
      "Epoch [3/5], Step [700/10336], Loss: 0.3003\n",
      "Epoch [3/5], Step [702/10336], Loss: 0.4818\n",
      "Epoch [3/5], Step [704/10336], Loss: 0.8514\n",
      "Epoch [3/5], Step [706/10336], Loss: 0.4584\n",
      "Epoch [3/5], Step [708/10336], Loss: 0.0510\n",
      "Epoch [3/5], Step [710/10336], Loss: 0.9654\n",
      "Epoch [3/5], Step [712/10336], Loss: 0.0261\n",
      "Epoch [3/5], Step [714/10336], Loss: 1.8325\n",
      "Epoch [3/5], Step [716/10336], Loss: 0.2390\n",
      "Epoch [3/5], Step [718/10336], Loss: 0.0342\n",
      "Epoch [3/5], Step [720/10336], Loss: 0.7339\n",
      "Epoch [3/5], Step [722/10336], Loss: 1.6278\n",
      "Epoch [3/5], Step [724/10336], Loss: 0.2174\n",
      "Epoch [3/5], Step [726/10336], Loss: 0.2217\n",
      "Epoch [3/5], Step [728/10336], Loss: 2.6722\n",
      "Epoch [3/5], Step [730/10336], Loss: 0.0349\n",
      "Epoch [3/5], Step [732/10336], Loss: 1.5573\n",
      "Epoch [3/5], Step [734/10336], Loss: 0.0173\n",
      "Epoch [3/5], Step [736/10336], Loss: 2.7360\n",
      "Epoch [3/5], Step [738/10336], Loss: 0.9488\n",
      "Epoch [3/5], Step [740/10336], Loss: 0.1824\n",
      "Epoch [3/5], Step [742/10336], Loss: 0.6436\n",
      "Epoch [3/5], Step [744/10336], Loss: 0.2051\n",
      "Epoch [3/5], Step [746/10336], Loss: 5.2389\n",
      "Epoch [3/5], Step [748/10336], Loss: 0.0470\n",
      "Epoch [3/5], Step [750/10336], Loss: 0.2932\n",
      "Epoch [3/5], Step [752/10336], Loss: 1.3925\n",
      "Epoch [3/5], Step [754/10336], Loss: 0.9217\n",
      "Epoch [3/5], Step [756/10336], Loss: 0.8437\n",
      "Epoch [3/5], Step [758/10336], Loss: 1.5061\n",
      "Epoch [3/5], Step [760/10336], Loss: 1.3801\n",
      "Epoch [3/5], Step [762/10336], Loss: 0.1475\n",
      "Epoch [3/5], Step [764/10336], Loss: 0.5441\n",
      "Epoch [3/5], Step [766/10336], Loss: 2.2542\n",
      "Epoch [3/5], Step [768/10336], Loss: 0.0203\n",
      "Epoch [3/5], Step [770/10336], Loss: 0.0268\n",
      "Epoch [3/5], Step [772/10336], Loss: 1.9299\n",
      "Epoch [3/5], Step [774/10336], Loss: 0.0153\n",
      "Epoch [3/5], Step [776/10336], Loss: 0.0750\n",
      "Epoch [3/5], Step [778/10336], Loss: 1.7561\n",
      "Epoch [3/5], Step [780/10336], Loss: 0.0994\n",
      "Epoch [3/5], Step [782/10336], Loss: 3.3255\n",
      "Epoch [3/5], Step [784/10336], Loss: 0.5154\n",
      "Epoch [3/5], Step [786/10336], Loss: 0.5625\n",
      "Epoch [3/5], Step [788/10336], Loss: 0.0606\n",
      "Epoch [3/5], Step [790/10336], Loss: 1.0630\n",
      "Epoch [3/5], Step [792/10336], Loss: 0.4133\n",
      "Epoch [3/5], Step [794/10336], Loss: 0.4867\n",
      "Epoch [3/5], Step [796/10336], Loss: 0.0029\n",
      "Epoch [3/5], Step [798/10336], Loss: 0.1256\n",
      "Epoch [3/5], Step [800/10336], Loss: 1.5727\n",
      "Epoch [3/5], Step [802/10336], Loss: 0.0704\n",
      "Epoch [3/5], Step [804/10336], Loss: 0.9755\n",
      "Epoch [3/5], Step [806/10336], Loss: 0.0927\n",
      "Epoch [3/5], Step [808/10336], Loss: 0.2151\n",
      "Epoch [3/5], Step [810/10336], Loss: 0.0082\n",
      "Epoch [3/5], Step [812/10336], Loss: 0.0080\n",
      "Epoch [3/5], Step [814/10336], Loss: 0.0280\n",
      "Epoch [3/5], Step [816/10336], Loss: 1.6775\n",
      "Epoch [3/5], Step [818/10336], Loss: 0.2688\n",
      "Epoch [3/5], Step [820/10336], Loss: 0.0169\n",
      "Epoch [3/5], Step [822/10336], Loss: 0.3944\n",
      "Epoch [3/5], Step [824/10336], Loss: 0.3188\n",
      "Epoch [3/5], Step [826/10336], Loss: 0.0051\n",
      "Epoch [3/5], Step [828/10336], Loss: 0.4948\n",
      "Epoch [3/5], Step [830/10336], Loss: 3.2971\n",
      "Epoch [3/5], Step [832/10336], Loss: 0.6347\n",
      "Epoch [3/5], Step [834/10336], Loss: 0.1017\n",
      "Epoch [3/5], Step [836/10336], Loss: 3.9928\n",
      "Epoch [3/5], Step [838/10336], Loss: 0.9640\n",
      "Epoch [3/5], Step [840/10336], Loss: 0.2547\n",
      "Epoch [3/5], Step [842/10336], Loss: 2.1239\n",
      "Epoch [3/5], Step [844/10336], Loss: 2.0180\n",
      "Epoch [3/5], Step [846/10336], Loss: 1.0915\n",
      "Epoch [3/5], Step [848/10336], Loss: 2.4070\n",
      "Epoch [3/5], Step [850/10336], Loss: 1.6558\n",
      "Epoch [3/5], Step [852/10336], Loss: 0.1196\n",
      "Epoch [3/5], Step [854/10336], Loss: 1.5620\n",
      "Epoch [3/5], Step [856/10336], Loss: 0.2795\n",
      "Epoch [3/5], Step [858/10336], Loss: 2.3156\n",
      "Epoch [3/5], Step [860/10336], Loss: 0.0202\n",
      "Epoch [3/5], Step [862/10336], Loss: 0.6415\n",
      "Epoch [3/5], Step [864/10336], Loss: 0.0176\n",
      "Epoch [3/5], Step [866/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [868/10336], Loss: 0.1827\n",
      "Epoch [3/5], Step [870/10336], Loss: 0.3426\n",
      "Epoch [3/5], Step [872/10336], Loss: 0.2227\n",
      "Epoch [3/5], Step [874/10336], Loss: 1.1419\n",
      "Epoch [3/5], Step [876/10336], Loss: 0.7412\n",
      "Epoch [3/5], Step [878/10336], Loss: 1.3989\n",
      "Epoch [3/5], Step [880/10336], Loss: 0.2380\n",
      "Epoch [3/5], Step [882/10336], Loss: 0.1637\n",
      "Epoch [3/5], Step [884/10336], Loss: 0.1177\n",
      "Epoch [3/5], Step [886/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [888/10336], Loss: 0.0123\n",
      "Epoch [3/5], Step [890/10336], Loss: 0.0568\n",
      "Epoch [3/5], Step [892/10336], Loss: 0.3175\n",
      "Epoch [3/5], Step [894/10336], Loss: 0.4552\n",
      "Epoch [3/5], Step [896/10336], Loss: 0.3895\n",
      "Epoch [3/5], Step [898/10336], Loss: 1.7612\n",
      "Epoch [3/5], Step [900/10336], Loss: 0.5021\n",
      "Epoch [3/5], Step [902/10336], Loss: 0.3544\n",
      "Epoch [3/5], Step [904/10336], Loss: 0.0546\n",
      "Epoch [3/5], Step [906/10336], Loss: 0.1569\n",
      "Epoch [3/5], Step [908/10336], Loss: 0.3930\n",
      "Epoch [3/5], Step [910/10336], Loss: 0.3176\n",
      "Epoch [3/5], Step [912/10336], Loss: 0.0091\n",
      "Epoch [3/5], Step [914/10336], Loss: 1.5684\n",
      "Epoch [3/5], Step [916/10336], Loss: 0.7862\n",
      "Epoch [3/5], Step [918/10336], Loss: 1.2637\n",
      "Epoch [3/5], Step [920/10336], Loss: 0.2122\n",
      "Epoch [3/5], Step [922/10336], Loss: 0.2026\n",
      "Epoch [3/5], Step [924/10336], Loss: 0.3567\n",
      "Epoch [3/5], Step [926/10336], Loss: 0.0741\n",
      "Epoch [3/5], Step [928/10336], Loss: 0.3714\n",
      "Epoch [3/5], Step [930/10336], Loss: 4.3774\n",
      "Epoch [3/5], Step [932/10336], Loss: 0.0291\n",
      "Epoch [3/5], Step [934/10336], Loss: 0.5433\n",
      "Epoch [3/5], Step [936/10336], Loss: 0.5338\n",
      "Epoch [3/5], Step [938/10336], Loss: 0.0095\n",
      "Epoch [3/5], Step [940/10336], Loss: 1.1200\n",
      "Epoch [3/5], Step [942/10336], Loss: 0.3501\n",
      "Epoch [3/5], Step [944/10336], Loss: 0.4176\n",
      "Epoch [3/5], Step [946/10336], Loss: 1.5704\n",
      "Epoch [3/5], Step [948/10336], Loss: 0.0378\n",
      "Epoch [3/5], Step [950/10336], Loss: 0.5876\n",
      "Epoch [3/5], Step [952/10336], Loss: 0.1325\n",
      "Epoch [3/5], Step [954/10336], Loss: 0.4103\n",
      "Epoch [3/5], Step [956/10336], Loss: 0.0411\n",
      "Epoch [3/5], Step [958/10336], Loss: 2.4366\n",
      "Epoch [3/5], Step [960/10336], Loss: 0.5856\n",
      "Epoch [3/5], Step [962/10336], Loss: 0.0385\n",
      "Epoch [3/5], Step [964/10336], Loss: 3.9712\n",
      "Epoch [3/5], Step [966/10336], Loss: 1.9001\n",
      "Epoch [3/5], Step [968/10336], Loss: 0.5547\n",
      "Epoch [3/5], Step [970/10336], Loss: 0.0091\n",
      "Epoch [3/5], Step [972/10336], Loss: 0.3659\n",
      "Epoch [3/5], Step [974/10336], Loss: 4.4526\n",
      "Epoch [3/5], Step [976/10336], Loss: 0.0522\n",
      "Epoch [3/5], Step [978/10336], Loss: 2.8437\n",
      "Epoch [3/5], Step [980/10336], Loss: 0.0772\n",
      "Epoch [3/5], Step [982/10336], Loss: 0.0083\n",
      "Epoch [3/5], Step [984/10336], Loss: 0.0697\n",
      "Epoch [3/5], Step [986/10336], Loss: 0.0206\n",
      "Epoch [3/5], Step [988/10336], Loss: 0.0534\n",
      "Epoch [3/5], Step [990/10336], Loss: 1.3646\n",
      "Epoch [3/5], Step [992/10336], Loss: 0.0042\n",
      "Epoch [3/5], Step [994/10336], Loss: 0.0433\n",
      "Epoch [3/5], Step [996/10336], Loss: 0.2695\n",
      "Epoch [3/5], Step [998/10336], Loss: 0.9907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [1000/10336], Loss: 2.7148\n",
      "Epoch [3/5], Step [1002/10336], Loss: 0.7514\n",
      "Epoch [3/5], Step [1004/10336], Loss: 2.1991\n",
      "Epoch [3/5], Step [1006/10336], Loss: 0.3091\n",
      "Epoch [3/5], Step [1008/10336], Loss: 2.2954\n",
      "Epoch [3/5], Step [1010/10336], Loss: 0.2669\n",
      "Epoch [3/5], Step [1012/10336], Loss: 0.0589\n",
      "Epoch [3/5], Step [1014/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [1016/10336], Loss: 0.5536\n",
      "Epoch [3/5], Step [1018/10336], Loss: 0.7420\n",
      "Epoch [3/5], Step [1020/10336], Loss: 2.0914\n",
      "Epoch [3/5], Step [1022/10336], Loss: 0.0449\n",
      "Epoch [3/5], Step [1024/10336], Loss: 1.4292\n",
      "Epoch [3/5], Step [1026/10336], Loss: 0.1680\n",
      "Epoch [3/5], Step [1028/10336], Loss: 0.0024\n",
      "Epoch [3/5], Step [1030/10336], Loss: 0.0135\n",
      "Epoch [3/5], Step [1032/10336], Loss: 0.1105\n",
      "Epoch [3/5], Step [1034/10336], Loss: 1.8336\n",
      "Epoch [3/5], Step [1036/10336], Loss: 0.5392\n",
      "Epoch [3/5], Step [1038/10336], Loss: 0.0162\n",
      "Epoch [3/5], Step [1040/10336], Loss: 0.1639\n",
      "Epoch [3/5], Step [1042/10336], Loss: 0.2611\n",
      "Epoch [3/5], Step [1044/10336], Loss: 0.1860\n",
      "Epoch [3/5], Step [1046/10336], Loss: 1.5571\n",
      "Epoch [3/5], Step [1048/10336], Loss: 0.0764\n",
      "Epoch [3/5], Step [1050/10336], Loss: 0.0289\n",
      "Epoch [3/5], Step [1052/10336], Loss: 0.6138\n",
      "Epoch [3/5], Step [1054/10336], Loss: 0.2139\n",
      "Epoch [3/5], Step [1056/10336], Loss: 0.1346\n",
      "Epoch [3/5], Step [1058/10336], Loss: 0.3985\n",
      "Epoch [3/5], Step [1060/10336], Loss: 0.4325\n",
      "Epoch [3/5], Step [1062/10336], Loss: 0.2459\n",
      "Epoch [3/5], Step [1064/10336], Loss: 1.5787\n",
      "Epoch [3/5], Step [1066/10336], Loss: 0.0579\n",
      "Epoch [3/5], Step [1068/10336], Loss: 0.2918\n",
      "Epoch [3/5], Step [1070/10336], Loss: 0.4573\n",
      "Epoch [3/5], Step [1072/10336], Loss: 1.2820\n",
      "Epoch [3/5], Step [1074/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [1076/10336], Loss: 0.0011\n",
      "Epoch [3/5], Step [1078/10336], Loss: 0.4618\n",
      "Epoch [3/5], Step [1080/10336], Loss: 0.2549\n",
      "Epoch [3/5], Step [1082/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [1084/10336], Loss: 0.2193\n",
      "Epoch [3/5], Step [1086/10336], Loss: 0.2585\n",
      "Epoch [3/5], Step [1088/10336], Loss: 1.7440\n",
      "Epoch [3/5], Step [1090/10336], Loss: 0.0607\n",
      "Epoch [3/5], Step [1092/10336], Loss: 0.2025\n",
      "Epoch [3/5], Step [1094/10336], Loss: 0.0776\n",
      "Epoch [3/5], Step [1096/10336], Loss: 0.2365\n",
      "Epoch [3/5], Step [1098/10336], Loss: 1.8714\n",
      "Epoch [3/5], Step [1100/10336], Loss: 1.0298\n",
      "Epoch [3/5], Step [1102/10336], Loss: 0.0119\n",
      "Epoch [3/5], Step [1104/10336], Loss: 0.4129\n",
      "Epoch [3/5], Step [1106/10336], Loss: 4.7066\n",
      "Epoch [3/5], Step [1108/10336], Loss: 0.2999\n",
      "Epoch [3/5], Step [1110/10336], Loss: 0.1768\n",
      "Epoch [3/5], Step [1112/10336], Loss: 0.7885\n",
      "Epoch [3/5], Step [1114/10336], Loss: 0.3798\n",
      "Epoch [3/5], Step [1116/10336], Loss: 0.0607\n",
      "Epoch [3/5], Step [1118/10336], Loss: 0.0192\n",
      "Epoch [3/5], Step [1120/10336], Loss: 0.1399\n",
      "Epoch [3/5], Step [1122/10336], Loss: 0.3437\n",
      "Epoch [3/5], Step [1124/10336], Loss: 0.1872\n",
      "Epoch [3/5], Step [1126/10336], Loss: 0.2263\n",
      "Epoch [3/5], Step [1128/10336], Loss: 0.1827\n",
      "Epoch [3/5], Step [1130/10336], Loss: 0.4605\n",
      "Epoch [3/5], Step [1132/10336], Loss: 0.5249\n",
      "Epoch [3/5], Step [1134/10336], Loss: 0.5552\n",
      "Epoch [3/5], Step [1136/10336], Loss: 0.2197\n",
      "Epoch [3/5], Step [1138/10336], Loss: 1.5819\n",
      "Epoch [3/5], Step [1140/10336], Loss: 0.5564\n",
      "Epoch [3/5], Step [1142/10336], Loss: 7.6972\n",
      "Epoch [3/5], Step [1144/10336], Loss: 0.0960\n",
      "Epoch [3/5], Step [1146/10336], Loss: 0.5276\n",
      "Epoch [3/5], Step [1148/10336], Loss: 1.1211\n",
      "Epoch [3/5], Step [1150/10336], Loss: 0.1781\n",
      "Epoch [3/5], Step [1152/10336], Loss: 1.1862\n",
      "Epoch [3/5], Step [1154/10336], Loss: 5.1463\n",
      "Epoch [3/5], Step [1156/10336], Loss: 1.1890\n",
      "Epoch [3/5], Step [1158/10336], Loss: 0.3117\n",
      "Epoch [3/5], Step [1160/10336], Loss: 0.4429\n",
      "Epoch [3/5], Step [1162/10336], Loss: 0.6773\n",
      "Epoch [3/5], Step [1164/10336], Loss: 0.1371\n",
      "Epoch [3/5], Step [1166/10336], Loss: 0.1447\n",
      "Epoch [3/5], Step [1168/10336], Loss: 2.8242\n",
      "Epoch [3/5], Step [1170/10336], Loss: 0.1090\n",
      "Epoch [3/5], Step [1172/10336], Loss: 1.1702\n",
      "Epoch [3/5], Step [1174/10336], Loss: 0.7258\n",
      "Epoch [3/5], Step [1176/10336], Loss: 0.1548\n",
      "Epoch [3/5], Step [1178/10336], Loss: 0.3559\n",
      "Epoch [3/5], Step [1180/10336], Loss: 0.3018\n",
      "Epoch [3/5], Step [1182/10336], Loss: 0.0530\n",
      "Epoch [3/5], Step [1184/10336], Loss: 0.2530\n",
      "Epoch [3/5], Step [1186/10336], Loss: 0.0452\n",
      "Epoch [3/5], Step [1188/10336], Loss: 0.0588\n",
      "Epoch [3/5], Step [1190/10336], Loss: 1.0966\n",
      "Epoch [3/5], Step [1192/10336], Loss: 0.3629\n",
      "Epoch [3/5], Step [1194/10336], Loss: 4.7433\n",
      "Epoch [3/5], Step [1196/10336], Loss: 0.4549\n",
      "Epoch [3/5], Step [1198/10336], Loss: 0.3424\n",
      "Epoch [3/5], Step [1200/10336], Loss: 2.2001\n",
      "Epoch [3/5], Step [1202/10336], Loss: 0.3081\n",
      "Epoch [3/5], Step [1204/10336], Loss: 0.4248\n",
      "Epoch [3/5], Step [1206/10336], Loss: 0.6555\n",
      "Epoch [3/5], Step [1208/10336], Loss: 0.3031\n",
      "Epoch [3/5], Step [1210/10336], Loss: 0.2552\n",
      "Epoch [3/5], Step [1212/10336], Loss: 3.8246\n",
      "Epoch [3/5], Step [1214/10336], Loss: 0.2190\n",
      "Epoch [3/5], Step [1216/10336], Loss: 0.1888\n",
      "Epoch [3/5], Step [1218/10336], Loss: 0.3648\n",
      "Epoch [3/5], Step [1220/10336], Loss: 2.9170\n",
      "Epoch [3/5], Step [1222/10336], Loss: 2.1612\n",
      "Epoch [3/5], Step [1224/10336], Loss: 0.1053\n",
      "Epoch [3/5], Step [1226/10336], Loss: 0.0517\n",
      "Epoch [3/5], Step [1228/10336], Loss: 0.0072\n",
      "Epoch [3/5], Step [1230/10336], Loss: 0.0805\n",
      "Epoch [3/5], Step [1232/10336], Loss: 0.0183\n",
      "Epoch [3/5], Step [1234/10336], Loss: 0.6491\n",
      "Epoch [3/5], Step [1236/10336], Loss: 0.1705\n",
      "Epoch [3/5], Step [1238/10336], Loss: 1.6010\n",
      "Epoch [3/5], Step [1240/10336], Loss: 0.8154\n",
      "Epoch [3/5], Step [1242/10336], Loss: 0.1241\n",
      "Epoch [3/5], Step [1244/10336], Loss: 0.1535\n",
      "Epoch [3/5], Step [1246/10336], Loss: 0.9389\n",
      "Epoch [3/5], Step [1248/10336], Loss: 0.7413\n",
      "Epoch [3/5], Step [1250/10336], Loss: 0.0274\n",
      "Epoch [3/5], Step [1252/10336], Loss: 0.2663\n",
      "Epoch [3/5], Step [1254/10336], Loss: 0.6348\n",
      "Epoch [3/5], Step [1256/10336], Loss: 0.1448\n",
      "Epoch [3/5], Step [1258/10336], Loss: 0.0107\n",
      "Epoch [3/5], Step [1260/10336], Loss: 1.4266\n",
      "Epoch [3/5], Step [1262/10336], Loss: 4.6385\n",
      "Epoch [3/5], Step [1264/10336], Loss: 0.1982\n",
      "Epoch [3/5], Step [1266/10336], Loss: 0.0756\n",
      "Epoch [3/5], Step [1268/10336], Loss: 0.1461\n",
      "Epoch [3/5], Step [1270/10336], Loss: 1.2283\n",
      "Epoch [3/5], Step [1272/10336], Loss: 0.6065\n",
      "Epoch [3/5], Step [1274/10336], Loss: 0.1967\n",
      "Epoch [3/5], Step [1276/10336], Loss: 0.3646\n",
      "Epoch [3/5], Step [1278/10336], Loss: 0.7866\n",
      "Epoch [3/5], Step [1280/10336], Loss: 0.1862\n",
      "Epoch [3/5], Step [1282/10336], Loss: 0.6847\n",
      "Epoch [3/5], Step [1284/10336], Loss: 1.9204\n",
      "Epoch [3/5], Step [1286/10336], Loss: 0.2316\n",
      "Epoch [3/5], Step [1288/10336], Loss: 0.7365\n",
      "Epoch [3/5], Step [1290/10336], Loss: 1.1757\n",
      "Epoch [3/5], Step [1292/10336], Loss: 0.0792\n",
      "Epoch [3/5], Step [1294/10336], Loss: 0.7326\n",
      "Epoch [3/5], Step [1296/10336], Loss: 0.1907\n",
      "Epoch [3/5], Step [1298/10336], Loss: 0.0132\n",
      "Epoch [3/5], Step [1300/10336], Loss: 0.2651\n",
      "Epoch [3/5], Step [1302/10336], Loss: 0.2755\n",
      "Epoch [3/5], Step [1304/10336], Loss: 0.2570\n",
      "Epoch [3/5], Step [1306/10336], Loss: 0.1011\n",
      "Epoch [3/5], Step [1308/10336], Loss: 0.0245\n",
      "Epoch [3/5], Step [1310/10336], Loss: 0.0417\n",
      "Epoch [3/5], Step [1312/10336], Loss: 0.0259\n",
      "Epoch [3/5], Step [1314/10336], Loss: 0.0057\n",
      "Epoch [3/5], Step [1316/10336], Loss: 0.1491\n",
      "Epoch [3/5], Step [1318/10336], Loss: 0.4119\n",
      "Epoch [3/5], Step [1320/10336], Loss: 2.7478\n",
      "Epoch [3/5], Step [1322/10336], Loss: 0.6556\n",
      "Epoch [3/5], Step [1324/10336], Loss: 0.0941\n",
      "Epoch [3/5], Step [1326/10336], Loss: 0.0702\n",
      "Epoch [3/5], Step [1328/10336], Loss: 0.3906\n",
      "Epoch [3/5], Step [1330/10336], Loss: 0.3001\n",
      "Epoch [3/5], Step [1332/10336], Loss: 0.0022\n",
      "Epoch [3/5], Step [1334/10336], Loss: 0.3199\n",
      "Epoch [3/5], Step [1336/10336], Loss: 0.0250\n",
      "Epoch [3/5], Step [1338/10336], Loss: 0.0828\n",
      "Epoch [3/5], Step [1340/10336], Loss: 3.2071\n",
      "Epoch [3/5], Step [1342/10336], Loss: 0.4262\n",
      "Epoch [3/5], Step [1344/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [1346/10336], Loss: 0.4748\n",
      "Epoch [3/5], Step [1348/10336], Loss: 0.0927\n",
      "Epoch [3/5], Step [1350/10336], Loss: 0.0657\n",
      "Epoch [3/5], Step [1352/10336], Loss: 0.8499\n",
      "Epoch [3/5], Step [1354/10336], Loss: 0.0026\n",
      "Epoch [3/5], Step [1356/10336], Loss: 3.1358\n",
      "Epoch [3/5], Step [1358/10336], Loss: 0.7405\n",
      "Epoch [3/5], Step [1360/10336], Loss: 0.1214\n",
      "Epoch [3/5], Step [1362/10336], Loss: 0.0084\n",
      "Epoch [3/5], Step [1364/10336], Loss: 0.0702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [1366/10336], Loss: 0.0421\n",
      "Epoch [3/5], Step [1368/10336], Loss: 0.1208\n",
      "Epoch [3/5], Step [1370/10336], Loss: 0.0824\n",
      "Epoch [3/5], Step [1372/10336], Loss: 0.9395\n",
      "Epoch [3/5], Step [1374/10336], Loss: 3.0653\n",
      "Epoch [3/5], Step [1376/10336], Loss: 0.2106\n",
      "Epoch [3/5], Step [1378/10336], Loss: 0.4855\n",
      "Epoch [3/5], Step [1380/10336], Loss: 0.5526\n",
      "Epoch [3/5], Step [1382/10336], Loss: 1.4258\n",
      "Epoch [3/5], Step [1384/10336], Loss: 0.2003\n",
      "Epoch [3/5], Step [1386/10336], Loss: 0.3710\n",
      "Epoch [3/5], Step [1388/10336], Loss: 0.1614\n",
      "Epoch [3/5], Step [1390/10336], Loss: 1.4362\n",
      "Epoch [3/5], Step [1392/10336], Loss: 0.0108\n",
      "Epoch [3/5], Step [1394/10336], Loss: 0.0065\n",
      "Epoch [3/5], Step [1396/10336], Loss: 0.0792\n",
      "Epoch [3/5], Step [1398/10336], Loss: 0.4574\n",
      "Epoch [3/5], Step [1400/10336], Loss: 2.0269\n",
      "Epoch [3/5], Step [1402/10336], Loss: 0.0182\n",
      "Epoch [3/5], Step [1404/10336], Loss: 3.2596\n",
      "Epoch [3/5], Step [1406/10336], Loss: 0.3181\n",
      "Epoch [3/5], Step [1408/10336], Loss: 0.3464\n",
      "Epoch [3/5], Step [1410/10336], Loss: 2.1785\n",
      "Epoch [3/5], Step [1412/10336], Loss: 0.2926\n",
      "Epoch [3/5], Step [1414/10336], Loss: 0.0709\n",
      "Epoch [3/5], Step [1416/10336], Loss: 0.0121\n",
      "Epoch [3/5], Step [1418/10336], Loss: 2.8097\n",
      "Epoch [3/5], Step [1420/10336], Loss: 0.2618\n",
      "Epoch [3/5], Step [1422/10336], Loss: 0.2656\n",
      "Epoch [3/5], Step [1424/10336], Loss: 0.6628\n",
      "Epoch [3/5], Step [1426/10336], Loss: 0.2529\n",
      "Epoch [3/5], Step [1428/10336], Loss: 0.3475\n",
      "Epoch [3/5], Step [1430/10336], Loss: 0.3795\n",
      "Epoch [3/5], Step [1432/10336], Loss: 0.1747\n",
      "Epoch [3/5], Step [1434/10336], Loss: 0.6801\n",
      "Epoch [3/5], Step [1436/10336], Loss: 0.2646\n",
      "Epoch [3/5], Step [1438/10336], Loss: 0.1520\n",
      "Epoch [3/5], Step [1440/10336], Loss: 0.0622\n",
      "Epoch [3/5], Step [1442/10336], Loss: 2.6624\n",
      "Epoch [3/5], Step [1444/10336], Loss: 0.2501\n",
      "Epoch [3/5], Step [1446/10336], Loss: 0.1069\n",
      "Epoch [3/5], Step [1448/10336], Loss: 2.5339\n",
      "Epoch [3/5], Step [1450/10336], Loss: 3.4545\n",
      "Epoch [3/5], Step [1452/10336], Loss: 5.3692\n",
      "Epoch [3/5], Step [1454/10336], Loss: 0.0670\n",
      "Epoch [3/5], Step [1456/10336], Loss: 0.0284\n",
      "Epoch [3/5], Step [1458/10336], Loss: 0.1707\n",
      "Epoch [3/5], Step [1460/10336], Loss: 0.1301\n",
      "Epoch [3/5], Step [1462/10336], Loss: 0.0411\n",
      "Epoch [3/5], Step [1464/10336], Loss: 0.7108\n",
      "Epoch [3/5], Step [1466/10336], Loss: 0.2556\n",
      "Epoch [3/5], Step [1468/10336], Loss: 0.0506\n",
      "Epoch [3/5], Step [1470/10336], Loss: 0.3239\n",
      "Epoch [3/5], Step [1472/10336], Loss: 0.5243\n",
      "Epoch [3/5], Step [1474/10336], Loss: 0.5620\n",
      "Epoch [3/5], Step [1476/10336], Loss: 0.0270\n",
      "Epoch [3/5], Step [1478/10336], Loss: 2.9944\n",
      "Epoch [3/5], Step [1480/10336], Loss: 4.4412\n",
      "Epoch [3/5], Step [1482/10336], Loss: 0.1960\n",
      "Epoch [3/5], Step [1484/10336], Loss: 2.7021\n",
      "Epoch [3/5], Step [1486/10336], Loss: 0.2659\n",
      "Epoch [3/5], Step [1488/10336], Loss: 0.0152\n",
      "Epoch [3/5], Step [1490/10336], Loss: 2.2855\n",
      "Epoch [3/5], Step [1492/10336], Loss: 0.0847\n",
      "Epoch [3/5], Step [1494/10336], Loss: 0.3989\n",
      "Epoch [3/5], Step [1496/10336], Loss: 1.6343\n",
      "Epoch [3/5], Step [1498/10336], Loss: 0.1505\n",
      "Epoch [3/5], Step [1500/10336], Loss: 0.0104\n",
      "Epoch [3/5], Step [1502/10336], Loss: 0.9471\n",
      "Epoch [3/5], Step [1504/10336], Loss: 1.6786\n",
      "Epoch [3/5], Step [1506/10336], Loss: 0.7205\n",
      "Epoch [3/5], Step [1508/10336], Loss: 0.3615\n",
      "Epoch [3/5], Step [1510/10336], Loss: 0.3231\n",
      "Epoch [3/5], Step [1512/10336], Loss: 1.4737\n",
      "Epoch [3/5], Step [1514/10336], Loss: 0.5173\n",
      "Epoch [3/5], Step [1516/10336], Loss: 0.0610\n",
      "Epoch [3/5], Step [1518/10336], Loss: 1.8757\n",
      "Epoch [3/5], Step [1520/10336], Loss: 3.1266\n",
      "Epoch [3/5], Step [1522/10336], Loss: 0.1029\n",
      "Epoch [3/5], Step [1524/10336], Loss: 0.0391\n",
      "Epoch [3/5], Step [1526/10336], Loss: 0.2555\n",
      "Epoch [3/5], Step [1528/10336], Loss: 1.7670\n",
      "Epoch [3/5], Step [1530/10336], Loss: 0.1144\n",
      "Epoch [3/5], Step [1532/10336], Loss: 0.9463\n",
      "Epoch [3/5], Step [1534/10336], Loss: 0.0296\n",
      "Epoch [3/5], Step [1536/10336], Loss: 0.1398\n",
      "Epoch [3/5], Step [1538/10336], Loss: 0.0716\n",
      "Epoch [3/5], Step [1540/10336], Loss: 3.0262\n",
      "Epoch [3/5], Step [1542/10336], Loss: 1.4135\n",
      "Epoch [3/5], Step [1544/10336], Loss: 0.5146\n",
      "Epoch [3/5], Step [1546/10336], Loss: 0.0967\n",
      "Epoch [3/5], Step [1548/10336], Loss: 0.5319\n",
      "Epoch [3/5], Step [1550/10336], Loss: 0.4047\n",
      "Epoch [3/5], Step [1552/10336], Loss: 0.0286\n",
      "Epoch [3/5], Step [1554/10336], Loss: 0.2685\n",
      "Epoch [3/5], Step [1556/10336], Loss: 0.1446\n",
      "Epoch [3/5], Step [1558/10336], Loss: 1.6439\n",
      "Epoch [3/5], Step [1560/10336], Loss: 0.5261\n",
      "Epoch [3/5], Step [1562/10336], Loss: 0.6780\n",
      "Epoch [3/5], Step [1564/10336], Loss: 0.2363\n",
      "Epoch [3/5], Step [1566/10336], Loss: 0.4572\n",
      "Epoch [3/5], Step [1568/10336], Loss: 0.4275\n",
      "Epoch [3/5], Step [1570/10336], Loss: 0.0031\n",
      "Epoch [3/5], Step [1572/10336], Loss: 0.4955\n",
      "Epoch [3/5], Step [1574/10336], Loss: 2.5752\n",
      "Epoch [3/5], Step [1576/10336], Loss: 0.6837\n",
      "Epoch [3/5], Step [1578/10336], Loss: 0.0424\n",
      "Epoch [3/5], Step [1580/10336], Loss: 3.8040\n",
      "Epoch [3/5], Step [1582/10336], Loss: 0.0381\n",
      "Epoch [3/5], Step [1584/10336], Loss: 0.9285\n",
      "Epoch [3/5], Step [1586/10336], Loss: 0.3297\n",
      "Epoch [3/5], Step [1588/10336], Loss: 0.2804\n",
      "Epoch [3/5], Step [1590/10336], Loss: 0.0021\n",
      "Epoch [3/5], Step [1592/10336], Loss: 0.0760\n",
      "Epoch [3/5], Step [1594/10336], Loss: 0.0511\n",
      "Epoch [3/5], Step [1596/10336], Loss: 0.4774\n",
      "Epoch [3/5], Step [1598/10336], Loss: 2.7519\n",
      "Epoch [3/5], Step [1600/10336], Loss: 0.3747\n",
      "Epoch [3/5], Step [1602/10336], Loss: 1.2058\n",
      "Epoch [3/5], Step [1604/10336], Loss: 0.0610\n",
      "Epoch [3/5], Step [1606/10336], Loss: 3.0280\n",
      "Epoch [3/5], Step [1608/10336], Loss: 0.5251\n",
      "Epoch [3/5], Step [1610/10336], Loss: 0.2015\n",
      "Epoch [3/5], Step [1612/10336], Loss: 0.6111\n",
      "Epoch [3/5], Step [1614/10336], Loss: 1.2256\n",
      "Epoch [3/5], Step [1616/10336], Loss: 0.3063\n",
      "Epoch [3/5], Step [1618/10336], Loss: 0.4377\n",
      "Epoch [3/5], Step [1620/10336], Loss: 0.5912\n",
      "Epoch [3/5], Step [1622/10336], Loss: 0.1664\n",
      "Epoch [3/5], Step [1624/10336], Loss: 0.6716\n",
      "Epoch [3/5], Step [1626/10336], Loss: 0.5664\n",
      "Epoch [3/5], Step [1628/10336], Loss: 2.1790\n",
      "Epoch [3/5], Step [1630/10336], Loss: 0.9658\n",
      "Epoch [3/5], Step [1632/10336], Loss: 0.1883\n",
      "Epoch [3/5], Step [1634/10336], Loss: 0.1654\n",
      "Epoch [3/5], Step [1636/10336], Loss: 1.1038\n",
      "Epoch [3/5], Step [1638/10336], Loss: 0.2715\n",
      "Epoch [3/5], Step [1640/10336], Loss: 3.1161\n",
      "Epoch [3/5], Step [1642/10336], Loss: 2.4863\n",
      "Epoch [3/5], Step [1644/10336], Loss: 0.6674\n",
      "Epoch [3/5], Step [1646/10336], Loss: 0.1010\n",
      "Epoch [3/5], Step [1648/10336], Loss: 0.8507\n",
      "Epoch [3/5], Step [1650/10336], Loss: 0.2872\n",
      "Epoch [3/5], Step [1652/10336], Loss: 0.1128\n",
      "Epoch [3/5], Step [1654/10336], Loss: 0.0060\n",
      "Epoch [3/5], Step [1656/10336], Loss: 0.0678\n",
      "Epoch [3/5], Step [1658/10336], Loss: 0.1378\n",
      "Epoch [3/5], Step [1660/10336], Loss: 0.0649\n",
      "Epoch [3/5], Step [1662/10336], Loss: 0.2249\n",
      "Epoch [3/5], Step [1664/10336], Loss: 0.0890\n",
      "Epoch [3/5], Step [1666/10336], Loss: 0.0406\n",
      "Epoch [3/5], Step [1668/10336], Loss: 0.0378\n",
      "Epoch [3/5], Step [1670/10336], Loss: 1.0393\n",
      "Epoch [3/5], Step [1672/10336], Loss: 1.7945\n",
      "Epoch [3/5], Step [1674/10336], Loss: 0.2984\n",
      "Epoch [3/5], Step [1676/10336], Loss: 0.0508\n",
      "Epoch [3/5], Step [1678/10336], Loss: 0.3151\n",
      "Epoch [3/5], Step [1680/10336], Loss: 0.0176\n",
      "Epoch [3/5], Step [1682/10336], Loss: 0.0090\n",
      "Epoch [3/5], Step [1684/10336], Loss: 0.0016\n",
      "Epoch [3/5], Step [1686/10336], Loss: 0.1747\n",
      "Epoch [3/5], Step [1688/10336], Loss: 0.3966\n",
      "Epoch [3/5], Step [1690/10336], Loss: 4.3145\n",
      "Epoch [3/5], Step [1692/10336], Loss: 0.2596\n",
      "Epoch [3/5], Step [1694/10336], Loss: 0.5875\n",
      "Epoch [3/5], Step [1696/10336], Loss: 0.3590\n",
      "Epoch [3/5], Step [1698/10336], Loss: 0.2101\n",
      "Epoch [3/5], Step [1700/10336], Loss: 0.3101\n",
      "Epoch [3/5], Step [1702/10336], Loss: 0.5312\n",
      "Epoch [3/5], Step [1704/10336], Loss: 0.9876\n",
      "Epoch [3/5], Step [1706/10336], Loss: 0.0048\n",
      "Epoch [3/5], Step [1708/10336], Loss: 0.1179\n",
      "Epoch [3/5], Step [1710/10336], Loss: 0.0012\n",
      "Epoch [3/5], Step [1712/10336], Loss: 0.1676\n",
      "Epoch [3/5], Step [1714/10336], Loss: 0.3223\n",
      "Epoch [3/5], Step [1716/10336], Loss: 0.1456\n",
      "Epoch [3/5], Step [1718/10336], Loss: 0.8484\n",
      "Epoch [3/5], Step [1720/10336], Loss: 0.0307\n",
      "Epoch [3/5], Step [1722/10336], Loss: 1.5719\n",
      "Epoch [3/5], Step [1724/10336], Loss: 1.6183\n",
      "Epoch [3/5], Step [1726/10336], Loss: 0.1482\n",
      "Epoch [3/5], Step [1728/10336], Loss: 0.0207\n",
      "Epoch [3/5], Step [1730/10336], Loss: 1.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [1732/10336], Loss: 0.0762\n",
      "Epoch [3/5], Step [1734/10336], Loss: 0.4795\n",
      "Epoch [3/5], Step [1736/10336], Loss: 0.6901\n",
      "Epoch [3/5], Step [1738/10336], Loss: 2.3613\n",
      "Epoch [3/5], Step [1740/10336], Loss: 3.0832\n",
      "Epoch [3/5], Step [1742/10336], Loss: 0.2477\n",
      "Epoch [3/5], Step [1744/10336], Loss: 0.9287\n",
      "Epoch [3/5], Step [1746/10336], Loss: 2.9295\n",
      "Epoch [3/5], Step [1748/10336], Loss: 0.1717\n",
      "Epoch [3/5], Step [1750/10336], Loss: 0.1834\n",
      "Epoch [3/5], Step [1752/10336], Loss: 0.3956\n",
      "Epoch [3/5], Step [1754/10336], Loss: 0.0252\n",
      "Epoch [3/5], Step [1756/10336], Loss: 0.8058\n",
      "Epoch [3/5], Step [1758/10336], Loss: 0.1370\n",
      "Epoch [3/5], Step [1760/10336], Loss: 0.4360\n",
      "Epoch [3/5], Step [1762/10336], Loss: 0.0344\n",
      "Epoch [3/5], Step [1764/10336], Loss: 3.5802\n",
      "Epoch [3/5], Step [1766/10336], Loss: 0.5027\n",
      "Epoch [3/5], Step [1768/10336], Loss: 0.0451\n",
      "Epoch [3/5], Step [1770/10336], Loss: 0.1498\n",
      "Epoch [3/5], Step [1772/10336], Loss: 3.1148\n",
      "Epoch [3/5], Step [1774/10336], Loss: 0.9227\n",
      "Epoch [3/5], Step [1776/10336], Loss: 0.3990\n",
      "Epoch [3/5], Step [1778/10336], Loss: 0.0444\n",
      "Epoch [3/5], Step [1780/10336], Loss: 3.8995\n",
      "Epoch [3/5], Step [1782/10336], Loss: 0.0638\n",
      "Epoch [3/5], Step [1784/10336], Loss: 0.0531\n",
      "Epoch [3/5], Step [1786/10336], Loss: 1.7624\n",
      "Epoch [3/5], Step [1788/10336], Loss: 1.0467\n",
      "Epoch [3/5], Step [1790/10336], Loss: 1.7725\n",
      "Epoch [3/5], Step [1792/10336], Loss: 0.0691\n",
      "Epoch [3/5], Step [1794/10336], Loss: 2.0304\n",
      "Epoch [3/5], Step [1796/10336], Loss: 0.1626\n",
      "Epoch [3/5], Step [1798/10336], Loss: 0.4279\n",
      "Epoch [3/5], Step [1800/10336], Loss: 0.2643\n",
      "Epoch [3/5], Step [1802/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [1804/10336], Loss: 2.6846\n",
      "Epoch [3/5], Step [1806/10336], Loss: 0.7844\n",
      "Epoch [3/5], Step [1808/10336], Loss: 0.0934\n",
      "Epoch [3/5], Step [1810/10336], Loss: 0.0197\n",
      "Epoch [3/5], Step [1812/10336], Loss: 2.7920\n",
      "Epoch [3/5], Step [1814/10336], Loss: 1.2638\n",
      "Epoch [3/5], Step [1816/10336], Loss: 0.3537\n",
      "Epoch [3/5], Step [1818/10336], Loss: 0.1697\n",
      "Epoch [3/5], Step [1820/10336], Loss: 1.7280\n",
      "Epoch [3/5], Step [1822/10336], Loss: 3.6100\n",
      "Epoch [3/5], Step [1824/10336], Loss: 0.0324\n",
      "Epoch [3/5], Step [1826/10336], Loss: 0.0807\n",
      "Epoch [3/5], Step [1828/10336], Loss: 1.6849\n",
      "Epoch [3/5], Step [1830/10336], Loss: 0.2515\n",
      "Epoch [3/5], Step [1832/10336], Loss: 0.2975\n",
      "Epoch [3/5], Step [1834/10336], Loss: 2.6799\n",
      "Epoch [3/5], Step [1836/10336], Loss: 0.0258\n",
      "Epoch [3/5], Step [1838/10336], Loss: 0.1384\n",
      "Epoch [3/5], Step [1840/10336], Loss: 0.1382\n",
      "Epoch [3/5], Step [1842/10336], Loss: 0.2285\n",
      "Epoch [3/5], Step [1844/10336], Loss: 0.0848\n",
      "Epoch [3/5], Step [1846/10336], Loss: 3.0987\n",
      "Epoch [3/5], Step [1848/10336], Loss: 1.0997\n",
      "Epoch [3/5], Step [1850/10336], Loss: 1.8230\n",
      "Epoch [3/5], Step [1852/10336], Loss: 0.3767\n",
      "Epoch [3/5], Step [1854/10336], Loss: 0.2867\n",
      "Epoch [3/5], Step [1856/10336], Loss: 0.0045\n",
      "Epoch [3/5], Step [1858/10336], Loss: 0.9109\n",
      "Epoch [3/5], Step [1860/10336], Loss: 0.0781\n",
      "Epoch [3/5], Step [1862/10336], Loss: 0.2176\n",
      "Epoch [3/5], Step [1864/10336], Loss: 0.5888\n",
      "Epoch [3/5], Step [1866/10336], Loss: 1.0740\n",
      "Epoch [3/5], Step [1868/10336], Loss: 0.0102\n",
      "Epoch [3/5], Step [1870/10336], Loss: 1.4597\n",
      "Epoch [3/5], Step [1872/10336], Loss: 0.0198\n",
      "Epoch [3/5], Step [1874/10336], Loss: 0.3117\n",
      "Epoch [3/5], Step [1876/10336], Loss: 1.6332\n",
      "Epoch [3/5], Step [1878/10336], Loss: 0.2178\n",
      "Epoch [3/5], Step [1880/10336], Loss: 0.2836\n",
      "Epoch [3/5], Step [1882/10336], Loss: 1.4767\n",
      "Epoch [3/5], Step [1884/10336], Loss: 0.0489\n",
      "Epoch [3/5], Step [1886/10336], Loss: 0.2955\n",
      "Epoch [3/5], Step [1888/10336], Loss: 0.2303\n",
      "Epoch [3/5], Step [1890/10336], Loss: 1.4719\n",
      "Epoch [3/5], Step [1892/10336], Loss: 0.1597\n",
      "Epoch [3/5], Step [1894/10336], Loss: 0.8260\n",
      "Epoch [3/5], Step [1896/10336], Loss: 0.0022\n",
      "Epoch [3/5], Step [1898/10336], Loss: 0.6906\n",
      "Epoch [3/5], Step [1900/10336], Loss: 1.9367\n",
      "Epoch [3/5], Step [1902/10336], Loss: 0.0004\n",
      "Epoch [3/5], Step [1904/10336], Loss: 0.0584\n",
      "Epoch [3/5], Step [1906/10336], Loss: 4.8752\n",
      "Epoch [3/5], Step [1908/10336], Loss: 3.8575\n",
      "Epoch [3/5], Step [1910/10336], Loss: 0.0053\n",
      "Epoch [3/5], Step [1912/10336], Loss: 0.6806\n",
      "Epoch [3/5], Step [1914/10336], Loss: 0.0249\n",
      "Epoch [3/5], Step [1916/10336], Loss: 0.0951\n",
      "Epoch [3/5], Step [1918/10336], Loss: 0.7960\n",
      "Epoch [3/5], Step [1920/10336], Loss: 0.2504\n",
      "Epoch [3/5], Step [1922/10336], Loss: 2.1889\n",
      "Epoch [3/5], Step [1924/10336], Loss: 0.0949\n",
      "Epoch [3/5], Step [1926/10336], Loss: 0.1745\n",
      "Epoch [3/5], Step [1928/10336], Loss: 0.1118\n",
      "Epoch [3/5], Step [1930/10336], Loss: 0.0776\n",
      "Epoch [3/5], Step [1932/10336], Loss: 0.2511\n",
      "Epoch [3/5], Step [1934/10336], Loss: 0.0483\n",
      "Epoch [3/5], Step [1936/10336], Loss: 2.2399\n",
      "Epoch [3/5], Step [1938/10336], Loss: 0.0401\n",
      "Epoch [3/5], Step [1940/10336], Loss: 5.7528\n",
      "Epoch [3/5], Step [1942/10336], Loss: 0.3087\n",
      "Epoch [3/5], Step [1944/10336], Loss: 0.3853\n",
      "Epoch [3/5], Step [1946/10336], Loss: 1.3248\n",
      "Epoch [3/5], Step [1948/10336], Loss: 0.0204\n",
      "Epoch [3/5], Step [1950/10336], Loss: 0.2078\n",
      "Epoch [3/5], Step [1952/10336], Loss: 0.3289\n",
      "Epoch [3/5], Step [1954/10336], Loss: 0.0465\n",
      "Epoch [3/5], Step [1956/10336], Loss: 0.0240\n",
      "Epoch [3/5], Step [1958/10336], Loss: 0.0968\n",
      "Epoch [3/5], Step [1960/10336], Loss: 0.0004\n",
      "Epoch [3/5], Step [1962/10336], Loss: 5.3463\n",
      "Epoch [3/5], Step [1964/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [1966/10336], Loss: 0.4376\n",
      "Epoch [3/5], Step [1968/10336], Loss: 0.1261\n",
      "Epoch [3/5], Step [1970/10336], Loss: 0.4781\n",
      "Epoch [3/5], Step [1972/10336], Loss: 0.3734\n",
      "Epoch [3/5], Step [1974/10336], Loss: 4.1518\n",
      "Epoch [3/5], Step [1976/10336], Loss: 0.0149\n",
      "Epoch [3/5], Step [1978/10336], Loss: 2.3274\n",
      "Epoch [3/5], Step [1980/10336], Loss: 0.0352\n",
      "Epoch [3/5], Step [1982/10336], Loss: 0.1116\n",
      "Epoch [3/5], Step [1984/10336], Loss: 3.9044\n",
      "Epoch [3/5], Step [1986/10336], Loss: 2.7670\n",
      "Epoch [3/5], Step [1988/10336], Loss: 0.1586\n",
      "Epoch [3/5], Step [1990/10336], Loss: 0.0450\n",
      "Epoch [3/5], Step [1992/10336], Loss: 0.0963\n",
      "Epoch [3/5], Step [1994/10336], Loss: 0.6935\n",
      "Epoch [3/5], Step [1996/10336], Loss: 0.0313\n",
      "Epoch [3/5], Step [1998/10336], Loss: 0.0250\n",
      "Epoch [3/5], Step [2000/10336], Loss: 1.2934\n",
      "Epoch [3/5], Step [2002/10336], Loss: 1.0893\n",
      "Epoch [3/5], Step [2004/10336], Loss: 0.1298\n",
      "Epoch [3/5], Step [2006/10336], Loss: 0.0782\n",
      "Epoch [3/5], Step [2008/10336], Loss: 2.4242\n",
      "Epoch [3/5], Step [2010/10336], Loss: 1.4684\n",
      "Epoch [3/5], Step [2012/10336], Loss: 0.2426\n",
      "Epoch [3/5], Step [2014/10336], Loss: 0.2995\n",
      "Epoch [3/5], Step [2016/10336], Loss: 2.4089\n",
      "Epoch [3/5], Step [2018/10336], Loss: 0.1530\n",
      "Epoch [3/5], Step [2020/10336], Loss: 0.0436\n",
      "Epoch [3/5], Step [2022/10336], Loss: 1.1967\n",
      "Epoch [3/5], Step [2024/10336], Loss: 0.9338\n",
      "Epoch [3/5], Step [2026/10336], Loss: 1.6519\n",
      "Epoch [3/5], Step [2028/10336], Loss: 0.1045\n",
      "Epoch [3/5], Step [2030/10336], Loss: 0.0765\n",
      "Epoch [3/5], Step [2032/10336], Loss: 0.0203\n",
      "Epoch [3/5], Step [2034/10336], Loss: 5.0078\n",
      "Epoch [3/5], Step [2036/10336], Loss: 0.1198\n",
      "Epoch [3/5], Step [2038/10336], Loss: 0.1726\n",
      "Epoch [3/5], Step [2040/10336], Loss: 0.1221\n",
      "Epoch [3/5], Step [2042/10336], Loss: 1.9911\n",
      "Epoch [3/5], Step [2044/10336], Loss: 0.0065\n",
      "Epoch [3/5], Step [2046/10336], Loss: 5.0870\n",
      "Epoch [3/5], Step [2048/10336], Loss: 0.5637\n",
      "Epoch [3/5], Step [2050/10336], Loss: 0.0579\n",
      "Epoch [3/5], Step [2052/10336], Loss: 0.1454\n",
      "Epoch [3/5], Step [2054/10336], Loss: 0.3777\n",
      "Epoch [3/5], Step [2056/10336], Loss: 2.7575\n",
      "Epoch [3/5], Step [2058/10336], Loss: 0.0686\n",
      "Epoch [3/5], Step [2060/10336], Loss: 1.3975\n",
      "Epoch [3/5], Step [2062/10336], Loss: 3.0564\n",
      "Epoch [3/5], Step [2064/10336], Loss: 0.2127\n",
      "Epoch [3/5], Step [2066/10336], Loss: 0.0400\n",
      "Epoch [3/5], Step [2068/10336], Loss: 0.1019\n",
      "Epoch [3/5], Step [2070/10336], Loss: 0.1806\n",
      "Epoch [3/5], Step [2072/10336], Loss: 0.2745\n",
      "Epoch [3/5], Step [2074/10336], Loss: 3.8553\n",
      "Epoch [3/5], Step [2076/10336], Loss: 0.3323\n",
      "Epoch [3/5], Step [2078/10336], Loss: 0.1017\n",
      "Epoch [3/5], Step [2080/10336], Loss: 1.9797\n",
      "Epoch [3/5], Step [2082/10336], Loss: 0.4553\n",
      "Epoch [3/5], Step [2084/10336], Loss: 0.2070\n",
      "Epoch [3/5], Step [2086/10336], Loss: 3.0694\n",
      "Epoch [3/5], Step [2088/10336], Loss: 0.6352\n",
      "Epoch [3/5], Step [2090/10336], Loss: 3.8045\n",
      "Epoch [3/5], Step [2092/10336], Loss: 0.3504\n",
      "Epoch [3/5], Step [2094/10336], Loss: 0.3443\n",
      "Epoch [3/5], Step [2096/10336], Loss: 0.6045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [2098/10336], Loss: 0.2081\n",
      "Epoch [3/5], Step [2100/10336], Loss: 0.9560\n",
      "Epoch [3/5], Step [2102/10336], Loss: 0.2767\n",
      "Epoch [3/5], Step [2104/10336], Loss: 0.4501\n",
      "Epoch [3/5], Step [2106/10336], Loss: 0.5410\n",
      "Epoch [3/5], Step [2108/10336], Loss: 1.8061\n",
      "Epoch [3/5], Step [2110/10336], Loss: 0.3209\n",
      "Epoch [3/5], Step [2112/10336], Loss: 3.8228\n",
      "Epoch [3/5], Step [2114/10336], Loss: 1.0810\n",
      "Epoch [3/5], Step [2116/10336], Loss: 0.1449\n",
      "Epoch [3/5], Step [2118/10336], Loss: 0.0050\n",
      "Epoch [3/5], Step [2120/10336], Loss: 0.5779\n",
      "Epoch [3/5], Step [2122/10336], Loss: 3.4032\n",
      "Epoch [3/5], Step [2124/10336], Loss: 0.9493\n",
      "Epoch [3/5], Step [2126/10336], Loss: 0.3435\n",
      "Epoch [3/5], Step [2128/10336], Loss: 0.3658\n",
      "Epoch [3/5], Step [2130/10336], Loss: 3.6151\n",
      "Epoch [3/5], Step [2132/10336], Loss: 0.8302\n",
      "Epoch [3/5], Step [2134/10336], Loss: 0.2201\n",
      "Epoch [3/5], Step [2136/10336], Loss: 0.2226\n",
      "Epoch [3/5], Step [2138/10336], Loss: 0.1279\n",
      "Epoch [3/5], Step [2140/10336], Loss: 2.8041\n",
      "Epoch [3/5], Step [2142/10336], Loss: 1.4884\n",
      "Epoch [3/5], Step [2144/10336], Loss: 0.1198\n",
      "Epoch [3/5], Step [2146/10336], Loss: 0.0333\n",
      "Epoch [3/5], Step [2148/10336], Loss: 0.3125\n",
      "Epoch [3/5], Step [2150/10336], Loss: 1.4267\n",
      "Epoch [3/5], Step [2152/10336], Loss: 3.1823\n",
      "Epoch [3/5], Step [2154/10336], Loss: 0.2232\n",
      "Epoch [3/5], Step [2156/10336], Loss: 0.8998\n",
      "Epoch [3/5], Step [2158/10336], Loss: 0.0427\n",
      "Epoch [3/5], Step [2160/10336], Loss: 0.6724\n",
      "Epoch [3/5], Step [2162/10336], Loss: 0.1752\n",
      "Epoch [3/5], Step [2164/10336], Loss: 0.2017\n",
      "Epoch [3/5], Step [2166/10336], Loss: 1.0340\n",
      "Epoch [3/5], Step [2168/10336], Loss: 1.9408\n",
      "Epoch [3/5], Step [2170/10336], Loss: 0.4835\n",
      "Epoch [3/5], Step [2172/10336], Loss: 0.2312\n",
      "Epoch [3/5], Step [2174/10336], Loss: 0.3465\n",
      "Epoch [3/5], Step [2176/10336], Loss: 0.9098\n",
      "Epoch [3/5], Step [2178/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [2180/10336], Loss: 0.0164\n",
      "Epoch [3/5], Step [2182/10336], Loss: 1.6224\n",
      "Epoch [3/5], Step [2184/10336], Loss: 2.7640\n",
      "Epoch [3/5], Step [2186/10336], Loss: 3.9038\n",
      "Epoch [3/5], Step [2188/10336], Loss: 0.3543\n",
      "Epoch [3/5], Step [2190/10336], Loss: 0.1029\n",
      "Epoch [3/5], Step [2192/10336], Loss: 1.1078\n",
      "Epoch [3/5], Step [2194/10336], Loss: 0.2491\n",
      "Epoch [3/5], Step [2196/10336], Loss: 0.3362\n",
      "Epoch [3/5], Step [2198/10336], Loss: 1.4287\n",
      "Epoch [3/5], Step [2200/10336], Loss: 0.2100\n",
      "Epoch [3/5], Step [2202/10336], Loss: 0.2468\n",
      "Epoch [3/5], Step [2204/10336], Loss: 0.0481\n",
      "Epoch [3/5], Step [2206/10336], Loss: 2.3003\n",
      "Epoch [3/5], Step [2208/10336], Loss: 1.6211\n",
      "Epoch [3/5], Step [2210/10336], Loss: 0.1854\n",
      "Epoch [3/5], Step [2212/10336], Loss: 0.5273\n",
      "Epoch [3/5], Step [2214/10336], Loss: 2.7234\n",
      "Epoch [3/5], Step [2216/10336], Loss: 0.3473\n",
      "Epoch [3/5], Step [2218/10336], Loss: 0.3598\n",
      "Epoch [3/5], Step [2220/10336], Loss: 0.4204\n",
      "Epoch [3/5], Step [2222/10336], Loss: 0.4424\n",
      "Epoch [3/5], Step [2224/10336], Loss: 0.0515\n",
      "Epoch [3/5], Step [2226/10336], Loss: 2.3537\n",
      "Epoch [3/5], Step [2228/10336], Loss: 0.0695\n",
      "Epoch [3/5], Step [2230/10336], Loss: 0.0242\n",
      "Epoch [3/5], Step [2232/10336], Loss: 0.5840\n",
      "Epoch [3/5], Step [2234/10336], Loss: 1.2756\n",
      "Epoch [3/5], Step [2236/10336], Loss: 0.0391\n",
      "Epoch [3/5], Step [2238/10336], Loss: 0.1525\n",
      "Epoch [3/5], Step [2240/10336], Loss: 0.9953\n",
      "Epoch [3/5], Step [2242/10336], Loss: 2.8730\n",
      "Epoch [3/5], Step [2244/10336], Loss: 0.0363\n",
      "Epoch [3/5], Step [2246/10336], Loss: 0.1854\n",
      "Epoch [3/5], Step [2248/10336], Loss: 0.2183\n",
      "Epoch [3/5], Step [2250/10336], Loss: 0.0221\n",
      "Epoch [3/5], Step [2252/10336], Loss: 2.0061\n",
      "Epoch [3/5], Step [2254/10336], Loss: 2.3257\n",
      "Epoch [3/5], Step [2256/10336], Loss: 0.2411\n",
      "Epoch [3/5], Step [2258/10336], Loss: 2.5695\n",
      "Epoch [3/5], Step [2260/10336], Loss: 0.1466\n",
      "Epoch [3/5], Step [2262/10336], Loss: 2.5564\n",
      "Epoch [3/5], Step [2264/10336], Loss: 0.1327\n",
      "Epoch [3/5], Step [2266/10336], Loss: 1.2509\n",
      "Epoch [3/5], Step [2268/10336], Loss: 0.0057\n",
      "Epoch [3/5], Step [2270/10336], Loss: 0.3769\n",
      "Epoch [3/5], Step [2272/10336], Loss: 1.5234\n",
      "Epoch [3/5], Step [2274/10336], Loss: 0.8032\n",
      "Epoch [3/5], Step [2276/10336], Loss: 1.3363\n",
      "Epoch [3/5], Step [2278/10336], Loss: 0.0223\n",
      "Epoch [3/5], Step [2280/10336], Loss: 0.1125\n",
      "Epoch [3/5], Step [2282/10336], Loss: 0.6250\n",
      "Epoch [3/5], Step [2284/10336], Loss: 0.1780\n",
      "Epoch [3/5], Step [2286/10336], Loss: 0.4649\n",
      "Epoch [3/5], Step [2288/10336], Loss: 0.0825\n",
      "Epoch [3/5], Step [2290/10336], Loss: 2.7514\n",
      "Epoch [3/5], Step [2292/10336], Loss: 0.1903\n",
      "Epoch [3/5], Step [2294/10336], Loss: 2.9633\n",
      "Epoch [3/5], Step [2296/10336], Loss: 0.8796\n",
      "Epoch [3/5], Step [2298/10336], Loss: 0.0489\n",
      "Epoch [3/5], Step [2300/10336], Loss: 0.5212\n",
      "Epoch [3/5], Step [2302/10336], Loss: 0.1003\n",
      "Epoch [3/5], Step [2304/10336], Loss: 1.3171\n",
      "Epoch [3/5], Step [2306/10336], Loss: 0.0314\n",
      "Epoch [3/5], Step [2308/10336], Loss: 2.4088\n",
      "Epoch [3/5], Step [2310/10336], Loss: 0.3700\n",
      "Epoch [3/5], Step [2312/10336], Loss: 0.2766\n",
      "Epoch [3/5], Step [2314/10336], Loss: 0.0148\n",
      "Epoch [3/5], Step [2316/10336], Loss: 0.0933\n",
      "Epoch [3/5], Step [2318/10336], Loss: 0.2584\n",
      "Epoch [3/5], Step [2320/10336], Loss: 0.2813\n",
      "Epoch [3/5], Step [2322/10336], Loss: 0.2030\n",
      "Epoch [3/5], Step [2324/10336], Loss: 0.1082\n",
      "Epoch [3/5], Step [2326/10336], Loss: 1.0426\n",
      "Epoch [3/5], Step [2328/10336], Loss: 0.9675\n",
      "Epoch [3/5], Step [2330/10336], Loss: 0.3518\n",
      "Epoch [3/5], Step [2332/10336], Loss: 0.3226\n",
      "Epoch [3/5], Step [2334/10336], Loss: 0.6432\n",
      "Epoch [3/5], Step [2336/10336], Loss: 2.3963\n",
      "Epoch [3/5], Step [2338/10336], Loss: 0.0112\n",
      "Epoch [3/5], Step [2340/10336], Loss: 0.0220\n",
      "Epoch [3/5], Step [2342/10336], Loss: 0.2002\n",
      "Epoch [3/5], Step [2344/10336], Loss: 0.5864\n",
      "Epoch [3/5], Step [2346/10336], Loss: 1.6476\n",
      "Epoch [3/5], Step [2348/10336], Loss: 0.6131\n",
      "Epoch [3/5], Step [2350/10336], Loss: 0.4923\n",
      "Epoch [3/5], Step [2352/10336], Loss: 0.3546\n",
      "Epoch [3/5], Step [2354/10336], Loss: 2.3740\n",
      "Epoch [3/5], Step [2356/10336], Loss: 0.2566\n",
      "Epoch [3/5], Step [2358/10336], Loss: 1.1652\n",
      "Epoch [3/5], Step [2360/10336], Loss: 2.5454\n",
      "Epoch [3/5], Step [2362/10336], Loss: 0.3174\n",
      "Epoch [3/5], Step [2364/10336], Loss: 6.5849\n",
      "Epoch [3/5], Step [2366/10336], Loss: 0.5630\n",
      "Epoch [3/5], Step [2368/10336], Loss: 0.0704\n",
      "Epoch [3/5], Step [2370/10336], Loss: 0.9789\n",
      "Epoch [3/5], Step [2372/10336], Loss: 0.3160\n",
      "Epoch [3/5], Step [2374/10336], Loss: 4.4022\n",
      "Epoch [3/5], Step [2376/10336], Loss: 0.3851\n",
      "Epoch [3/5], Step [2378/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [2380/10336], Loss: 0.0247\n",
      "Epoch [3/5], Step [2382/10336], Loss: 3.5697\n",
      "Epoch [3/5], Step [2384/10336], Loss: 0.1274\n",
      "Epoch [3/5], Step [2386/10336], Loss: 0.1500\n",
      "Epoch [3/5], Step [2388/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [2390/10336], Loss: 0.0983\n",
      "Epoch [3/5], Step [2392/10336], Loss: 3.5592\n",
      "Epoch [3/5], Step [2394/10336], Loss: 0.0482\n",
      "Epoch [3/5], Step [2396/10336], Loss: 0.0345\n",
      "Epoch [3/5], Step [2398/10336], Loss: 0.0838\n",
      "Epoch [3/5], Step [2400/10336], Loss: 0.4100\n",
      "Epoch [3/5], Step [2402/10336], Loss: 1.1091\n",
      "Epoch [3/5], Step [2404/10336], Loss: 0.1674\n",
      "Epoch [3/5], Step [2406/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [2408/10336], Loss: 2.0068\n",
      "Epoch [3/5], Step [2410/10336], Loss: 0.1888\n",
      "Epoch [3/5], Step [2412/10336], Loss: 0.3301\n",
      "Epoch [3/5], Step [2414/10336], Loss: 0.2026\n",
      "Epoch [3/5], Step [2416/10336], Loss: 1.7452\n",
      "Epoch [3/5], Step [2418/10336], Loss: 0.1067\n",
      "Epoch [3/5], Step [2420/10336], Loss: 0.0642\n",
      "Epoch [3/5], Step [2422/10336], Loss: 0.1455\n",
      "Epoch [3/5], Step [2424/10336], Loss: 0.2781\n",
      "Epoch [3/5], Step [2426/10336], Loss: 3.0989\n",
      "Epoch [3/5], Step [2428/10336], Loss: 0.1144\n",
      "Epoch [3/5], Step [2430/10336], Loss: 0.1615\n",
      "Epoch [3/5], Step [2432/10336], Loss: 0.2573\n",
      "Epoch [3/5], Step [2434/10336], Loss: 0.7538\n",
      "Epoch [3/5], Step [2436/10336], Loss: 0.2420\n",
      "Epoch [3/5], Step [2438/10336], Loss: 3.5331\n",
      "Epoch [3/5], Step [2440/10336], Loss: 0.0653\n",
      "Epoch [3/5], Step [2442/10336], Loss: 0.6234\n",
      "Epoch [3/5], Step [2444/10336], Loss: 0.3873\n",
      "Epoch [3/5], Step [2446/10336], Loss: 0.0967\n",
      "Epoch [3/5], Step [2448/10336], Loss: 1.4894\n",
      "Epoch [3/5], Step [2450/10336], Loss: 0.3784\n",
      "Epoch [3/5], Step [2452/10336], Loss: 2.8186\n",
      "Epoch [3/5], Step [2454/10336], Loss: 0.1570\n",
      "Epoch [3/5], Step [2456/10336], Loss: 0.4764\n",
      "Epoch [3/5], Step [2458/10336], Loss: 0.4303\n",
      "Epoch [3/5], Step [2460/10336], Loss: 0.0388\n",
      "Epoch [3/5], Step [2462/10336], Loss: 0.0309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [2464/10336], Loss: 2.2798\n",
      "Epoch [3/5], Step [2466/10336], Loss: 0.0025\n",
      "Epoch [3/5], Step [2468/10336], Loss: 1.2050\n",
      "Epoch [3/5], Step [2470/10336], Loss: 0.3060\n",
      "Epoch [3/5], Step [2472/10336], Loss: 0.1369\n",
      "Epoch [3/5], Step [2474/10336], Loss: 2.2077\n",
      "Epoch [3/5], Step [2476/10336], Loss: 0.3731\n",
      "Epoch [3/5], Step [2478/10336], Loss: 0.6697\n",
      "Epoch [3/5], Step [2480/10336], Loss: 2.3759\n",
      "Epoch [3/5], Step [2482/10336], Loss: 2.7561\n",
      "Epoch [3/5], Step [2484/10336], Loss: 3.8879\n",
      "Epoch [3/5], Step [2486/10336], Loss: 1.5784\n",
      "Epoch [3/5], Step [2488/10336], Loss: 1.0253\n",
      "Epoch [3/5], Step [2490/10336], Loss: 0.0602\n",
      "Epoch [3/5], Step [2492/10336], Loss: 0.0051\n",
      "Epoch [3/5], Step [2494/10336], Loss: 0.1982\n",
      "Epoch [3/5], Step [2496/10336], Loss: 0.3189\n",
      "Epoch [3/5], Step [2498/10336], Loss: 0.0350\n",
      "Epoch [3/5], Step [2500/10336], Loss: 0.3981\n",
      "Epoch [3/5], Step [2502/10336], Loss: 1.5370\n",
      "Epoch [3/5], Step [2504/10336], Loss: 0.1826\n",
      "Epoch [3/5], Step [2506/10336], Loss: 0.0156\n",
      "Epoch [3/5], Step [2508/10336], Loss: 0.1804\n",
      "Epoch [3/5], Step [2510/10336], Loss: 0.1798\n",
      "Epoch [3/5], Step [2512/10336], Loss: 0.1582\n",
      "Epoch [3/5], Step [2514/10336], Loss: 0.1226\n",
      "Epoch [3/5], Step [2516/10336], Loss: 0.3639\n",
      "Epoch [3/5], Step [2518/10336], Loss: 0.7767\n",
      "Epoch [3/5], Step [2520/10336], Loss: 0.2365\n",
      "Epoch [3/5], Step [2522/10336], Loss: 0.2486\n",
      "Epoch [3/5], Step [2524/10336], Loss: 0.2684\n",
      "Epoch [3/5], Step [2526/10336], Loss: 0.0396\n",
      "Epoch [3/5], Step [2528/10336], Loss: 0.6751\n",
      "Epoch [3/5], Step [2530/10336], Loss: 0.0084\n",
      "Epoch [3/5], Step [2532/10336], Loss: 0.1947\n",
      "Epoch [3/5], Step [2534/10336], Loss: 4.9473\n",
      "Epoch [3/5], Step [2536/10336], Loss: 0.0285\n",
      "Epoch [3/5], Step [2538/10336], Loss: 0.1185\n",
      "Epoch [3/5], Step [2540/10336], Loss: 1.2771\n",
      "Epoch [3/5], Step [2542/10336], Loss: 0.9064\n",
      "Epoch [3/5], Step [2544/10336], Loss: 0.1165\n",
      "Epoch [3/5], Step [2546/10336], Loss: 0.3108\n",
      "Epoch [3/5], Step [2548/10336], Loss: 0.9738\n",
      "Epoch [3/5], Step [2550/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [2552/10336], Loss: 0.6438\n",
      "Epoch [3/5], Step [2554/10336], Loss: 0.2719\n",
      "Epoch [3/5], Step [2556/10336], Loss: 4.6437\n",
      "Epoch [3/5], Step [2558/10336], Loss: 0.1795\n",
      "Epoch [3/5], Step [2560/10336], Loss: 0.0032\n",
      "Epoch [3/5], Step [2562/10336], Loss: 0.0533\n",
      "Epoch [3/5], Step [2564/10336], Loss: 0.5998\n",
      "Epoch [3/5], Step [2566/10336], Loss: 0.3726\n",
      "Epoch [3/5], Step [2568/10336], Loss: 0.3133\n",
      "Epoch [3/5], Step [2570/10336], Loss: 0.0641\n",
      "Epoch [3/5], Step [2572/10336], Loss: 0.1635\n",
      "Epoch [3/5], Step [2574/10336], Loss: 0.5133\n",
      "Epoch [3/5], Step [2576/10336], Loss: 1.8926\n",
      "Epoch [3/5], Step [2578/10336], Loss: 0.3338\n",
      "Epoch [3/5], Step [2580/10336], Loss: 0.3295\n",
      "Epoch [3/5], Step [2582/10336], Loss: 0.0811\n",
      "Epoch [3/5], Step [2584/10336], Loss: 0.3439\n",
      "Epoch [3/5], Step [2586/10336], Loss: 0.0259\n",
      "Epoch [3/5], Step [2588/10336], Loss: 1.2711\n",
      "Epoch [3/5], Step [2590/10336], Loss: 1.7590\n",
      "Epoch [3/5], Step [2592/10336], Loss: 0.1528\n",
      "Epoch [3/5], Step [2594/10336], Loss: 0.6652\n",
      "Epoch [3/5], Step [2596/10336], Loss: 2.5864\n",
      "Epoch [3/5], Step [2598/10336], Loss: 0.0234\n",
      "Epoch [3/5], Step [2600/10336], Loss: 0.3147\n",
      "Epoch [3/5], Step [2602/10336], Loss: 0.0162\n",
      "Epoch [3/5], Step [2604/10336], Loss: 1.4030\n",
      "Epoch [3/5], Step [2606/10336], Loss: 1.0684\n",
      "Epoch [3/5], Step [2608/10336], Loss: 1.2941\n",
      "Epoch [3/5], Step [2610/10336], Loss: 0.3442\n",
      "Epoch [3/5], Step [2612/10336], Loss: 0.0966\n",
      "Epoch [3/5], Step [2614/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [2616/10336], Loss: 0.2922\n",
      "Epoch [3/5], Step [2618/10336], Loss: 0.0132\n",
      "Epoch [3/5], Step [2620/10336], Loss: 1.6182\n",
      "Epoch [3/5], Step [2622/10336], Loss: 0.1458\n",
      "Epoch [3/5], Step [2624/10336], Loss: 0.6561\n",
      "Epoch [3/5], Step [2626/10336], Loss: 0.4462\n",
      "Epoch [3/5], Step [2628/10336], Loss: 0.1549\n",
      "Epoch [3/5], Step [2630/10336], Loss: 0.1515\n",
      "Epoch [3/5], Step [2632/10336], Loss: 0.1286\n",
      "Epoch [3/5], Step [2634/10336], Loss: 1.1216\n",
      "Epoch [3/5], Step [2636/10336], Loss: 0.3296\n",
      "Epoch [3/5], Step [2638/10336], Loss: 0.1365\n",
      "Epoch [3/5], Step [2640/10336], Loss: 1.1534\n",
      "Epoch [3/5], Step [2642/10336], Loss: 0.2211\n",
      "Epoch [3/5], Step [2644/10336], Loss: 0.2534\n",
      "Epoch [3/5], Step [2646/10336], Loss: 0.0112\n",
      "Epoch [3/5], Step [2648/10336], Loss: 0.0023\n",
      "Epoch [3/5], Step [2650/10336], Loss: 0.1411\n",
      "Epoch [3/5], Step [2652/10336], Loss: 0.4019\n",
      "Epoch [3/5], Step [2654/10336], Loss: 0.3643\n",
      "Epoch [3/5], Step [2656/10336], Loss: 0.3779\n",
      "Epoch [3/5], Step [2658/10336], Loss: 1.2899\n",
      "Epoch [3/5], Step [2660/10336], Loss: 1.3597\n",
      "Epoch [3/5], Step [2662/10336], Loss: 0.1184\n",
      "Epoch [3/5], Step [2664/10336], Loss: 0.0323\n",
      "Epoch [3/5], Step [2666/10336], Loss: 0.1692\n",
      "Epoch [3/5], Step [2668/10336], Loss: 2.8221\n",
      "Epoch [3/5], Step [2670/10336], Loss: 0.2212\n",
      "Epoch [3/5], Step [2672/10336], Loss: 1.6490\n",
      "Epoch [3/5], Step [2674/10336], Loss: 2.2867\n",
      "Epoch [3/5], Step [2676/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [2678/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [2680/10336], Loss: 0.5672\n",
      "Epoch [3/5], Step [2682/10336], Loss: 0.1017\n",
      "Epoch [3/5], Step [2684/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [2686/10336], Loss: 5.2748\n",
      "Epoch [3/5], Step [2688/10336], Loss: 0.1429\n",
      "Epoch [3/5], Step [2690/10336], Loss: 0.8747\n",
      "Epoch [3/5], Step [2692/10336], Loss: 0.0185\n",
      "Epoch [3/5], Step [2694/10336], Loss: 0.3036\n",
      "Epoch [3/5], Step [2696/10336], Loss: 2.3232\n",
      "Epoch [3/5], Step [2698/10336], Loss: 2.3173\n",
      "Epoch [3/5], Step [2700/10336], Loss: 0.0324\n",
      "Epoch [3/5], Step [2702/10336], Loss: 0.7691\n",
      "Epoch [3/5], Step [2704/10336], Loss: 0.0521\n",
      "Epoch [3/5], Step [2706/10336], Loss: 0.0199\n",
      "Epoch [3/5], Step [2708/10336], Loss: 1.3314\n",
      "Epoch [3/5], Step [2710/10336], Loss: 0.0886\n",
      "Epoch [3/5], Step [2712/10336], Loss: 0.0099\n",
      "Epoch [3/5], Step [2714/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [2716/10336], Loss: 4.4421\n",
      "Epoch [3/5], Step [2718/10336], Loss: 1.1260\n",
      "Epoch [3/5], Step [2720/10336], Loss: 0.1709\n",
      "Epoch [3/5], Step [2722/10336], Loss: 0.2021\n",
      "Epoch [3/5], Step [2724/10336], Loss: 0.7483\n",
      "Epoch [3/5], Step [2726/10336], Loss: 0.0069\n",
      "Epoch [3/5], Step [2728/10336], Loss: 2.2296\n",
      "Epoch [3/5], Step [2730/10336], Loss: 0.5567\n",
      "Epoch [3/5], Step [2732/10336], Loss: 1.0470\n",
      "Epoch [3/5], Step [2734/10336], Loss: 3.9782\n",
      "Epoch [3/5], Step [2736/10336], Loss: 0.0292\n",
      "Epoch [3/5], Step [2738/10336], Loss: 3.8777\n",
      "Epoch [3/5], Step [2740/10336], Loss: 0.1428\n",
      "Epoch [3/5], Step [2742/10336], Loss: 1.9301\n",
      "Epoch [3/5], Step [2744/10336], Loss: 0.1874\n",
      "Epoch [3/5], Step [2746/10336], Loss: 0.2439\n",
      "Epoch [3/5], Step [2748/10336], Loss: 0.1460\n",
      "Epoch [3/5], Step [2750/10336], Loss: 0.6697\n",
      "Epoch [3/5], Step [2752/10336], Loss: 4.1359\n",
      "Epoch [3/5], Step [2754/10336], Loss: 0.5846\n",
      "Epoch [3/5], Step [2756/10336], Loss: 0.1055\n",
      "Epoch [3/5], Step [2758/10336], Loss: 0.0776\n",
      "Epoch [3/5], Step [2760/10336], Loss: 0.0342\n",
      "Epoch [3/5], Step [2762/10336], Loss: 0.3005\n",
      "Epoch [3/5], Step [2764/10336], Loss: 0.3305\n",
      "Epoch [3/5], Step [2766/10336], Loss: 0.0084\n",
      "Epoch [3/5], Step [2768/10336], Loss: 0.7531\n",
      "Epoch [3/5], Step [2770/10336], Loss: 0.2361\n",
      "Epoch [3/5], Step [2772/10336], Loss: 0.1493\n",
      "Epoch [3/5], Step [2774/10336], Loss: 0.2440\n",
      "Epoch [3/5], Step [2776/10336], Loss: 1.8035\n",
      "Epoch [3/5], Step [2778/10336], Loss: 0.1648\n",
      "Epoch [3/5], Step [2780/10336], Loss: 0.3072\n",
      "Epoch [3/5], Step [2782/10336], Loss: 0.2122\n",
      "Epoch [3/5], Step [2784/10336], Loss: 0.3005\n",
      "Epoch [3/5], Step [2786/10336], Loss: 0.0755\n",
      "Epoch [3/5], Step [2788/10336], Loss: 0.3035\n",
      "Epoch [3/5], Step [2790/10336], Loss: 0.3154\n",
      "Epoch [3/5], Step [2792/10336], Loss: 0.0662\n",
      "Epoch [3/5], Step [2794/10336], Loss: 0.0444\n",
      "Epoch [3/5], Step [2796/10336], Loss: 0.3061\n",
      "Epoch [3/5], Step [2798/10336], Loss: 0.1056\n",
      "Epoch [3/5], Step [2800/10336], Loss: 0.4624\n",
      "Epoch [3/5], Step [2802/10336], Loss: 0.1413\n",
      "Epoch [3/5], Step [2804/10336], Loss: 0.6123\n",
      "Epoch [3/5], Step [2806/10336], Loss: 0.6004\n",
      "Epoch [3/5], Step [2808/10336], Loss: 0.1470\n",
      "Epoch [3/5], Step [2810/10336], Loss: 0.1102\n",
      "Epoch [3/5], Step [2812/10336], Loss: 0.1818\n",
      "Epoch [3/5], Step [2814/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [2816/10336], Loss: 0.2617\n",
      "Epoch [3/5], Step [2818/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [2820/10336], Loss: 0.1251\n",
      "Epoch [3/5], Step [2822/10336], Loss: 0.3520\n",
      "Epoch [3/5], Step [2824/10336], Loss: 2.8154\n",
      "Epoch [3/5], Step [2826/10336], Loss: 0.0336\n",
      "Epoch [3/5], Step [2828/10336], Loss: 0.0531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [2830/10336], Loss: 0.3245\n",
      "Epoch [3/5], Step [2832/10336], Loss: 0.4012\n",
      "Epoch [3/5], Step [2834/10336], Loss: 1.0785\n",
      "Epoch [3/5], Step [2836/10336], Loss: 0.1796\n",
      "Epoch [3/5], Step [2838/10336], Loss: 1.2791\n",
      "Epoch [3/5], Step [2840/10336], Loss: 0.0751\n",
      "Epoch [3/5], Step [2842/10336], Loss: 0.2621\n",
      "Epoch [3/5], Step [2844/10336], Loss: 0.3365\n",
      "Epoch [3/5], Step [2846/10336], Loss: 0.6059\n",
      "Epoch [3/5], Step [2848/10336], Loss: 0.2284\n",
      "Epoch [3/5], Step [2850/10336], Loss: 0.0339\n",
      "Epoch [3/5], Step [2852/10336], Loss: 0.5099\n",
      "Epoch [3/5], Step [2854/10336], Loss: 0.1308\n",
      "Epoch [3/5], Step [2856/10336], Loss: 0.6782\n",
      "Epoch [3/5], Step [2858/10336], Loss: 0.0203\n",
      "Epoch [3/5], Step [2860/10336], Loss: 0.3483\n",
      "Epoch [3/5], Step [2862/10336], Loss: 0.0112\n",
      "Epoch [3/5], Step [2864/10336], Loss: 0.4982\n",
      "Epoch [3/5], Step [2866/10336], Loss: 2.3460\n",
      "Epoch [3/5], Step [2868/10336], Loss: 0.2440\n",
      "Epoch [3/5], Step [2870/10336], Loss: 0.9832\n",
      "Epoch [3/5], Step [2872/10336], Loss: 0.1703\n",
      "Epoch [3/5], Step [2874/10336], Loss: 2.9825\n",
      "Epoch [3/5], Step [2876/10336], Loss: 1.3280\n",
      "Epoch [3/5], Step [2878/10336], Loss: 1.6753\n",
      "Epoch [3/5], Step [2880/10336], Loss: 2.4502\n",
      "Epoch [3/5], Step [2882/10336], Loss: 0.0633\n",
      "Epoch [3/5], Step [2884/10336], Loss: 0.0355\n",
      "Epoch [3/5], Step [2886/10336], Loss: 0.0231\n",
      "Epoch [3/5], Step [2888/10336], Loss: 0.8452\n",
      "Epoch [3/5], Step [2890/10336], Loss: 0.7014\n",
      "Epoch [3/5], Step [2892/10336], Loss: 1.0018\n",
      "Epoch [3/5], Step [2894/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [2896/10336], Loss: 0.0187\n",
      "Epoch [3/5], Step [2898/10336], Loss: 0.5825\n",
      "Epoch [3/5], Step [2900/10336], Loss: 0.3097\n",
      "Epoch [3/5], Step [2902/10336], Loss: 0.3265\n",
      "Epoch [3/5], Step [2904/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [2906/10336], Loss: 0.6750\n",
      "Epoch [3/5], Step [2908/10336], Loss: 0.3370\n",
      "Epoch [3/5], Step [2910/10336], Loss: 0.2344\n",
      "Epoch [3/5], Step [2912/10336], Loss: 0.2109\n",
      "Epoch [3/5], Step [2914/10336], Loss: 0.1424\n",
      "Epoch [3/5], Step [2916/10336], Loss: 2.9897\n",
      "Epoch [3/5], Step [2918/10336], Loss: 0.5983\n",
      "Epoch [3/5], Step [2920/10336], Loss: 0.6759\n",
      "Epoch [3/5], Step [2922/10336], Loss: 0.0004\n",
      "Epoch [3/5], Step [2924/10336], Loss: 0.0003\n",
      "Epoch [3/5], Step [2926/10336], Loss: 0.2253\n",
      "Epoch [3/5], Step [2928/10336], Loss: 0.0054\n",
      "Epoch [3/5], Step [2930/10336], Loss: 0.0202\n",
      "Epoch [3/5], Step [2932/10336], Loss: 0.3550\n",
      "Epoch [3/5], Step [2934/10336], Loss: 0.8203\n",
      "Epoch [3/5], Step [2936/10336], Loss: 5.3842\n",
      "Epoch [3/5], Step [2938/10336], Loss: 0.1299\n",
      "Epoch [3/5], Step [2940/10336], Loss: 0.0153\n",
      "Epoch [3/5], Step [2942/10336], Loss: 0.0929\n",
      "Epoch [3/5], Step [2944/10336], Loss: 0.8509\n",
      "Epoch [3/5], Step [2946/10336], Loss: 0.0345\n",
      "Epoch [3/5], Step [2948/10336], Loss: 1.8993\n",
      "Epoch [3/5], Step [2950/10336], Loss: 1.0730\n",
      "Epoch [3/5], Step [2952/10336], Loss: 3.0522\n",
      "Epoch [3/5], Step [2954/10336], Loss: 0.4124\n",
      "Epoch [3/5], Step [2956/10336], Loss: 0.0080\n",
      "Epoch [3/5], Step [2958/10336], Loss: 1.0887\n",
      "Epoch [3/5], Step [2960/10336], Loss: 0.6544\n",
      "Epoch [3/5], Step [2962/10336], Loss: 0.1067\n",
      "Epoch [3/5], Step [2964/10336], Loss: 1.3665\n",
      "Epoch [3/5], Step [2966/10336], Loss: 0.5597\n",
      "Epoch [3/5], Step [2968/10336], Loss: 1.6548\n",
      "Epoch [3/5], Step [2970/10336], Loss: 3.6608\n",
      "Epoch [3/5], Step [2972/10336], Loss: 0.0067\n",
      "Epoch [3/5], Step [2974/10336], Loss: 0.0302\n",
      "Epoch [3/5], Step [2976/10336], Loss: 0.0217\n",
      "Epoch [3/5], Step [2978/10336], Loss: 0.2722\n",
      "Epoch [3/5], Step [2980/10336], Loss: 0.5073\n",
      "Epoch [3/5], Step [2982/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [2984/10336], Loss: 0.0194\n",
      "Epoch [3/5], Step [2986/10336], Loss: 1.7010\n",
      "Epoch [3/5], Step [2988/10336], Loss: 0.0463\n",
      "Epoch [3/5], Step [2990/10336], Loss: 0.5177\n",
      "Epoch [3/5], Step [2992/10336], Loss: 0.0283\n",
      "Epoch [3/5], Step [2994/10336], Loss: 0.1811\n",
      "Epoch [3/5], Step [2996/10336], Loss: 0.1257\n",
      "Epoch [3/5], Step [2998/10336], Loss: 0.0120\n",
      "Epoch [3/5], Step [3000/10336], Loss: 0.2260\n",
      "Epoch [3/5], Step [3002/10336], Loss: 0.0039\n",
      "Epoch [3/5], Step [3004/10336], Loss: 0.1799\n",
      "Epoch [3/5], Step [3006/10336], Loss: 0.2186\n",
      "Epoch [3/5], Step [3008/10336], Loss: 0.2801\n",
      "Epoch [3/5], Step [3010/10336], Loss: 0.0574\n",
      "Epoch [3/5], Step [3012/10336], Loss: 0.0605\n",
      "Epoch [3/5], Step [3014/10336], Loss: 0.0072\n",
      "Epoch [3/5], Step [3016/10336], Loss: 0.1839\n",
      "Epoch [3/5], Step [3018/10336], Loss: 0.1469\n",
      "Epoch [3/5], Step [3020/10336], Loss: 1.4967\n",
      "Epoch [3/5], Step [3022/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [3024/10336], Loss: 0.2837\n",
      "Epoch [3/5], Step [3026/10336], Loss: 3.6074\n",
      "Epoch [3/5], Step [3028/10336], Loss: 4.0329\n",
      "Epoch [3/5], Step [3030/10336], Loss: 0.7241\n",
      "Epoch [3/5], Step [3032/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [3034/10336], Loss: 0.0933\n",
      "Epoch [3/5], Step [3036/10336], Loss: 0.0037\n",
      "Epoch [3/5], Step [3038/10336], Loss: 0.1071\n",
      "Epoch [3/5], Step [3040/10336], Loss: 2.0516\n",
      "Epoch [3/5], Step [3042/10336], Loss: 0.0278\n",
      "Epoch [3/5], Step [3044/10336], Loss: 0.0214\n",
      "Epoch [3/5], Step [3046/10336], Loss: 0.0020\n",
      "Epoch [3/5], Step [3048/10336], Loss: 0.2480\n",
      "Epoch [3/5], Step [3050/10336], Loss: 0.7348\n",
      "Epoch [3/5], Step [3052/10336], Loss: 0.5288\n",
      "Epoch [3/5], Step [3054/10336], Loss: 1.3749\n",
      "Epoch [3/5], Step [3056/10336], Loss: 0.0163\n",
      "Epoch [3/5], Step [3058/10336], Loss: 3.8946\n",
      "Epoch [3/5], Step [3060/10336], Loss: 0.1232\n",
      "Epoch [3/5], Step [3062/10336], Loss: 1.6777\n",
      "Epoch [3/5], Step [3064/10336], Loss: 0.4645\n",
      "Epoch [3/5], Step [3066/10336], Loss: 0.1499\n",
      "Epoch [3/5], Step [3068/10336], Loss: 1.5246\n",
      "Epoch [3/5], Step [3070/10336], Loss: 0.6396\n",
      "Epoch [3/5], Step [3072/10336], Loss: 2.4629\n",
      "Epoch [3/5], Step [3074/10336], Loss: 0.1898\n",
      "Epoch [3/5], Step [3076/10336], Loss: 0.0029\n",
      "Epoch [3/5], Step [3078/10336], Loss: 0.6893\n",
      "Epoch [3/5], Step [3080/10336], Loss: 0.1416\n",
      "Epoch [3/5], Step [3082/10336], Loss: 4.3444\n",
      "Epoch [3/5], Step [3084/10336], Loss: 0.9689\n",
      "Epoch [3/5], Step [3086/10336], Loss: 1.1338\n",
      "Epoch [3/5], Step [3088/10336], Loss: 1.1101\n",
      "Epoch [3/5], Step [3090/10336], Loss: 1.8668\n",
      "Epoch [3/5], Step [3092/10336], Loss: 0.3028\n",
      "Epoch [3/5], Step [3094/10336], Loss: 0.2537\n",
      "Epoch [3/5], Step [3096/10336], Loss: 0.4950\n",
      "Epoch [3/5], Step [3098/10336], Loss: 0.1527\n",
      "Epoch [3/5], Step [3100/10336], Loss: 0.0073\n",
      "Epoch [3/5], Step [3102/10336], Loss: 2.7242\n",
      "Epoch [3/5], Step [3104/10336], Loss: 0.2673\n",
      "Epoch [3/5], Step [3106/10336], Loss: 0.0280\n",
      "Epoch [3/5], Step [3108/10336], Loss: 4.1048\n",
      "Epoch [3/5], Step [3110/10336], Loss: 0.3948\n",
      "Epoch [3/5], Step [3112/10336], Loss: 2.7914\n",
      "Epoch [3/5], Step [3114/10336], Loss: 0.1263\n",
      "Epoch [3/5], Step [3116/10336], Loss: 0.0552\n",
      "Epoch [3/5], Step [3118/10336], Loss: 0.4541\n",
      "Epoch [3/5], Step [3120/10336], Loss: 0.1954\n",
      "Epoch [3/5], Step [3122/10336], Loss: 3.8731\n",
      "Epoch [3/5], Step [3124/10336], Loss: 0.7880\n",
      "Epoch [3/5], Step [3126/10336], Loss: 0.2210\n",
      "Epoch [3/5], Step [3128/10336], Loss: 0.0123\n",
      "Epoch [3/5], Step [3130/10336], Loss: 1.3879\n",
      "Epoch [3/5], Step [3132/10336], Loss: 0.3242\n",
      "Epoch [3/5], Step [3134/10336], Loss: 0.0752\n",
      "Epoch [3/5], Step [3136/10336], Loss: 3.4781\n",
      "Epoch [3/5], Step [3138/10336], Loss: 0.4803\n",
      "Epoch [3/5], Step [3140/10336], Loss: 2.2139\n",
      "Epoch [3/5], Step [3142/10336], Loss: 0.4969\n",
      "Epoch [3/5], Step [3144/10336], Loss: 0.0370\n",
      "Epoch [3/5], Step [3146/10336], Loss: 2.3408\n",
      "Epoch [3/5], Step [3148/10336], Loss: 1.4192\n",
      "Epoch [3/5], Step [3150/10336], Loss: 0.1036\n",
      "Epoch [3/5], Step [3152/10336], Loss: 0.1123\n",
      "Epoch [3/5], Step [3154/10336], Loss: 5.1505\n",
      "Epoch [3/5], Step [3156/10336], Loss: 0.9021\n",
      "Epoch [3/5], Step [3158/10336], Loss: 0.1774\n",
      "Epoch [3/5], Step [3160/10336], Loss: 0.2897\n",
      "Epoch [3/5], Step [3162/10336], Loss: 1.0899\n",
      "Epoch [3/5], Step [3164/10336], Loss: 0.4160\n",
      "Epoch [3/5], Step [3166/10336], Loss: 0.2023\n",
      "Epoch [3/5], Step [3168/10336], Loss: 0.0711\n",
      "Epoch [3/5], Step [3170/10336], Loss: 0.0144\n",
      "Epoch [3/5], Step [3172/10336], Loss: 0.0089\n",
      "Epoch [3/5], Step [3174/10336], Loss: 0.6194\n",
      "Epoch [3/5], Step [3176/10336], Loss: 0.0274\n",
      "Epoch [3/5], Step [3178/10336], Loss: 1.4744\n",
      "Epoch [3/5], Step [3180/10336], Loss: 0.1548\n",
      "Epoch [3/5], Step [3182/10336], Loss: 0.8341\n",
      "Epoch [3/5], Step [3184/10336], Loss: 0.9142\n",
      "Epoch [3/5], Step [3186/10336], Loss: 2.0855\n",
      "Epoch [3/5], Step [3188/10336], Loss: 0.2129\n",
      "Epoch [3/5], Step [3190/10336], Loss: 0.4035\n",
      "Epoch [3/5], Step [3192/10336], Loss: 0.9346\n",
      "Epoch [3/5], Step [3194/10336], Loss: 0.2147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [3196/10336], Loss: 0.0817\n",
      "Epoch [3/5], Step [3198/10336], Loss: 0.0049\n",
      "Epoch [3/5], Step [3200/10336], Loss: 0.1766\n",
      "Epoch [3/5], Step [3202/10336], Loss: 0.1887\n",
      "Epoch [3/5], Step [3204/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [3206/10336], Loss: 1.6145\n",
      "Epoch [3/5], Step [3208/10336], Loss: 0.3281\n",
      "Epoch [3/5], Step [3210/10336], Loss: 0.0747\n",
      "Epoch [3/5], Step [3212/10336], Loss: 1.6631\n",
      "Epoch [3/5], Step [3214/10336], Loss: 1.4477\n",
      "Epoch [3/5], Step [3216/10336], Loss: 0.5373\n",
      "Epoch [3/5], Step [3218/10336], Loss: 0.0241\n",
      "Epoch [3/5], Step [3220/10336], Loss: 0.3687\n",
      "Epoch [3/5], Step [3222/10336], Loss: 0.3263\n",
      "Epoch [3/5], Step [3224/10336], Loss: 0.1380\n",
      "Epoch [3/5], Step [3226/10336], Loss: 1.6239\n",
      "Epoch [3/5], Step [3228/10336], Loss: 0.0014\n",
      "Epoch [3/5], Step [3230/10336], Loss: 0.6793\n",
      "Epoch [3/5], Step [3232/10336], Loss: 1.8694\n",
      "Epoch [3/5], Step [3234/10336], Loss: 1.6560\n",
      "Epoch [3/5], Step [3236/10336], Loss: 0.2238\n",
      "Epoch [3/5], Step [3238/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [3240/10336], Loss: 6.2801\n",
      "Epoch [3/5], Step [3242/10336], Loss: 0.4363\n",
      "Epoch [3/5], Step [3244/10336], Loss: 0.0574\n",
      "Epoch [3/5], Step [3246/10336], Loss: 0.0158\n",
      "Epoch [3/5], Step [3248/10336], Loss: 0.3828\n",
      "Epoch [3/5], Step [3250/10336], Loss: 2.7769\n",
      "Epoch [3/5], Step [3252/10336], Loss: 0.2091\n",
      "Epoch [3/5], Step [3254/10336], Loss: 2.3731\n",
      "Epoch [3/5], Step [3256/10336], Loss: 1.6119\n",
      "Epoch [3/5], Step [3258/10336], Loss: 0.2709\n",
      "Epoch [3/5], Step [3260/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [3262/10336], Loss: 2.4033\n",
      "Epoch [3/5], Step [3264/10336], Loss: 0.2305\n",
      "Epoch [3/5], Step [3266/10336], Loss: 0.0972\n",
      "Epoch [3/5], Step [3268/10336], Loss: 1.5723\n",
      "Epoch [3/5], Step [3270/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [3272/10336], Loss: 0.0640\n",
      "Epoch [3/5], Step [3274/10336], Loss: 2.5798\n",
      "Epoch [3/5], Step [3276/10336], Loss: 0.8862\n",
      "Epoch [3/5], Step [3278/10336], Loss: 0.3913\n",
      "Epoch [3/5], Step [3280/10336], Loss: 1.0563\n",
      "Epoch [3/5], Step [3282/10336], Loss: 0.5192\n",
      "Epoch [3/5], Step [3284/10336], Loss: 0.0845\n",
      "Epoch [3/5], Step [3286/10336], Loss: 0.0739\n",
      "Epoch [3/5], Step [3288/10336], Loss: 0.2240\n",
      "Epoch [3/5], Step [3290/10336], Loss: 0.6753\n",
      "Epoch [3/5], Step [3292/10336], Loss: 0.0166\n",
      "Epoch [3/5], Step [3294/10336], Loss: 2.0160\n",
      "Epoch [3/5], Step [3296/10336], Loss: 0.0092\n",
      "Epoch [3/5], Step [3298/10336], Loss: 0.0063\n",
      "Epoch [3/5], Step [3300/10336], Loss: 1.4281\n",
      "Epoch [3/5], Step [3302/10336], Loss: 1.0106\n",
      "Epoch [3/5], Step [3304/10336], Loss: 0.5306\n",
      "Epoch [3/5], Step [3306/10336], Loss: 0.0086\n",
      "Epoch [3/5], Step [3308/10336], Loss: 2.3664\n",
      "Epoch [3/5], Step [3310/10336], Loss: 0.2462\n",
      "Epoch [3/5], Step [3312/10336], Loss: 1.0089\n",
      "Epoch [3/5], Step [3314/10336], Loss: 0.2242\n",
      "Epoch [3/5], Step [3316/10336], Loss: 0.2756\n",
      "Epoch [3/5], Step [3318/10336], Loss: 0.6448\n",
      "Epoch [3/5], Step [3320/10336], Loss: 0.0247\n",
      "Epoch [3/5], Step [3322/10336], Loss: 0.3642\n",
      "Epoch [3/5], Step [3324/10336], Loss: 0.1955\n",
      "Epoch [3/5], Step [3326/10336], Loss: 0.0036\n",
      "Epoch [3/5], Step [3328/10336], Loss: 0.1175\n",
      "Epoch [3/5], Step [3330/10336], Loss: 3.3191\n",
      "Epoch [3/5], Step [3332/10336], Loss: 0.3282\n",
      "Epoch [3/5], Step [3334/10336], Loss: 0.4184\n",
      "Epoch [3/5], Step [3336/10336], Loss: 0.1872\n",
      "Epoch [3/5], Step [3338/10336], Loss: 0.1802\n",
      "Epoch [3/5], Step [3340/10336], Loss: 0.6904\n",
      "Epoch [3/5], Step [3342/10336], Loss: 2.8830\n",
      "Epoch [3/5], Step [3344/10336], Loss: 0.2232\n",
      "Epoch [3/5], Step [3346/10336], Loss: 0.1578\n",
      "Epoch [3/5], Step [3348/10336], Loss: 2.4706\n",
      "Epoch [3/5], Step [3350/10336], Loss: 0.7502\n",
      "Epoch [3/5], Step [3352/10336], Loss: 0.6377\n",
      "Epoch [3/5], Step [3354/10336], Loss: 0.1001\n",
      "Epoch [3/5], Step [3356/10336], Loss: 2.8204\n",
      "Epoch [3/5], Step [3358/10336], Loss: 4.7944\n",
      "Epoch [3/5], Step [3360/10336], Loss: 0.2901\n",
      "Epoch [3/5], Step [3362/10336], Loss: 0.1944\n",
      "Epoch [3/5], Step [3364/10336], Loss: 0.3845\n",
      "Epoch [3/5], Step [3366/10336], Loss: 0.2789\n",
      "Epoch [3/5], Step [3368/10336], Loss: 1.2935\n",
      "Epoch [3/5], Step [3370/10336], Loss: 2.6971\n",
      "Epoch [3/5], Step [3372/10336], Loss: 0.6499\n",
      "Epoch [3/5], Step [3374/10336], Loss: 2.7813\n",
      "Epoch [3/5], Step [3376/10336], Loss: 0.1729\n",
      "Epoch [3/5], Step [3378/10336], Loss: 0.0817\n",
      "Epoch [3/5], Step [3380/10336], Loss: 1.0269\n",
      "Epoch [3/5], Step [3382/10336], Loss: 0.3352\n",
      "Epoch [3/5], Step [3384/10336], Loss: 3.0716\n",
      "Epoch [3/5], Step [3386/10336], Loss: 0.4099\n",
      "Epoch [3/5], Step [3388/10336], Loss: 0.0042\n",
      "Epoch [3/5], Step [3390/10336], Loss: 0.6078\n",
      "Epoch [3/5], Step [3392/10336], Loss: 1.4405\n",
      "Epoch [3/5], Step [3394/10336], Loss: 3.6131\n",
      "Epoch [3/5], Step [3396/10336], Loss: 0.0494\n",
      "Epoch [3/5], Step [3398/10336], Loss: 0.2175\n",
      "Epoch [3/5], Step [3400/10336], Loss: 1.5896\n",
      "Epoch [3/5], Step [3402/10336], Loss: 0.1827\n",
      "Epoch [3/5], Step [3404/10336], Loss: 0.1543\n",
      "Epoch [3/5], Step [3406/10336], Loss: 0.0055\n",
      "Epoch [3/5], Step [3408/10336], Loss: 4.6227\n",
      "Epoch [3/5], Step [3410/10336], Loss: 0.6751\n",
      "Epoch [3/5], Step [3412/10336], Loss: 0.2268\n",
      "Epoch [3/5], Step [3414/10336], Loss: 0.5274\n",
      "Epoch [3/5], Step [3416/10336], Loss: 2.0790\n",
      "Epoch [3/5], Step [3418/10336], Loss: 2.0040\n",
      "Epoch [3/5], Step [3420/10336], Loss: 0.2632\n",
      "Epoch [3/5], Step [3422/10336], Loss: 0.1474\n",
      "Epoch [3/5], Step [3424/10336], Loss: 0.6404\n",
      "Epoch [3/5], Step [3426/10336], Loss: 0.2003\n",
      "Epoch [3/5], Step [3428/10336], Loss: 0.4803\n",
      "Epoch [3/5], Step [3430/10336], Loss: 0.2338\n",
      "Epoch [3/5], Step [3432/10336], Loss: 1.0180\n",
      "Epoch [3/5], Step [3434/10336], Loss: 0.7850\n",
      "Epoch [3/5], Step [3436/10336], Loss: 0.2011\n",
      "Epoch [3/5], Step [3438/10336], Loss: 0.7588\n",
      "Epoch [3/5], Step [3440/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [3442/10336], Loss: 2.8494\n",
      "Epoch [3/5], Step [3444/10336], Loss: 0.7766\n",
      "Epoch [3/5], Step [3446/10336], Loss: 0.1011\n",
      "Epoch [3/5], Step [3448/10336], Loss: 0.3409\n",
      "Epoch [3/5], Step [3450/10336], Loss: 0.1598\n",
      "Epoch [3/5], Step [3452/10336], Loss: 0.3160\n",
      "Epoch [3/5], Step [3454/10336], Loss: 0.0109\n",
      "Epoch [3/5], Step [3456/10336], Loss: 0.1249\n",
      "Epoch [3/5], Step [3458/10336], Loss: 0.8154\n",
      "Epoch [3/5], Step [3460/10336], Loss: 0.0023\n",
      "Epoch [3/5], Step [3462/10336], Loss: 0.0965\n",
      "Epoch [3/5], Step [3464/10336], Loss: 3.5362\n",
      "Epoch [3/5], Step [3466/10336], Loss: 0.1515\n",
      "Epoch [3/5], Step [3468/10336], Loss: 0.0397\n",
      "Epoch [3/5], Step [3470/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [3472/10336], Loss: 1.5707\n",
      "Epoch [3/5], Step [3474/10336], Loss: 0.5943\n",
      "Epoch [3/5], Step [3476/10336], Loss: 0.1033\n",
      "Epoch [3/5], Step [3478/10336], Loss: 2.2133\n",
      "Epoch [3/5], Step [3480/10336], Loss: 0.3589\n",
      "Epoch [3/5], Step [3482/10336], Loss: 0.2325\n",
      "Epoch [3/5], Step [3484/10336], Loss: 0.2179\n",
      "Epoch [3/5], Step [3486/10336], Loss: 0.6249\n",
      "Epoch [3/5], Step [3488/10336], Loss: 0.0429\n",
      "Epoch [3/5], Step [3490/10336], Loss: 0.3210\n",
      "Epoch [3/5], Step [3492/10336], Loss: 0.2383\n",
      "Epoch [3/5], Step [3494/10336], Loss: 0.7198\n",
      "Epoch [3/5], Step [3496/10336], Loss: 0.6376\n",
      "Epoch [3/5], Step [3498/10336], Loss: 0.0297\n",
      "Epoch [3/5], Step [3500/10336], Loss: 0.1326\n",
      "Epoch [3/5], Step [3502/10336], Loss: 0.0901\n",
      "Epoch [3/5], Step [3504/10336], Loss: 0.6647\n",
      "Epoch [3/5], Step [3506/10336], Loss: 0.0221\n",
      "Epoch [3/5], Step [3508/10336], Loss: 0.1153\n",
      "Epoch [3/5], Step [3510/10336], Loss: 0.0473\n",
      "Epoch [3/5], Step [3512/10336], Loss: 0.0231\n",
      "Epoch [3/5], Step [3514/10336], Loss: 0.6810\n",
      "Epoch [3/5], Step [3516/10336], Loss: 2.9047\n",
      "Epoch [3/5], Step [3518/10336], Loss: 0.0341\n",
      "Epoch [3/5], Step [3520/10336], Loss: 0.0636\n",
      "Epoch [3/5], Step [3522/10336], Loss: 0.1342\n",
      "Epoch [3/5], Step [3524/10336], Loss: 3.1979\n",
      "Epoch [3/5], Step [3526/10336], Loss: 0.1762\n",
      "Epoch [3/5], Step [3528/10336], Loss: 0.0799\n",
      "Epoch [3/5], Step [3530/10336], Loss: 0.2705\n",
      "Epoch [3/5], Step [3532/10336], Loss: 0.4518\n",
      "Epoch [3/5], Step [3534/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [3536/10336], Loss: 1.7510\n",
      "Epoch [3/5], Step [3538/10336], Loss: 0.4116\n",
      "Epoch [3/5], Step [3540/10336], Loss: 4.2191\n",
      "Epoch [3/5], Step [3542/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [3544/10336], Loss: 3.3077\n",
      "Epoch [3/5], Step [3546/10336], Loss: 0.1307\n",
      "Epoch [3/5], Step [3548/10336], Loss: 0.1398\n",
      "Epoch [3/5], Step [3550/10336], Loss: 0.2367\n",
      "Epoch [3/5], Step [3552/10336], Loss: 0.8230\n",
      "Epoch [3/5], Step [3554/10336], Loss: 1.5547\n",
      "Epoch [3/5], Step [3556/10336], Loss: 0.1665\n",
      "Epoch [3/5], Step [3558/10336], Loss: 2.3081\n",
      "Epoch [3/5], Step [3560/10336], Loss: 0.2240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [3562/10336], Loss: 0.2269\n",
      "Epoch [3/5], Step [3564/10336], Loss: 1.8601\n",
      "Epoch [3/5], Step [3566/10336], Loss: 0.1713\n",
      "Epoch [3/5], Step [3568/10336], Loss: 0.0766\n",
      "Epoch [3/5], Step [3570/10336], Loss: 0.0268\n",
      "Epoch [3/5], Step [3572/10336], Loss: 0.0350\n",
      "Epoch [3/5], Step [3574/10336], Loss: 0.0364\n",
      "Epoch [3/5], Step [3576/10336], Loss: 0.3911\n",
      "Epoch [3/5], Step [3578/10336], Loss: 1.9053\n",
      "Epoch [3/5], Step [3580/10336], Loss: 0.7970\n",
      "Epoch [3/5], Step [3582/10336], Loss: 0.0175\n",
      "Epoch [3/5], Step [3584/10336], Loss: 0.0791\n",
      "Epoch [3/5], Step [3586/10336], Loss: 0.1029\n",
      "Epoch [3/5], Step [3588/10336], Loss: 1.9227\n",
      "Epoch [3/5], Step [3590/10336], Loss: 0.3227\n",
      "Epoch [3/5], Step [3592/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [3594/10336], Loss: 0.1948\n",
      "Epoch [3/5], Step [3596/10336], Loss: 0.1283\n",
      "Epoch [3/5], Step [3598/10336], Loss: 0.2338\n",
      "Epoch [3/5], Step [3600/10336], Loss: 0.0670\n",
      "Epoch [3/5], Step [3602/10336], Loss: 0.6731\n",
      "Epoch [3/5], Step [3604/10336], Loss: 3.1144\n",
      "Epoch [3/5], Step [3606/10336], Loss: 0.3516\n",
      "Epoch [3/5], Step [3608/10336], Loss: 0.0897\n",
      "Epoch [3/5], Step [3610/10336], Loss: 0.0808\n",
      "Epoch [3/5], Step [3612/10336], Loss: 2.3260\n",
      "Epoch [3/5], Step [3614/10336], Loss: 0.8284\n",
      "Epoch [3/5], Step [3616/10336], Loss: 0.2063\n",
      "Epoch [3/5], Step [3618/10336], Loss: 0.1035\n",
      "Epoch [3/5], Step [3620/10336], Loss: 0.0076\n",
      "Epoch [3/5], Step [3622/10336], Loss: 1.9010\n",
      "Epoch [3/5], Step [3624/10336], Loss: 0.4827\n",
      "Epoch [3/5], Step [3626/10336], Loss: 0.5264\n",
      "Epoch [3/5], Step [3628/10336], Loss: 0.0431\n",
      "Epoch [3/5], Step [3630/10336], Loss: 0.7878\n",
      "Epoch [3/5], Step [3632/10336], Loss: 0.7285\n",
      "Epoch [3/5], Step [3634/10336], Loss: 2.6211\n",
      "Epoch [3/5], Step [3636/10336], Loss: 0.3893\n",
      "Epoch [3/5], Step [3638/10336], Loss: 2.4827\n",
      "Epoch [3/5], Step [3640/10336], Loss: 0.3456\n",
      "Epoch [3/5], Step [3642/10336], Loss: 0.1575\n",
      "Epoch [3/5], Step [3644/10336], Loss: 0.2400\n",
      "Epoch [3/5], Step [3646/10336], Loss: 0.0862\n",
      "Epoch [3/5], Step [3648/10336], Loss: 3.3444\n",
      "Epoch [3/5], Step [3650/10336], Loss: 0.6263\n",
      "Epoch [3/5], Step [3652/10336], Loss: 0.2261\n",
      "Epoch [3/5], Step [3654/10336], Loss: 0.1815\n",
      "Epoch [3/5], Step [3656/10336], Loss: 0.1000\n",
      "Epoch [3/5], Step [3658/10336], Loss: 0.5039\n",
      "Epoch [3/5], Step [3660/10336], Loss: 0.0213\n",
      "Epoch [3/5], Step [3662/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [3664/10336], Loss: 0.1725\n",
      "Epoch [3/5], Step [3666/10336], Loss: 0.0031\n",
      "Epoch [3/5], Step [3668/10336], Loss: 0.4709\n",
      "Epoch [3/5], Step [3670/10336], Loss: 0.2173\n",
      "Epoch [3/5], Step [3672/10336], Loss: 0.0974\n",
      "Epoch [3/5], Step [3674/10336], Loss: 0.1614\n",
      "Epoch [3/5], Step [3676/10336], Loss: 0.0058\n",
      "Epoch [3/5], Step [3678/10336], Loss: 0.2340\n",
      "Epoch [3/5], Step [3680/10336], Loss: 0.4439\n",
      "Epoch [3/5], Step [3682/10336], Loss: 0.0444\n",
      "Epoch [3/5], Step [3684/10336], Loss: 0.5857\n",
      "Epoch [3/5], Step [3686/10336], Loss: 0.1557\n",
      "Epoch [3/5], Step [3688/10336], Loss: 2.5117\n",
      "Epoch [3/5], Step [3690/10336], Loss: 1.3313\n",
      "Epoch [3/5], Step [3692/10336], Loss: 0.3466\n",
      "Epoch [3/5], Step [3694/10336], Loss: 0.0014\n",
      "Epoch [3/5], Step [3696/10336], Loss: 0.2521\n",
      "Epoch [3/5], Step [3698/10336], Loss: 3.6600\n",
      "Epoch [3/5], Step [3700/10336], Loss: 1.1028\n",
      "Epoch [3/5], Step [3702/10336], Loss: 0.6425\n",
      "Epoch [3/5], Step [3704/10336], Loss: 0.2240\n",
      "Epoch [3/5], Step [3706/10336], Loss: 2.3957\n",
      "Epoch [3/5], Step [3708/10336], Loss: 0.4683\n",
      "Epoch [3/5], Step [3710/10336], Loss: 1.2472\n",
      "Epoch [3/5], Step [3712/10336], Loss: 0.2376\n",
      "Epoch [3/5], Step [3714/10336], Loss: 1.3854\n",
      "Epoch [3/5], Step [3716/10336], Loss: 2.6094\n",
      "Epoch [3/5], Step [3718/10336], Loss: 0.6245\n",
      "Epoch [3/5], Step [3720/10336], Loss: 0.7776\n",
      "Epoch [3/5], Step [3722/10336], Loss: 0.0562\n",
      "Epoch [3/5], Step [3724/10336], Loss: 0.5348\n",
      "Epoch [3/5], Step [3726/10336], Loss: 5.8664\n",
      "Epoch [3/5], Step [3728/10336], Loss: 0.2167\n",
      "Epoch [3/5], Step [3730/10336], Loss: 0.4688\n",
      "Epoch [3/5], Step [3732/10336], Loss: 0.1451\n",
      "Epoch [3/5], Step [3734/10336], Loss: 0.5173\n",
      "Epoch [3/5], Step [3736/10336], Loss: 0.0024\n",
      "Epoch [3/5], Step [3738/10336], Loss: 1.9700\n",
      "Epoch [3/5], Step [3740/10336], Loss: 0.2126\n",
      "Epoch [3/5], Step [3742/10336], Loss: 0.3025\n",
      "Epoch [3/5], Step [3744/10336], Loss: 0.3022\n",
      "Epoch [3/5], Step [3746/10336], Loss: 0.3131\n",
      "Epoch [3/5], Step [3748/10336], Loss: 0.2621\n",
      "Epoch [3/5], Step [3750/10336], Loss: 0.0595\n",
      "Epoch [3/5], Step [3752/10336], Loss: 0.0012\n",
      "Epoch [3/5], Step [3754/10336], Loss: 0.0138\n",
      "Epoch [3/5], Step [3756/10336], Loss: 0.0380\n",
      "Epoch [3/5], Step [3758/10336], Loss: 0.0661\n",
      "Epoch [3/5], Step [3760/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [3762/10336], Loss: 0.0695\n",
      "Epoch [3/5], Step [3764/10336], Loss: 4.5181\n",
      "Epoch [3/5], Step [3766/10336], Loss: 0.1893\n",
      "Epoch [3/5], Step [3768/10336], Loss: 0.2252\n",
      "Epoch [3/5], Step [3770/10336], Loss: 0.1763\n",
      "Epoch [3/5], Step [3772/10336], Loss: 0.0065\n",
      "Epoch [3/5], Step [3774/10336], Loss: 0.0037\n",
      "Epoch [3/5], Step [3776/10336], Loss: 0.7945\n",
      "Epoch [3/5], Step [3778/10336], Loss: 3.6266\n",
      "Epoch [3/5], Step [3780/10336], Loss: 0.2333\n",
      "Epoch [3/5], Step [3782/10336], Loss: 0.1882\n",
      "Epoch [3/5], Step [3784/10336], Loss: 0.1106\n",
      "Epoch [3/5], Step [3786/10336], Loss: 2.7948\n",
      "Epoch [3/5], Step [3788/10336], Loss: 1.6728\n",
      "Epoch [3/5], Step [3790/10336], Loss: 1.1293\n",
      "Epoch [3/5], Step [3792/10336], Loss: 3.9136\n",
      "Epoch [3/5], Step [3794/10336], Loss: 0.3338\n",
      "Epoch [3/5], Step [3796/10336], Loss: 0.2412\n",
      "Epoch [3/5], Step [3798/10336], Loss: 0.1157\n",
      "Epoch [3/5], Step [3800/10336], Loss: 0.1394\n",
      "Epoch [3/5], Step [3802/10336], Loss: 2.4509\n",
      "Epoch [3/5], Step [3804/10336], Loss: 0.8578\n",
      "Epoch [3/5], Step [3806/10336], Loss: 0.0478\n",
      "Epoch [3/5], Step [3808/10336], Loss: 1.7119\n",
      "Epoch [3/5], Step [3810/10336], Loss: 0.4915\n",
      "Epoch [3/5], Step [3812/10336], Loss: 0.2834\n",
      "Epoch [3/5], Step [3814/10336], Loss: 0.6924\n",
      "Epoch [3/5], Step [3816/10336], Loss: 4.4879\n",
      "Epoch [3/5], Step [3818/10336], Loss: 1.4883\n",
      "Epoch [3/5], Step [3820/10336], Loss: 0.4724\n",
      "Epoch [3/5], Step [3822/10336], Loss: 0.3800\n",
      "Epoch [3/5], Step [3824/10336], Loss: 0.0399\n",
      "Epoch [3/5], Step [3826/10336], Loss: 0.0868\n",
      "Epoch [3/5], Step [3828/10336], Loss: 0.0481\n",
      "Epoch [3/5], Step [3830/10336], Loss: 0.2659\n",
      "Epoch [3/5], Step [3832/10336], Loss: 0.0393\n",
      "Epoch [3/5], Step [3834/10336], Loss: 0.0156\n",
      "Epoch [3/5], Step [3836/10336], Loss: 0.8094\n",
      "Epoch [3/5], Step [3838/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [3840/10336], Loss: 0.2587\n",
      "Epoch [3/5], Step [3842/10336], Loss: 0.4514\n",
      "Epoch [3/5], Step [3844/10336], Loss: 0.0466\n",
      "Epoch [3/5], Step [3846/10336], Loss: 0.1492\n",
      "Epoch [3/5], Step [3848/10336], Loss: 1.2078\n",
      "Epoch [3/5], Step [3850/10336], Loss: 0.1569\n",
      "Epoch [3/5], Step [3852/10336], Loss: 1.1533\n",
      "Epoch [3/5], Step [3854/10336], Loss: 0.0162\n",
      "Epoch [3/5], Step [3856/10336], Loss: 3.3982\n",
      "Epoch [3/5], Step [3858/10336], Loss: 0.1344\n",
      "Epoch [3/5], Step [3860/10336], Loss: 0.3359\n",
      "Epoch [3/5], Step [3862/10336], Loss: 0.1686\n",
      "Epoch [3/5], Step [3864/10336], Loss: 0.0001\n",
      "Epoch [3/5], Step [3866/10336], Loss: 0.0024\n",
      "Epoch [3/5], Step [3868/10336], Loss: 0.4504\n",
      "Epoch [3/5], Step [3870/10336], Loss: 0.1893\n",
      "Epoch [3/5], Step [3872/10336], Loss: 1.9364\n",
      "Epoch [3/5], Step [3874/10336], Loss: 0.1487\n",
      "Epoch [3/5], Step [3876/10336], Loss: 0.6090\n",
      "Epoch [3/5], Step [3878/10336], Loss: 1.1992\n",
      "Epoch [3/5], Step [3880/10336], Loss: 0.0873\n",
      "Epoch [3/5], Step [3882/10336], Loss: 0.1204\n",
      "Epoch [3/5], Step [3884/10336], Loss: 0.0029\n",
      "Epoch [3/5], Step [3886/10336], Loss: 0.0054\n",
      "Epoch [3/5], Step [3888/10336], Loss: 0.2208\n",
      "Epoch [3/5], Step [3890/10336], Loss: 0.3308\n",
      "Epoch [3/5], Step [3892/10336], Loss: 0.0969\n",
      "Epoch [3/5], Step [3894/10336], Loss: 0.1439\n",
      "Epoch [3/5], Step [3896/10336], Loss: 0.3697\n",
      "Epoch [3/5], Step [3898/10336], Loss: 0.0896\n",
      "Epoch [3/5], Step [3900/10336], Loss: 0.1234\n",
      "Epoch [3/5], Step [3902/10336], Loss: 0.3951\n",
      "Epoch [3/5], Step [3904/10336], Loss: 0.0153\n",
      "Epoch [3/5], Step [3906/10336], Loss: 0.0206\n",
      "Epoch [3/5], Step [3908/10336], Loss: 0.0298\n",
      "Epoch [3/5], Step [3910/10336], Loss: 0.0960\n",
      "Epoch [3/5], Step [3912/10336], Loss: 0.2462\n",
      "Epoch [3/5], Step [3914/10336], Loss: 0.1954\n",
      "Epoch [3/5], Step [3916/10336], Loss: 0.6686\n",
      "Epoch [3/5], Step [3918/10336], Loss: 0.0263\n",
      "Epoch [3/5], Step [3920/10336], Loss: 1.8178\n",
      "Epoch [3/5], Step [3922/10336], Loss: 0.1226\n",
      "Epoch [3/5], Step [3924/10336], Loss: 0.0401\n",
      "Epoch [3/5], Step [3926/10336], Loss: 0.0425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [3928/10336], Loss: 0.1032\n",
      "Epoch [3/5], Step [3930/10336], Loss: 0.2605\n",
      "Epoch [3/5], Step [3932/10336], Loss: 1.0061\n",
      "Epoch [3/5], Step [3934/10336], Loss: 0.6741\n",
      "Epoch [3/5], Step [3936/10336], Loss: 0.3401\n",
      "Epoch [3/5], Step [3938/10336], Loss: 0.0555\n",
      "Epoch [3/5], Step [3940/10336], Loss: 0.4016\n",
      "Epoch [3/5], Step [3942/10336], Loss: 0.0266\n",
      "Epoch [3/5], Step [3944/10336], Loss: 0.0849\n",
      "Epoch [3/5], Step [3946/10336], Loss: 0.0130\n",
      "Epoch [3/5], Step [3948/10336], Loss: 3.4995\n",
      "Epoch [3/5], Step [3950/10336], Loss: 0.4151\n",
      "Epoch [3/5], Step [3952/10336], Loss: 0.2736\n",
      "Epoch [3/5], Step [3954/10336], Loss: 0.6356\n",
      "Epoch [3/5], Step [3956/10336], Loss: 0.0245\n",
      "Epoch [3/5], Step [3958/10336], Loss: 0.6173\n",
      "Epoch [3/5], Step [3960/10336], Loss: 0.1430\n",
      "Epoch [3/5], Step [3962/10336], Loss: 0.0083\n",
      "Epoch [3/5], Step [3964/10336], Loss: 1.4366\n",
      "Epoch [3/5], Step [3966/10336], Loss: 0.2982\n",
      "Epoch [3/5], Step [3968/10336], Loss: 0.1639\n",
      "Epoch [3/5], Step [3970/10336], Loss: 1.7654\n",
      "Epoch [3/5], Step [3972/10336], Loss: 0.6001\n",
      "Epoch [3/5], Step [3974/10336], Loss: 0.0563\n",
      "Epoch [3/5], Step [3976/10336], Loss: 0.6868\n",
      "Epoch [3/5], Step [3978/10336], Loss: 1.5283\n",
      "Epoch [3/5], Step [3980/10336], Loss: 0.0039\n",
      "Epoch [3/5], Step [3982/10336], Loss: 0.2332\n",
      "Epoch [3/5], Step [3984/10336], Loss: 0.2607\n",
      "Epoch [3/5], Step [3986/10336], Loss: 0.1173\n",
      "Epoch [3/5], Step [3988/10336], Loss: 3.8317\n",
      "Epoch [3/5], Step [3990/10336], Loss: 0.3150\n",
      "Epoch [3/5], Step [3992/10336], Loss: 2.3695\n",
      "Epoch [3/5], Step [3994/10336], Loss: 1.2428\n",
      "Epoch [3/5], Step [3996/10336], Loss: 0.1522\n",
      "Epoch [3/5], Step [3998/10336], Loss: 1.0970\n",
      "Epoch [3/5], Step [4000/10336], Loss: 0.1812\n",
      "Epoch [3/5], Step [4002/10336], Loss: 0.4882\n",
      "Epoch [3/5], Step [4004/10336], Loss: 0.2033\n",
      "Epoch [3/5], Step [4006/10336], Loss: 0.0034\n",
      "Epoch [3/5], Step [4008/10336], Loss: 0.6179\n",
      "Epoch [3/5], Step [4010/10336], Loss: 0.7478\n",
      "Epoch [3/5], Step [4012/10336], Loss: 0.4710\n",
      "Epoch [3/5], Step [4014/10336], Loss: 0.1355\n",
      "Epoch [3/5], Step [4016/10336], Loss: 3.5169\n",
      "Epoch [3/5], Step [4018/10336], Loss: 0.2799\n",
      "Epoch [3/5], Step [4020/10336], Loss: 0.0037\n",
      "Epoch [3/5], Step [4022/10336], Loss: 0.2198\n",
      "Epoch [3/5], Step [4024/10336], Loss: 0.0142\n",
      "Epoch [3/5], Step [4026/10336], Loss: 1.0921\n",
      "Epoch [3/5], Step [4028/10336], Loss: 0.6788\n",
      "Epoch [3/5], Step [4030/10336], Loss: 0.5205\n",
      "Epoch [3/5], Step [4032/10336], Loss: 0.9905\n",
      "Epoch [3/5], Step [4034/10336], Loss: 0.5949\n",
      "Epoch [3/5], Step [4036/10336], Loss: 0.4696\n",
      "Epoch [3/5], Step [4038/10336], Loss: 0.1992\n",
      "Epoch [3/5], Step [4040/10336], Loss: 0.1482\n",
      "Epoch [3/5], Step [4042/10336], Loss: 0.5260\n",
      "Epoch [3/5], Step [4044/10336], Loss: 0.2535\n",
      "Epoch [3/5], Step [4046/10336], Loss: 3.2718\n",
      "Epoch [3/5], Step [4048/10336], Loss: 0.3322\n",
      "Epoch [3/5], Step [4050/10336], Loss: 0.3510\n",
      "Epoch [3/5], Step [4052/10336], Loss: 1.2331\n",
      "Epoch [3/5], Step [4054/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [4056/10336], Loss: 0.9383\n",
      "Epoch [3/5], Step [4058/10336], Loss: 0.3292\n",
      "Epoch [3/5], Step [4060/10336], Loss: 0.2110\n",
      "Epoch [3/5], Step [4062/10336], Loss: 0.0296\n",
      "Epoch [3/5], Step [4064/10336], Loss: 1.5807\n",
      "Epoch [3/5], Step [4066/10336], Loss: 0.1597\n",
      "Epoch [3/5], Step [4068/10336], Loss: 0.3210\n",
      "Epoch [3/5], Step [4070/10336], Loss: 0.2375\n",
      "Epoch [3/5], Step [4072/10336], Loss: 0.6231\n",
      "Epoch [3/5], Step [4074/10336], Loss: 0.1679\n",
      "Epoch [3/5], Step [4076/10336], Loss: 2.1160\n",
      "Epoch [3/5], Step [4078/10336], Loss: 0.0287\n",
      "Epoch [3/5], Step [4080/10336], Loss: 0.1013\n",
      "Epoch [3/5], Step [4082/10336], Loss: 0.4351\n",
      "Epoch [3/5], Step [4084/10336], Loss: 7.5486\n",
      "Epoch [3/5], Step [4086/10336], Loss: 0.2708\n",
      "Epoch [3/5], Step [4088/10336], Loss: 0.1961\n",
      "Epoch [3/5], Step [4090/10336], Loss: 0.0366\n",
      "Epoch [3/5], Step [4092/10336], Loss: 0.1055\n",
      "Epoch [3/5], Step [4094/10336], Loss: 0.0457\n",
      "Epoch [3/5], Step [4096/10336], Loss: 1.0073\n",
      "Epoch [3/5], Step [4098/10336], Loss: 3.3945\n",
      "Epoch [3/5], Step [4100/10336], Loss: 0.1601\n",
      "Epoch [3/5], Step [4102/10336], Loss: 0.5698\n",
      "Epoch [3/5], Step [4104/10336], Loss: 0.2275\n",
      "Epoch [3/5], Step [4106/10336], Loss: 0.0364\n",
      "Epoch [3/5], Step [4108/10336], Loss: 0.3756\n",
      "Epoch [3/5], Step [4110/10336], Loss: 0.0232\n",
      "Epoch [3/5], Step [4112/10336], Loss: 0.0347\n",
      "Epoch [3/5], Step [4114/10336], Loss: 1.9926\n",
      "Epoch [3/5], Step [4116/10336], Loss: 0.5623\n",
      "Epoch [3/5], Step [4118/10336], Loss: 1.9374\n",
      "Epoch [3/5], Step [4120/10336], Loss: 0.3340\n",
      "Epoch [3/5], Step [4122/10336], Loss: 0.0419\n",
      "Epoch [3/5], Step [4124/10336], Loss: 0.3346\n",
      "Epoch [3/5], Step [4126/10336], Loss: 0.0242\n",
      "Epoch [3/5], Step [4128/10336], Loss: 0.2818\n",
      "Epoch [3/5], Step [4130/10336], Loss: 0.1551\n",
      "Epoch [3/5], Step [4132/10336], Loss: 0.2064\n",
      "Epoch [3/5], Step [4134/10336], Loss: 2.7944\n",
      "Epoch [3/5], Step [4136/10336], Loss: 0.9338\n",
      "Epoch [3/5], Step [4138/10336], Loss: 0.9037\n",
      "Epoch [3/5], Step [4140/10336], Loss: 0.1017\n",
      "Epoch [3/5], Step [4142/10336], Loss: 0.7177\n",
      "Epoch [3/5], Step [4144/10336], Loss: 0.7426\n",
      "Epoch [3/5], Step [4146/10336], Loss: 2.4336\n",
      "Epoch [3/5], Step [4148/10336], Loss: 0.0941\n",
      "Epoch [3/5], Step [4150/10336], Loss: 0.1204\n",
      "Epoch [3/5], Step [4152/10336], Loss: 0.4046\n",
      "Epoch [3/5], Step [4154/10336], Loss: 1.1533\n",
      "Epoch [3/5], Step [4156/10336], Loss: 0.2325\n",
      "Epoch [3/5], Step [4158/10336], Loss: 0.5230\n",
      "Epoch [3/5], Step [4160/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [4162/10336], Loss: 0.9101\n",
      "Epoch [3/5], Step [4164/10336], Loss: 2.1515\n",
      "Epoch [3/5], Step [4166/10336], Loss: 1.1420\n",
      "Epoch [3/5], Step [4168/10336], Loss: 0.2835\n",
      "Epoch [3/5], Step [4170/10336], Loss: 0.0303\n",
      "Epoch [3/5], Step [4172/10336], Loss: 0.4344\n",
      "Epoch [3/5], Step [4174/10336], Loss: 2.7466\n",
      "Epoch [3/5], Step [4176/10336], Loss: 0.2064\n",
      "Epoch [3/5], Step [4178/10336], Loss: 0.0892\n",
      "Epoch [3/5], Step [4180/10336], Loss: 0.4666\n",
      "Epoch [3/5], Step [4182/10336], Loss: 1.3615\n",
      "Epoch [3/5], Step [4184/10336], Loss: 2.5042\n",
      "Epoch [3/5], Step [4186/10336], Loss: 0.2293\n",
      "Epoch [3/5], Step [4188/10336], Loss: 1.1006\n",
      "Epoch [3/5], Step [4190/10336], Loss: 0.0254\n",
      "Epoch [3/5], Step [4192/10336], Loss: 0.0135\n",
      "Epoch [3/5], Step [4194/10336], Loss: 3.8012\n",
      "Epoch [3/5], Step [4196/10336], Loss: 1.2429\n",
      "Epoch [3/5], Step [4198/10336], Loss: 0.0808\n",
      "Epoch [3/5], Step [4200/10336], Loss: 0.2212\n",
      "Epoch [3/5], Step [4202/10336], Loss: 0.1481\n",
      "Epoch [3/5], Step [4204/10336], Loss: 0.3335\n",
      "Epoch [3/5], Step [4206/10336], Loss: 1.2031\n",
      "Epoch [3/5], Step [4208/10336], Loss: 4.0863\n",
      "Epoch [3/5], Step [4210/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [4212/10336], Loss: 0.1426\n",
      "Epoch [3/5], Step [4214/10336], Loss: 0.3193\n",
      "Epoch [3/5], Step [4216/10336], Loss: 0.4536\n",
      "Epoch [3/5], Step [4218/10336], Loss: 1.0604\n",
      "Epoch [3/5], Step [4220/10336], Loss: 1.1603\n",
      "Epoch [3/5], Step [4222/10336], Loss: 0.7074\n",
      "Epoch [3/5], Step [4224/10336], Loss: 0.4268\n",
      "Epoch [3/5], Step [4226/10336], Loss: 1.1650\n",
      "Epoch [3/5], Step [4228/10336], Loss: 0.1021\n",
      "Epoch [3/5], Step [4230/10336], Loss: 0.0665\n",
      "Epoch [3/5], Step [4232/10336], Loss: 0.4638\n",
      "Epoch [3/5], Step [4234/10336], Loss: 0.2078\n",
      "Epoch [3/5], Step [4236/10336], Loss: 0.0597\n",
      "Epoch [3/5], Step [4238/10336], Loss: 0.8713\n",
      "Epoch [3/5], Step [4240/10336], Loss: 0.6819\n",
      "Epoch [3/5], Step [4242/10336], Loss: 0.4393\n",
      "Epoch [3/5], Step [4244/10336], Loss: 1.6001\n",
      "Epoch [3/5], Step [4246/10336], Loss: 0.0123\n",
      "Epoch [3/5], Step [4248/10336], Loss: 0.3562\n",
      "Epoch [3/5], Step [4250/10336], Loss: 0.1283\n",
      "Epoch [3/5], Step [4252/10336], Loss: 4.0160\n",
      "Epoch [3/5], Step [4254/10336], Loss: 0.3337\n",
      "Epoch [3/5], Step [4256/10336], Loss: 0.1227\n",
      "Epoch [3/5], Step [4258/10336], Loss: 0.0068\n",
      "Epoch [3/5], Step [4260/10336], Loss: 0.5288\n",
      "Epoch [3/5], Step [4262/10336], Loss: 0.9358\n",
      "Epoch [3/5], Step [4264/10336], Loss: 0.1723\n",
      "Epoch [3/5], Step [4266/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [4268/10336], Loss: 0.3377\n",
      "Epoch [3/5], Step [4270/10336], Loss: 0.1730\n",
      "Epoch [3/5], Step [4272/10336], Loss: 0.4002\n",
      "Epoch [3/5], Step [4274/10336], Loss: 0.5398\n",
      "Epoch [3/5], Step [4276/10336], Loss: 0.2718\n",
      "Epoch [3/5], Step [4278/10336], Loss: 0.3163\n",
      "Epoch [3/5], Step [4280/10336], Loss: 1.7242\n",
      "Epoch [3/5], Step [4282/10336], Loss: 2.1767\n",
      "Epoch [3/5], Step [4284/10336], Loss: 2.1614\n",
      "Epoch [3/5], Step [4286/10336], Loss: 0.3123\n",
      "Epoch [3/5], Step [4288/10336], Loss: 0.0377\n",
      "Epoch [3/5], Step [4290/10336], Loss: 1.5799\n",
      "Epoch [3/5], Step [4292/10336], Loss: 0.1417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [4294/10336], Loss: 0.2704\n",
      "Epoch [3/5], Step [4296/10336], Loss: 0.7843\n",
      "Epoch [3/5], Step [4298/10336], Loss: 0.0096\n",
      "Epoch [3/5], Step [4300/10336], Loss: 0.1799\n",
      "Epoch [3/5], Step [4302/10336], Loss: 0.2185\n",
      "Epoch [3/5], Step [4304/10336], Loss: 3.3313\n",
      "Epoch [3/5], Step [4306/10336], Loss: 0.1412\n",
      "Epoch [3/5], Step [4308/10336], Loss: 0.1266\n",
      "Epoch [3/5], Step [4310/10336], Loss: 0.0026\n",
      "Epoch [3/5], Step [4312/10336], Loss: 3.3109\n",
      "Epoch [3/5], Step [4314/10336], Loss: 1.7239\n",
      "Epoch [3/5], Step [4316/10336], Loss: 0.8402\n",
      "Epoch [3/5], Step [4318/10336], Loss: 0.4149\n",
      "Epoch [3/5], Step [4320/10336], Loss: 0.4631\n",
      "Epoch [3/5], Step [4322/10336], Loss: 0.3157\n",
      "Epoch [3/5], Step [4324/10336], Loss: 0.5942\n",
      "Epoch [3/5], Step [4326/10336], Loss: 0.1020\n",
      "Epoch [3/5], Step [4328/10336], Loss: 0.1719\n",
      "Epoch [3/5], Step [4330/10336], Loss: 0.1835\n",
      "Epoch [3/5], Step [4332/10336], Loss: 1.0528\n",
      "Epoch [3/5], Step [4334/10336], Loss: 0.1717\n",
      "Epoch [3/5], Step [4336/10336], Loss: 0.0025\n",
      "Epoch [3/5], Step [4338/10336], Loss: 1.8348\n",
      "Epoch [3/5], Step [4340/10336], Loss: 0.4032\n",
      "Epoch [3/5], Step [4342/10336], Loss: 2.8837\n",
      "Epoch [3/5], Step [4344/10336], Loss: 0.1244\n",
      "Epoch [3/5], Step [4346/10336], Loss: 0.8897\n",
      "Epoch [3/5], Step [4348/10336], Loss: 0.1177\n",
      "Epoch [3/5], Step [4350/10336], Loss: 1.5257\n",
      "Epoch [3/5], Step [4352/10336], Loss: 0.3196\n",
      "Epoch [3/5], Step [4354/10336], Loss: 1.8844\n",
      "Epoch [3/5], Step [4356/10336], Loss: 0.0257\n",
      "Epoch [3/5], Step [4358/10336], Loss: 0.5391\n",
      "Epoch [3/5], Step [4360/10336], Loss: 0.2383\n",
      "Epoch [3/5], Step [4362/10336], Loss: 0.0893\n",
      "Epoch [3/5], Step [4364/10336], Loss: 0.9553\n",
      "Epoch [3/5], Step [4366/10336], Loss: 0.1176\n",
      "Epoch [3/5], Step [4368/10336], Loss: 0.9931\n",
      "Epoch [3/5], Step [4370/10336], Loss: 1.1690\n",
      "Epoch [3/5], Step [4372/10336], Loss: 0.3575\n",
      "Epoch [3/5], Step [4374/10336], Loss: 0.0454\n",
      "Epoch [3/5], Step [4376/10336], Loss: 0.8387\n",
      "Epoch [3/5], Step [4378/10336], Loss: 0.0495\n",
      "Epoch [3/5], Step [4380/10336], Loss: 0.3191\n",
      "Epoch [3/5], Step [4382/10336], Loss: 1.4106\n",
      "Epoch [3/5], Step [4384/10336], Loss: 0.0763\n",
      "Epoch [3/5], Step [4386/10336], Loss: 2.5994\n",
      "Epoch [3/5], Step [4388/10336], Loss: 1.3449\n",
      "Epoch [3/5], Step [4390/10336], Loss: 0.3424\n",
      "Epoch [3/5], Step [4392/10336], Loss: 0.1315\n",
      "Epoch [3/5], Step [4394/10336], Loss: 4.0648\n",
      "Epoch [3/5], Step [4396/10336], Loss: 1.0897\n",
      "Epoch [3/5], Step [4398/10336], Loss: 3.1632\n",
      "Epoch [3/5], Step [4400/10336], Loss: 0.3225\n",
      "Epoch [3/5], Step [4402/10336], Loss: 0.1465\n",
      "Epoch [3/5], Step [4404/10336], Loss: 0.0511\n",
      "Epoch [3/5], Step [4406/10336], Loss: 0.6766\n",
      "Epoch [3/5], Step [4408/10336], Loss: 0.1584\n",
      "Epoch [3/5], Step [4410/10336], Loss: 0.4944\n",
      "Epoch [3/5], Step [4412/10336], Loss: 0.1488\n",
      "Epoch [3/5], Step [4414/10336], Loss: 0.5643\n",
      "Epoch [3/5], Step [4416/10336], Loss: 0.0908\n",
      "Epoch [3/5], Step [4418/10336], Loss: 0.0250\n",
      "Epoch [3/5], Step [4420/10336], Loss: 0.2825\n",
      "Epoch [3/5], Step [4422/10336], Loss: 0.3822\n",
      "Epoch [3/5], Step [4424/10336], Loss: 0.1934\n",
      "Epoch [3/5], Step [4426/10336], Loss: 0.8719\n",
      "Epoch [3/5], Step [4428/10336], Loss: 1.5536\n",
      "Epoch [3/5], Step [4430/10336], Loss: 0.1572\n",
      "Epoch [3/5], Step [4432/10336], Loss: 0.0001\n",
      "Epoch [3/5], Step [4434/10336], Loss: 0.6165\n",
      "Epoch [3/5], Step [4436/10336], Loss: 0.0091\n",
      "Epoch [3/5], Step [4438/10336], Loss: 0.0715\n",
      "Epoch [3/5], Step [4440/10336], Loss: 0.0072\n",
      "Epoch [3/5], Step [4442/10336], Loss: 0.7111\n",
      "Epoch [3/5], Step [4444/10336], Loss: 4.2268\n",
      "Epoch [3/5], Step [4446/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [4448/10336], Loss: 1.8273\n",
      "Epoch [3/5], Step [4450/10336], Loss: 3.7646\n",
      "Epoch [3/5], Step [4452/10336], Loss: 0.0087\n",
      "Epoch [3/5], Step [4454/10336], Loss: 2.8216\n",
      "Epoch [3/5], Step [4456/10336], Loss: 0.5556\n",
      "Epoch [3/5], Step [4458/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [4460/10336], Loss: 0.2614\n",
      "Epoch [3/5], Step [4462/10336], Loss: 0.1213\n",
      "Epoch [3/5], Step [4464/10336], Loss: 0.3650\n",
      "Epoch [3/5], Step [4466/10336], Loss: 0.0277\n",
      "Epoch [3/5], Step [4468/10336], Loss: 0.1688\n",
      "Epoch [3/5], Step [4470/10336], Loss: 0.0020\n",
      "Epoch [3/5], Step [4472/10336], Loss: 0.6093\n",
      "Epoch [3/5], Step [4474/10336], Loss: 0.2342\n",
      "Epoch [3/5], Step [4476/10336], Loss: 0.2209\n",
      "Epoch [3/5], Step [4478/10336], Loss: 0.4055\n",
      "Epoch [3/5], Step [4480/10336], Loss: 0.0472\n",
      "Epoch [3/5], Step [4482/10336], Loss: 0.0047\n",
      "Epoch [3/5], Step [4484/10336], Loss: 0.2604\n",
      "Epoch [3/5], Step [4486/10336], Loss: 0.1778\n",
      "Epoch [3/5], Step [4488/10336], Loss: 0.0113\n",
      "Epoch [3/5], Step [4490/10336], Loss: 0.4490\n",
      "Epoch [3/5], Step [4492/10336], Loss: 0.3358\n",
      "Epoch [3/5], Step [4494/10336], Loss: 1.2642\n",
      "Epoch [3/5], Step [4496/10336], Loss: 0.0101\n",
      "Epoch [3/5], Step [4498/10336], Loss: 0.1124\n",
      "Epoch [3/5], Step [4500/10336], Loss: 3.9063\n",
      "Epoch [3/5], Step [4502/10336], Loss: 0.1732\n",
      "Epoch [3/5], Step [4504/10336], Loss: 3.6191\n",
      "Epoch [3/5], Step [4506/10336], Loss: 2.5808\n",
      "Epoch [3/5], Step [4508/10336], Loss: 0.5894\n",
      "Epoch [3/5], Step [4510/10336], Loss: 0.0112\n",
      "Epoch [3/5], Step [4512/10336], Loss: 0.1750\n",
      "Epoch [3/5], Step [4514/10336], Loss: 0.0877\n",
      "Epoch [3/5], Step [4516/10336], Loss: 1.6216\n",
      "Epoch [3/5], Step [4518/10336], Loss: 1.4105\n",
      "Epoch [3/5], Step [4520/10336], Loss: 1.8950\n",
      "Epoch [3/5], Step [4522/10336], Loss: 0.7967\n",
      "Epoch [3/5], Step [4524/10336], Loss: 1.0332\n",
      "Epoch [3/5], Step [4526/10336], Loss: 2.6972\n",
      "Epoch [3/5], Step [4528/10336], Loss: 0.3058\n",
      "Epoch [3/5], Step [4530/10336], Loss: 0.0210\n",
      "Epoch [3/5], Step [4532/10336], Loss: 0.1290\n",
      "Epoch [3/5], Step [4534/10336], Loss: 0.0302\n",
      "Epoch [3/5], Step [4536/10336], Loss: 0.1876\n",
      "Epoch [3/5], Step [4538/10336], Loss: 0.1880\n",
      "Epoch [3/5], Step [4540/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [4542/10336], Loss: 4.5460\n",
      "Epoch [3/5], Step [4544/10336], Loss: 0.0521\n",
      "Epoch [3/5], Step [4546/10336], Loss: 0.4382\n",
      "Epoch [3/5], Step [4548/10336], Loss: 4.0913\n",
      "Epoch [3/5], Step [4550/10336], Loss: 0.0141\n",
      "Epoch [3/5], Step [4552/10336], Loss: 1.7304\n",
      "Epoch [3/5], Step [4554/10336], Loss: 0.4261\n",
      "Epoch [3/5], Step [4556/10336], Loss: 0.5837\n",
      "Epoch [3/5], Step [4558/10336], Loss: 0.1926\n",
      "Epoch [3/5], Step [4560/10336], Loss: 0.0558\n",
      "Epoch [3/5], Step [4562/10336], Loss: 3.2185\n",
      "Epoch [3/5], Step [4564/10336], Loss: 0.0970\n",
      "Epoch [3/5], Step [4566/10336], Loss: 4.2468\n",
      "Epoch [3/5], Step [4568/10336], Loss: 0.1145\n",
      "Epoch [3/5], Step [4570/10336], Loss: 0.6921\n",
      "Epoch [3/5], Step [4572/10336], Loss: 0.4395\n",
      "Epoch [3/5], Step [4574/10336], Loss: 0.1355\n",
      "Epoch [3/5], Step [4576/10336], Loss: 0.1940\n",
      "Epoch [3/5], Step [4578/10336], Loss: 0.0781\n",
      "Epoch [3/5], Step [4580/10336], Loss: 1.2057\n",
      "Epoch [3/5], Step [4582/10336], Loss: 0.0301\n",
      "Epoch [3/5], Step [4584/10336], Loss: 0.2762\n",
      "Epoch [3/5], Step [4586/10336], Loss: 0.7364\n",
      "Epoch [3/5], Step [4588/10336], Loss: 0.2940\n",
      "Epoch [3/5], Step [4590/10336], Loss: 3.3978\n",
      "Epoch [3/5], Step [4592/10336], Loss: 0.3345\n",
      "Epoch [3/5], Step [4594/10336], Loss: 2.4190\n",
      "Epoch [3/5], Step [4596/10336], Loss: 1.4506\n",
      "Epoch [3/5], Step [4598/10336], Loss: 3.0049\n",
      "Epoch [3/5], Step [4600/10336], Loss: 0.0039\n",
      "Epoch [3/5], Step [4602/10336], Loss: 1.5424\n",
      "Epoch [3/5], Step [4604/10336], Loss: 0.1032\n",
      "Epoch [3/5], Step [4606/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [4608/10336], Loss: 0.0522\n",
      "Epoch [3/5], Step [4610/10336], Loss: 0.0149\n",
      "Epoch [3/5], Step [4612/10336], Loss: 0.3593\n",
      "Epoch [3/5], Step [4614/10336], Loss: 0.3571\n",
      "Epoch [3/5], Step [4616/10336], Loss: 1.8125\n",
      "Epoch [3/5], Step [4618/10336], Loss: 0.3571\n",
      "Epoch [3/5], Step [4620/10336], Loss: 2.1005\n",
      "Epoch [3/5], Step [4622/10336], Loss: 0.0024\n",
      "Epoch [3/5], Step [4624/10336], Loss: 1.9349\n",
      "Epoch [3/5], Step [4626/10336], Loss: 0.3028\n",
      "Epoch [3/5], Step [4628/10336], Loss: 0.0614\n",
      "Epoch [3/5], Step [4630/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [4632/10336], Loss: 0.4055\n",
      "Epoch [3/5], Step [4634/10336], Loss: 0.6437\n",
      "Epoch [3/5], Step [4636/10336], Loss: 0.1364\n",
      "Epoch [3/5], Step [4638/10336], Loss: 0.0683\n",
      "Epoch [3/5], Step [4640/10336], Loss: 0.1216\n",
      "Epoch [3/5], Step [4642/10336], Loss: 0.0213\n",
      "Epoch [3/5], Step [4644/10336], Loss: 0.0817\n",
      "Epoch [3/5], Step [4646/10336], Loss: 3.5274\n",
      "Epoch [3/5], Step [4648/10336], Loss: 1.1289\n",
      "Epoch [3/5], Step [4650/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [4652/10336], Loss: 0.4864\n",
      "Epoch [3/5], Step [4654/10336], Loss: 0.0552\n",
      "Epoch [3/5], Step [4656/10336], Loss: 0.6211\n",
      "Epoch [3/5], Step [4658/10336], Loss: 0.9934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [4660/10336], Loss: 0.5006\n",
      "Epoch [3/5], Step [4662/10336], Loss: 0.5328\n",
      "Epoch [3/5], Step [4664/10336], Loss: 0.1017\n",
      "Epoch [3/5], Step [4666/10336], Loss: 0.1672\n",
      "Epoch [3/5], Step [4668/10336], Loss: 0.0918\n",
      "Epoch [3/5], Step [4670/10336], Loss: 0.5705\n",
      "Epoch [3/5], Step [4672/10336], Loss: 0.3124\n",
      "Epoch [3/5], Step [4674/10336], Loss: 4.4862\n",
      "Epoch [3/5], Step [4676/10336], Loss: 0.3810\n",
      "Epoch [3/5], Step [4678/10336], Loss: 0.0489\n",
      "Epoch [3/5], Step [4680/10336], Loss: 0.3299\n",
      "Epoch [3/5], Step [4682/10336], Loss: 1.7397\n",
      "Epoch [3/5], Step [4684/10336], Loss: 0.0067\n",
      "Epoch [3/5], Step [4686/10336], Loss: 0.4463\n",
      "Epoch [3/5], Step [4688/10336], Loss: 2.8401\n",
      "Epoch [3/5], Step [4690/10336], Loss: 0.1866\n",
      "Epoch [3/5], Step [4692/10336], Loss: 0.9272\n",
      "Epoch [3/5], Step [4694/10336], Loss: 0.7543\n",
      "Epoch [3/5], Step [4696/10336], Loss: 0.0223\n",
      "Epoch [3/5], Step [4698/10336], Loss: 0.8097\n",
      "Epoch [3/5], Step [4700/10336], Loss: 0.2333\n",
      "Epoch [3/5], Step [4702/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [4704/10336], Loss: 5.5761\n",
      "Epoch [3/5], Step [4706/10336], Loss: 0.2063\n",
      "Epoch [3/5], Step [4708/10336], Loss: 0.0051\n",
      "Epoch [3/5], Step [4710/10336], Loss: 4.5058\n",
      "Epoch [3/5], Step [4712/10336], Loss: 0.0116\n",
      "Epoch [3/5], Step [4714/10336], Loss: 1.5752\n",
      "Epoch [3/5], Step [4716/10336], Loss: 2.7247\n",
      "Epoch [3/5], Step [4718/10336], Loss: 1.2144\n",
      "Epoch [3/5], Step [4720/10336], Loss: 0.1484\n",
      "Epoch [3/5], Step [4722/10336], Loss: 2.2259\n",
      "Epoch [3/5], Step [4724/10336], Loss: 0.0744\n",
      "Epoch [3/5], Step [4726/10336], Loss: 0.1826\n",
      "Epoch [3/5], Step [4728/10336], Loss: 0.4720\n",
      "Epoch [3/5], Step [4730/10336], Loss: 0.0411\n",
      "Epoch [3/5], Step [4732/10336], Loss: 1.5957\n",
      "Epoch [3/5], Step [4734/10336], Loss: 1.0320\n",
      "Epoch [3/5], Step [4736/10336], Loss: 0.5676\n",
      "Epoch [3/5], Step [4738/10336], Loss: 0.0779\n",
      "Epoch [3/5], Step [4740/10336], Loss: 0.0046\n",
      "Epoch [3/5], Step [4742/10336], Loss: 0.0603\n",
      "Epoch [3/5], Step [4744/10336], Loss: 0.0195\n",
      "Epoch [3/5], Step [4746/10336], Loss: 0.3381\n",
      "Epoch [3/5], Step [4748/10336], Loss: 0.2299\n",
      "Epoch [3/5], Step [4750/10336], Loss: 0.0127\n",
      "Epoch [3/5], Step [4752/10336], Loss: 1.2331\n",
      "Epoch [3/5], Step [4754/10336], Loss: 0.1233\n",
      "Epoch [3/5], Step [4756/10336], Loss: 0.8222\n",
      "Epoch [3/5], Step [4758/10336], Loss: 0.0095\n",
      "Epoch [3/5], Step [4760/10336], Loss: 0.1010\n",
      "Epoch [3/5], Step [4762/10336], Loss: 0.0236\n",
      "Epoch [3/5], Step [4764/10336], Loss: 2.5290\n",
      "Epoch [3/5], Step [4766/10336], Loss: 0.3476\n",
      "Epoch [3/5], Step [4768/10336], Loss: 1.7598\n",
      "Epoch [3/5], Step [4770/10336], Loss: 0.2477\n",
      "Epoch [3/5], Step [4772/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [4774/10336], Loss: 0.2074\n",
      "Epoch [3/5], Step [4776/10336], Loss: 0.0035\n",
      "Epoch [3/5], Step [4778/10336], Loss: 0.2735\n",
      "Epoch [3/5], Step [4780/10336], Loss: 0.6386\n",
      "Epoch [3/5], Step [4782/10336], Loss: 0.2495\n",
      "Epoch [3/5], Step [4784/10336], Loss: 0.2151\n",
      "Epoch [3/5], Step [4786/10336], Loss: 0.1081\n",
      "Epoch [3/5], Step [4788/10336], Loss: 0.0164\n",
      "Epoch [3/5], Step [4790/10336], Loss: 3.1642\n",
      "Epoch [3/5], Step [4792/10336], Loss: 1.0400\n",
      "Epoch [3/5], Step [4794/10336], Loss: 0.7909\n",
      "Epoch [3/5], Step [4796/10336], Loss: 0.2981\n",
      "Epoch [3/5], Step [4798/10336], Loss: 0.1099\n",
      "Epoch [3/5], Step [4800/10336], Loss: 0.9042\n",
      "Epoch [3/5], Step [4802/10336], Loss: 0.6941\n",
      "Epoch [3/5], Step [4804/10336], Loss: 0.3188\n",
      "Epoch [3/5], Step [4806/10336], Loss: 4.0386\n",
      "Epoch [3/5], Step [4808/10336], Loss: 0.2060\n",
      "Epoch [3/5], Step [4810/10336], Loss: 0.1020\n",
      "Epoch [3/5], Step [4812/10336], Loss: 2.8075\n",
      "Epoch [3/5], Step [4814/10336], Loss: 0.4002\n",
      "Epoch [3/5], Step [4816/10336], Loss: 1.6458\n",
      "Epoch [3/5], Step [4818/10336], Loss: 0.1844\n",
      "Epoch [3/5], Step [4820/10336], Loss: 0.8565\n",
      "Epoch [3/5], Step [4822/10336], Loss: 1.1733\n",
      "Epoch [3/5], Step [4824/10336], Loss: 0.0646\n",
      "Epoch [3/5], Step [4826/10336], Loss: 0.0060\n",
      "Epoch [3/5], Step [4828/10336], Loss: 0.0673\n",
      "Epoch [3/5], Step [4830/10336], Loss: 0.0942\n",
      "Epoch [3/5], Step [4832/10336], Loss: 0.0234\n",
      "Epoch [3/5], Step [4834/10336], Loss: 0.0109\n",
      "Epoch [3/5], Step [4836/10336], Loss: 0.4571\n",
      "Epoch [3/5], Step [4838/10336], Loss: 0.1713\n",
      "Epoch [3/5], Step [4840/10336], Loss: 0.0123\n",
      "Epoch [3/5], Step [4842/10336], Loss: 0.6113\n",
      "Epoch [3/5], Step [4844/10336], Loss: 0.6280\n",
      "Epoch [3/5], Step [4846/10336], Loss: 0.8606\n",
      "Epoch [3/5], Step [4848/10336], Loss: 0.2384\n",
      "Epoch [3/5], Step [4850/10336], Loss: 0.6767\n",
      "Epoch [3/5], Step [4852/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [4854/10336], Loss: 4.9446\n",
      "Epoch [3/5], Step [4856/10336], Loss: 0.1159\n",
      "Epoch [3/5], Step [4858/10336], Loss: 0.1595\n",
      "Epoch [3/5], Step [4860/10336], Loss: 0.0591\n",
      "Epoch [3/5], Step [4862/10336], Loss: 0.1037\n",
      "Epoch [3/5], Step [4864/10336], Loss: 0.1362\n",
      "Epoch [3/5], Step [4866/10336], Loss: 0.4588\n",
      "Epoch [3/5], Step [4868/10336], Loss: 0.9408\n",
      "Epoch [3/5], Step [4870/10336], Loss: 0.4787\n",
      "Epoch [3/5], Step [4872/10336], Loss: 0.0082\n",
      "Epoch [3/5], Step [4874/10336], Loss: 0.2757\n",
      "Epoch [3/5], Step [4876/10336], Loss: 0.0042\n",
      "Epoch [3/5], Step [4878/10336], Loss: 0.7734\n",
      "Epoch [3/5], Step [4880/10336], Loss: 0.0489\n",
      "Epoch [3/5], Step [4882/10336], Loss: 1.5153\n",
      "Epoch [3/5], Step [4884/10336], Loss: 0.9107\n",
      "Epoch [3/5], Step [4886/10336], Loss: 0.0246\n",
      "Epoch [3/5], Step [4888/10336], Loss: 2.2785\n",
      "Epoch [3/5], Step [4890/10336], Loss: 0.3500\n",
      "Epoch [3/5], Step [4892/10336], Loss: 0.1976\n",
      "Epoch [3/5], Step [4894/10336], Loss: 0.1463\n",
      "Epoch [3/5], Step [4896/10336], Loss: 0.8731\n",
      "Epoch [3/5], Step [4898/10336], Loss: 1.9748\n",
      "Epoch [3/5], Step [4900/10336], Loss: 0.3953\n",
      "Epoch [3/5], Step [4902/10336], Loss: 0.3781\n",
      "Epoch [3/5], Step [4904/10336], Loss: 0.4375\n",
      "Epoch [3/5], Step [4906/10336], Loss: 4.1181\n",
      "Epoch [3/5], Step [4908/10336], Loss: 0.7606\n",
      "Epoch [3/5], Step [4910/10336], Loss: 0.1012\n",
      "Epoch [3/5], Step [4912/10336], Loss: 0.1423\n",
      "Epoch [3/5], Step [4914/10336], Loss: 0.9039\n",
      "Epoch [3/5], Step [4916/10336], Loss: 0.0660\n",
      "Epoch [3/5], Step [4918/10336], Loss: 0.0067\n",
      "Epoch [3/5], Step [4920/10336], Loss: 0.2565\n",
      "Epoch [3/5], Step [4922/10336], Loss: 0.2877\n",
      "Epoch [3/5], Step [4924/10336], Loss: 1.1090\n",
      "Epoch [3/5], Step [4926/10336], Loss: 1.0157\n",
      "Epoch [3/5], Step [4928/10336], Loss: 0.4500\n",
      "Epoch [3/5], Step [4930/10336], Loss: 0.0405\n",
      "Epoch [3/5], Step [4932/10336], Loss: 0.7585\n",
      "Epoch [3/5], Step [4934/10336], Loss: 0.4673\n",
      "Epoch [3/5], Step [4936/10336], Loss: 0.1441\n",
      "Epoch [3/5], Step [4938/10336], Loss: 1.1292\n",
      "Epoch [3/5], Step [4940/10336], Loss: 2.1021\n",
      "Epoch [3/5], Step [4942/10336], Loss: 1.2382\n",
      "Epoch [3/5], Step [4944/10336], Loss: 1.4839\n",
      "Epoch [3/5], Step [4946/10336], Loss: 0.4108\n",
      "Epoch [3/5], Step [4948/10336], Loss: 0.2377\n",
      "Epoch [3/5], Step [4950/10336], Loss: 0.2038\n",
      "Epoch [3/5], Step [4952/10336], Loss: 0.3173\n",
      "Epoch [3/5], Step [4954/10336], Loss: 0.0054\n",
      "Epoch [3/5], Step [4956/10336], Loss: 2.1159\n",
      "Epoch [3/5], Step [4958/10336], Loss: 0.5556\n",
      "Epoch [3/5], Step [4960/10336], Loss: 0.1273\n",
      "Epoch [3/5], Step [4962/10336], Loss: 0.0957\n",
      "Epoch [3/5], Step [4964/10336], Loss: 4.5034\n",
      "Epoch [3/5], Step [4966/10336], Loss: 0.0219\n",
      "Epoch [3/5], Step [4968/10336], Loss: 0.1750\n",
      "Epoch [3/5], Step [4970/10336], Loss: 2.3629\n",
      "Epoch [3/5], Step [4972/10336], Loss: 0.0291\n",
      "Epoch [3/5], Step [4974/10336], Loss: 0.1524\n",
      "Epoch [3/5], Step [4976/10336], Loss: 0.8583\n",
      "Epoch [3/5], Step [4978/10336], Loss: 0.0223\n",
      "Epoch [3/5], Step [4980/10336], Loss: 0.0765\n",
      "Epoch [3/5], Step [4982/10336], Loss: 0.0112\n",
      "Epoch [3/5], Step [4984/10336], Loss: 0.0397\n",
      "Epoch [3/5], Step [4986/10336], Loss: 0.0123\n",
      "Epoch [3/5], Step [4988/10336], Loss: 0.0581\n",
      "Epoch [3/5], Step [4990/10336], Loss: 0.1739\n",
      "Epoch [3/5], Step [4992/10336], Loss: 0.2305\n",
      "Epoch [3/5], Step [4994/10336], Loss: 0.4642\n",
      "Epoch [3/5], Step [4996/10336], Loss: 0.0051\n",
      "Epoch [3/5], Step [4998/10336], Loss: 0.1968\n",
      "Epoch [3/5], Step [5000/10336], Loss: 0.0931\n",
      "Epoch [3/5], Step [5002/10336], Loss: 0.3884\n",
      "Epoch [3/5], Step [5004/10336], Loss: 0.3011\n",
      "Epoch [3/5], Step [5006/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [5008/10336], Loss: 0.9664\n",
      "Epoch [3/5], Step [5010/10336], Loss: 0.5664\n",
      "Epoch [3/5], Step [5012/10336], Loss: 0.1559\n",
      "Epoch [3/5], Step [5014/10336], Loss: 0.9356\n",
      "Epoch [3/5], Step [5016/10336], Loss: 0.0053\n",
      "Epoch [3/5], Step [5018/10336], Loss: 0.2840\n",
      "Epoch [3/5], Step [5020/10336], Loss: 0.2718\n",
      "Epoch [3/5], Step [5022/10336], Loss: 0.0729\n",
      "Epoch [3/5], Step [5024/10336], Loss: 0.2934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [5026/10336], Loss: 0.6140\n",
      "Epoch [3/5], Step [5028/10336], Loss: 0.2132\n",
      "Epoch [3/5], Step [5030/10336], Loss: 0.1149\n",
      "Epoch [3/5], Step [5032/10336], Loss: 1.2658\n",
      "Epoch [3/5], Step [5034/10336], Loss: 0.0398\n",
      "Epoch [3/5], Step [5036/10336], Loss: 0.2626\n",
      "Epoch [3/5], Step [5038/10336], Loss: 0.8066\n",
      "Epoch [3/5], Step [5040/10336], Loss: 0.4197\n",
      "Epoch [3/5], Step [5042/10336], Loss: 0.7204\n",
      "Epoch [3/5], Step [5044/10336], Loss: 3.5101\n",
      "Epoch [3/5], Step [5046/10336], Loss: 0.9224\n",
      "Epoch [3/5], Step [5048/10336], Loss: 0.0064\n",
      "Epoch [3/5], Step [5050/10336], Loss: 1.5262\n",
      "Epoch [3/5], Step [5052/10336], Loss: 0.0055\n",
      "Epoch [3/5], Step [5054/10336], Loss: 3.1965\n",
      "Epoch [3/5], Step [5056/10336], Loss: 0.6257\n",
      "Epoch [3/5], Step [5058/10336], Loss: 0.0085\n",
      "Epoch [3/5], Step [5060/10336], Loss: 0.0026\n",
      "Epoch [3/5], Step [5062/10336], Loss: 0.2176\n",
      "Epoch [3/5], Step [5064/10336], Loss: 0.4438\n",
      "Epoch [3/5], Step [5066/10336], Loss: 0.0372\n",
      "Epoch [3/5], Step [5068/10336], Loss: 0.0128\n",
      "Epoch [3/5], Step [5070/10336], Loss: 0.2838\n",
      "Epoch [3/5], Step [5072/10336], Loss: 0.1220\n",
      "Epoch [3/5], Step [5074/10336], Loss: 0.0628\n",
      "Epoch [3/5], Step [5076/10336], Loss: 0.2673\n",
      "Epoch [3/5], Step [5078/10336], Loss: 1.6178\n",
      "Epoch [3/5], Step [5080/10336], Loss: 0.0103\n",
      "Epoch [3/5], Step [5082/10336], Loss: 2.8200\n",
      "Epoch [3/5], Step [5084/10336], Loss: 0.1234\n",
      "Epoch [3/5], Step [5086/10336], Loss: 3.1778\n",
      "Epoch [3/5], Step [5088/10336], Loss: 0.1506\n",
      "Epoch [3/5], Step [5090/10336], Loss: 0.0063\n",
      "Epoch [3/5], Step [5092/10336], Loss: 0.0003\n",
      "Epoch [3/5], Step [5094/10336], Loss: 0.0170\n",
      "Epoch [3/5], Step [5096/10336], Loss: 0.0559\n",
      "Epoch [3/5], Step [5098/10336], Loss: 0.8625\n",
      "Epoch [3/5], Step [5100/10336], Loss: 0.0995\n",
      "Epoch [3/5], Step [5102/10336], Loss: 1.9513\n",
      "Epoch [3/5], Step [5104/10336], Loss: 0.5619\n",
      "Epoch [3/5], Step [5106/10336], Loss: 3.9865\n",
      "Epoch [3/5], Step [5108/10336], Loss: 0.2692\n",
      "Epoch [3/5], Step [5110/10336], Loss: 2.0045\n",
      "Epoch [3/5], Step [5112/10336], Loss: 0.1020\n",
      "Epoch [3/5], Step [5114/10336], Loss: 0.8482\n",
      "Epoch [3/5], Step [5116/10336], Loss: 2.1556\n",
      "Epoch [3/5], Step [5118/10336], Loss: 0.2964\n",
      "Epoch [3/5], Step [5120/10336], Loss: 0.2503\n",
      "Epoch [3/5], Step [5122/10336], Loss: 0.5452\n",
      "Epoch [3/5], Step [5124/10336], Loss: 0.2941\n",
      "Epoch [3/5], Step [5126/10336], Loss: 1.2816\n",
      "Epoch [3/5], Step [5128/10336], Loss: 1.3977\n",
      "Epoch [3/5], Step [5130/10336], Loss: 0.0738\n",
      "Epoch [3/5], Step [5132/10336], Loss: 0.0882\n",
      "Epoch [3/5], Step [5134/10336], Loss: 0.0814\n",
      "Epoch [3/5], Step [5136/10336], Loss: 0.8312\n",
      "Epoch [3/5], Step [5138/10336], Loss: 0.1717\n",
      "Epoch [3/5], Step [5140/10336], Loss: 1.3311\n",
      "Epoch [3/5], Step [5142/10336], Loss: 0.0088\n",
      "Epoch [3/5], Step [5144/10336], Loss: 0.9904\n",
      "Epoch [3/5], Step [5146/10336], Loss: 0.0786\n",
      "Epoch [3/5], Step [5148/10336], Loss: 1.7093\n",
      "Epoch [3/5], Step [5150/10336], Loss: 0.1501\n",
      "Epoch [3/5], Step [5152/10336], Loss: 1.7940\n",
      "Epoch [3/5], Step [5154/10336], Loss: 0.6365\n",
      "Epoch [3/5], Step [5156/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [5158/10336], Loss: 3.5808\n",
      "Epoch [3/5], Step [5160/10336], Loss: 1.8255\n",
      "Epoch [3/5], Step [5162/10336], Loss: 0.0219\n",
      "Epoch [3/5], Step [5164/10336], Loss: 0.1195\n",
      "Epoch [3/5], Step [5166/10336], Loss: 3.3372\n",
      "Epoch [3/5], Step [5168/10336], Loss: 0.0581\n",
      "Epoch [3/5], Step [5170/10336], Loss: 0.0704\n",
      "Epoch [3/5], Step [5172/10336], Loss: 0.6621\n",
      "Epoch [3/5], Step [5174/10336], Loss: 0.1564\n",
      "Epoch [3/5], Step [5176/10336], Loss: 1.3170\n",
      "Epoch [3/5], Step [5178/10336], Loss: 0.3461\n",
      "Epoch [3/5], Step [5180/10336], Loss: 0.1623\n",
      "Epoch [3/5], Step [5182/10336], Loss: 0.3214\n",
      "Epoch [3/5], Step [5184/10336], Loss: 0.0814\n",
      "Epoch [3/5], Step [5186/10336], Loss: 2.1700\n",
      "Epoch [3/5], Step [5188/10336], Loss: 1.1878\n",
      "Epoch [3/5], Step [5190/10336], Loss: 0.0756\n",
      "Epoch [3/5], Step [5192/10336], Loss: 0.2229\n",
      "Epoch [3/5], Step [5194/10336], Loss: 0.0603\n",
      "Epoch [3/5], Step [5196/10336], Loss: 1.6679\n",
      "Epoch [3/5], Step [5198/10336], Loss: 0.6906\n",
      "Epoch [3/5], Step [5200/10336], Loss: 2.8714\n",
      "Epoch [3/5], Step [5202/10336], Loss: 1.0527\n",
      "Epoch [3/5], Step [5204/10336], Loss: 0.2654\n",
      "Epoch [3/5], Step [5206/10336], Loss: 2.0016\n",
      "Epoch [3/5], Step [5208/10336], Loss: 0.2890\n",
      "Epoch [3/5], Step [5210/10336], Loss: 0.5433\n",
      "Epoch [3/5], Step [5212/10336], Loss: 0.4327\n",
      "Epoch [3/5], Step [5214/10336], Loss: 0.0118\n",
      "Epoch [3/5], Step [5216/10336], Loss: 3.4177\n",
      "Epoch [3/5], Step [5218/10336], Loss: 2.0429\n",
      "Epoch [3/5], Step [5220/10336], Loss: 2.7152\n",
      "Epoch [3/5], Step [5222/10336], Loss: 0.1842\n",
      "Epoch [3/5], Step [5224/10336], Loss: 0.7010\n",
      "Epoch [3/5], Step [5226/10336], Loss: 0.2039\n",
      "Epoch [3/5], Step [5228/10336], Loss: 0.0898\n",
      "Epoch [3/5], Step [5230/10336], Loss: 2.1030\n",
      "Epoch [3/5], Step [5232/10336], Loss: 0.0049\n",
      "Epoch [3/5], Step [5234/10336], Loss: 0.0065\n",
      "Epoch [3/5], Step [5236/10336], Loss: 0.0126\n",
      "Epoch [3/5], Step [5238/10336], Loss: 0.4130\n",
      "Epoch [3/5], Step [5240/10336], Loss: 0.1722\n",
      "Epoch [3/5], Step [5242/10336], Loss: 0.0039\n",
      "Epoch [3/5], Step [5244/10336], Loss: 0.0092\n",
      "Epoch [3/5], Step [5246/10336], Loss: 3.6413\n",
      "Epoch [3/5], Step [5248/10336], Loss: 0.4547\n",
      "Epoch [3/5], Step [5250/10336], Loss: 0.3114\n",
      "Epoch [3/5], Step [5252/10336], Loss: 0.0519\n",
      "Epoch [3/5], Step [5254/10336], Loss: 4.1071\n",
      "Epoch [3/5], Step [5256/10336], Loss: 2.0183\n",
      "Epoch [3/5], Step [5258/10336], Loss: 0.9715\n",
      "Epoch [3/5], Step [5260/10336], Loss: 0.8082\n",
      "Epoch [3/5], Step [5262/10336], Loss: 0.0090\n",
      "Epoch [3/5], Step [5264/10336], Loss: 2.0688\n",
      "Epoch [3/5], Step [5266/10336], Loss: 0.0430\n",
      "Epoch [3/5], Step [5268/10336], Loss: 1.9774\n",
      "Epoch [3/5], Step [5270/10336], Loss: 1.9920\n",
      "Epoch [3/5], Step [5272/10336], Loss: 3.6905\n",
      "Epoch [3/5], Step [5274/10336], Loss: 1.8448\n",
      "Epoch [3/5], Step [5276/10336], Loss: 0.3692\n",
      "Epoch [3/5], Step [5278/10336], Loss: 0.4510\n",
      "Epoch [3/5], Step [5280/10336], Loss: 0.9367\n",
      "Epoch [3/5], Step [5282/10336], Loss: 1.0632\n",
      "Epoch [3/5], Step [5284/10336], Loss: 0.1729\n",
      "Epoch [3/5], Step [5286/10336], Loss: 0.7556\n",
      "Epoch [3/5], Step [5288/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [5290/10336], Loss: 0.6985\n",
      "Epoch [3/5], Step [5292/10336], Loss: 0.3348\n",
      "Epoch [3/5], Step [5294/10336], Loss: 0.4438\n",
      "Epoch [3/5], Step [5296/10336], Loss: 0.3122\n",
      "Epoch [3/5], Step [5298/10336], Loss: 0.0418\n",
      "Epoch [3/5], Step [5300/10336], Loss: 0.1933\n",
      "Epoch [3/5], Step [5302/10336], Loss: 0.4548\n",
      "Epoch [3/5], Step [5304/10336], Loss: 0.0146\n",
      "Epoch [3/5], Step [5306/10336], Loss: 0.2096\n",
      "Epoch [3/5], Step [5308/10336], Loss: 0.0977\n",
      "Epoch [3/5], Step [5310/10336], Loss: 0.4820\n",
      "Epoch [3/5], Step [5312/10336], Loss: 2.1210\n",
      "Epoch [3/5], Step [5314/10336], Loss: 1.2743\n",
      "Epoch [3/5], Step [5316/10336], Loss: 0.6234\n",
      "Epoch [3/5], Step [5318/10336], Loss: 0.0083\n",
      "Epoch [3/5], Step [5320/10336], Loss: 0.1033\n",
      "Epoch [3/5], Step [5322/10336], Loss: 0.0751\n",
      "Epoch [3/5], Step [5324/10336], Loss: 0.5268\n",
      "Epoch [3/5], Step [5326/10336], Loss: 0.0983\n",
      "Epoch [3/5], Step [5328/10336], Loss: 1.5310\n",
      "Epoch [3/5], Step [5330/10336], Loss: 0.0495\n",
      "Epoch [3/5], Step [5332/10336], Loss: 0.1378\n",
      "Epoch [3/5], Step [5334/10336], Loss: 3.2285\n",
      "Epoch [3/5], Step [5336/10336], Loss: 0.0049\n",
      "Epoch [3/5], Step [5338/10336], Loss: 0.2335\n",
      "Epoch [3/5], Step [5340/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [5342/10336], Loss: 1.2852\n",
      "Epoch [3/5], Step [5344/10336], Loss: 0.0573\n",
      "Epoch [3/5], Step [5346/10336], Loss: 0.0016\n",
      "Epoch [3/5], Step [5348/10336], Loss: 2.4988\n",
      "Epoch [3/5], Step [5350/10336], Loss: 4.9724\n",
      "Epoch [3/5], Step [5352/10336], Loss: 0.5503\n",
      "Epoch [3/5], Step [5354/10336], Loss: 0.0030\n",
      "Epoch [3/5], Step [5356/10336], Loss: 0.1616\n",
      "Epoch [3/5], Step [5358/10336], Loss: 1.1524\n",
      "Epoch [3/5], Step [5360/10336], Loss: 0.3160\n",
      "Epoch [3/5], Step [5362/10336], Loss: 0.1767\n",
      "Epoch [3/5], Step [5364/10336], Loss: 0.8141\n",
      "Epoch [3/5], Step [5366/10336], Loss: 1.8005\n",
      "Epoch [3/5], Step [5368/10336], Loss: 1.1708\n",
      "Epoch [3/5], Step [5370/10336], Loss: 0.0845\n",
      "Epoch [3/5], Step [5372/10336], Loss: 0.1659\n",
      "Epoch [3/5], Step [5374/10336], Loss: 0.6177\n",
      "Epoch [3/5], Step [5376/10336], Loss: 0.1932\n",
      "Epoch [3/5], Step [5378/10336], Loss: 3.1307\n",
      "Epoch [3/5], Step [5380/10336], Loss: 0.0542\n",
      "Epoch [3/5], Step [5382/10336], Loss: 2.3341\n",
      "Epoch [3/5], Step [5384/10336], Loss: 0.0068\n",
      "Epoch [3/5], Step [5386/10336], Loss: 0.1492\n",
      "Epoch [3/5], Step [5388/10336], Loss: 0.6888\n",
      "Epoch [3/5], Step [5390/10336], Loss: 0.5116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [5392/10336], Loss: 0.2501\n",
      "Epoch [3/5], Step [5394/10336], Loss: 0.0568\n",
      "Epoch [3/5], Step [5396/10336], Loss: 0.3279\n",
      "Epoch [3/5], Step [5398/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [5400/10336], Loss: 4.3869\n",
      "Epoch [3/5], Step [5402/10336], Loss: 1.1392\n",
      "Epoch [3/5], Step [5404/10336], Loss: 0.4887\n",
      "Epoch [3/5], Step [5406/10336], Loss: 0.8829\n",
      "Epoch [3/5], Step [5408/10336], Loss: 0.0183\n",
      "Epoch [3/5], Step [5410/10336], Loss: 0.4510\n",
      "Epoch [3/5], Step [5412/10336], Loss: 0.2687\n",
      "Epoch [3/5], Step [5414/10336], Loss: 1.1800\n",
      "Epoch [3/5], Step [5416/10336], Loss: 0.7764\n",
      "Epoch [3/5], Step [5418/10336], Loss: 0.0003\n",
      "Epoch [3/5], Step [5420/10336], Loss: 3.7976\n",
      "Epoch [3/5], Step [5422/10336], Loss: 0.6215\n",
      "Epoch [3/5], Step [5424/10336], Loss: 0.4115\n",
      "Epoch [3/5], Step [5426/10336], Loss: 0.2648\n",
      "Epoch [3/5], Step [5428/10336], Loss: 3.1111\n",
      "Epoch [3/5], Step [5430/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [5432/10336], Loss: 4.5775\n",
      "Epoch [3/5], Step [5434/10336], Loss: 0.0981\n",
      "Epoch [3/5], Step [5436/10336], Loss: 0.1671\n",
      "Epoch [3/5], Step [5438/10336], Loss: 0.6692\n",
      "Epoch [3/5], Step [5440/10336], Loss: 0.3660\n",
      "Epoch [3/5], Step [5442/10336], Loss: 0.2962\n",
      "Epoch [3/5], Step [5444/10336], Loss: 2.9230\n",
      "Epoch [3/5], Step [5446/10336], Loss: 0.1519\n",
      "Epoch [3/5], Step [5448/10336], Loss: 0.2220\n",
      "Epoch [3/5], Step [5450/10336], Loss: 0.3371\n",
      "Epoch [3/5], Step [5452/10336], Loss: 0.3675\n",
      "Epoch [3/5], Step [5454/10336], Loss: 0.2387\n",
      "Epoch [3/5], Step [5456/10336], Loss: 0.5329\n",
      "Epoch [3/5], Step [5458/10336], Loss: 0.2172\n",
      "Epoch [3/5], Step [5460/10336], Loss: 0.2369\n",
      "Epoch [3/5], Step [5462/10336], Loss: 1.4340\n",
      "Epoch [3/5], Step [5464/10336], Loss: 3.4781\n",
      "Epoch [3/5], Step [5466/10336], Loss: 0.9526\n",
      "Epoch [3/5], Step [5468/10336], Loss: 1.5045\n",
      "Epoch [3/5], Step [5470/10336], Loss: 0.5182\n",
      "Epoch [3/5], Step [5472/10336], Loss: 0.2706\n",
      "Epoch [3/5], Step [5474/10336], Loss: 1.3748\n",
      "Epoch [3/5], Step [5476/10336], Loss: 1.4374\n",
      "Epoch [3/5], Step [5478/10336], Loss: 2.8518\n",
      "Epoch [3/5], Step [5480/10336], Loss: 3.5602\n",
      "Epoch [3/5], Step [5482/10336], Loss: 0.0731\n",
      "Epoch [3/5], Step [5484/10336], Loss: 0.5156\n",
      "Epoch [3/5], Step [5486/10336], Loss: 1.0879\n",
      "Epoch [3/5], Step [5488/10336], Loss: 0.6668\n",
      "Epoch [3/5], Step [5490/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [5492/10336], Loss: 0.0705\n",
      "Epoch [3/5], Step [5494/10336], Loss: 1.3082\n",
      "Epoch [3/5], Step [5496/10336], Loss: 1.0991\n",
      "Epoch [3/5], Step [5498/10336], Loss: 1.4718\n",
      "Epoch [3/5], Step [5500/10336], Loss: 0.2226\n",
      "Epoch [3/5], Step [5502/10336], Loss: 0.3620\n",
      "Epoch [3/5], Step [5504/10336], Loss: 1.9843\n",
      "Epoch [3/5], Step [5506/10336], Loss: 0.0190\n",
      "Epoch [3/5], Step [5508/10336], Loss: 0.4449\n",
      "Epoch [3/5], Step [5510/10336], Loss: 0.0464\n",
      "Epoch [3/5], Step [5512/10336], Loss: 0.5687\n",
      "Epoch [3/5], Step [5514/10336], Loss: 0.1347\n",
      "Epoch [3/5], Step [5516/10336], Loss: 2.8301\n",
      "Epoch [3/5], Step [5518/10336], Loss: 0.0065\n",
      "Epoch [3/5], Step [5520/10336], Loss: 0.0804\n",
      "Epoch [3/5], Step [5522/10336], Loss: 0.0893\n",
      "Epoch [3/5], Step [5524/10336], Loss: 0.0798\n",
      "Epoch [3/5], Step [5526/10336], Loss: 2.7387\n",
      "Epoch [3/5], Step [5528/10336], Loss: 0.0080\n",
      "Epoch [3/5], Step [5530/10336], Loss: 0.1652\n",
      "Epoch [3/5], Step [5532/10336], Loss: 0.1330\n",
      "Epoch [3/5], Step [5534/10336], Loss: 3.8177\n",
      "Epoch [3/5], Step [5536/10336], Loss: 0.9322\n",
      "Epoch [3/5], Step [5538/10336], Loss: 0.2420\n",
      "Epoch [3/5], Step [5540/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [5542/10336], Loss: 0.0715\n",
      "Epoch [3/5], Step [5544/10336], Loss: 0.6099\n",
      "Epoch [3/5], Step [5546/10336], Loss: 0.2797\n",
      "Epoch [3/5], Step [5548/10336], Loss: 0.7448\n",
      "Epoch [3/5], Step [5550/10336], Loss: 0.2207\n",
      "Epoch [3/5], Step [5552/10336], Loss: 0.3717\n",
      "Epoch [3/5], Step [5554/10336], Loss: 0.6753\n",
      "Epoch [3/5], Step [5556/10336], Loss: 0.0032\n",
      "Epoch [3/5], Step [5558/10336], Loss: 3.8923\n",
      "Epoch [3/5], Step [5560/10336], Loss: 0.7518\n",
      "Epoch [3/5], Step [5562/10336], Loss: 0.0673\n",
      "Epoch [3/5], Step [5564/10336], Loss: 0.1290\n",
      "Epoch [3/5], Step [5566/10336], Loss: 3.3292\n",
      "Epoch [3/5], Step [5568/10336], Loss: 0.8497\n",
      "Epoch [3/5], Step [5570/10336], Loss: 0.0538\n",
      "Epoch [3/5], Step [5572/10336], Loss: 0.0028\n",
      "Epoch [3/5], Step [5574/10336], Loss: 1.3728\n",
      "Epoch [3/5], Step [5576/10336], Loss: 2.0712\n",
      "Epoch [3/5], Step [5578/10336], Loss: 0.0664\n",
      "Epoch [3/5], Step [5580/10336], Loss: 0.1354\n",
      "Epoch [3/5], Step [5582/10336], Loss: 0.1073\n",
      "Epoch [3/5], Step [5584/10336], Loss: 0.6563\n",
      "Epoch [3/5], Step [5586/10336], Loss: 0.6875\n",
      "Epoch [3/5], Step [5588/10336], Loss: 0.0173\n",
      "Epoch [3/5], Step [5590/10336], Loss: 0.4729\n",
      "Epoch [3/5], Step [5592/10336], Loss: 0.1515\n",
      "Epoch [3/5], Step [5594/10336], Loss: 1.8638\n",
      "Epoch [3/5], Step [5596/10336], Loss: 4.0512\n",
      "Epoch [3/5], Step [5598/10336], Loss: 0.0020\n",
      "Epoch [3/5], Step [5600/10336], Loss: 0.0067\n",
      "Epoch [3/5], Step [5602/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [5604/10336], Loss: 0.2499\n",
      "Epoch [3/5], Step [5606/10336], Loss: 0.6382\n",
      "Epoch [3/5], Step [5608/10336], Loss: 0.1186\n",
      "Epoch [3/5], Step [5610/10336], Loss: 0.8615\n",
      "Epoch [3/5], Step [5612/10336], Loss: 0.0028\n",
      "Epoch [3/5], Step [5614/10336], Loss: 0.0183\n",
      "Epoch [3/5], Step [5616/10336], Loss: 0.5147\n",
      "Epoch [3/5], Step [5618/10336], Loss: 0.2417\n",
      "Epoch [3/5], Step [5620/10336], Loss: 0.0526\n",
      "Epoch [3/5], Step [5622/10336], Loss: 0.4687\n",
      "Epoch [3/5], Step [5624/10336], Loss: 0.0107\n",
      "Epoch [3/5], Step [5626/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [5628/10336], Loss: 5.5993\n",
      "Epoch [3/5], Step [5630/10336], Loss: 2.4588\n",
      "Epoch [3/5], Step [5632/10336], Loss: 0.4018\n",
      "Epoch [3/5], Step [5634/10336], Loss: 0.0235\n",
      "Epoch [3/5], Step [5636/10336], Loss: 4.7006\n",
      "Epoch [3/5], Step [5638/10336], Loss: 0.0038\n",
      "Epoch [3/5], Step [5640/10336], Loss: 0.2295\n",
      "Epoch [3/5], Step [5642/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [5644/10336], Loss: 0.0815\n",
      "Epoch [3/5], Step [5646/10336], Loss: 0.0152\n",
      "Epoch [3/5], Step [5648/10336], Loss: 2.6431\n",
      "Epoch [3/5], Step [5650/10336], Loss: 0.0839\n",
      "Epoch [3/5], Step [5652/10336], Loss: 2.0284\n",
      "Epoch [3/5], Step [5654/10336], Loss: 0.1977\n",
      "Epoch [3/5], Step [5656/10336], Loss: 0.0134\n",
      "Epoch [3/5], Step [5658/10336], Loss: 0.2150\n",
      "Epoch [3/5], Step [5660/10336], Loss: 0.2591\n",
      "Epoch [3/5], Step [5662/10336], Loss: 1.0081\n",
      "Epoch [3/5], Step [5664/10336], Loss: 0.5725\n",
      "Epoch [3/5], Step [5666/10336], Loss: 0.2604\n",
      "Epoch [3/5], Step [5668/10336], Loss: 0.0298\n",
      "Epoch [3/5], Step [5670/10336], Loss: 0.1570\n",
      "Epoch [3/5], Step [5672/10336], Loss: 0.0444\n",
      "Epoch [3/5], Step [5674/10336], Loss: 1.1933\n",
      "Epoch [3/5], Step [5676/10336], Loss: 0.2516\n",
      "Epoch [3/5], Step [5678/10336], Loss: 1.4262\n",
      "Epoch [3/5], Step [5680/10336], Loss: 0.2104\n",
      "Epoch [3/5], Step [5682/10336], Loss: 1.0729\n",
      "Epoch [3/5], Step [5684/10336], Loss: 0.0067\n",
      "Epoch [3/5], Step [5686/10336], Loss: 4.5001\n",
      "Epoch [3/5], Step [5688/10336], Loss: 0.1246\n",
      "Epoch [3/5], Step [5690/10336], Loss: 0.4401\n",
      "Epoch [3/5], Step [5692/10336], Loss: 0.9958\n",
      "Epoch [3/5], Step [5694/10336], Loss: 0.1818\n",
      "Epoch [3/5], Step [5696/10336], Loss: 0.0119\n",
      "Epoch [3/5], Step [5698/10336], Loss: 0.2739\n",
      "Epoch [3/5], Step [5700/10336], Loss: 0.1231\n",
      "Epoch [3/5], Step [5702/10336], Loss: 0.1510\n",
      "Epoch [3/5], Step [5704/10336], Loss: 0.1810\n",
      "Epoch [3/5], Step [5706/10336], Loss: 0.0095\n",
      "Epoch [3/5], Step [5708/10336], Loss: 0.0944\n",
      "Epoch [3/5], Step [5710/10336], Loss: 0.1884\n",
      "Epoch [3/5], Step [5712/10336], Loss: 0.1251\n",
      "Epoch [3/5], Step [5714/10336], Loss: 0.1293\n",
      "Epoch [3/5], Step [5716/10336], Loss: 0.0518\n",
      "Epoch [3/5], Step [5718/10336], Loss: 0.0869\n",
      "Epoch [3/5], Step [5720/10336], Loss: 0.2425\n",
      "Epoch [3/5], Step [5722/10336], Loss: 0.0938\n",
      "Epoch [3/5], Step [5724/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [5726/10336], Loss: 0.5763\n",
      "Epoch [3/5], Step [5728/10336], Loss: 0.3070\n",
      "Epoch [3/5], Step [5730/10336], Loss: 3.0165\n",
      "Epoch [3/5], Step [5732/10336], Loss: 0.5233\n",
      "Epoch [3/5], Step [5734/10336], Loss: 0.5913\n",
      "Epoch [3/5], Step [5736/10336], Loss: 0.0068\n",
      "Epoch [3/5], Step [5738/10336], Loss: 0.2607\n",
      "Epoch [3/5], Step [5740/10336], Loss: 0.1130\n",
      "Epoch [3/5], Step [5742/10336], Loss: 0.4397\n",
      "Epoch [3/5], Step [5744/10336], Loss: 0.0655\n",
      "Epoch [3/5], Step [5746/10336], Loss: 0.5127\n",
      "Epoch [3/5], Step [5748/10336], Loss: 0.3278\n",
      "Epoch [3/5], Step [5750/10336], Loss: 0.2356\n",
      "Epoch [3/5], Step [5752/10336], Loss: 0.0175\n",
      "Epoch [3/5], Step [5754/10336], Loss: 0.0084\n",
      "Epoch [3/5], Step [5756/10336], Loss: 0.2660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [5758/10336], Loss: 0.2640\n",
      "Epoch [3/5], Step [5760/10336], Loss: 0.1198\n",
      "Epoch [3/5], Step [5762/10336], Loss: 0.7613\n",
      "Epoch [3/5], Step [5764/10336], Loss: 0.0165\n",
      "Epoch [3/5], Step [5766/10336], Loss: 0.1277\n",
      "Epoch [3/5], Step [5768/10336], Loss: 0.3280\n",
      "Epoch [3/5], Step [5770/10336], Loss: 0.0032\n",
      "Epoch [3/5], Step [5772/10336], Loss: 0.2012\n",
      "Epoch [3/5], Step [5774/10336], Loss: 1.6220\n",
      "Epoch [3/5], Step [5776/10336], Loss: 3.2458\n",
      "Epoch [3/5], Step [5778/10336], Loss: 0.2029\n",
      "Epoch [3/5], Step [5780/10336], Loss: 1.0114\n",
      "Epoch [3/5], Step [5782/10336], Loss: 1.8511\n",
      "Epoch [3/5], Step [5784/10336], Loss: 0.1930\n",
      "Epoch [3/5], Step [5786/10336], Loss: 0.0249\n",
      "Epoch [3/5], Step [5788/10336], Loss: 1.0587\n",
      "Epoch [3/5], Step [5790/10336], Loss: 0.1862\n",
      "Epoch [3/5], Step [5792/10336], Loss: 0.2431\n",
      "Epoch [3/5], Step [5794/10336], Loss: 0.3047\n",
      "Epoch [3/5], Step [5796/10336], Loss: 0.0139\n",
      "Epoch [3/5], Step [5798/10336], Loss: 2.0727\n",
      "Epoch [3/5], Step [5800/10336], Loss: 0.1491\n",
      "Epoch [3/5], Step [5802/10336], Loss: 2.0269\n",
      "Epoch [3/5], Step [5804/10336], Loss: 0.1781\n",
      "Epoch [3/5], Step [5806/10336], Loss: 2.0641\n",
      "Epoch [3/5], Step [5808/10336], Loss: 0.2571\n",
      "Epoch [3/5], Step [5810/10336], Loss: 0.4332\n",
      "Epoch [3/5], Step [5812/10336], Loss: 0.1466\n",
      "Epoch [3/5], Step [5814/10336], Loss: 2.6720\n",
      "Epoch [3/5], Step [5816/10336], Loss: 3.2691\n",
      "Epoch [3/5], Step [5818/10336], Loss: 0.7233\n",
      "Epoch [3/5], Step [5820/10336], Loss: 0.0257\n",
      "Epoch [3/5], Step [5822/10336], Loss: 1.0285\n",
      "Epoch [3/5], Step [5824/10336], Loss: 0.8926\n",
      "Epoch [3/5], Step [5826/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [5828/10336], Loss: 0.0426\n",
      "Epoch [3/5], Step [5830/10336], Loss: 2.5977\n",
      "Epoch [3/5], Step [5832/10336], Loss: 0.2366\n",
      "Epoch [3/5], Step [5834/10336], Loss: 1.1281\n",
      "Epoch [3/5], Step [5836/10336], Loss: 0.7749\n",
      "Epoch [3/5], Step [5838/10336], Loss: 0.1546\n",
      "Epoch [3/5], Step [5840/10336], Loss: 0.0075\n",
      "Epoch [3/5], Step [5842/10336], Loss: 0.0603\n",
      "Epoch [3/5], Step [5844/10336], Loss: 0.0022\n",
      "Epoch [3/5], Step [5846/10336], Loss: 0.0488\n",
      "Epoch [3/5], Step [5848/10336], Loss: 1.4686\n",
      "Epoch [3/5], Step [5850/10336], Loss: 0.1586\n",
      "Epoch [3/5], Step [5852/10336], Loss: 0.3954\n",
      "Epoch [3/5], Step [5854/10336], Loss: 0.8992\n",
      "Epoch [3/5], Step [5856/10336], Loss: 1.0696\n",
      "Epoch [3/5], Step [5858/10336], Loss: 0.7168\n",
      "Epoch [3/5], Step [5860/10336], Loss: 0.0067\n",
      "Epoch [3/5], Step [5862/10336], Loss: 3.8733\n",
      "Epoch [3/5], Step [5864/10336], Loss: 0.0962\n",
      "Epoch [3/5], Step [5866/10336], Loss: 1.0964\n",
      "Epoch [3/5], Step [5868/10336], Loss: 0.0222\n",
      "Epoch [3/5], Step [5870/10336], Loss: 1.0937\n",
      "Epoch [3/5], Step [5872/10336], Loss: 0.2346\n",
      "Epoch [3/5], Step [5874/10336], Loss: 0.0167\n",
      "Epoch [3/5], Step [5876/10336], Loss: 0.1459\n",
      "Epoch [3/5], Step [5878/10336], Loss: 0.1447\n",
      "Epoch [3/5], Step [5880/10336], Loss: 0.0323\n",
      "Epoch [3/5], Step [5882/10336], Loss: 2.9296\n",
      "Epoch [3/5], Step [5884/10336], Loss: 0.0683\n",
      "Epoch [3/5], Step [5886/10336], Loss: 0.5498\n",
      "Epoch [3/5], Step [5888/10336], Loss: 0.0259\n",
      "Epoch [3/5], Step [5890/10336], Loss: 0.6151\n",
      "Epoch [3/5], Step [5892/10336], Loss: 0.6171\n",
      "Epoch [3/5], Step [5894/10336], Loss: 1.9187\n",
      "Epoch [3/5], Step [5896/10336], Loss: 0.2060\n",
      "Epoch [3/5], Step [5898/10336], Loss: 1.0591\n",
      "Epoch [3/5], Step [5900/10336], Loss: 4.9597\n",
      "Epoch [3/5], Step [5902/10336], Loss: 2.7091\n",
      "Epoch [3/5], Step [5904/10336], Loss: 2.6553\n",
      "Epoch [3/5], Step [5906/10336], Loss: 0.1242\n",
      "Epoch [3/5], Step [5908/10336], Loss: 0.2573\n",
      "Epoch [3/5], Step [5910/10336], Loss: 0.0982\n",
      "Epoch [3/5], Step [5912/10336], Loss: 0.0132\n",
      "Epoch [3/5], Step [5914/10336], Loss: 0.3950\n",
      "Epoch [3/5], Step [5916/10336], Loss: 0.0084\n",
      "Epoch [3/5], Step [5918/10336], Loss: 0.1210\n",
      "Epoch [3/5], Step [5920/10336], Loss: 0.0905\n",
      "Epoch [3/5], Step [5922/10336], Loss: 0.0685\n",
      "Epoch [3/5], Step [5924/10336], Loss: 0.2682\n",
      "Epoch [3/5], Step [5926/10336], Loss: 0.1052\n",
      "Epoch [3/5], Step [5928/10336], Loss: 0.5536\n",
      "Epoch [3/5], Step [5930/10336], Loss: 0.1863\n",
      "Epoch [3/5], Step [5932/10336], Loss: 0.4019\n",
      "Epoch [3/5], Step [5934/10336], Loss: 2.5772\n",
      "Epoch [3/5], Step [5936/10336], Loss: 0.1796\n",
      "Epoch [3/5], Step [5938/10336], Loss: 3.3926\n",
      "Epoch [3/5], Step [5940/10336], Loss: 0.0652\n",
      "Epoch [3/5], Step [5942/10336], Loss: 0.1030\n",
      "Epoch [3/5], Step [5944/10336], Loss: 0.1876\n",
      "Epoch [3/5], Step [5946/10336], Loss: 1.7699\n",
      "Epoch [3/5], Step [5948/10336], Loss: 0.1588\n",
      "Epoch [3/5], Step [5950/10336], Loss: 0.0109\n",
      "Epoch [3/5], Step [5952/10336], Loss: 0.2039\n",
      "Epoch [3/5], Step [5954/10336], Loss: 0.3081\n",
      "Epoch [3/5], Step [5956/10336], Loss: 0.0305\n",
      "Epoch [3/5], Step [5958/10336], Loss: 0.1476\n",
      "Epoch [3/5], Step [5960/10336], Loss: 0.1415\n",
      "Epoch [3/5], Step [5962/10336], Loss: 2.0533\n",
      "Epoch [3/5], Step [5964/10336], Loss: 0.6863\n",
      "Epoch [3/5], Step [5966/10336], Loss: 0.2046\n",
      "Epoch [3/5], Step [5968/10336], Loss: 0.6103\n",
      "Epoch [3/5], Step [5970/10336], Loss: 0.6410\n",
      "Epoch [3/5], Step [5972/10336], Loss: 0.2647\n",
      "Epoch [3/5], Step [5974/10336], Loss: 0.1966\n",
      "Epoch [3/5], Step [5976/10336], Loss: 0.2195\n",
      "Epoch [3/5], Step [5978/10336], Loss: 1.2955\n",
      "Epoch [3/5], Step [5980/10336], Loss: 0.0412\n",
      "Epoch [3/5], Step [5982/10336], Loss: 0.0787\n",
      "Epoch [3/5], Step [5984/10336], Loss: 0.1498\n",
      "Epoch [3/5], Step [5986/10336], Loss: 1.3078\n",
      "Epoch [3/5], Step [5988/10336], Loss: 0.0416\n",
      "Epoch [3/5], Step [5990/10336], Loss: 0.6532\n",
      "Epoch [3/5], Step [5992/10336], Loss: 0.1306\n",
      "Epoch [3/5], Step [5994/10336], Loss: 0.0672\n",
      "Epoch [3/5], Step [5996/10336], Loss: 3.0981\n",
      "Epoch [3/5], Step [5998/10336], Loss: 0.1789\n",
      "Epoch [3/5], Step [6000/10336], Loss: 6.0542\n",
      "Epoch [3/5], Step [6002/10336], Loss: 0.7065\n",
      "Epoch [3/5], Step [6004/10336], Loss: 1.8174\n",
      "Epoch [3/5], Step [6006/10336], Loss: 0.3822\n",
      "Epoch [3/5], Step [6008/10336], Loss: 0.0567\n",
      "Epoch [3/5], Step [6010/10336], Loss: 0.1194\n",
      "Epoch [3/5], Step [6012/10336], Loss: 0.0607\n",
      "Epoch [3/5], Step [6014/10336], Loss: 0.4519\n",
      "Epoch [3/5], Step [6016/10336], Loss: 0.0899\n",
      "Epoch [3/5], Step [6018/10336], Loss: 0.4088\n",
      "Epoch [3/5], Step [6020/10336], Loss: 0.2659\n",
      "Epoch [3/5], Step [6022/10336], Loss: 0.2256\n",
      "Epoch [3/5], Step [6024/10336], Loss: 0.8195\n",
      "Epoch [3/5], Step [6026/10336], Loss: 1.2697\n",
      "Epoch [3/5], Step [6028/10336], Loss: 1.0151\n",
      "Epoch [3/5], Step [6030/10336], Loss: 0.0039\n",
      "Epoch [3/5], Step [6032/10336], Loss: 0.2684\n",
      "Epoch [3/5], Step [6034/10336], Loss: 0.0028\n",
      "Epoch [3/5], Step [6036/10336], Loss: 0.1814\n",
      "Epoch [3/5], Step [6038/10336], Loss: 0.0121\n",
      "Epoch [3/5], Step [6040/10336], Loss: 0.1035\n",
      "Epoch [3/5], Step [6042/10336], Loss: 0.3988\n",
      "Epoch [3/5], Step [6044/10336], Loss: 2.5068\n",
      "Epoch [3/5], Step [6046/10336], Loss: 0.1303\n",
      "Epoch [3/5], Step [6048/10336], Loss: 4.5727\n",
      "Epoch [3/5], Step [6050/10336], Loss: 3.9799\n",
      "Epoch [3/5], Step [6052/10336], Loss: 0.0782\n",
      "Epoch [3/5], Step [6054/10336], Loss: 0.3070\n",
      "Epoch [3/5], Step [6056/10336], Loss: 0.0122\n",
      "Epoch [3/5], Step [6058/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [6060/10336], Loss: 0.2539\n",
      "Epoch [3/5], Step [6062/10336], Loss: 0.5366\n",
      "Epoch [3/5], Step [6064/10336], Loss: 5.5941\n",
      "Epoch [3/5], Step [6066/10336], Loss: 1.7736\n",
      "Epoch [3/5], Step [6068/10336], Loss: 0.0389\n",
      "Epoch [3/5], Step [6070/10336], Loss: 0.0700\n",
      "Epoch [3/5], Step [6072/10336], Loss: 0.3696\n",
      "Epoch [3/5], Step [6074/10336], Loss: 0.0356\n",
      "Epoch [3/5], Step [6076/10336], Loss: 0.1482\n",
      "Epoch [3/5], Step [6078/10336], Loss: 1.4886\n",
      "Epoch [3/5], Step [6080/10336], Loss: 1.1841\n",
      "Epoch [3/5], Step [6082/10336], Loss: 0.1963\n",
      "Epoch [3/5], Step [6084/10336], Loss: 0.3229\n",
      "Epoch [3/5], Step [6086/10336], Loss: 1.2988\n",
      "Epoch [3/5], Step [6088/10336], Loss: 0.0646\n",
      "Epoch [3/5], Step [6090/10336], Loss: 0.1948\n",
      "Epoch [3/5], Step [6092/10336], Loss: 0.5965\n",
      "Epoch [3/5], Step [6094/10336], Loss: 0.2317\n",
      "Epoch [3/5], Step [6096/10336], Loss: 0.1375\n",
      "Epoch [3/5], Step [6098/10336], Loss: 0.0047\n",
      "Epoch [3/5], Step [6100/10336], Loss: 1.0501\n",
      "Epoch [3/5], Step [6102/10336], Loss: 0.1639\n",
      "Epoch [3/5], Step [6104/10336], Loss: 0.5711\n",
      "Epoch [3/5], Step [6106/10336], Loss: 0.2989\n",
      "Epoch [3/5], Step [6108/10336], Loss: 0.0094\n",
      "Epoch [3/5], Step [6110/10336], Loss: 0.2878\n",
      "Epoch [3/5], Step [6112/10336], Loss: 0.0157\n",
      "Epoch [3/5], Step [6114/10336], Loss: 1.8204\n",
      "Epoch [3/5], Step [6116/10336], Loss: 0.0443\n",
      "Epoch [3/5], Step [6118/10336], Loss: 0.0047\n",
      "Epoch [3/5], Step [6120/10336], Loss: 0.2952\n",
      "Epoch [3/5], Step [6122/10336], Loss: 0.3443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [6124/10336], Loss: 0.1585\n",
      "Epoch [3/5], Step [6126/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [6128/10336], Loss: 0.1904\n",
      "Epoch [3/5], Step [6130/10336], Loss: 0.4733\n",
      "Epoch [3/5], Step [6132/10336], Loss: 3.3728\n",
      "Epoch [3/5], Step [6134/10336], Loss: 1.3638\n",
      "Epoch [3/5], Step [6136/10336], Loss: 1.5296\n",
      "Epoch [3/5], Step [6138/10336], Loss: 2.1518\n",
      "Epoch [3/5], Step [6140/10336], Loss: 0.1646\n",
      "Epoch [3/5], Step [6142/10336], Loss: 0.0237\n",
      "Epoch [3/5], Step [6144/10336], Loss: 0.1635\n",
      "Epoch [3/5], Step [6146/10336], Loss: 0.2382\n",
      "Epoch [3/5], Step [6148/10336], Loss: 0.3554\n",
      "Epoch [3/5], Step [6150/10336], Loss: 3.8538\n",
      "Epoch [3/5], Step [6152/10336], Loss: 0.6715\n",
      "Epoch [3/5], Step [6154/10336], Loss: 2.2256\n",
      "Epoch [3/5], Step [6156/10336], Loss: 0.7528\n",
      "Epoch [3/5], Step [6158/10336], Loss: 0.0045\n",
      "Epoch [3/5], Step [6160/10336], Loss: 11.3374\n",
      "Epoch [3/5], Step [6162/10336], Loss: 1.9252\n",
      "Epoch [3/5], Step [6164/10336], Loss: 0.0057\n",
      "Epoch [3/5], Step [6166/10336], Loss: 0.2214\n",
      "Epoch [3/5], Step [6168/10336], Loss: 0.9444\n",
      "Epoch [3/5], Step [6170/10336], Loss: 1.6270\n",
      "Epoch [3/5], Step [6172/10336], Loss: 0.0100\n",
      "Epoch [3/5], Step [6174/10336], Loss: 0.2243\n",
      "Epoch [3/5], Step [6176/10336], Loss: 0.2499\n",
      "Epoch [3/5], Step [6178/10336], Loss: 0.1111\n",
      "Epoch [3/5], Step [6180/10336], Loss: 0.1186\n",
      "Epoch [3/5], Step [6182/10336], Loss: 2.5976\n",
      "Epoch [3/5], Step [6184/10336], Loss: 0.2747\n",
      "Epoch [3/5], Step [6186/10336], Loss: 0.2231\n",
      "Epoch [3/5], Step [6188/10336], Loss: 0.1336\n",
      "Epoch [3/5], Step [6190/10336], Loss: 0.6137\n",
      "Epoch [3/5], Step [6192/10336], Loss: 0.3175\n",
      "Epoch [3/5], Step [6194/10336], Loss: 1.9465\n",
      "Epoch [3/5], Step [6196/10336], Loss: 0.5302\n",
      "Epoch [3/5], Step [6198/10336], Loss: 0.6767\n",
      "Epoch [3/5], Step [6200/10336], Loss: 1.3273\n",
      "Epoch [3/5], Step [6202/10336], Loss: 1.1493\n",
      "Epoch [3/5], Step [6204/10336], Loss: 0.5398\n",
      "Epoch [3/5], Step [6206/10336], Loss: 0.4065\n",
      "Epoch [3/5], Step [6208/10336], Loss: 0.7386\n",
      "Epoch [3/5], Step [6210/10336], Loss: 0.0478\n",
      "Epoch [3/5], Step [6212/10336], Loss: 0.9797\n",
      "Epoch [3/5], Step [6214/10336], Loss: 2.3523\n",
      "Epoch [3/5], Step [6216/10336], Loss: 0.0025\n",
      "Epoch [3/5], Step [6218/10336], Loss: 1.9038\n",
      "Epoch [3/5], Step [6220/10336], Loss: 1.9095\n",
      "Epoch [3/5], Step [6222/10336], Loss: 0.1441\n",
      "Epoch [3/5], Step [6224/10336], Loss: 1.6208\n",
      "Epoch [3/5], Step [6226/10336], Loss: 1.6754\n",
      "Epoch [3/5], Step [6228/10336], Loss: 0.0343\n",
      "Epoch [3/5], Step [6230/10336], Loss: 0.0709\n",
      "Epoch [3/5], Step [6232/10336], Loss: 0.0315\n",
      "Epoch [3/5], Step [6234/10336], Loss: 0.4171\n",
      "Epoch [3/5], Step [6236/10336], Loss: 0.4062\n",
      "Epoch [3/5], Step [6238/10336], Loss: 0.0021\n",
      "Epoch [3/5], Step [6240/10336], Loss: 2.8761\n",
      "Epoch [3/5], Step [6242/10336], Loss: 1.4086\n",
      "Epoch [3/5], Step [6244/10336], Loss: 0.0025\n",
      "Epoch [3/5], Step [6246/10336], Loss: 0.2041\n",
      "Epoch [3/5], Step [6248/10336], Loss: 0.9783\n",
      "Epoch [3/5], Step [6250/10336], Loss: 0.0775\n",
      "Epoch [3/5], Step [6252/10336], Loss: 0.1018\n",
      "Epoch [3/5], Step [6254/10336], Loss: 1.3210\n",
      "Epoch [3/5], Step [6256/10336], Loss: 0.3487\n",
      "Epoch [3/5], Step [6258/10336], Loss: 0.4847\n",
      "Epoch [3/5], Step [6260/10336], Loss: 0.2160\n",
      "Epoch [3/5], Step [6262/10336], Loss: 1.5217\n",
      "Epoch [3/5], Step [6264/10336], Loss: 0.2088\n",
      "Epoch [3/5], Step [6266/10336], Loss: 0.3790\n",
      "Epoch [3/5], Step [6268/10336], Loss: 4.5282\n",
      "Epoch [3/5], Step [6270/10336], Loss: 0.5268\n",
      "Epoch [3/5], Step [6272/10336], Loss: 0.0281\n",
      "Epoch [3/5], Step [6274/10336], Loss: 0.8413\n",
      "Epoch [3/5], Step [6276/10336], Loss: 2.2213\n",
      "Epoch [3/5], Step [6278/10336], Loss: 0.5497\n",
      "Epoch [3/5], Step [6280/10336], Loss: 0.3134\n",
      "Epoch [3/5], Step [6282/10336], Loss: 1.4521\n",
      "Epoch [3/5], Step [6284/10336], Loss: 0.7057\n",
      "Epoch [3/5], Step [6286/10336], Loss: 1.6086\n",
      "Epoch [3/5], Step [6288/10336], Loss: 0.6989\n",
      "Epoch [3/5], Step [6290/10336], Loss: 4.3352\n",
      "Epoch [3/5], Step [6292/10336], Loss: 0.2359\n",
      "Epoch [3/5], Step [6294/10336], Loss: 0.0058\n",
      "Epoch [3/5], Step [6296/10336], Loss: 1.3907\n",
      "Epoch [3/5], Step [6298/10336], Loss: 0.0140\n",
      "Epoch [3/5], Step [6300/10336], Loss: 2.5828\n",
      "Epoch [3/5], Step [6302/10336], Loss: 0.0108\n",
      "Epoch [3/5], Step [6304/10336], Loss: 0.0981\n",
      "Epoch [3/5], Step [6306/10336], Loss: 0.1256\n",
      "Epoch [3/5], Step [6308/10336], Loss: 4.4389\n",
      "Epoch [3/5], Step [6310/10336], Loss: 1.1828\n",
      "Epoch [3/5], Step [6312/10336], Loss: 0.0245\n",
      "Epoch [3/5], Step [6314/10336], Loss: 0.3241\n",
      "Epoch [3/5], Step [6316/10336], Loss: 1.2887\n",
      "Epoch [3/5], Step [6318/10336], Loss: 0.0043\n",
      "Epoch [3/5], Step [6320/10336], Loss: 0.0907\n",
      "Epoch [3/5], Step [6322/10336], Loss: 1.0928\n",
      "Epoch [3/5], Step [6324/10336], Loss: 0.0134\n",
      "Epoch [3/5], Step [6326/10336], Loss: 0.0516\n",
      "Epoch [3/5], Step [6328/10336], Loss: 0.2259\n",
      "Epoch [3/5], Step [6330/10336], Loss: 3.0468\n",
      "Epoch [3/5], Step [6332/10336], Loss: 0.1101\n",
      "Epoch [3/5], Step [6334/10336], Loss: 0.8282\n",
      "Epoch [3/5], Step [6336/10336], Loss: 1.3752\n",
      "Epoch [3/5], Step [6338/10336], Loss: 0.3215\n",
      "Epoch [3/5], Step [6340/10336], Loss: 0.5780\n",
      "Epoch [3/5], Step [6342/10336], Loss: 0.4904\n",
      "Epoch [3/5], Step [6344/10336], Loss: 5.6143\n",
      "Epoch [3/5], Step [6346/10336], Loss: 3.7011\n",
      "Epoch [3/5], Step [6348/10336], Loss: 2.6705\n",
      "Epoch [3/5], Step [6350/10336], Loss: 0.1613\n",
      "Epoch [3/5], Step [6352/10336], Loss: 1.8262\n",
      "Epoch [3/5], Step [6354/10336], Loss: 1.5613\n",
      "Epoch [3/5], Step [6356/10336], Loss: 0.0286\n",
      "Epoch [3/5], Step [6358/10336], Loss: 0.0412\n",
      "Epoch [3/5], Step [6360/10336], Loss: 1.0710\n",
      "Epoch [3/5], Step [6362/10336], Loss: 0.1519\n",
      "Epoch [3/5], Step [6364/10336], Loss: 0.2782\n",
      "Epoch [3/5], Step [6366/10336], Loss: 0.2991\n",
      "Epoch [3/5], Step [6368/10336], Loss: 0.0914\n",
      "Epoch [3/5], Step [6370/10336], Loss: 0.1560\n",
      "Epoch [3/5], Step [6372/10336], Loss: 0.7628\n",
      "Epoch [3/5], Step [6374/10336], Loss: 0.2762\n",
      "Epoch [3/5], Step [6376/10336], Loss: 0.0083\n",
      "Epoch [3/5], Step [6378/10336], Loss: 0.7269\n",
      "Epoch [3/5], Step [6380/10336], Loss: 0.5168\n",
      "Epoch [3/5], Step [6382/10336], Loss: 0.4000\n",
      "Epoch [3/5], Step [6384/10336], Loss: 0.8316\n",
      "Epoch [3/5], Step [6386/10336], Loss: 0.1253\n",
      "Epoch [3/5], Step [6388/10336], Loss: 0.0094\n",
      "Epoch [3/5], Step [6390/10336], Loss: 0.3401\n",
      "Epoch [3/5], Step [6392/10336], Loss: 0.0574\n",
      "Epoch [3/5], Step [6394/10336], Loss: 0.6109\n",
      "Epoch [3/5], Step [6396/10336], Loss: 1.3280\n",
      "Epoch [3/5], Step [6398/10336], Loss: 0.0589\n",
      "Epoch [3/5], Step [6400/10336], Loss: 0.6689\n",
      "Epoch [3/5], Step [6402/10336], Loss: 0.0736\n",
      "Epoch [3/5], Step [6404/10336], Loss: 0.4800\n",
      "Epoch [3/5], Step [6406/10336], Loss: 1.2094\n",
      "Epoch [3/5], Step [6408/10336], Loss: 0.1471\n",
      "Epoch [3/5], Step [6410/10336], Loss: 1.0117\n",
      "Epoch [3/5], Step [6412/10336], Loss: 0.3744\n",
      "Epoch [3/5], Step [6414/10336], Loss: 0.3825\n",
      "Epoch [3/5], Step [6416/10336], Loss: 0.2334\n",
      "Epoch [3/5], Step [6418/10336], Loss: 0.1248\n",
      "Epoch [3/5], Step [6420/10336], Loss: 1.9198\n",
      "Epoch [3/5], Step [6422/10336], Loss: 1.9703\n",
      "Epoch [3/5], Step [6424/10336], Loss: 0.8110\n",
      "Epoch [3/5], Step [6426/10336], Loss: 0.1946\n",
      "Epoch [3/5], Step [6428/10336], Loss: 3.0251\n",
      "Epoch [3/5], Step [6430/10336], Loss: 0.1762\n",
      "Epoch [3/5], Step [6432/10336], Loss: 0.3359\n",
      "Epoch [3/5], Step [6434/10336], Loss: 0.3828\n",
      "Epoch [3/5], Step [6436/10336], Loss: 0.3202\n",
      "Epoch [3/5], Step [6438/10336], Loss: 0.7016\n",
      "Epoch [3/5], Step [6440/10336], Loss: 0.1725\n",
      "Epoch [3/5], Step [6442/10336], Loss: 0.9577\n",
      "Epoch [3/5], Step [6444/10336], Loss: 0.0188\n",
      "Epoch [3/5], Step [6446/10336], Loss: 0.1980\n",
      "Epoch [3/5], Step [6448/10336], Loss: 0.2180\n",
      "Epoch [3/5], Step [6450/10336], Loss: 0.1235\n",
      "Epoch [3/5], Step [6452/10336], Loss: 0.0175\n",
      "Epoch [3/5], Step [6454/10336], Loss: 0.5126\n",
      "Epoch [3/5], Step [6456/10336], Loss: 0.2115\n",
      "Epoch [3/5], Step [6458/10336], Loss: 3.4982\n",
      "Epoch [3/5], Step [6460/10336], Loss: 0.1968\n",
      "Epoch [3/5], Step [6462/10336], Loss: 0.6148\n",
      "Epoch [3/5], Step [6464/10336], Loss: 0.2213\n",
      "Epoch [3/5], Step [6466/10336], Loss: 0.0035\n",
      "Epoch [3/5], Step [6468/10336], Loss: 0.0919\n",
      "Epoch [3/5], Step [6470/10336], Loss: 0.2884\n",
      "Epoch [3/5], Step [6472/10336], Loss: 0.1432\n",
      "Epoch [3/5], Step [6474/10336], Loss: 0.1434\n",
      "Epoch [3/5], Step [6476/10336], Loss: 0.3903\n",
      "Epoch [3/5], Step [6478/10336], Loss: 0.1272\n",
      "Epoch [3/5], Step [6480/10336], Loss: 0.3044\n",
      "Epoch [3/5], Step [6482/10336], Loss: 0.6384\n",
      "Epoch [3/5], Step [6484/10336], Loss: 0.4189\n",
      "Epoch [3/5], Step [6486/10336], Loss: 0.0055\n",
      "Epoch [3/5], Step [6488/10336], Loss: 1.8598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [6490/10336], Loss: 0.2771\n",
      "Epoch [3/5], Step [6492/10336], Loss: 0.2916\n",
      "Epoch [3/5], Step [6494/10336], Loss: 0.0960\n",
      "Epoch [3/5], Step [6496/10336], Loss: 0.1390\n",
      "Epoch [3/5], Step [6498/10336], Loss: 0.2356\n",
      "Epoch [3/5], Step [6500/10336], Loss: 1.6435\n",
      "Epoch [3/5], Step [6502/10336], Loss: 0.0632\n",
      "Epoch [3/5], Step [6504/10336], Loss: 0.1633\n",
      "Epoch [3/5], Step [6506/10336], Loss: 0.0826\n",
      "Epoch [3/5], Step [6508/10336], Loss: 0.6527\n",
      "Epoch [3/5], Step [6510/10336], Loss: 0.3343\n",
      "Epoch [3/5], Step [6512/10336], Loss: 0.0047\n",
      "Epoch [3/5], Step [6514/10336], Loss: 0.0914\n",
      "Epoch [3/5], Step [6516/10336], Loss: 0.0984\n",
      "Epoch [3/5], Step [6518/10336], Loss: 0.1305\n",
      "Epoch [3/5], Step [6520/10336], Loss: 0.1766\n",
      "Epoch [3/5], Step [6522/10336], Loss: 0.3233\n",
      "Epoch [3/5], Step [6524/10336], Loss: 3.8425\n",
      "Epoch [3/5], Step [6526/10336], Loss: 0.9459\n",
      "Epoch [3/5], Step [6528/10336], Loss: 0.1943\n",
      "Epoch [3/5], Step [6530/10336], Loss: 0.2113\n",
      "Epoch [3/5], Step [6532/10336], Loss: 0.2918\n",
      "Epoch [3/5], Step [6534/10336], Loss: 0.8121\n",
      "Epoch [3/5], Step [6536/10336], Loss: 0.0181\n",
      "Epoch [3/5], Step [6538/10336], Loss: 2.1613\n",
      "Epoch [3/5], Step [6540/10336], Loss: 2.8962\n",
      "Epoch [3/5], Step [6542/10336], Loss: 0.1429\n",
      "Epoch [3/5], Step [6544/10336], Loss: 1.7268\n",
      "Epoch [3/5], Step [6546/10336], Loss: 0.7339\n",
      "Epoch [3/5], Step [6548/10336], Loss: 0.5006\n",
      "Epoch [3/5], Step [6550/10336], Loss: 0.6388\n",
      "Epoch [3/5], Step [6552/10336], Loss: 0.3900\n",
      "Epoch [3/5], Step [6554/10336], Loss: 0.6319\n",
      "Epoch [3/5], Step [6556/10336], Loss: 0.0056\n",
      "Epoch [3/5], Step [6558/10336], Loss: 0.3436\n",
      "Epoch [3/5], Step [6560/10336], Loss: 0.3457\n",
      "Epoch [3/5], Step [6562/10336], Loss: 0.2161\n",
      "Epoch [3/5], Step [6564/10336], Loss: 3.9757\n",
      "Epoch [3/5], Step [6566/10336], Loss: 1.3859\n",
      "Epoch [3/5], Step [6568/10336], Loss: 0.1071\n",
      "Epoch [3/5], Step [6570/10336], Loss: 0.4115\n",
      "Epoch [3/5], Step [6572/10336], Loss: 0.1634\n",
      "Epoch [3/5], Step [6574/10336], Loss: 0.1000\n",
      "Epoch [3/5], Step [6576/10336], Loss: 0.1775\n",
      "Epoch [3/5], Step [6578/10336], Loss: 0.8014\n",
      "Epoch [3/5], Step [6580/10336], Loss: 0.0165\n",
      "Epoch [3/5], Step [6582/10336], Loss: 0.0774\n",
      "Epoch [3/5], Step [6584/10336], Loss: 0.3043\n",
      "Epoch [3/5], Step [6586/10336], Loss: 0.5308\n",
      "Epoch [3/5], Step [6588/10336], Loss: 0.0516\n",
      "Epoch [3/5], Step [6590/10336], Loss: 0.8344\n",
      "Epoch [3/5], Step [6592/10336], Loss: 0.0354\n",
      "Epoch [3/5], Step [6594/10336], Loss: 0.0912\n",
      "Epoch [3/5], Step [6596/10336], Loss: 0.2788\n",
      "Epoch [3/5], Step [6598/10336], Loss: 1.1140\n",
      "Epoch [3/5], Step [6600/10336], Loss: 0.0496\n",
      "Epoch [3/5], Step [6602/10336], Loss: 0.3758\n",
      "Epoch [3/5], Step [6604/10336], Loss: 0.2618\n",
      "Epoch [3/5], Step [6606/10336], Loss: 5.5567\n",
      "Epoch [3/5], Step [6608/10336], Loss: 1.2118\n",
      "Epoch [3/5], Step [6610/10336], Loss: 0.0050\n",
      "Epoch [3/5], Step [6612/10336], Loss: 1.1114\n",
      "Epoch [3/5], Step [6614/10336], Loss: 0.1139\n",
      "Epoch [3/5], Step [6616/10336], Loss: 1.7008\n",
      "Epoch [3/5], Step [6618/10336], Loss: 0.2679\n",
      "Epoch [3/5], Step [6620/10336], Loss: 0.0016\n",
      "Epoch [3/5], Step [6622/10336], Loss: 0.9238\n",
      "Epoch [3/5], Step [6624/10336], Loss: 0.4659\n",
      "Epoch [3/5], Step [6626/10336], Loss: 0.1761\n",
      "Epoch [3/5], Step [6628/10336], Loss: 0.5024\n",
      "Epoch [3/5], Step [6630/10336], Loss: 1.1964\n",
      "Epoch [3/5], Step [6632/10336], Loss: 0.0090\n",
      "Epoch [3/5], Step [6634/10336], Loss: 0.1668\n",
      "Epoch [3/5], Step [6636/10336], Loss: 0.1822\n",
      "Epoch [3/5], Step [6638/10336], Loss: 0.0254\n",
      "Epoch [3/5], Step [6640/10336], Loss: 2.5826\n",
      "Epoch [3/5], Step [6642/10336], Loss: 0.3383\n",
      "Epoch [3/5], Step [6644/10336], Loss: 0.2149\n",
      "Epoch [3/5], Step [6646/10336], Loss: 0.0067\n",
      "Epoch [3/5], Step [6648/10336], Loss: 0.2878\n",
      "Epoch [3/5], Step [6650/10336], Loss: 0.1643\n",
      "Epoch [3/5], Step [6652/10336], Loss: 0.5217\n",
      "Epoch [3/5], Step [6654/10336], Loss: 0.4506\n",
      "Epoch [3/5], Step [6656/10336], Loss: 0.1045\n",
      "Epoch [3/5], Step [6658/10336], Loss: 0.0056\n",
      "Epoch [3/5], Step [6660/10336], Loss: 0.0443\n",
      "Epoch [3/5], Step [6662/10336], Loss: 0.1178\n",
      "Epoch [3/5], Step [6664/10336], Loss: 0.2999\n",
      "Epoch [3/5], Step [6666/10336], Loss: 0.4166\n",
      "Epoch [3/5], Step [6668/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [6670/10336], Loss: 0.0339\n",
      "Epoch [3/5], Step [6672/10336], Loss: 0.5118\n",
      "Epoch [3/5], Step [6674/10336], Loss: 1.4275\n",
      "Epoch [3/5], Step [6676/10336], Loss: 1.0800\n",
      "Epoch [3/5], Step [6678/10336], Loss: 0.8326\n",
      "Epoch [3/5], Step [6680/10336], Loss: 0.0030\n",
      "Epoch [3/5], Step [6682/10336], Loss: 0.7411\n",
      "Epoch [3/5], Step [6684/10336], Loss: 2.5371\n",
      "Epoch [3/5], Step [6686/10336], Loss: 0.0463\n",
      "Epoch [3/5], Step [6688/10336], Loss: 0.1774\n",
      "Epoch [3/5], Step [6690/10336], Loss: 0.4881\n",
      "Epoch [3/5], Step [6692/10336], Loss: 0.0256\n",
      "Epoch [3/5], Step [6694/10336], Loss: 0.0059\n",
      "Epoch [3/5], Step [6696/10336], Loss: 0.3614\n",
      "Epoch [3/5], Step [6698/10336], Loss: 0.8020\n",
      "Epoch [3/5], Step [6700/10336], Loss: 0.1181\n",
      "Epoch [3/5], Step [6702/10336], Loss: 0.2480\n",
      "Epoch [3/5], Step [6704/10336], Loss: 0.0265\n",
      "Epoch [3/5], Step [6706/10336], Loss: 0.7755\n",
      "Epoch [3/5], Step [6708/10336], Loss: 5.7379\n",
      "Epoch [3/5], Step [6710/10336], Loss: 0.8160\n",
      "Epoch [3/5], Step [6712/10336], Loss: 0.2866\n",
      "Epoch [3/5], Step [6714/10336], Loss: 0.1091\n",
      "Epoch [3/5], Step [6716/10336], Loss: 0.0016\n",
      "Epoch [3/5], Step [6718/10336], Loss: 0.3412\n",
      "Epoch [3/5], Step [6720/10336], Loss: 0.0153\n",
      "Epoch [3/5], Step [6722/10336], Loss: 0.0079\n",
      "Epoch [3/5], Step [6724/10336], Loss: 0.1153\n",
      "Epoch [3/5], Step [6726/10336], Loss: 0.0485\n",
      "Epoch [3/5], Step [6728/10336], Loss: 3.8835\n",
      "Epoch [3/5], Step [6730/10336], Loss: 0.1361\n",
      "Epoch [3/5], Step [6732/10336], Loss: 0.1502\n",
      "Epoch [3/5], Step [6734/10336], Loss: 0.0139\n",
      "Epoch [3/5], Step [6736/10336], Loss: 0.0438\n",
      "Epoch [3/5], Step [6738/10336], Loss: 1.3206\n",
      "Epoch [3/5], Step [6740/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [6742/10336], Loss: 0.7271\n",
      "Epoch [3/5], Step [6744/10336], Loss: 0.8955\n",
      "Epoch [3/5], Step [6746/10336], Loss: 0.0177\n",
      "Epoch [3/5], Step [6748/10336], Loss: 0.0012\n",
      "Epoch [3/5], Step [6750/10336], Loss: 0.0556\n",
      "Epoch [3/5], Step [6752/10336], Loss: 0.1427\n",
      "Epoch [3/5], Step [6754/10336], Loss: 0.2183\n",
      "Epoch [3/5], Step [6756/10336], Loss: 2.3786\n",
      "Epoch [3/5], Step [6758/10336], Loss: 2.2744\n",
      "Epoch [3/5], Step [6760/10336], Loss: 0.0263\n",
      "Epoch [3/5], Step [6762/10336], Loss: 0.0961\n",
      "Epoch [3/5], Step [6764/10336], Loss: 0.1557\n",
      "Epoch [3/5], Step [6766/10336], Loss: 0.9958\n",
      "Epoch [3/5], Step [6768/10336], Loss: 0.0298\n",
      "Epoch [3/5], Step [6770/10336], Loss: 1.0669\n",
      "Epoch [3/5], Step [6772/10336], Loss: 0.1597\n",
      "Epoch [3/5], Step [6774/10336], Loss: 0.1511\n",
      "Epoch [3/5], Step [6776/10336], Loss: 0.4666\n",
      "Epoch [3/5], Step [6778/10336], Loss: 1.2611\n",
      "Epoch [3/5], Step [6780/10336], Loss: 0.5640\n",
      "Epoch [3/5], Step [6782/10336], Loss: 2.5435\n",
      "Epoch [3/5], Step [6784/10336], Loss: 0.0458\n",
      "Epoch [3/5], Step [6786/10336], Loss: 0.0156\n",
      "Epoch [3/5], Step [6788/10336], Loss: 0.0275\n",
      "Epoch [3/5], Step [6790/10336], Loss: 0.2855\n",
      "Epoch [3/5], Step [6792/10336], Loss: 0.0689\n",
      "Epoch [3/5], Step [6794/10336], Loss: 0.0767\n",
      "Epoch [3/5], Step [6796/10336], Loss: 2.0260\n",
      "Epoch [3/5], Step [6798/10336], Loss: 0.1271\n",
      "Epoch [3/5], Step [6800/10336], Loss: 0.3841\n",
      "Epoch [3/5], Step [6802/10336], Loss: 0.0535\n",
      "Epoch [3/5], Step [6804/10336], Loss: 0.1066\n",
      "Epoch [3/5], Step [6806/10336], Loss: 4.1559\n",
      "Epoch [3/5], Step [6808/10336], Loss: 0.6266\n",
      "Epoch [3/5], Step [6810/10336], Loss: 0.0389\n",
      "Epoch [3/5], Step [6812/10336], Loss: 1.4589\n",
      "Epoch [3/5], Step [6814/10336], Loss: 4.1000\n",
      "Epoch [3/5], Step [6816/10336], Loss: 3.1557\n",
      "Epoch [3/5], Step [6818/10336], Loss: 1.4127\n",
      "Epoch [3/5], Step [6820/10336], Loss: 2.0491\n",
      "Epoch [3/5], Step [6822/10336], Loss: 0.0197\n",
      "Epoch [3/5], Step [6824/10336], Loss: 0.8271\n",
      "Epoch [3/5], Step [6826/10336], Loss: 1.2440\n",
      "Epoch [3/5], Step [6828/10336], Loss: 0.0660\n",
      "Epoch [3/5], Step [6830/10336], Loss: 0.6089\n",
      "Epoch [3/5], Step [6832/10336], Loss: 0.2499\n",
      "Epoch [3/5], Step [6834/10336], Loss: 0.8069\n",
      "Epoch [3/5], Step [6836/10336], Loss: 1.8624\n",
      "Epoch [3/5], Step [6838/10336], Loss: 0.3119\n",
      "Epoch [3/5], Step [6840/10336], Loss: 0.4899\n",
      "Epoch [3/5], Step [6842/10336], Loss: 3.4895\n",
      "Epoch [3/5], Step [6844/10336], Loss: 0.0028\n",
      "Epoch [3/5], Step [6846/10336], Loss: 0.8661\n",
      "Epoch [3/5], Step [6848/10336], Loss: 1.4565\n",
      "Epoch [3/5], Step [6850/10336], Loss: 0.4177\n",
      "Epoch [3/5], Step [6852/10336], Loss: 0.4143\n",
      "Epoch [3/5], Step [6854/10336], Loss: 0.4082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [6856/10336], Loss: 0.0329\n",
      "Epoch [3/5], Step [6858/10336], Loss: 0.0499\n",
      "Epoch [3/5], Step [6860/10336], Loss: 3.3432\n",
      "Epoch [3/5], Step [6862/10336], Loss: 1.6191\n",
      "Epoch [3/5], Step [6864/10336], Loss: 0.2667\n",
      "Epoch [3/5], Step [6866/10336], Loss: 0.3163\n",
      "Epoch [3/5], Step [6868/10336], Loss: 0.2467\n",
      "Epoch [3/5], Step [6870/10336], Loss: 2.1925\n",
      "Epoch [3/5], Step [6872/10336], Loss: 0.1106\n",
      "Epoch [3/5], Step [6874/10336], Loss: 0.0380\n",
      "Epoch [3/5], Step [6876/10336], Loss: 0.0802\n",
      "Epoch [3/5], Step [6878/10336], Loss: 0.4531\n",
      "Epoch [3/5], Step [6880/10336], Loss: 1.9420\n",
      "Epoch [3/5], Step [6882/10336], Loss: 0.0027\n",
      "Epoch [3/5], Step [6884/10336], Loss: 0.2307\n",
      "Epoch [3/5], Step [6886/10336], Loss: 0.4290\n",
      "Epoch [3/5], Step [6888/10336], Loss: 0.0110\n",
      "Epoch [3/5], Step [6890/10336], Loss: 2.9256\n",
      "Epoch [3/5], Step [6892/10336], Loss: 0.1317\n",
      "Epoch [3/5], Step [6894/10336], Loss: 0.1053\n",
      "Epoch [3/5], Step [6896/10336], Loss: 0.1838\n",
      "Epoch [3/5], Step [6898/10336], Loss: 0.5755\n",
      "Epoch [3/5], Step [6900/10336], Loss: 0.5577\n",
      "Epoch [3/5], Step [6902/10336], Loss: 0.1643\n",
      "Epoch [3/5], Step [6904/10336], Loss: 0.7539\n",
      "Epoch [3/5], Step [6906/10336], Loss: 0.0219\n",
      "Epoch [3/5], Step [6908/10336], Loss: 0.2758\n",
      "Epoch [3/5], Step [6910/10336], Loss: 0.1662\n",
      "Epoch [3/5], Step [6912/10336], Loss: 0.4627\n",
      "Epoch [3/5], Step [6914/10336], Loss: 0.1977\n",
      "Epoch [3/5], Step [6916/10336], Loss: 0.1179\n",
      "Epoch [3/5], Step [6918/10336], Loss: 0.3491\n",
      "Epoch [3/5], Step [6920/10336], Loss: 0.4298\n",
      "Epoch [3/5], Step [6922/10336], Loss: 0.2058\n",
      "Epoch [3/5], Step [6924/10336], Loss: 2.2877\n",
      "Epoch [3/5], Step [6926/10336], Loss: 0.1581\n",
      "Epoch [3/5], Step [6928/10336], Loss: 0.6722\n",
      "Epoch [3/5], Step [6930/10336], Loss: 0.0047\n",
      "Epoch [3/5], Step [6932/10336], Loss: 1.1054\n",
      "Epoch [3/5], Step [6934/10336], Loss: 0.0782\n",
      "Epoch [3/5], Step [6936/10336], Loss: 0.2771\n",
      "Epoch [3/5], Step [6938/10336], Loss: 0.1243\n",
      "Epoch [3/5], Step [6940/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [6942/10336], Loss: 0.2195\n",
      "Epoch [3/5], Step [6944/10336], Loss: 0.1246\n",
      "Epoch [3/5], Step [6946/10336], Loss: 0.0092\n",
      "Epoch [3/5], Step [6948/10336], Loss: 0.8683\n",
      "Epoch [3/5], Step [6950/10336], Loss: 0.8380\n",
      "Epoch [3/5], Step [6952/10336], Loss: 0.0962\n",
      "Epoch [3/5], Step [6954/10336], Loss: 0.3070\n",
      "Epoch [3/5], Step [6956/10336], Loss: 0.2425\n",
      "Epoch [3/5], Step [6958/10336], Loss: 0.0437\n",
      "Epoch [3/5], Step [6960/10336], Loss: 1.6438\n",
      "Epoch [3/5], Step [6962/10336], Loss: 0.1655\n",
      "Epoch [3/5], Step [6964/10336], Loss: 0.2638\n",
      "Epoch [3/5], Step [6966/10336], Loss: 0.1871\n",
      "Epoch [3/5], Step [6968/10336], Loss: 2.4931\n",
      "Epoch [3/5], Step [6970/10336], Loss: 0.0003\n",
      "Epoch [3/5], Step [6972/10336], Loss: 1.1356\n",
      "Epoch [3/5], Step [6974/10336], Loss: 0.2134\n",
      "Epoch [3/5], Step [6976/10336], Loss: 0.0058\n",
      "Epoch [3/5], Step [6978/10336], Loss: 0.7950\n",
      "Epoch [3/5], Step [6980/10336], Loss: 0.2483\n",
      "Epoch [3/5], Step [6982/10336], Loss: 0.1926\n",
      "Epoch [3/5], Step [6984/10336], Loss: 0.3816\n",
      "Epoch [3/5], Step [6986/10336], Loss: 0.0676\n",
      "Epoch [3/5], Step [6988/10336], Loss: 3.5137\n",
      "Epoch [3/5], Step [6990/10336], Loss: 1.3582\n",
      "Epoch [3/5], Step [6992/10336], Loss: 0.4087\n",
      "Epoch [3/5], Step [6994/10336], Loss: 0.0063\n",
      "Epoch [3/5], Step [6996/10336], Loss: 0.0355\n",
      "Epoch [3/5], Step [6998/10336], Loss: 0.9179\n",
      "Epoch [3/5], Step [7000/10336], Loss: 0.5427\n",
      "Epoch [3/5], Step [7002/10336], Loss: 0.0247\n",
      "Epoch [3/5], Step [7004/10336], Loss: 3.5169\n",
      "Epoch [3/5], Step [7006/10336], Loss: 0.1155\n",
      "Epoch [3/5], Step [7008/10336], Loss: 0.2593\n",
      "Epoch [3/5], Step [7010/10336], Loss: 1.8850\n",
      "Epoch [3/5], Step [7012/10336], Loss: 0.0419\n",
      "Epoch [3/5], Step [7014/10336], Loss: 0.2375\n",
      "Epoch [3/5], Step [7016/10336], Loss: 0.1684\n",
      "Epoch [3/5], Step [7018/10336], Loss: 0.1939\n",
      "Epoch [3/5], Step [7020/10336], Loss: 0.5236\n",
      "Epoch [3/5], Step [7022/10336], Loss: 0.1593\n",
      "Epoch [3/5], Step [7024/10336], Loss: 0.0193\n",
      "Epoch [3/5], Step [7026/10336], Loss: 0.4740\n",
      "Epoch [3/5], Step [7028/10336], Loss: 1.5847\n",
      "Epoch [3/5], Step [7030/10336], Loss: 1.1870\n",
      "Epoch [3/5], Step [7032/10336], Loss: 1.5588\n",
      "Epoch [3/5], Step [7034/10336], Loss: 0.2589\n",
      "Epoch [3/5], Step [7036/10336], Loss: 0.2552\n",
      "Epoch [3/5], Step [7038/10336], Loss: 0.1137\n",
      "Epoch [3/5], Step [7040/10336], Loss: 0.0424\n",
      "Epoch [3/5], Step [7042/10336], Loss: 0.2266\n",
      "Epoch [3/5], Step [7044/10336], Loss: 0.3609\n",
      "Epoch [3/5], Step [7046/10336], Loss: 0.3194\n",
      "Epoch [3/5], Step [7048/10336], Loss: 0.2368\n",
      "Epoch [3/5], Step [7050/10336], Loss: 0.0916\n",
      "Epoch [3/5], Step [7052/10336], Loss: 0.1170\n",
      "Epoch [3/5], Step [7054/10336], Loss: 0.0806\n",
      "Epoch [3/5], Step [7056/10336], Loss: 0.6815\n",
      "Epoch [3/5], Step [7058/10336], Loss: 0.0027\n",
      "Epoch [3/5], Step [7060/10336], Loss: 0.0042\n",
      "Epoch [3/5], Step [7062/10336], Loss: 1.1610\n",
      "Epoch [3/5], Step [7064/10336], Loss: 0.1914\n",
      "Epoch [3/5], Step [7066/10336], Loss: 0.6826\n",
      "Epoch [3/5], Step [7068/10336], Loss: 2.1672\n",
      "Epoch [3/5], Step [7070/10336], Loss: 0.1576\n",
      "Epoch [3/5], Step [7072/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [7074/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [7076/10336], Loss: 0.7640\n",
      "Epoch [3/5], Step [7078/10336], Loss: 0.6641\n",
      "Epoch [3/5], Step [7080/10336], Loss: 0.0793\n",
      "Epoch [3/5], Step [7082/10336], Loss: 0.2206\n",
      "Epoch [3/5], Step [7084/10336], Loss: 0.1442\n",
      "Epoch [3/5], Step [7086/10336], Loss: 2.2193\n",
      "Epoch [3/5], Step [7088/10336], Loss: 0.1515\n",
      "Epoch [3/5], Step [7090/10336], Loss: 0.0552\n",
      "Epoch [3/5], Step [7092/10336], Loss: 0.0226\n",
      "Epoch [3/5], Step [7094/10336], Loss: 0.0003\n",
      "Epoch [3/5], Step [7096/10336], Loss: 0.3006\n",
      "Epoch [3/5], Step [7098/10336], Loss: 0.0003\n",
      "Epoch [3/5], Step [7100/10336], Loss: 0.5646\n",
      "Epoch [3/5], Step [7102/10336], Loss: 0.0261\n",
      "Epoch [3/5], Step [7104/10336], Loss: 1.3628\n",
      "Epoch [3/5], Step [7106/10336], Loss: 0.1429\n",
      "Epoch [3/5], Step [7108/10336], Loss: 0.0034\n",
      "Epoch [3/5], Step [7110/10336], Loss: 1.0137\n",
      "Epoch [3/5], Step [7112/10336], Loss: 0.3518\n",
      "Epoch [3/5], Step [7114/10336], Loss: 5.9169\n",
      "Epoch [3/5], Step [7116/10336], Loss: 0.4307\n",
      "Epoch [3/5], Step [7118/10336], Loss: 0.1616\n",
      "Epoch [3/5], Step [7120/10336], Loss: 0.0465\n",
      "Epoch [3/5], Step [7122/10336], Loss: 0.0060\n",
      "Epoch [3/5], Step [7124/10336], Loss: 2.1761\n",
      "Epoch [3/5], Step [7126/10336], Loss: 1.2457\n",
      "Epoch [3/5], Step [7128/10336], Loss: 0.0961\n",
      "Epoch [3/5], Step [7130/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [7132/10336], Loss: 0.8638\n",
      "Epoch [3/5], Step [7134/10336], Loss: 1.0736\n",
      "Epoch [3/5], Step [7136/10336], Loss: 0.4232\n",
      "Epoch [3/5], Step [7138/10336], Loss: 0.0074\n",
      "Epoch [3/5], Step [7140/10336], Loss: 2.3141\n",
      "Epoch [3/5], Step [7142/10336], Loss: 0.4713\n",
      "Epoch [3/5], Step [7144/10336], Loss: 1.7136\n",
      "Epoch [3/5], Step [7146/10336], Loss: 0.4091\n",
      "Epoch [3/5], Step [7148/10336], Loss: 0.3082\n",
      "Epoch [3/5], Step [7150/10336], Loss: 4.6498\n",
      "Epoch [3/5], Step [7152/10336], Loss: 0.1837\n",
      "Epoch [3/5], Step [7154/10336], Loss: 0.7196\n",
      "Epoch [3/5], Step [7156/10336], Loss: 2.3796\n",
      "Epoch [3/5], Step [7158/10336], Loss: 0.4801\n",
      "Epoch [3/5], Step [7160/10336], Loss: 0.4360\n",
      "Epoch [3/5], Step [7162/10336], Loss: 0.2171\n",
      "Epoch [3/5], Step [7164/10336], Loss: 0.8800\n",
      "Epoch [3/5], Step [7166/10336], Loss: 0.3659\n",
      "Epoch [3/5], Step [7168/10336], Loss: 0.1899\n",
      "Epoch [3/5], Step [7170/10336], Loss: 0.1031\n",
      "Epoch [3/5], Step [7172/10336], Loss: 0.0739\n",
      "Epoch [3/5], Step [7174/10336], Loss: 0.0097\n",
      "Epoch [3/5], Step [7176/10336], Loss: 1.4448\n",
      "Epoch [3/5], Step [7178/10336], Loss: 0.7689\n",
      "Epoch [3/5], Step [7180/10336], Loss: 0.4665\n",
      "Epoch [3/5], Step [7182/10336], Loss: 0.0118\n",
      "Epoch [3/5], Step [7184/10336], Loss: 1.0628\n",
      "Epoch [3/5], Step [7186/10336], Loss: 0.5411\n",
      "Epoch [3/5], Step [7188/10336], Loss: 0.1275\n",
      "Epoch [3/5], Step [7190/10336], Loss: 5.6911\n",
      "Epoch [3/5], Step [7192/10336], Loss: 0.0296\n",
      "Epoch [3/5], Step [7194/10336], Loss: 0.2066\n",
      "Epoch [3/5], Step [7196/10336], Loss: 2.3960\n",
      "Epoch [3/5], Step [7198/10336], Loss: 0.2756\n",
      "Epoch [3/5], Step [7200/10336], Loss: 0.1437\n",
      "Epoch [3/5], Step [7202/10336], Loss: 0.1755\n",
      "Epoch [3/5], Step [7204/10336], Loss: 0.0268\n",
      "Epoch [3/5], Step [7206/10336], Loss: 0.5832\n",
      "Epoch [3/5], Step [7208/10336], Loss: 1.6788\n",
      "Epoch [3/5], Step [7210/10336], Loss: 1.4613\n",
      "Epoch [3/5], Step [7212/10336], Loss: 0.0272\n",
      "Epoch [3/5], Step [7214/10336], Loss: 0.5948\n",
      "Epoch [3/5], Step [7216/10336], Loss: 0.2639\n",
      "Epoch [3/5], Step [7218/10336], Loss: 0.0123\n",
      "Epoch [3/5], Step [7220/10336], Loss: 1.9494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [7222/10336], Loss: 1.7742\n",
      "Epoch [3/5], Step [7224/10336], Loss: 0.1182\n",
      "Epoch [3/5], Step [7226/10336], Loss: 0.0751\n",
      "Epoch [3/5], Step [7228/10336], Loss: 0.0847\n",
      "Epoch [3/5], Step [7230/10336], Loss: 1.1857\n",
      "Epoch [3/5], Step [7232/10336], Loss: 0.7599\n",
      "Epoch [3/5], Step [7234/10336], Loss: 0.1769\n",
      "Epoch [3/5], Step [7236/10336], Loss: 1.7187\n",
      "Epoch [3/5], Step [7238/10336], Loss: 0.0088\n",
      "Epoch [3/5], Step [7240/10336], Loss: 1.8388\n",
      "Epoch [3/5], Step [7242/10336], Loss: 0.0071\n",
      "Epoch [3/5], Step [7244/10336], Loss: 0.0031\n",
      "Epoch [3/5], Step [7246/10336], Loss: 0.0024\n",
      "Epoch [3/5], Step [7248/10336], Loss: 0.8682\n",
      "Epoch [3/5], Step [7250/10336], Loss: 0.0014\n",
      "Epoch [3/5], Step [7252/10336], Loss: 0.1413\n",
      "Epoch [3/5], Step [7254/10336], Loss: 0.0138\n",
      "Epoch [3/5], Step [7256/10336], Loss: 6.7033\n",
      "Epoch [3/5], Step [7258/10336], Loss: 0.7394\n",
      "Epoch [3/5], Step [7260/10336], Loss: 1.8750\n",
      "Epoch [3/5], Step [7262/10336], Loss: 1.1236\n",
      "Epoch [3/5], Step [7264/10336], Loss: 0.1808\n",
      "Epoch [3/5], Step [7266/10336], Loss: 0.9870\n",
      "Epoch [3/5], Step [7268/10336], Loss: 0.3861\n",
      "Epoch [3/5], Step [7270/10336], Loss: 2.2300\n",
      "Epoch [3/5], Step [7272/10336], Loss: 0.0048\n",
      "Epoch [3/5], Step [7274/10336], Loss: 0.3529\n",
      "Epoch [3/5], Step [7276/10336], Loss: 0.7580\n",
      "Epoch [3/5], Step [7278/10336], Loss: 0.0111\n",
      "Epoch [3/5], Step [7280/10336], Loss: 0.3140\n",
      "Epoch [3/5], Step [7282/10336], Loss: 2.4641\n",
      "Epoch [3/5], Step [7284/10336], Loss: 0.0197\n",
      "Epoch [3/5], Step [7286/10336], Loss: 0.3888\n",
      "Epoch [3/5], Step [7288/10336], Loss: 0.7854\n",
      "Epoch [3/5], Step [7290/10336], Loss: 0.0353\n",
      "Epoch [3/5], Step [7292/10336], Loss: 1.2526\n",
      "Epoch [3/5], Step [7294/10336], Loss: 0.0011\n",
      "Epoch [3/5], Step [7296/10336], Loss: 0.1768\n",
      "Epoch [3/5], Step [7298/10336], Loss: 0.2264\n",
      "Epoch [3/5], Step [7300/10336], Loss: 0.1707\n",
      "Epoch [3/5], Step [7302/10336], Loss: 0.2742\n",
      "Epoch [3/5], Step [7304/10336], Loss: 0.2473\n",
      "Epoch [3/5], Step [7306/10336], Loss: 3.6828\n",
      "Epoch [3/5], Step [7308/10336], Loss: 2.0131\n",
      "Epoch [3/5], Step [7310/10336], Loss: 1.5944\n",
      "Epoch [3/5], Step [7312/10336], Loss: 0.8255\n",
      "Epoch [3/5], Step [7314/10336], Loss: 1.6024\n",
      "Epoch [3/5], Step [7316/10336], Loss: 0.4211\n",
      "Epoch [3/5], Step [7318/10336], Loss: 1.2480\n",
      "Epoch [3/5], Step [7320/10336], Loss: 0.0419\n",
      "Epoch [3/5], Step [7322/10336], Loss: 0.8045\n",
      "Epoch [3/5], Step [7324/10336], Loss: 0.2121\n",
      "Epoch [3/5], Step [7326/10336], Loss: 0.8858\n",
      "Epoch [3/5], Step [7328/10336], Loss: 0.2271\n",
      "Epoch [3/5], Step [7330/10336], Loss: 0.0254\n",
      "Epoch [3/5], Step [7332/10336], Loss: 0.2343\n",
      "Epoch [3/5], Step [7334/10336], Loss: 0.2567\n",
      "Epoch [3/5], Step [7336/10336], Loss: 1.2768\n",
      "Epoch [3/5], Step [7338/10336], Loss: 5.1942\n",
      "Epoch [3/5], Step [7340/10336], Loss: 0.0023\n",
      "Epoch [3/5], Step [7342/10336], Loss: 0.1069\n",
      "Epoch [3/5], Step [7344/10336], Loss: 4.1057\n",
      "Epoch [3/5], Step [7346/10336], Loss: 0.0974\n",
      "Epoch [3/5], Step [7348/10336], Loss: 0.2558\n",
      "Epoch [3/5], Step [7350/10336], Loss: 0.0011\n",
      "Epoch [3/5], Step [7352/10336], Loss: 0.0193\n",
      "Epoch [3/5], Step [7354/10336], Loss: 1.2599\n",
      "Epoch [3/5], Step [7356/10336], Loss: 2.1760\n",
      "Epoch [3/5], Step [7358/10336], Loss: 0.6261\n",
      "Epoch [3/5], Step [7360/10336], Loss: 2.1637\n",
      "Epoch [3/5], Step [7362/10336], Loss: 0.1474\n",
      "Epoch [3/5], Step [7364/10336], Loss: 0.2108\n",
      "Epoch [3/5], Step [7366/10336], Loss: 0.0042\n",
      "Epoch [3/5], Step [7368/10336], Loss: 3.2521\n",
      "Epoch [3/5], Step [7370/10336], Loss: 4.2305\n",
      "Epoch [3/5], Step [7372/10336], Loss: 0.3461\n",
      "Epoch [3/5], Step [7374/10336], Loss: 1.7909\n",
      "Epoch [3/5], Step [7376/10336], Loss: 1.2445\n",
      "Epoch [3/5], Step [7378/10336], Loss: 0.3636\n",
      "Epoch [3/5], Step [7380/10336], Loss: 3.3600\n",
      "Epoch [3/5], Step [7382/10336], Loss: 0.4026\n",
      "Epoch [3/5], Step [7384/10336], Loss: 0.0442\n",
      "Epoch [3/5], Step [7386/10336], Loss: 0.1985\n",
      "Epoch [3/5], Step [7388/10336], Loss: 0.3821\n",
      "Epoch [3/5], Step [7390/10336], Loss: 0.4571\n",
      "Epoch [3/5], Step [7392/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [7394/10336], Loss: 0.7368\n",
      "Epoch [3/5], Step [7396/10336], Loss: 1.0173\n",
      "Epoch [3/5], Step [7398/10336], Loss: 2.0744\n",
      "Epoch [3/5], Step [7400/10336], Loss: 0.3523\n",
      "Epoch [3/5], Step [7402/10336], Loss: 0.1836\n",
      "Epoch [3/5], Step [7404/10336], Loss: 0.0076\n",
      "Epoch [3/5], Step [7406/10336], Loss: 0.0044\n",
      "Epoch [3/5], Step [7408/10336], Loss: 0.0020\n",
      "Epoch [3/5], Step [7410/10336], Loss: 0.3596\n",
      "Epoch [3/5], Step [7412/10336], Loss: 0.7824\n",
      "Epoch [3/5], Step [7414/10336], Loss: 0.2944\n",
      "Epoch [3/5], Step [7416/10336], Loss: 0.2941\n",
      "Epoch [3/5], Step [7418/10336], Loss: 0.2031\n",
      "Epoch [3/5], Step [7420/10336], Loss: 0.9731\n",
      "Epoch [3/5], Step [7422/10336], Loss: 0.2808\n",
      "Epoch [3/5], Step [7424/10336], Loss: 1.4074\n",
      "Epoch [3/5], Step [7426/10336], Loss: 0.4184\n",
      "Epoch [3/5], Step [7428/10336], Loss: 0.0124\n",
      "Epoch [3/5], Step [7430/10336], Loss: 0.1395\n",
      "Epoch [3/5], Step [7432/10336], Loss: 0.3726\n",
      "Epoch [3/5], Step [7434/10336], Loss: 0.3116\n",
      "Epoch [3/5], Step [7436/10336], Loss: 2.1632\n",
      "Epoch [3/5], Step [7438/10336], Loss: 0.5261\n",
      "Epoch [3/5], Step [7440/10336], Loss: 0.0266\n",
      "Epoch [3/5], Step [7442/10336], Loss: 0.0064\n",
      "Epoch [3/5], Step [7444/10336], Loss: 0.2996\n",
      "Epoch [3/5], Step [7446/10336], Loss: 3.5023\n",
      "Epoch [3/5], Step [7448/10336], Loss: 0.0865\n",
      "Epoch [3/5], Step [7450/10336], Loss: 0.8417\n",
      "Epoch [3/5], Step [7452/10336], Loss: 1.0522\n",
      "Epoch [3/5], Step [7454/10336], Loss: 2.0301\n",
      "Epoch [3/5], Step [7456/10336], Loss: 3.5628\n",
      "Epoch [3/5], Step [7458/10336], Loss: 0.1932\n",
      "Epoch [3/5], Step [7460/10336], Loss: 0.6682\n",
      "Epoch [3/5], Step [7462/10336], Loss: 0.2768\n",
      "Epoch [3/5], Step [7464/10336], Loss: 1.3066\n",
      "Epoch [3/5], Step [7466/10336], Loss: 0.0008\n",
      "Epoch [3/5], Step [7468/10336], Loss: 0.0374\n",
      "Epoch [3/5], Step [7470/10336], Loss: 2.3380\n",
      "Epoch [3/5], Step [7472/10336], Loss: 0.5027\n",
      "Epoch [3/5], Step [7474/10336], Loss: 0.4103\n",
      "Epoch [3/5], Step [7476/10336], Loss: 0.0345\n",
      "Epoch [3/5], Step [7478/10336], Loss: 0.2896\n",
      "Epoch [3/5], Step [7480/10336], Loss: 0.0087\n",
      "Epoch [3/5], Step [7482/10336], Loss: 1.0780\n",
      "Epoch [3/5], Step [7484/10336], Loss: 0.0027\n",
      "Epoch [3/5], Step [7486/10336], Loss: 1.8519\n",
      "Epoch [3/5], Step [7488/10336], Loss: 1.1204\n",
      "Epoch [3/5], Step [7490/10336], Loss: 0.2523\n",
      "Epoch [3/5], Step [7492/10336], Loss: 0.0147\n",
      "Epoch [3/5], Step [7494/10336], Loss: 0.0625\n",
      "Epoch [3/5], Step [7496/10336], Loss: 0.0397\n",
      "Epoch [3/5], Step [7498/10336], Loss: 0.0175\n",
      "Epoch [3/5], Step [7500/10336], Loss: 0.6504\n",
      "Epoch [3/5], Step [7502/10336], Loss: 0.9449\n",
      "Epoch [3/5], Step [7504/10336], Loss: 0.1251\n",
      "Epoch [3/5], Step [7506/10336], Loss: 0.2782\n",
      "Epoch [3/5], Step [7508/10336], Loss: 2.5873\n",
      "Epoch [3/5], Step [7510/10336], Loss: 0.0031\n",
      "Epoch [3/5], Step [7512/10336], Loss: 0.8414\n",
      "Epoch [3/5], Step [7514/10336], Loss: 0.6716\n",
      "Epoch [3/5], Step [7516/10336], Loss: 0.0304\n",
      "Epoch [3/5], Step [7518/10336], Loss: 0.0264\n",
      "Epoch [3/5], Step [7520/10336], Loss: 0.1237\n",
      "Epoch [3/5], Step [7522/10336], Loss: 0.3007\n",
      "Epoch [3/5], Step [7524/10336], Loss: 0.3960\n",
      "Epoch [3/5], Step [7526/10336], Loss: 0.1372\n",
      "Epoch [3/5], Step [7528/10336], Loss: 0.0273\n",
      "Epoch [3/5], Step [7530/10336], Loss: 0.0036\n",
      "Epoch [3/5], Step [7532/10336], Loss: 0.1285\n",
      "Epoch [3/5], Step [7534/10336], Loss: 0.0505\n",
      "Epoch [3/5], Step [7536/10336], Loss: 0.9360\n",
      "Epoch [3/5], Step [7538/10336], Loss: 0.6606\n",
      "Epoch [3/5], Step [7540/10336], Loss: 0.0745\n",
      "Epoch [3/5], Step [7542/10336], Loss: 0.3571\n",
      "Epoch [3/5], Step [7544/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [7546/10336], Loss: 1.0638\n",
      "Epoch [3/5], Step [7548/10336], Loss: 0.5987\n",
      "Epoch [3/5], Step [7550/10336], Loss: 0.1906\n",
      "Epoch [3/5], Step [7552/10336], Loss: 1.1794\n",
      "Epoch [3/5], Step [7554/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [7556/10336], Loss: 1.0206\n",
      "Epoch [3/5], Step [7558/10336], Loss: 3.1903\n",
      "Epoch [3/5], Step [7560/10336], Loss: 0.0253\n",
      "Epoch [3/5], Step [7562/10336], Loss: 2.8806\n",
      "Epoch [3/5], Step [7564/10336], Loss: 1.4968\n",
      "Epoch [3/5], Step [7566/10336], Loss: 0.0240\n",
      "Epoch [3/5], Step [7568/10336], Loss: 3.5712\n",
      "Epoch [3/5], Step [7570/10336], Loss: 0.5085\n",
      "Epoch [3/5], Step [7572/10336], Loss: 0.3921\n",
      "Epoch [3/5], Step [7574/10336], Loss: 1.7662\n",
      "Epoch [3/5], Step [7576/10336], Loss: 0.2757\n",
      "Epoch [3/5], Step [7578/10336], Loss: 0.5874\n",
      "Epoch [3/5], Step [7580/10336], Loss: 0.1679\n",
      "Epoch [3/5], Step [7582/10336], Loss: 0.3492\n",
      "Epoch [3/5], Step [7584/10336], Loss: 0.1896\n",
      "Epoch [3/5], Step [7586/10336], Loss: 0.9574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [7588/10336], Loss: 0.3218\n",
      "Epoch [3/5], Step [7590/10336], Loss: 1.1260\n",
      "Epoch [3/5], Step [7592/10336], Loss: 0.1823\n",
      "Epoch [3/5], Step [7594/10336], Loss: 1.0768\n",
      "Epoch [3/5], Step [7596/10336], Loss: 0.6331\n",
      "Epoch [3/5], Step [7598/10336], Loss: 0.0201\n",
      "Epoch [3/5], Step [7600/10336], Loss: 1.6501\n",
      "Epoch [3/5], Step [7602/10336], Loss: 3.0156\n",
      "Epoch [3/5], Step [7604/10336], Loss: 0.1357\n",
      "Epoch [3/5], Step [7606/10336], Loss: 1.5654\n",
      "Epoch [3/5], Step [7608/10336], Loss: 0.1081\n",
      "Epoch [3/5], Step [7610/10336], Loss: 1.6641\n",
      "Epoch [3/5], Step [7612/10336], Loss: 0.7298\n",
      "Epoch [3/5], Step [7614/10336], Loss: 0.0391\n",
      "Epoch [3/5], Step [7616/10336], Loss: 0.6600\n",
      "Epoch [3/5], Step [7618/10336], Loss: 0.0155\n",
      "Epoch [3/5], Step [7620/10336], Loss: 0.3898\n",
      "Epoch [3/5], Step [7622/10336], Loss: 6.6537\n",
      "Epoch [3/5], Step [7624/10336], Loss: 0.0701\n",
      "Epoch [3/5], Step [7626/10336], Loss: 0.7996\n",
      "Epoch [3/5], Step [7628/10336], Loss: 0.2743\n",
      "Epoch [3/5], Step [7630/10336], Loss: 0.3627\n",
      "Epoch [3/5], Step [7632/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [7634/10336], Loss: 0.1133\n",
      "Epoch [3/5], Step [7636/10336], Loss: 0.0454\n",
      "Epoch [3/5], Step [7638/10336], Loss: 0.0366\n",
      "Epoch [3/5], Step [7640/10336], Loss: 0.6934\n",
      "Epoch [3/5], Step [7642/10336], Loss: 2.1905\n",
      "Epoch [3/5], Step [7644/10336], Loss: 1.8246\n",
      "Epoch [3/5], Step [7646/10336], Loss: 0.3852\n",
      "Epoch [3/5], Step [7648/10336], Loss: 0.2176\n",
      "Epoch [3/5], Step [7650/10336], Loss: 0.0511\n",
      "Epoch [3/5], Step [7652/10336], Loss: 1.4288\n",
      "Epoch [3/5], Step [7654/10336], Loss: 0.0945\n",
      "Epoch [3/5], Step [7656/10336], Loss: 0.0584\n",
      "Epoch [3/5], Step [7658/10336], Loss: 0.2019\n",
      "Epoch [3/5], Step [7660/10336], Loss: 0.0022\n",
      "Epoch [3/5], Step [7662/10336], Loss: 0.2016\n",
      "Epoch [3/5], Step [7664/10336], Loss: 0.3499\n",
      "Epoch [3/5], Step [7666/10336], Loss: 0.0215\n",
      "Epoch [3/5], Step [7668/10336], Loss: 2.9220\n",
      "Epoch [3/5], Step [7670/10336], Loss: 0.0508\n",
      "Epoch [3/5], Step [7672/10336], Loss: 0.0248\n",
      "Epoch [3/5], Step [7674/10336], Loss: 0.0191\n",
      "Epoch [3/5], Step [7676/10336], Loss: 0.1207\n",
      "Epoch [3/5], Step [7678/10336], Loss: 0.1994\n",
      "Epoch [3/5], Step [7680/10336], Loss: 0.0874\n",
      "Epoch [3/5], Step [7682/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [7684/10336], Loss: 0.2320\n",
      "Epoch [3/5], Step [7686/10336], Loss: 0.1501\n",
      "Epoch [3/5], Step [7688/10336], Loss: 0.1093\n",
      "Epoch [3/5], Step [7690/10336], Loss: 0.0561\n",
      "Epoch [3/5], Step [7692/10336], Loss: 0.0566\n",
      "Epoch [3/5], Step [7694/10336], Loss: 0.3330\n",
      "Epoch [3/5], Step [7696/10336], Loss: 0.5497\n",
      "Epoch [3/5], Step [7698/10336], Loss: 0.1858\n",
      "Epoch [3/5], Step [7700/10336], Loss: 0.1711\n",
      "Epoch [3/5], Step [7702/10336], Loss: 0.0459\n",
      "Epoch [3/5], Step [7704/10336], Loss: 1.5730\n",
      "Epoch [3/5], Step [7706/10336], Loss: 0.2375\n",
      "Epoch [3/5], Step [7708/10336], Loss: 0.5240\n",
      "Epoch [3/5], Step [7710/10336], Loss: 1.7627\n",
      "Epoch [3/5], Step [7712/10336], Loss: 0.0074\n",
      "Epoch [3/5], Step [7714/10336], Loss: 0.4438\n",
      "Epoch [3/5], Step [7716/10336], Loss: 0.1328\n",
      "Epoch [3/5], Step [7718/10336], Loss: 2.8520\n",
      "Epoch [3/5], Step [7720/10336], Loss: 0.4079\n",
      "Epoch [3/5], Step [7722/10336], Loss: 0.0002\n",
      "Epoch [3/5], Step [7724/10336], Loss: 0.0789\n",
      "Epoch [3/5], Step [7726/10336], Loss: 0.4686\n",
      "Epoch [3/5], Step [7728/10336], Loss: 1.3542\n",
      "Epoch [3/5], Step [7730/10336], Loss: 0.3521\n",
      "Epoch [3/5], Step [7732/10336], Loss: 0.6464\n",
      "Epoch [3/5], Step [7734/10336], Loss: 0.0350\n",
      "Epoch [3/5], Step [7736/10336], Loss: 2.3711\n",
      "Epoch [3/5], Step [7738/10336], Loss: 0.0863\n",
      "Epoch [3/5], Step [7740/10336], Loss: 1.9187\n",
      "Epoch [3/5], Step [7742/10336], Loss: 0.3299\n",
      "Epoch [3/5], Step [7744/10336], Loss: 0.8335\n",
      "Epoch [3/5], Step [7746/10336], Loss: 3.7430\n",
      "Epoch [3/5], Step [7748/10336], Loss: 0.0630\n",
      "Epoch [3/5], Step [7750/10336], Loss: 0.0016\n",
      "Epoch [3/5], Step [7752/10336], Loss: 0.3272\n",
      "Epoch [3/5], Step [7754/10336], Loss: 0.4950\n",
      "Epoch [3/5], Step [7756/10336], Loss: 2.1046\n",
      "Epoch [3/5], Step [7758/10336], Loss: 0.0027\n",
      "Epoch [3/5], Step [7760/10336], Loss: 1.7684\n",
      "Epoch [3/5], Step [7762/10336], Loss: 0.0931\n",
      "Epoch [3/5], Step [7764/10336], Loss: 0.2239\n",
      "Epoch [3/5], Step [7766/10336], Loss: 0.0008\n",
      "Epoch [3/5], Step [7768/10336], Loss: 0.4185\n",
      "Epoch [3/5], Step [7770/10336], Loss: 0.5316\n",
      "Epoch [3/5], Step [7772/10336], Loss: 0.2312\n",
      "Epoch [3/5], Step [7774/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [7776/10336], Loss: 1.3696\n",
      "Epoch [3/5], Step [7778/10336], Loss: 0.0254\n",
      "Epoch [3/5], Step [7780/10336], Loss: 0.2233\n",
      "Epoch [3/5], Step [7782/10336], Loss: 0.2512\n",
      "Epoch [3/5], Step [7784/10336], Loss: 0.0831\n",
      "Epoch [3/5], Step [7786/10336], Loss: 0.0168\n",
      "Epoch [3/5], Step [7788/10336], Loss: 3.7658\n",
      "Epoch [3/5], Step [7790/10336], Loss: 0.1045\n",
      "Epoch [3/5], Step [7792/10336], Loss: 1.1503\n",
      "Epoch [3/5], Step [7794/10336], Loss: 0.0165\n",
      "Epoch [3/5], Step [7796/10336], Loss: 0.0724\n",
      "Epoch [3/5], Step [7798/10336], Loss: 0.7043\n",
      "Epoch [3/5], Step [7800/10336], Loss: 0.3145\n",
      "Epoch [3/5], Step [7802/10336], Loss: 0.6666\n",
      "Epoch [3/5], Step [7804/10336], Loss: 0.0031\n",
      "Epoch [3/5], Step [7806/10336], Loss: 0.1039\n",
      "Epoch [3/5], Step [7808/10336], Loss: 0.8355\n",
      "Epoch [3/5], Step [7810/10336], Loss: 0.5293\n",
      "Epoch [3/5], Step [7812/10336], Loss: 0.0903\n",
      "Epoch [3/5], Step [7814/10336], Loss: 1.1102\n",
      "Epoch [3/5], Step [7816/10336], Loss: 0.2211\n",
      "Epoch [3/5], Step [7818/10336], Loss: 0.5771\n",
      "Epoch [3/5], Step [7820/10336], Loss: 0.1580\n",
      "Epoch [3/5], Step [7822/10336], Loss: 0.0305\n",
      "Epoch [3/5], Step [7824/10336], Loss: 0.2261\n",
      "Epoch [3/5], Step [7826/10336], Loss: 0.3023\n",
      "Epoch [3/5], Step [7828/10336], Loss: 0.4411\n",
      "Epoch [3/5], Step [7830/10336], Loss: 0.8741\n",
      "Epoch [3/5], Step [7832/10336], Loss: 0.7254\n",
      "Epoch [3/5], Step [7834/10336], Loss: 0.6952\n",
      "Epoch [3/5], Step [7836/10336], Loss: 0.1886\n",
      "Epoch [3/5], Step [7838/10336], Loss: 1.1470\n",
      "Epoch [3/5], Step [7840/10336], Loss: 0.1134\n",
      "Epoch [3/5], Step [7842/10336], Loss: 0.1335\n",
      "Epoch [3/5], Step [7844/10336], Loss: 3.3042\n",
      "Epoch [3/5], Step [7846/10336], Loss: 3.4602\n",
      "Epoch [3/5], Step [7848/10336], Loss: 1.3741\n",
      "Epoch [3/5], Step [7850/10336], Loss: 2.5218\n",
      "Epoch [3/5], Step [7852/10336], Loss: 0.6435\n",
      "Epoch [3/5], Step [7854/10336], Loss: 2.0472\n",
      "Epoch [3/5], Step [7856/10336], Loss: 1.5840\n",
      "Epoch [3/5], Step [7858/10336], Loss: 0.5917\n",
      "Epoch [3/5], Step [7860/10336], Loss: 1.5486\n",
      "Epoch [3/5], Step [7862/10336], Loss: 1.0182\n",
      "Epoch [3/5], Step [7864/10336], Loss: 2.6922\n",
      "Epoch [3/5], Step [7866/10336], Loss: 0.0072\n",
      "Epoch [3/5], Step [7868/10336], Loss: 0.9206\n",
      "Epoch [3/5], Step [7870/10336], Loss: 0.0685\n",
      "Epoch [3/5], Step [7872/10336], Loss: 0.4091\n",
      "Epoch [3/5], Step [7874/10336], Loss: 2.3573\n",
      "Epoch [3/5], Step [7876/10336], Loss: 3.6658\n",
      "Epoch [3/5], Step [7878/10336], Loss: 0.0329\n",
      "Epoch [3/5], Step [7880/10336], Loss: 0.2524\n",
      "Epoch [3/5], Step [7882/10336], Loss: 0.2228\n",
      "Epoch [3/5], Step [7884/10336], Loss: 1.9006\n",
      "Epoch [3/5], Step [7886/10336], Loss: 0.0082\n",
      "Epoch [3/5], Step [7888/10336], Loss: 3.6122\n",
      "Epoch [3/5], Step [7890/10336], Loss: 1.0187\n",
      "Epoch [3/5], Step [7892/10336], Loss: 0.2217\n",
      "Epoch [3/5], Step [7894/10336], Loss: 0.0172\n",
      "Epoch [3/5], Step [7896/10336], Loss: 0.0416\n",
      "Epoch [3/5], Step [7898/10336], Loss: 0.4833\n",
      "Epoch [3/5], Step [7900/10336], Loss: 0.4930\n",
      "Epoch [3/5], Step [7902/10336], Loss: 0.9434\n",
      "Epoch [3/5], Step [7904/10336], Loss: 0.0452\n",
      "Epoch [3/5], Step [7906/10336], Loss: 0.0161\n",
      "Epoch [3/5], Step [7908/10336], Loss: 1.6602\n",
      "Epoch [3/5], Step [7910/10336], Loss: 2.5895\n",
      "Epoch [3/5], Step [7912/10336], Loss: 0.1851\n",
      "Epoch [3/5], Step [7914/10336], Loss: 0.2247\n",
      "Epoch [3/5], Step [7916/10336], Loss: 0.2994\n",
      "Epoch [3/5], Step [7918/10336], Loss: 0.5086\n",
      "Epoch [3/5], Step [7920/10336], Loss: 0.6617\n",
      "Epoch [3/5], Step [7922/10336], Loss: 0.7502\n",
      "Epoch [3/5], Step [7924/10336], Loss: 3.4600\n",
      "Epoch [3/5], Step [7926/10336], Loss: 0.0016\n",
      "Epoch [3/5], Step [7928/10336], Loss: 0.5153\n",
      "Epoch [3/5], Step [7930/10336], Loss: 0.2300\n",
      "Epoch [3/5], Step [7932/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [7934/10336], Loss: 0.4373\n",
      "Epoch [3/5], Step [7936/10336], Loss: 0.0484\n",
      "Epoch [3/5], Step [7938/10336], Loss: 0.8610\n",
      "Epoch [3/5], Step [7940/10336], Loss: 0.1507\n",
      "Epoch [3/5], Step [7942/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [7944/10336], Loss: 0.6640\n",
      "Epoch [3/5], Step [7946/10336], Loss: 2.4478\n",
      "Epoch [3/5], Step [7948/10336], Loss: 0.9626\n",
      "Epoch [3/5], Step [7950/10336], Loss: 0.9360\n",
      "Epoch [3/5], Step [7952/10336], Loss: 0.0531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [7954/10336], Loss: 0.3686\n",
      "Epoch [3/5], Step [7956/10336], Loss: 0.0537\n",
      "Epoch [3/5], Step [7958/10336], Loss: 0.0048\n",
      "Epoch [3/5], Step [7960/10336], Loss: 0.0731\n",
      "Epoch [3/5], Step [7962/10336], Loss: 0.0175\n",
      "Epoch [3/5], Step [7964/10336], Loss: 0.0083\n",
      "Epoch [3/5], Step [7966/10336], Loss: 0.3742\n",
      "Epoch [3/5], Step [7968/10336], Loss: 0.0476\n",
      "Epoch [3/5], Step [7970/10336], Loss: 0.5476\n",
      "Epoch [3/5], Step [7972/10336], Loss: 0.0975\n",
      "Epoch [3/5], Step [7974/10336], Loss: 0.4032\n",
      "Epoch [3/5], Step [7976/10336], Loss: 0.0107\n",
      "Epoch [3/5], Step [7978/10336], Loss: 0.6416\n",
      "Epoch [3/5], Step [7980/10336], Loss: 0.3986\n",
      "Epoch [3/5], Step [7982/10336], Loss: 1.1665\n",
      "Epoch [3/5], Step [7984/10336], Loss: 0.0625\n",
      "Epoch [3/5], Step [7986/10336], Loss: 0.1879\n",
      "Epoch [3/5], Step [7988/10336], Loss: 0.0532\n",
      "Epoch [3/5], Step [7990/10336], Loss: 3.6116\n",
      "Epoch [3/5], Step [7992/10336], Loss: 0.9851\n",
      "Epoch [3/5], Step [7994/10336], Loss: 0.0069\n",
      "Epoch [3/5], Step [7996/10336], Loss: 0.1666\n",
      "Epoch [3/5], Step [7998/10336], Loss: 0.3092\n",
      "Epoch [3/5], Step [8000/10336], Loss: 0.1532\n",
      "Epoch [3/5], Step [8002/10336], Loss: 0.0294\n",
      "Epoch [3/5], Step [8004/10336], Loss: 1.1470\n",
      "Epoch [3/5], Step [8006/10336], Loss: 0.0924\n",
      "Epoch [3/5], Step [8008/10336], Loss: 0.1195\n",
      "Epoch [3/5], Step [8010/10336], Loss: 0.0097\n",
      "Epoch [3/5], Step [8012/10336], Loss: 1.0202\n",
      "Epoch [3/5], Step [8014/10336], Loss: 0.0082\n",
      "Epoch [3/5], Step [8016/10336], Loss: 0.0612\n",
      "Epoch [3/5], Step [8018/10336], Loss: 0.1319\n",
      "Epoch [3/5], Step [8020/10336], Loss: 0.2859\n",
      "Epoch [3/5], Step [8022/10336], Loss: 0.0139\n",
      "Epoch [3/5], Step [8024/10336], Loss: 0.4376\n",
      "Epoch [3/5], Step [8026/10336], Loss: 0.0659\n",
      "Epoch [3/5], Step [8028/10336], Loss: 1.1180\n",
      "Epoch [3/5], Step [8030/10336], Loss: 0.1838\n",
      "Epoch [3/5], Step [8032/10336], Loss: 0.6230\n",
      "Epoch [3/5], Step [8034/10336], Loss: 0.8087\n",
      "Epoch [3/5], Step [8036/10336], Loss: 0.1304\n",
      "Epoch [3/5], Step [8038/10336], Loss: 0.0954\n",
      "Epoch [3/5], Step [8040/10336], Loss: 0.0040\n",
      "Epoch [3/5], Step [8042/10336], Loss: 1.6594\n",
      "Epoch [3/5], Step [8044/10336], Loss: 0.3564\n",
      "Epoch [3/5], Step [8046/10336], Loss: 0.2750\n",
      "Epoch [3/5], Step [8048/10336], Loss: 1.4589\n",
      "Epoch [3/5], Step [8050/10336], Loss: 0.0008\n",
      "Epoch [3/5], Step [8052/10336], Loss: 0.0162\n",
      "Epoch [3/5], Step [8054/10336], Loss: 0.5576\n",
      "Epoch [3/5], Step [8056/10336], Loss: 0.7919\n",
      "Epoch [3/5], Step [8058/10336], Loss: 0.1418\n",
      "Epoch [3/5], Step [8060/10336], Loss: 0.0203\n",
      "Epoch [3/5], Step [8062/10336], Loss: 0.2061\n",
      "Epoch [3/5], Step [8064/10336], Loss: 0.0459\n",
      "Epoch [3/5], Step [8066/10336], Loss: 2.3483\n",
      "Epoch [3/5], Step [8068/10336], Loss: 0.5888\n",
      "Epoch [3/5], Step [8070/10336], Loss: 0.2050\n",
      "Epoch [3/5], Step [8072/10336], Loss: 0.0788\n",
      "Epoch [3/5], Step [8074/10336], Loss: 0.3414\n",
      "Epoch [3/5], Step [8076/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [8078/10336], Loss: 0.0181\n",
      "Epoch [3/5], Step [8080/10336], Loss: 0.8222\n",
      "Epoch [3/5], Step [8082/10336], Loss: 0.8866\n",
      "Epoch [3/5], Step [8084/10336], Loss: 0.0042\n",
      "Epoch [3/5], Step [8086/10336], Loss: 0.7275\n",
      "Epoch [3/5], Step [8088/10336], Loss: 2.7638\n",
      "Epoch [3/5], Step [8090/10336], Loss: 0.2471\n",
      "Epoch [3/5], Step [8092/10336], Loss: 0.3274\n",
      "Epoch [3/5], Step [8094/10336], Loss: 0.0030\n",
      "Epoch [3/5], Step [8096/10336], Loss: 1.0762\n",
      "Epoch [3/5], Step [8098/10336], Loss: 2.1338\n",
      "Epoch [3/5], Step [8100/10336], Loss: 0.3400\n",
      "Epoch [3/5], Step [8102/10336], Loss: 0.1088\n",
      "Epoch [3/5], Step [8104/10336], Loss: 0.0837\n",
      "Epoch [3/5], Step [8106/10336], Loss: 0.2453\n",
      "Epoch [3/5], Step [8108/10336], Loss: 0.3525\n",
      "Epoch [3/5], Step [8110/10336], Loss: 0.0910\n",
      "Epoch [3/5], Step [8112/10336], Loss: 0.0217\n",
      "Epoch [3/5], Step [8114/10336], Loss: 0.0768\n",
      "Epoch [3/5], Step [8116/10336], Loss: 0.1192\n",
      "Epoch [3/5], Step [8118/10336], Loss: 0.2573\n",
      "Epoch [3/5], Step [8120/10336], Loss: 1.4838\n",
      "Epoch [3/5], Step [8122/10336], Loss: 0.7001\n",
      "Epoch [3/5], Step [8124/10336], Loss: 4.0237\n",
      "Epoch [3/5], Step [8126/10336], Loss: 0.0317\n",
      "Epoch [3/5], Step [8128/10336], Loss: 0.0026\n",
      "Epoch [3/5], Step [8130/10336], Loss: 0.0638\n",
      "Epoch [3/5], Step [8132/10336], Loss: 0.8334\n",
      "Epoch [3/5], Step [8134/10336], Loss: 1.0257\n",
      "Epoch [3/5], Step [8136/10336], Loss: 2.8322\n",
      "Epoch [3/5], Step [8138/10336], Loss: 0.1360\n",
      "Epoch [3/5], Step [8140/10336], Loss: 0.1309\n",
      "Epoch [3/5], Step [8142/10336], Loss: 0.0456\n",
      "Epoch [3/5], Step [8144/10336], Loss: 0.2124\n",
      "Epoch [3/5], Step [8146/10336], Loss: 0.2253\n",
      "Epoch [3/5], Step [8148/10336], Loss: 0.0423\n",
      "Epoch [3/5], Step [8150/10336], Loss: 0.8767\n",
      "Epoch [3/5], Step [8152/10336], Loss: 0.1097\n",
      "Epoch [3/5], Step [8154/10336], Loss: 1.0041\n",
      "Epoch [3/5], Step [8156/10336], Loss: 0.1642\n",
      "Epoch [3/5], Step [8158/10336], Loss: 0.0255\n",
      "Epoch [3/5], Step [8160/10336], Loss: 0.4284\n",
      "Epoch [3/5], Step [8162/10336], Loss: 0.0840\n",
      "Epoch [3/5], Step [8164/10336], Loss: 2.1012\n",
      "Epoch [3/5], Step [8166/10336], Loss: 2.4454\n",
      "Epoch [3/5], Step [8168/10336], Loss: 0.0428\n",
      "Epoch [3/5], Step [8170/10336], Loss: 0.9371\n",
      "Epoch [3/5], Step [8172/10336], Loss: 0.5343\n",
      "Epoch [3/5], Step [8174/10336], Loss: 0.4264\n",
      "Epoch [3/5], Step [8176/10336], Loss: 0.0600\n",
      "Epoch [3/5], Step [8178/10336], Loss: 2.9473\n",
      "Epoch [3/5], Step [8180/10336], Loss: 4.5102\n",
      "Epoch [3/5], Step [8182/10336], Loss: 0.2823\n",
      "Epoch [3/5], Step [8184/10336], Loss: 0.2624\n",
      "Epoch [3/5], Step [8186/10336], Loss: 0.1676\n",
      "Epoch [3/5], Step [8188/10336], Loss: 1.6014\n",
      "Epoch [3/5], Step [8190/10336], Loss: 0.1779\n",
      "Epoch [3/5], Step [8192/10336], Loss: 0.0535\n",
      "Epoch [3/5], Step [8194/10336], Loss: 0.2419\n",
      "Epoch [3/5], Step [8196/10336], Loss: 0.7677\n",
      "Epoch [3/5], Step [8198/10336], Loss: 0.3294\n",
      "Epoch [3/5], Step [8200/10336], Loss: 1.0449\n",
      "Epoch [3/5], Step [8202/10336], Loss: 0.0146\n",
      "Epoch [3/5], Step [8204/10336], Loss: 2.4848\n",
      "Epoch [3/5], Step [8206/10336], Loss: 0.6641\n",
      "Epoch [3/5], Step [8208/10336], Loss: 1.2378\n",
      "Epoch [3/5], Step [8210/10336], Loss: 1.5008\n",
      "Epoch [3/5], Step [8212/10336], Loss: 0.3552\n",
      "Epoch [3/5], Step [8214/10336], Loss: 0.1153\n",
      "Epoch [3/5], Step [8216/10336], Loss: 2.5489\n",
      "Epoch [3/5], Step [8218/10336], Loss: 0.2352\n",
      "Epoch [3/5], Step [8220/10336], Loss: 0.0567\n",
      "Epoch [3/5], Step [8222/10336], Loss: 0.0487\n",
      "Epoch [3/5], Step [8224/10336], Loss: 1.5899\n",
      "Epoch [3/5], Step [8226/10336], Loss: 0.0335\n",
      "Epoch [3/5], Step [8228/10336], Loss: 0.0014\n",
      "Epoch [3/5], Step [8230/10336], Loss: 0.3591\n",
      "Epoch [3/5], Step [8232/10336], Loss: 4.7362\n",
      "Epoch [3/5], Step [8234/10336], Loss: 1.0133\n",
      "Epoch [3/5], Step [8236/10336], Loss: 0.1445\n",
      "Epoch [3/5], Step [8238/10336], Loss: 2.8708\n",
      "Epoch [3/5], Step [8240/10336], Loss: 1.4960\n",
      "Epoch [3/5], Step [8242/10336], Loss: 0.3372\n",
      "Epoch [3/5], Step [8244/10336], Loss: 2.6691\n",
      "Epoch [3/5], Step [8246/10336], Loss: 2.1909\n",
      "Epoch [3/5], Step [8248/10336], Loss: 0.7320\n",
      "Epoch [3/5], Step [8250/10336], Loss: 1.0840\n",
      "Epoch [3/5], Step [8252/10336], Loss: 0.1740\n",
      "Epoch [3/5], Step [8254/10336], Loss: 0.2681\n",
      "Epoch [3/5], Step [8256/10336], Loss: 1.8797\n",
      "Epoch [3/5], Step [8258/10336], Loss: 0.2773\n",
      "Epoch [3/5], Step [8260/10336], Loss: 4.4684\n",
      "Epoch [3/5], Step [8262/10336], Loss: 1.3068\n",
      "Epoch [3/5], Step [8264/10336], Loss: 0.1270\n",
      "Epoch [3/5], Step [8266/10336], Loss: 3.0719\n",
      "Epoch [3/5], Step [8268/10336], Loss: 0.5761\n",
      "Epoch [3/5], Step [8270/10336], Loss: 0.2748\n",
      "Epoch [3/5], Step [8272/10336], Loss: 0.2953\n",
      "Epoch [3/5], Step [8274/10336], Loss: 0.9472\n",
      "Epoch [3/5], Step [8276/10336], Loss: 0.1225\n",
      "Epoch [3/5], Step [8278/10336], Loss: 0.1155\n",
      "Epoch [3/5], Step [8280/10336], Loss: 0.0892\n",
      "Epoch [3/5], Step [8282/10336], Loss: 0.0090\n",
      "Epoch [3/5], Step [8284/10336], Loss: 0.0419\n",
      "Epoch [3/5], Step [8286/10336], Loss: 1.3303\n",
      "Epoch [3/5], Step [8288/10336], Loss: 0.1717\n",
      "Epoch [3/5], Step [8290/10336], Loss: 1.1615\n",
      "Epoch [3/5], Step [8292/10336], Loss: 0.2055\n",
      "Epoch [3/5], Step [8294/10336], Loss: 0.3932\n",
      "Epoch [3/5], Step [8296/10336], Loss: 0.1703\n",
      "Epoch [3/5], Step [8298/10336], Loss: 1.8039\n",
      "Epoch [3/5], Step [8300/10336], Loss: 0.1358\n",
      "Epoch [3/5], Step [8302/10336], Loss: 0.0533\n",
      "Epoch [3/5], Step [8304/10336], Loss: 0.0984\n",
      "Epoch [3/5], Step [8306/10336], Loss: 0.2212\n",
      "Epoch [3/5], Step [8308/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [8310/10336], Loss: 0.1838\n",
      "Epoch [3/5], Step [8312/10336], Loss: 0.0045\n",
      "Epoch [3/5], Step [8314/10336], Loss: 0.0390\n",
      "Epoch [3/5], Step [8316/10336], Loss: 4.9015\n",
      "Epoch [3/5], Step [8318/10336], Loss: 1.0652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [8320/10336], Loss: 0.0241\n",
      "Epoch [3/5], Step [8322/10336], Loss: 0.5216\n",
      "Epoch [3/5], Step [8324/10336], Loss: 0.8030\n",
      "Epoch [3/5], Step [8326/10336], Loss: 0.3011\n",
      "Epoch [3/5], Step [8328/10336], Loss: 0.0625\n",
      "Epoch [3/5], Step [8330/10336], Loss: 0.4879\n",
      "Epoch [3/5], Step [8332/10336], Loss: 0.8448\n",
      "Epoch [3/5], Step [8334/10336], Loss: 3.8379\n",
      "Epoch [3/5], Step [8336/10336], Loss: 0.1261\n",
      "Epoch [3/5], Step [8338/10336], Loss: 1.8960\n",
      "Epoch [3/5], Step [8340/10336], Loss: 0.4422\n",
      "Epoch [3/5], Step [8342/10336], Loss: 0.7415\n",
      "Epoch [3/5], Step [8344/10336], Loss: 0.2483\n",
      "Epoch [3/5], Step [8346/10336], Loss: 0.0966\n",
      "Epoch [3/5], Step [8348/10336], Loss: 0.1203\n",
      "Epoch [3/5], Step [8350/10336], Loss: 0.4147\n",
      "Epoch [3/5], Step [8352/10336], Loss: 4.2656\n",
      "Epoch [3/5], Step [8354/10336], Loss: 0.1654\n",
      "Epoch [3/5], Step [8356/10336], Loss: 0.1833\n",
      "Epoch [3/5], Step [8358/10336], Loss: 0.1810\n",
      "Epoch [3/5], Step [8360/10336], Loss: 0.2819\n",
      "Epoch [3/5], Step [8362/10336], Loss: 0.0461\n",
      "Epoch [3/5], Step [8364/10336], Loss: 0.8043\n",
      "Epoch [3/5], Step [8366/10336], Loss: 0.2440\n",
      "Epoch [3/5], Step [8368/10336], Loss: 0.3121\n",
      "Epoch [3/5], Step [8370/10336], Loss: 1.4526\n",
      "Epoch [3/5], Step [8372/10336], Loss: 0.6393\n",
      "Epoch [3/5], Step [8374/10336], Loss: 0.0764\n",
      "Epoch [3/5], Step [8376/10336], Loss: 0.0192\n",
      "Epoch [3/5], Step [8378/10336], Loss: 0.2092\n",
      "Epoch [3/5], Step [8380/10336], Loss: 0.1578\n",
      "Epoch [3/5], Step [8382/10336], Loss: 0.0039\n",
      "Epoch [3/5], Step [8384/10336], Loss: 0.6224\n",
      "Epoch [3/5], Step [8386/10336], Loss: 0.0178\n",
      "Epoch [3/5], Step [8388/10336], Loss: 1.2423\n",
      "Epoch [3/5], Step [8390/10336], Loss: 0.2403\n",
      "Epoch [3/5], Step [8392/10336], Loss: 0.1301\n",
      "Epoch [3/5], Step [8394/10336], Loss: 0.4638\n",
      "Epoch [3/5], Step [8396/10336], Loss: 0.0621\n",
      "Epoch [3/5], Step [8398/10336], Loss: 0.0345\n",
      "Epoch [3/5], Step [8400/10336], Loss: 0.9578\n",
      "Epoch [3/5], Step [8402/10336], Loss: 0.4612\n",
      "Epoch [3/5], Step [8404/10336], Loss: 0.4203\n",
      "Epoch [3/5], Step [8406/10336], Loss: 0.0933\n",
      "Epoch [3/5], Step [8408/10336], Loss: 0.2473\n",
      "Epoch [3/5], Step [8410/10336], Loss: 0.0477\n",
      "Epoch [3/5], Step [8412/10336], Loss: 0.0053\n",
      "Epoch [3/5], Step [8414/10336], Loss: 0.0456\n",
      "Epoch [3/5], Step [8416/10336], Loss: 0.1693\n",
      "Epoch [3/5], Step [8418/10336], Loss: 0.1394\n",
      "Epoch [3/5], Step [8420/10336], Loss: 0.1577\n",
      "Epoch [3/5], Step [8422/10336], Loss: 0.8852\n",
      "Epoch [3/5], Step [8424/10336], Loss: 0.0062\n",
      "Epoch [3/5], Step [8426/10336], Loss: 0.0787\n",
      "Epoch [3/5], Step [8428/10336], Loss: 0.0509\n",
      "Epoch [3/5], Step [8430/10336], Loss: 1.0218\n",
      "Epoch [3/5], Step [8432/10336], Loss: 0.1282\n",
      "Epoch [3/5], Step [8434/10336], Loss: 0.2860\n",
      "Epoch [3/5], Step [8436/10336], Loss: 0.0246\n",
      "Epoch [3/5], Step [8438/10336], Loss: 0.2429\n",
      "Epoch [3/5], Step [8440/10336], Loss: 1.0730\n",
      "Epoch [3/5], Step [8442/10336], Loss: 4.6513\n",
      "Epoch [3/5], Step [8444/10336], Loss: 0.2763\n",
      "Epoch [3/5], Step [8446/10336], Loss: 0.0732\n",
      "Epoch [3/5], Step [8448/10336], Loss: 2.0101\n",
      "Epoch [3/5], Step [8450/10336], Loss: 2.2758\n",
      "Epoch [3/5], Step [8452/10336], Loss: 0.2056\n",
      "Epoch [3/5], Step [8454/10336], Loss: 0.0025\n",
      "Epoch [3/5], Step [8456/10336], Loss: 0.2160\n",
      "Epoch [3/5], Step [8458/10336], Loss: 0.3557\n",
      "Epoch [3/5], Step [8460/10336], Loss: 0.2628\n",
      "Epoch [3/5], Step [8462/10336], Loss: 0.0269\n",
      "Epoch [3/5], Step [8464/10336], Loss: 0.0012\n",
      "Epoch [3/5], Step [8466/10336], Loss: 0.3749\n",
      "Epoch [3/5], Step [8468/10336], Loss: 0.0650\n",
      "Epoch [3/5], Step [8470/10336], Loss: 0.0527\n",
      "Epoch [3/5], Step [8472/10336], Loss: 0.1575\n",
      "Epoch [3/5], Step [8474/10336], Loss: 0.0362\n",
      "Epoch [3/5], Step [8476/10336], Loss: 0.0153\n",
      "Epoch [3/5], Step [8478/10336], Loss: 0.2910\n",
      "Epoch [3/5], Step [8480/10336], Loss: 1.3033\n",
      "Epoch [3/5], Step [8482/10336], Loss: 0.1471\n",
      "Epoch [3/5], Step [8484/10336], Loss: 1.4330\n",
      "Epoch [3/5], Step [8486/10336], Loss: 0.6506\n",
      "Epoch [3/5], Step [8488/10336], Loss: 0.4015\n",
      "Epoch [3/5], Step [8490/10336], Loss: 0.0143\n",
      "Epoch [3/5], Step [8492/10336], Loss: 1.7473\n",
      "Epoch [3/5], Step [8494/10336], Loss: 3.2121\n",
      "Epoch [3/5], Step [8496/10336], Loss: 0.1235\n",
      "Epoch [3/5], Step [8498/10336], Loss: 0.7770\n",
      "Epoch [3/5], Step [8500/10336], Loss: 1.1911\n",
      "Epoch [3/5], Step [8502/10336], Loss: 0.0223\n",
      "Epoch [3/5], Step [8504/10336], Loss: 0.0381\n",
      "Epoch [3/5], Step [8506/10336], Loss: 0.4818\n",
      "Epoch [3/5], Step [8508/10336], Loss: 1.0379\n",
      "Epoch [3/5], Step [8510/10336], Loss: 0.0559\n",
      "Epoch [3/5], Step [8512/10336], Loss: 0.6099\n",
      "Epoch [3/5], Step [8514/10336], Loss: 2.0137\n",
      "Epoch [3/5], Step [8516/10336], Loss: 1.3946\n",
      "Epoch [3/5], Step [8518/10336], Loss: 0.0890\n",
      "Epoch [3/5], Step [8520/10336], Loss: 0.3315\n",
      "Epoch [3/5], Step [8522/10336], Loss: 0.2774\n",
      "Epoch [3/5], Step [8524/10336], Loss: 0.2303\n",
      "Epoch [3/5], Step [8526/10336], Loss: 0.3215\n",
      "Epoch [3/5], Step [8528/10336], Loss: 0.0473\n",
      "Epoch [3/5], Step [8530/10336], Loss: 0.0095\n",
      "Epoch [3/5], Step [8532/10336], Loss: 0.9508\n",
      "Epoch [3/5], Step [8534/10336], Loss: 0.1441\n",
      "Epoch [3/5], Step [8536/10336], Loss: 2.0663\n",
      "Epoch [3/5], Step [8538/10336], Loss: 1.4685\n",
      "Epoch [3/5], Step [8540/10336], Loss: 0.2050\n",
      "Epoch [3/5], Step [8542/10336], Loss: 0.1135\n",
      "Epoch [3/5], Step [8544/10336], Loss: 0.2089\n",
      "Epoch [3/5], Step [8546/10336], Loss: 0.2182\n",
      "Epoch [3/5], Step [8548/10336], Loss: 0.1375\n",
      "Epoch [3/5], Step [8550/10336], Loss: 0.4163\n",
      "Epoch [3/5], Step [8552/10336], Loss: 0.4410\n",
      "Epoch [3/5], Step [8554/10336], Loss: 0.2589\n",
      "Epoch [3/5], Step [8556/10336], Loss: 0.6193\n",
      "Epoch [3/5], Step [8558/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [8560/10336], Loss: 0.8148\n",
      "Epoch [3/5], Step [8562/10336], Loss: 0.2426\n",
      "Epoch [3/5], Step [8564/10336], Loss: 1.6798\n",
      "Epoch [3/5], Step [8566/10336], Loss: 0.5809\n",
      "Epoch [3/5], Step [8568/10336], Loss: 0.0224\n",
      "Epoch [3/5], Step [8570/10336], Loss: 0.1354\n",
      "Epoch [3/5], Step [8572/10336], Loss: 2.3320\n",
      "Epoch [3/5], Step [8574/10336], Loss: 0.0026\n",
      "Epoch [3/5], Step [8576/10336], Loss: 2.2581\n",
      "Epoch [3/5], Step [8578/10336], Loss: 0.3104\n",
      "Epoch [3/5], Step [8580/10336], Loss: 3.2487\n",
      "Epoch [3/5], Step [8582/10336], Loss: 2.4253\n",
      "Epoch [3/5], Step [8584/10336], Loss: 0.6958\n",
      "Epoch [3/5], Step [8586/10336], Loss: 0.2385\n",
      "Epoch [3/5], Step [8588/10336], Loss: 0.1105\n",
      "Epoch [3/5], Step [8590/10336], Loss: 1.1326\n",
      "Epoch [3/5], Step [8592/10336], Loss: 0.0459\n",
      "Epoch [3/5], Step [8594/10336], Loss: 0.0047\n",
      "Epoch [3/5], Step [8596/10336], Loss: 2.5252\n",
      "Epoch [3/5], Step [8598/10336], Loss: 1.5054\n",
      "Epoch [3/5], Step [8600/10336], Loss: 1.3354\n",
      "Epoch [3/5], Step [8602/10336], Loss: 0.0131\n",
      "Epoch [3/5], Step [8604/10336], Loss: 0.1988\n",
      "Epoch [3/5], Step [8606/10336], Loss: 1.7982\n",
      "Epoch [3/5], Step [8608/10336], Loss: 2.4258\n",
      "Epoch [3/5], Step [8610/10336], Loss: 0.4319\n",
      "Epoch [3/5], Step [8612/10336], Loss: 0.0388\n",
      "Epoch [3/5], Step [8614/10336], Loss: 0.2785\n",
      "Epoch [3/5], Step [8616/10336], Loss: 0.2162\n",
      "Epoch [3/5], Step [8618/10336], Loss: 0.0392\n",
      "Epoch [3/5], Step [8620/10336], Loss: 2.5534\n",
      "Epoch [3/5], Step [8622/10336], Loss: 0.7198\n",
      "Epoch [3/5], Step [8624/10336], Loss: 0.2319\n",
      "Epoch [3/5], Step [8626/10336], Loss: 0.2876\n",
      "Epoch [3/5], Step [8628/10336], Loss: 0.8417\n",
      "Epoch [3/5], Step [8630/10336], Loss: 0.3031\n",
      "Epoch [3/5], Step [8632/10336], Loss: 0.0980\n",
      "Epoch [3/5], Step [8634/10336], Loss: 0.0296\n",
      "Epoch [3/5], Step [8636/10336], Loss: 0.6286\n",
      "Epoch [3/5], Step [8638/10336], Loss: 0.0786\n",
      "Epoch [3/5], Step [8640/10336], Loss: 0.0011\n",
      "Epoch [3/5], Step [8642/10336], Loss: 0.1433\n",
      "Epoch [3/5], Step [8644/10336], Loss: 0.0930\n",
      "Epoch [3/5], Step [8646/10336], Loss: 1.0511\n",
      "Epoch [3/5], Step [8648/10336], Loss: 0.5386\n",
      "Epoch [3/5], Step [8650/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [8652/10336], Loss: 0.0073\n",
      "Epoch [3/5], Step [8654/10336], Loss: 0.0206\n",
      "Epoch [3/5], Step [8656/10336], Loss: 4.2487\n",
      "Epoch [3/5], Step [8658/10336], Loss: 0.4899\n",
      "Epoch [3/5], Step [8660/10336], Loss: 0.0246\n",
      "Epoch [3/5], Step [8662/10336], Loss: 0.3596\n",
      "Epoch [3/5], Step [8664/10336], Loss: 0.0050\n",
      "Epoch [3/5], Step [8666/10336], Loss: 2.4176\n",
      "Epoch [3/5], Step [8668/10336], Loss: 0.1444\n",
      "Epoch [3/5], Step [8670/10336], Loss: 0.1435\n",
      "Epoch [3/5], Step [8672/10336], Loss: 0.0300\n",
      "Epoch [3/5], Step [8674/10336], Loss: 2.2050\n",
      "Epoch [3/5], Step [8676/10336], Loss: 0.2668\n",
      "Epoch [3/5], Step [8678/10336], Loss: 1.4431\n",
      "Epoch [3/5], Step [8680/10336], Loss: 0.6910\n",
      "Epoch [3/5], Step [8682/10336], Loss: 0.2180\n",
      "Epoch [3/5], Step [8684/10336], Loss: 4.3352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [8686/10336], Loss: 2.2140\n",
      "Epoch [3/5], Step [8688/10336], Loss: 0.3473\n",
      "Epoch [3/5], Step [8690/10336], Loss: 0.7246\n",
      "Epoch [3/5], Step [8692/10336], Loss: 0.1942\n",
      "Epoch [3/5], Step [8694/10336], Loss: 0.2115\n",
      "Epoch [3/5], Step [8696/10336], Loss: 5.0791\n",
      "Epoch [3/5], Step [8698/10336], Loss: 0.0877\n",
      "Epoch [3/5], Step [8700/10336], Loss: 0.6548\n",
      "Epoch [3/5], Step [8702/10336], Loss: 0.6079\n",
      "Epoch [3/5], Step [8704/10336], Loss: 0.1603\n",
      "Epoch [3/5], Step [8706/10336], Loss: 0.3154\n",
      "Epoch [3/5], Step [8708/10336], Loss: 3.8086\n",
      "Epoch [3/5], Step [8710/10336], Loss: 0.0696\n",
      "Epoch [3/5], Step [8712/10336], Loss: 0.8921\n",
      "Epoch [3/5], Step [8714/10336], Loss: 0.2904\n",
      "Epoch [3/5], Step [8716/10336], Loss: 0.4211\n",
      "Epoch [3/5], Step [8718/10336], Loss: 0.0156\n",
      "Epoch [3/5], Step [8720/10336], Loss: 1.7946\n",
      "Epoch [3/5], Step [8722/10336], Loss: 0.0085\n",
      "Epoch [3/5], Step [8724/10336], Loss: 0.4469\n",
      "Epoch [3/5], Step [8726/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [8728/10336], Loss: 0.2968\n",
      "Epoch [3/5], Step [8730/10336], Loss: 0.3601\n",
      "Epoch [3/5], Step [8732/10336], Loss: 0.0475\n",
      "Epoch [3/5], Step [8734/10336], Loss: 1.3897\n",
      "Epoch [3/5], Step [8736/10336], Loss: 0.2154\n",
      "Epoch [3/5], Step [8738/10336], Loss: 0.1746\n",
      "Epoch [3/5], Step [8740/10336], Loss: 1.1241\n",
      "Epoch [3/5], Step [8742/10336], Loss: 0.5958\n",
      "Epoch [3/5], Step [8744/10336], Loss: 0.0903\n",
      "Epoch [3/5], Step [8746/10336], Loss: 0.2956\n",
      "Epoch [3/5], Step [8748/10336], Loss: 0.6691\n",
      "Epoch [3/5], Step [8750/10336], Loss: 0.0945\n",
      "Epoch [3/5], Step [8752/10336], Loss: 5.7194\n",
      "Epoch [3/5], Step [8754/10336], Loss: 0.1786\n",
      "Epoch [3/5], Step [8756/10336], Loss: 0.0614\n",
      "Epoch [3/5], Step [8758/10336], Loss: 0.1103\n",
      "Epoch [3/5], Step [8760/10336], Loss: 0.0972\n",
      "Epoch [3/5], Step [8762/10336], Loss: 0.5833\n",
      "Epoch [3/5], Step [8764/10336], Loss: 0.1878\n",
      "Epoch [3/5], Step [8766/10336], Loss: 2.5018\n",
      "Epoch [3/5], Step [8768/10336], Loss: 0.5005\n",
      "Epoch [3/5], Step [8770/10336], Loss: 0.2260\n",
      "Epoch [3/5], Step [8772/10336], Loss: 0.2795\n",
      "Epoch [3/5], Step [8774/10336], Loss: 0.0270\n",
      "Epoch [3/5], Step [8776/10336], Loss: 1.5405\n",
      "Epoch [3/5], Step [8778/10336], Loss: 0.1288\n",
      "Epoch [3/5], Step [8780/10336], Loss: 0.6750\n",
      "Epoch [3/5], Step [8782/10336], Loss: 1.9746\n",
      "Epoch [3/5], Step [8784/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [8786/10336], Loss: 0.2649\n",
      "Epoch [3/5], Step [8788/10336], Loss: 1.7709\n",
      "Epoch [3/5], Step [8790/10336], Loss: 0.1877\n",
      "Epoch [3/5], Step [8792/10336], Loss: 1.0017\n",
      "Epoch [3/5], Step [8794/10336], Loss: 0.1831\n",
      "Epoch [3/5], Step [8796/10336], Loss: 0.0027\n",
      "Epoch [3/5], Step [8798/10336], Loss: 0.0392\n",
      "Epoch [3/5], Step [8800/10336], Loss: 2.4730\n",
      "Epoch [3/5], Step [8802/10336], Loss: 3.7420\n",
      "Epoch [3/5], Step [8804/10336], Loss: 4.7084\n",
      "Epoch [3/5], Step [8806/10336], Loss: 0.2693\n",
      "Epoch [3/5], Step [8808/10336], Loss: 0.1484\n",
      "Epoch [3/5], Step [8810/10336], Loss: 0.2853\n",
      "Epoch [3/5], Step [8812/10336], Loss: 0.2696\n",
      "Epoch [3/5], Step [8814/10336], Loss: 1.8194\n",
      "Epoch [3/5], Step [8816/10336], Loss: 2.6656\n",
      "Epoch [3/5], Step [8818/10336], Loss: 0.0433\n",
      "Epoch [3/5], Step [8820/10336], Loss: 0.6151\n",
      "Epoch [3/5], Step [8822/10336], Loss: 0.0245\n",
      "Epoch [3/5], Step [8824/10336], Loss: 0.0933\n",
      "Epoch [3/5], Step [8826/10336], Loss: 0.5540\n",
      "Epoch [3/5], Step [8828/10336], Loss: 0.0157\n",
      "Epoch [3/5], Step [8830/10336], Loss: 0.3396\n",
      "Epoch [3/5], Step [8832/10336], Loss: 0.0496\n",
      "Epoch [3/5], Step [8834/10336], Loss: 0.8245\n",
      "Epoch [3/5], Step [8836/10336], Loss: 0.0091\n",
      "Epoch [3/5], Step [8838/10336], Loss: 0.2884\n",
      "Epoch [3/5], Step [8840/10336], Loss: 0.0257\n",
      "Epoch [3/5], Step [8842/10336], Loss: 0.0840\n",
      "Epoch [3/5], Step [8844/10336], Loss: 0.4729\n",
      "Epoch [3/5], Step [8846/10336], Loss: 0.0089\n",
      "Epoch [3/5], Step [8848/10336], Loss: 2.6751\n",
      "Epoch [3/5], Step [8850/10336], Loss: 0.0591\n",
      "Epoch [3/5], Step [8852/10336], Loss: 0.1121\n",
      "Epoch [3/5], Step [8854/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [8856/10336], Loss: 2.6275\n",
      "Epoch [3/5], Step [8858/10336], Loss: 0.3947\n",
      "Epoch [3/5], Step [8860/10336], Loss: 0.0897\n",
      "Epoch [3/5], Step [8862/10336], Loss: 3.3137\n",
      "Epoch [3/5], Step [8864/10336], Loss: 0.2341\n",
      "Epoch [3/5], Step [8866/10336], Loss: 3.2332\n",
      "Epoch [3/5], Step [8868/10336], Loss: 0.0653\n",
      "Epoch [3/5], Step [8870/10336], Loss: 0.3018\n",
      "Epoch [3/5], Step [8872/10336], Loss: 0.9909\n",
      "Epoch [3/5], Step [8874/10336], Loss: 0.8652\n",
      "Epoch [3/5], Step [8876/10336], Loss: 0.4815\n",
      "Epoch [3/5], Step [8878/10336], Loss: 0.3002\n",
      "Epoch [3/5], Step [8880/10336], Loss: 0.3850\n",
      "Epoch [3/5], Step [8882/10336], Loss: 0.0371\n",
      "Epoch [3/5], Step [8884/10336], Loss: 2.8191\n",
      "Epoch [3/5], Step [8886/10336], Loss: 0.0592\n",
      "Epoch [3/5], Step [8888/10336], Loss: 0.1295\n",
      "Epoch [3/5], Step [8890/10336], Loss: 0.2685\n",
      "Epoch [3/5], Step [8892/10336], Loss: 0.2097\n",
      "Epoch [3/5], Step [8894/10336], Loss: 2.7591\n",
      "Epoch [3/5], Step [8896/10336], Loss: 0.2789\n",
      "Epoch [3/5], Step [8898/10336], Loss: 0.0844\n",
      "Epoch [3/5], Step [8900/10336], Loss: 1.4942\n",
      "Epoch [3/5], Step [8902/10336], Loss: 0.2907\n",
      "Epoch [3/5], Step [8904/10336], Loss: 0.1937\n",
      "Epoch [3/5], Step [8906/10336], Loss: 5.0466\n",
      "Epoch [3/5], Step [8908/10336], Loss: 0.3601\n",
      "Epoch [3/5], Step [8910/10336], Loss: 0.9584\n",
      "Epoch [3/5], Step [8912/10336], Loss: 0.0084\n",
      "Epoch [3/5], Step [8914/10336], Loss: 0.0228\n",
      "Epoch [3/5], Step [8916/10336], Loss: 1.6425\n",
      "Epoch [3/5], Step [8918/10336], Loss: 0.0758\n",
      "Epoch [3/5], Step [8920/10336], Loss: 0.3386\n",
      "Epoch [3/5], Step [8922/10336], Loss: 0.0490\n",
      "Epoch [3/5], Step [8924/10336], Loss: 0.4809\n",
      "Epoch [3/5], Step [8926/10336], Loss: 0.0606\n",
      "Epoch [3/5], Step [8928/10336], Loss: 0.1803\n",
      "Epoch [3/5], Step [8930/10336], Loss: 1.5307\n",
      "Epoch [3/5], Step [8932/10336], Loss: 0.0385\n",
      "Epoch [3/5], Step [8934/10336], Loss: 0.2241\n",
      "Epoch [3/5], Step [8936/10336], Loss: 0.0712\n",
      "Epoch [3/5], Step [8938/10336], Loss: 0.1216\n",
      "Epoch [3/5], Step [8940/10336], Loss: 0.0192\n",
      "Epoch [3/5], Step [8942/10336], Loss: 0.2117\n",
      "Epoch [3/5], Step [8944/10336], Loss: 0.6033\n",
      "Epoch [3/5], Step [8946/10336], Loss: 0.0695\n",
      "Epoch [3/5], Step [8948/10336], Loss: 4.1017\n",
      "Epoch [3/5], Step [8950/10336], Loss: 2.1493\n",
      "Epoch [3/5], Step [8952/10336], Loss: 0.0146\n",
      "Epoch [3/5], Step [8954/10336], Loss: 3.1247\n",
      "Epoch [3/5], Step [8956/10336], Loss: 4.2659\n",
      "Epoch [3/5], Step [8958/10336], Loss: 0.1646\n",
      "Epoch [3/5], Step [8960/10336], Loss: 0.0098\n",
      "Epoch [3/5], Step [8962/10336], Loss: 0.1190\n",
      "Epoch [3/5], Step [8964/10336], Loss: 0.3449\n",
      "Epoch [3/5], Step [8966/10336], Loss: 0.6005\n",
      "Epoch [3/5], Step [8968/10336], Loss: 0.2522\n",
      "Epoch [3/5], Step [8970/10336], Loss: 0.0845\n",
      "Epoch [3/5], Step [8972/10336], Loss: 2.8033\n",
      "Epoch [3/5], Step [8974/10336], Loss: 1.1701\n",
      "Epoch [3/5], Step [8976/10336], Loss: 0.2059\n",
      "Epoch [3/5], Step [8978/10336], Loss: 0.2945\n",
      "Epoch [3/5], Step [8980/10336], Loss: 0.1246\n",
      "Epoch [3/5], Step [8982/10336], Loss: 0.2307\n",
      "Epoch [3/5], Step [8984/10336], Loss: 0.5247\n",
      "Epoch [3/5], Step [8986/10336], Loss: 0.5307\n",
      "Epoch [3/5], Step [8988/10336], Loss: 0.3520\n",
      "Epoch [3/5], Step [8990/10336], Loss: 1.3485\n",
      "Epoch [3/5], Step [8992/10336], Loss: 1.0906\n",
      "Epoch [3/5], Step [8994/10336], Loss: 0.2115\n",
      "Epoch [3/5], Step [8996/10336], Loss: 0.4326\n",
      "Epoch [3/5], Step [8998/10336], Loss: 0.0611\n",
      "Epoch [3/5], Step [9000/10336], Loss: 0.2964\n",
      "Epoch [3/5], Step [9002/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [9004/10336], Loss: 0.5237\n",
      "Epoch [3/5], Step [9006/10336], Loss: 3.1263\n",
      "Epoch [3/5], Step [9008/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [9010/10336], Loss: 1.1320\n",
      "Epoch [3/5], Step [9012/10336], Loss: 4.9176\n",
      "Epoch [3/5], Step [9014/10336], Loss: 0.2134\n",
      "Epoch [3/5], Step [9016/10336], Loss: 0.6058\n",
      "Epoch [3/5], Step [9018/10336], Loss: 4.1618\n",
      "Epoch [3/5], Step [9020/10336], Loss: 0.0298\n",
      "Epoch [3/5], Step [9022/10336], Loss: 2.7354\n",
      "Epoch [3/5], Step [9024/10336], Loss: 0.1040\n",
      "Epoch [3/5], Step [9026/10336], Loss: 2.7992\n",
      "Epoch [3/5], Step [9028/10336], Loss: 0.7517\n",
      "Epoch [3/5], Step [9030/10336], Loss: 0.8510\n",
      "Epoch [3/5], Step [9032/10336], Loss: 0.3234\n",
      "Epoch [3/5], Step [9034/10336], Loss: 0.3989\n",
      "Epoch [3/5], Step [9036/10336], Loss: 0.3185\n",
      "Epoch [3/5], Step [9038/10336], Loss: 0.0496\n",
      "Epoch [3/5], Step [9040/10336], Loss: 0.9055\n",
      "Epoch [3/5], Step [9042/10336], Loss: 0.5430\n",
      "Epoch [3/5], Step [9044/10336], Loss: 0.1425\n",
      "Epoch [3/5], Step [9046/10336], Loss: 0.0780\n",
      "Epoch [3/5], Step [9048/10336], Loss: 0.4267\n",
      "Epoch [3/5], Step [9050/10336], Loss: 0.0038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [9052/10336], Loss: 0.0377\n",
      "Epoch [3/5], Step [9054/10336], Loss: 0.0667\n",
      "Epoch [3/5], Step [9056/10336], Loss: 0.6491\n",
      "Epoch [3/5], Step [9058/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [9060/10336], Loss: 0.1440\n",
      "Epoch [3/5], Step [9062/10336], Loss: 0.2136\n",
      "Epoch [3/5], Step [9064/10336], Loss: 0.2070\n",
      "Epoch [3/5], Step [9066/10336], Loss: 0.3932\n",
      "Epoch [3/5], Step [9068/10336], Loss: 0.0220\n",
      "Epoch [3/5], Step [9070/10336], Loss: 0.1481\n",
      "Epoch [3/5], Step [9072/10336], Loss: 0.0093\n",
      "Epoch [3/5], Step [9074/10336], Loss: 0.1092\n",
      "Epoch [3/5], Step [9076/10336], Loss: 0.0670\n",
      "Epoch [3/5], Step [9078/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [9080/10336], Loss: 0.8205\n",
      "Epoch [3/5], Step [9082/10336], Loss: 0.2336\n",
      "Epoch [3/5], Step [9084/10336], Loss: 0.0955\n",
      "Epoch [3/5], Step [9086/10336], Loss: 0.0108\n",
      "Epoch [3/5], Step [9088/10336], Loss: 0.1128\n",
      "Epoch [3/5], Step [9090/10336], Loss: 0.0467\n",
      "Epoch [3/5], Step [9092/10336], Loss: 0.0495\n",
      "Epoch [3/5], Step [9094/10336], Loss: 0.0227\n",
      "Epoch [3/5], Step [9096/10336], Loss: 0.1270\n",
      "Epoch [3/5], Step [9098/10336], Loss: 0.1790\n",
      "Epoch [3/5], Step [9100/10336], Loss: 1.5696\n",
      "Epoch [3/5], Step [9102/10336], Loss: 1.9284\n",
      "Epoch [3/5], Step [9104/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [9106/10336], Loss: 1.4977\n",
      "Epoch [3/5], Step [9108/10336], Loss: 2.3477\n",
      "Epoch [3/5], Step [9110/10336], Loss: 0.2622\n",
      "Epoch [3/5], Step [9112/10336], Loss: 2.6869\n",
      "Epoch [3/5], Step [9114/10336], Loss: 0.0423\n",
      "Epoch [3/5], Step [9116/10336], Loss: 0.1425\n",
      "Epoch [3/5], Step [9118/10336], Loss: 1.3227\n",
      "Epoch [3/5], Step [9120/10336], Loss: 0.1294\n",
      "Epoch [3/5], Step [9122/10336], Loss: 0.0908\n",
      "Epoch [3/5], Step [9124/10336], Loss: 0.3658\n",
      "Epoch [3/5], Step [9126/10336], Loss: 0.1694\n",
      "Epoch [3/5], Step [9128/10336], Loss: 0.2268\n",
      "Epoch [3/5], Step [9130/10336], Loss: 1.6961\n",
      "Epoch [3/5], Step [9132/10336], Loss: 0.0347\n",
      "Epoch [3/5], Step [9134/10336], Loss: 0.4359\n",
      "Epoch [3/5], Step [9136/10336], Loss: 4.3926\n",
      "Epoch [3/5], Step [9138/10336], Loss: 2.9688\n",
      "Epoch [3/5], Step [9140/10336], Loss: 0.7785\n",
      "Epoch [3/5], Step [9142/10336], Loss: 0.1198\n",
      "Epoch [3/5], Step [9144/10336], Loss: 0.3288\n",
      "Epoch [3/5], Step [9146/10336], Loss: 0.0571\n",
      "Epoch [3/5], Step [9148/10336], Loss: 0.0857\n",
      "Epoch [3/5], Step [9150/10336], Loss: 0.4796\n",
      "Epoch [3/5], Step [9152/10336], Loss: 3.4640\n",
      "Epoch [3/5], Step [9154/10336], Loss: 2.3003\n",
      "Epoch [3/5], Step [9156/10336], Loss: 0.0044\n",
      "Epoch [3/5], Step [9158/10336], Loss: 0.0930\n",
      "Epoch [3/5], Step [9160/10336], Loss: 0.3230\n",
      "Epoch [3/5], Step [9162/10336], Loss: 0.2938\n",
      "Epoch [3/5], Step [9164/10336], Loss: 1.5381\n",
      "Epoch [3/5], Step [9166/10336], Loss: 2.1150\n",
      "Epoch [3/5], Step [9168/10336], Loss: 1.9027\n",
      "Epoch [3/5], Step [9170/10336], Loss: 0.2079\n",
      "Epoch [3/5], Step [9172/10336], Loss: 3.5677\n",
      "Epoch [3/5], Step [9174/10336], Loss: 0.3371\n",
      "Epoch [3/5], Step [9176/10336], Loss: 0.1012\n",
      "Epoch [3/5], Step [9178/10336], Loss: 0.1128\n",
      "Epoch [3/5], Step [9180/10336], Loss: 0.1734\n",
      "Epoch [3/5], Step [9182/10336], Loss: 0.3161\n",
      "Epoch [3/5], Step [9184/10336], Loss: 0.0388\n",
      "Epoch [3/5], Step [9186/10336], Loss: 0.1088\n",
      "Epoch [3/5], Step [9188/10336], Loss: 0.1292\n",
      "Epoch [3/5], Step [9190/10336], Loss: 0.4562\n",
      "Epoch [3/5], Step [9192/10336], Loss: 0.0623\n",
      "Epoch [3/5], Step [9194/10336], Loss: 0.0223\n",
      "Epoch [3/5], Step [9196/10336], Loss: 0.1888\n",
      "Epoch [3/5], Step [9198/10336], Loss: 0.0190\n",
      "Epoch [3/5], Step [9200/10336], Loss: 0.0123\n",
      "Epoch [3/5], Step [9202/10336], Loss: 0.0397\n",
      "Epoch [3/5], Step [9204/10336], Loss: 4.8513\n",
      "Epoch [3/5], Step [9206/10336], Loss: 0.5385\n",
      "Epoch [3/5], Step [9208/10336], Loss: 0.1297\n",
      "Epoch [3/5], Step [9210/10336], Loss: 0.2171\n",
      "Epoch [3/5], Step [9212/10336], Loss: 0.5743\n",
      "Epoch [3/5], Step [9214/10336], Loss: 2.6818\n",
      "Epoch [3/5], Step [9216/10336], Loss: 0.4617\n",
      "Epoch [3/5], Step [9218/10336], Loss: 1.3910\n",
      "Epoch [3/5], Step [9220/10336], Loss: 0.0067\n",
      "Epoch [3/5], Step [9222/10336], Loss: 0.0290\n",
      "Epoch [3/5], Step [9224/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [9226/10336], Loss: 0.1593\n",
      "Epoch [3/5], Step [9228/10336], Loss: 0.5551\n",
      "Epoch [3/5], Step [9230/10336], Loss: 0.0307\n",
      "Epoch [3/5], Step [9232/10336], Loss: 0.0488\n",
      "Epoch [3/5], Step [9234/10336], Loss: 1.0503\n",
      "Epoch [3/5], Step [9236/10336], Loss: 0.1732\n",
      "Epoch [3/5], Step [9238/10336], Loss: 0.9702\n",
      "Epoch [3/5], Step [9240/10336], Loss: 0.1765\n",
      "Epoch [3/5], Step [9242/10336], Loss: 0.1394\n",
      "Epoch [3/5], Step [9244/10336], Loss: 0.1672\n",
      "Epoch [3/5], Step [9246/10336], Loss: 0.9606\n",
      "Epoch [3/5], Step [9248/10336], Loss: 0.0225\n",
      "Epoch [3/5], Step [9250/10336], Loss: 0.0183\n",
      "Epoch [3/5], Step [9252/10336], Loss: 1.1511\n",
      "Epoch [3/5], Step [9254/10336], Loss: 0.1005\n",
      "Epoch [3/5], Step [9256/10336], Loss: 0.0815\n",
      "Epoch [3/5], Step [9258/10336], Loss: 0.3063\n",
      "Epoch [3/5], Step [9260/10336], Loss: 1.4281\n",
      "Epoch [3/5], Step [9262/10336], Loss: 0.1106\n",
      "Epoch [3/5], Step [9264/10336], Loss: 0.0204\n",
      "Epoch [3/5], Step [9266/10336], Loss: 0.2732\n",
      "Epoch [3/5], Step [9268/10336], Loss: 0.2416\n",
      "Epoch [3/5], Step [9270/10336], Loss: 0.0573\n",
      "Epoch [3/5], Step [9272/10336], Loss: 0.2152\n",
      "Epoch [3/5], Step [9274/10336], Loss: 0.3030\n",
      "Epoch [3/5], Step [9276/10336], Loss: 4.6582\n",
      "Epoch [3/5], Step [9278/10336], Loss: 0.0298\n",
      "Epoch [3/5], Step [9280/10336], Loss: 0.1824\n",
      "Epoch [3/5], Step [9282/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [9284/10336], Loss: 0.9485\n",
      "Epoch [3/5], Step [9286/10336], Loss: 0.4157\n",
      "Epoch [3/5], Step [9288/10336], Loss: 0.0238\n",
      "Epoch [3/5], Step [9290/10336], Loss: 0.2775\n",
      "Epoch [3/5], Step [9292/10336], Loss: 0.0258\n",
      "Epoch [3/5], Step [9294/10336], Loss: 0.3049\n",
      "Epoch [3/5], Step [9296/10336], Loss: 0.3284\n",
      "Epoch [3/5], Step [9298/10336], Loss: 0.3062\n",
      "Epoch [3/5], Step [9300/10336], Loss: 1.3386\n",
      "Epoch [3/5], Step [9302/10336], Loss: 2.5847\n",
      "Epoch [3/5], Step [9304/10336], Loss: 0.2136\n",
      "Epoch [3/5], Step [9306/10336], Loss: 0.2938\n",
      "Epoch [3/5], Step [9308/10336], Loss: 0.4509\n",
      "Epoch [3/5], Step [9310/10336], Loss: 0.2493\n",
      "Epoch [3/5], Step [9312/10336], Loss: 0.2625\n",
      "Epoch [3/5], Step [9314/10336], Loss: 0.0935\n",
      "Epoch [3/5], Step [9316/10336], Loss: 3.3281\n",
      "Epoch [3/5], Step [9318/10336], Loss: 0.2888\n",
      "Epoch [3/5], Step [9320/10336], Loss: 0.0843\n",
      "Epoch [3/5], Step [9322/10336], Loss: 1.1076\n",
      "Epoch [3/5], Step [9324/10336], Loss: 0.4882\n",
      "Epoch [3/5], Step [9326/10336], Loss: 0.1443\n",
      "Epoch [3/5], Step [9328/10336], Loss: 0.1016\n",
      "Epoch [3/5], Step [9330/10336], Loss: 0.4255\n",
      "Epoch [3/5], Step [9332/10336], Loss: 0.3966\n",
      "Epoch [3/5], Step [9334/10336], Loss: 1.3330\n",
      "Epoch [3/5], Step [9336/10336], Loss: 0.2418\n",
      "Epoch [3/5], Step [9338/10336], Loss: 2.7460\n",
      "Epoch [3/5], Step [9340/10336], Loss: 0.0918\n",
      "Epoch [3/5], Step [9342/10336], Loss: 0.3477\n",
      "Epoch [3/5], Step [9344/10336], Loss: 0.8097\n",
      "Epoch [3/5], Step [9346/10336], Loss: 0.0061\n",
      "Epoch [3/5], Step [9348/10336], Loss: 1.5737\n",
      "Epoch [3/5], Step [9350/10336], Loss: 0.4115\n",
      "Epoch [3/5], Step [9352/10336], Loss: 0.1877\n",
      "Epoch [3/5], Step [9354/10336], Loss: 0.0778\n",
      "Epoch [3/5], Step [9356/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [9358/10336], Loss: 1.3131\n",
      "Epoch [3/5], Step [9360/10336], Loss: 1.1902\n",
      "Epoch [3/5], Step [9362/10336], Loss: 0.1636\n",
      "Epoch [3/5], Step [9364/10336], Loss: 2.4251\n",
      "Epoch [3/5], Step [9366/10336], Loss: 2.2352\n",
      "Epoch [3/5], Step [9368/10336], Loss: 0.0403\n",
      "Epoch [3/5], Step [9370/10336], Loss: 0.9004\n",
      "Epoch [3/5], Step [9372/10336], Loss: 0.2065\n",
      "Epoch [3/5], Step [9374/10336], Loss: 0.1052\n",
      "Epoch [3/5], Step [9376/10336], Loss: 0.3861\n",
      "Epoch [3/5], Step [9378/10336], Loss: 0.0613\n",
      "Epoch [3/5], Step [9380/10336], Loss: 4.4403\n",
      "Epoch [3/5], Step [9382/10336], Loss: 0.0607\n",
      "Epoch [3/5], Step [9384/10336], Loss: 1.3282\n",
      "Epoch [3/5], Step [9386/10336], Loss: 2.4789\n",
      "Epoch [3/5], Step [9388/10336], Loss: 2.7707\n",
      "Epoch [3/5], Step [9390/10336], Loss: 0.3393\n",
      "Epoch [3/5], Step [9392/10336], Loss: 3.0455\n",
      "Epoch [3/5], Step [9394/10336], Loss: 1.2304\n",
      "Epoch [3/5], Step [9396/10336], Loss: 0.6897\n",
      "Epoch [3/5], Step [9398/10336], Loss: 0.0139\n",
      "Epoch [3/5], Step [9400/10336], Loss: 0.1746\n",
      "Epoch [3/5], Step [9402/10336], Loss: 0.3828\n",
      "Epoch [3/5], Step [9404/10336], Loss: 0.2728\n",
      "Epoch [3/5], Step [9406/10336], Loss: 0.0531\n",
      "Epoch [3/5], Step [9408/10336], Loss: 1.0554\n",
      "Epoch [3/5], Step [9410/10336], Loss: 0.9977\n",
      "Epoch [3/5], Step [9412/10336], Loss: 0.5001\n",
      "Epoch [3/5], Step [9414/10336], Loss: 6.1648\n",
      "Epoch [3/5], Step [9416/10336], Loss: 0.0358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [9418/10336], Loss: 0.2703\n",
      "Epoch [3/5], Step [9420/10336], Loss: 0.0281\n",
      "Epoch [3/5], Step [9422/10336], Loss: 1.5165\n",
      "Epoch [3/5], Step [9424/10336], Loss: 0.4758\n",
      "Epoch [3/5], Step [9426/10336], Loss: 1.3009\n",
      "Epoch [3/5], Step [9428/10336], Loss: 0.1487\n",
      "Epoch [3/5], Step [9430/10336], Loss: 0.0357\n",
      "Epoch [3/5], Step [9432/10336], Loss: 0.6498\n",
      "Epoch [3/5], Step [9434/10336], Loss: 0.4400\n",
      "Epoch [3/5], Step [9436/10336], Loss: 3.0949\n",
      "Epoch [3/5], Step [9438/10336], Loss: 0.1029\n",
      "Epoch [3/5], Step [9440/10336], Loss: 0.3783\n",
      "Epoch [3/5], Step [9442/10336], Loss: 0.2786\n",
      "Epoch [3/5], Step [9444/10336], Loss: 3.2315\n",
      "Epoch [3/5], Step [9446/10336], Loss: 0.1790\n",
      "Epoch [3/5], Step [9448/10336], Loss: 0.6011\n",
      "Epoch [3/5], Step [9450/10336], Loss: 0.0246\n",
      "Epoch [3/5], Step [9452/10336], Loss: 6.8164\n",
      "Epoch [3/5], Step [9454/10336], Loss: 0.0088\n",
      "Epoch [3/5], Step [9456/10336], Loss: 0.0887\n",
      "Epoch [3/5], Step [9458/10336], Loss: 2.0355\n",
      "Epoch [3/5], Step [9460/10336], Loss: 0.0568\n",
      "Epoch [3/5], Step [9462/10336], Loss: 0.3548\n",
      "Epoch [3/5], Step [9464/10336], Loss: 0.4232\n",
      "Epoch [3/5], Step [9466/10336], Loss: 0.4454\n",
      "Epoch [3/5], Step [9468/10336], Loss: 0.1106\n",
      "Epoch [3/5], Step [9470/10336], Loss: 0.3503\n",
      "Epoch [3/5], Step [9472/10336], Loss: 0.8431\n",
      "Epoch [3/5], Step [9474/10336], Loss: 2.1187\n",
      "Epoch [3/5], Step [9476/10336], Loss: 1.1331\n",
      "Epoch [3/5], Step [9478/10336], Loss: 0.0442\n",
      "Epoch [3/5], Step [9480/10336], Loss: 0.4816\n",
      "Epoch [3/5], Step [9482/10336], Loss: 0.1049\n",
      "Epoch [3/5], Step [9484/10336], Loss: 1.5476\n",
      "Epoch [3/5], Step [9486/10336], Loss: 0.8801\n",
      "Epoch [3/5], Step [9488/10336], Loss: 0.4628\n",
      "Epoch [3/5], Step [9490/10336], Loss: 0.9436\n",
      "Epoch [3/5], Step [9492/10336], Loss: 0.0167\n",
      "Epoch [3/5], Step [9494/10336], Loss: 0.5800\n",
      "Epoch [3/5], Step [9496/10336], Loss: 0.1922\n",
      "Epoch [3/5], Step [9498/10336], Loss: 0.4563\n",
      "Epoch [3/5], Step [9500/10336], Loss: 0.8651\n",
      "Epoch [3/5], Step [9502/10336], Loss: 0.0036\n",
      "Epoch [3/5], Step [9504/10336], Loss: 0.0134\n",
      "Epoch [3/5], Step [9506/10336], Loss: 1.2891\n",
      "Epoch [3/5], Step [9508/10336], Loss: 3.6803\n",
      "Epoch [3/5], Step [9510/10336], Loss: 1.2961\n",
      "Epoch [3/5], Step [9512/10336], Loss: 0.0267\n",
      "Epoch [3/5], Step [9514/10336], Loss: 0.3522\n",
      "Epoch [3/5], Step [9516/10336], Loss: 1.2808\n",
      "Epoch [3/5], Step [9518/10336], Loss: 1.8134\n",
      "Epoch [3/5], Step [9520/10336], Loss: 1.5082\n",
      "Epoch [3/5], Step [9522/10336], Loss: 0.7842\n",
      "Epoch [3/5], Step [9524/10336], Loss: 1.8738\n",
      "Epoch [3/5], Step [9526/10336], Loss: 2.0694\n",
      "Epoch [3/5], Step [9528/10336], Loss: 0.3411\n",
      "Epoch [3/5], Step [9530/10336], Loss: 0.2389\n",
      "Epoch [3/5], Step [9532/10336], Loss: 0.2289\n",
      "Epoch [3/5], Step [9534/10336], Loss: 0.8270\n",
      "Epoch [3/5], Step [9536/10336], Loss: 0.0294\n",
      "Epoch [3/5], Step [9538/10336], Loss: 0.3366\n",
      "Epoch [3/5], Step [9540/10336], Loss: 0.9873\n",
      "Epoch [3/5], Step [9542/10336], Loss: 1.6432\n",
      "Epoch [3/5], Step [9544/10336], Loss: 0.1790\n",
      "Epoch [3/5], Step [9546/10336], Loss: 0.6311\n",
      "Epoch [3/5], Step [9548/10336], Loss: 0.4379\n",
      "Epoch [3/5], Step [9550/10336], Loss: 2.0256\n",
      "Epoch [3/5], Step [9552/10336], Loss: 0.0026\n",
      "Epoch [3/5], Step [9554/10336], Loss: 0.0706\n",
      "Epoch [3/5], Step [9556/10336], Loss: 0.0756\n",
      "Epoch [3/5], Step [9558/10336], Loss: 0.1171\n",
      "Epoch [3/5], Step [9560/10336], Loss: 0.0390\n",
      "Epoch [3/5], Step [9562/10336], Loss: 0.0113\n",
      "Epoch [3/5], Step [9564/10336], Loss: 0.2060\n",
      "Epoch [3/5], Step [9566/10336], Loss: 0.0488\n",
      "Epoch [3/5], Step [9568/10336], Loss: 0.0466\n",
      "Epoch [3/5], Step [9570/10336], Loss: 0.2400\n",
      "Epoch [3/5], Step [9572/10336], Loss: 0.2387\n",
      "Epoch [3/5], Step [9574/10336], Loss: 2.7105\n",
      "Epoch [3/5], Step [9576/10336], Loss: 0.6951\n",
      "Epoch [3/5], Step [9578/10336], Loss: 0.0457\n",
      "Epoch [3/5], Step [9580/10336], Loss: 0.3830\n",
      "Epoch [3/5], Step [9582/10336], Loss: 0.0689\n",
      "Epoch [3/5], Step [9584/10336], Loss: 0.1832\n",
      "Epoch [3/5], Step [9586/10336], Loss: 0.1184\n",
      "Epoch [3/5], Step [9588/10336], Loss: 0.1291\n",
      "Epoch [3/5], Step [9590/10336], Loss: 0.7140\n",
      "Epoch [3/5], Step [9592/10336], Loss: 0.0163\n",
      "Epoch [3/5], Step [9594/10336], Loss: 0.6150\n",
      "Epoch [3/5], Step [9596/10336], Loss: 1.2444\n",
      "Epoch [3/5], Step [9598/10336], Loss: 0.0063\n",
      "Epoch [3/5], Step [9600/10336], Loss: 0.0156\n",
      "Epoch [3/5], Step [9602/10336], Loss: 0.3120\n",
      "Epoch [3/5], Step [9604/10336], Loss: 0.0512\n",
      "Epoch [3/5], Step [9606/10336], Loss: 0.9396\n",
      "Epoch [3/5], Step [9608/10336], Loss: 0.1510\n",
      "Epoch [3/5], Step [9610/10336], Loss: 0.7409\n",
      "Epoch [3/5], Step [9612/10336], Loss: 0.0123\n",
      "Epoch [3/5], Step [9614/10336], Loss: 0.2030\n",
      "Epoch [3/5], Step [9616/10336], Loss: 0.4552\n",
      "Epoch [3/5], Step [9618/10336], Loss: 1.2426\n",
      "Epoch [3/5], Step [9620/10336], Loss: 1.3826\n",
      "Epoch [3/5], Step [9622/10336], Loss: 0.4409\n",
      "Epoch [3/5], Step [9624/10336], Loss: 0.1997\n",
      "Epoch [3/5], Step [9626/10336], Loss: 2.6599\n",
      "Epoch [3/5], Step [9628/10336], Loss: 0.1067\n",
      "Epoch [3/5], Step [9630/10336], Loss: 1.4806\n",
      "Epoch [3/5], Step [9632/10336], Loss: 0.0032\n",
      "Epoch [3/5], Step [9634/10336], Loss: 0.0672\n",
      "Epoch [3/5], Step [9636/10336], Loss: 0.2955\n",
      "Epoch [3/5], Step [9638/10336], Loss: 0.6475\n",
      "Epoch [3/5], Step [9640/10336], Loss: 0.1841\n",
      "Epoch [3/5], Step [9642/10336], Loss: 0.6324\n",
      "Epoch [3/5], Step [9644/10336], Loss: 0.1058\n",
      "Epoch [3/5], Step [9646/10336], Loss: 0.1599\n",
      "Epoch [3/5], Step [9648/10336], Loss: 5.3181\n",
      "Epoch [3/5], Step [9650/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [9652/10336], Loss: 0.8774\n",
      "Epoch [3/5], Step [9654/10336], Loss: 0.4687\n",
      "Epoch [3/5], Step [9656/10336], Loss: 1.4026\n",
      "Epoch [3/5], Step [9658/10336], Loss: 0.3438\n",
      "Epoch [3/5], Step [9660/10336], Loss: 0.0150\n",
      "Epoch [3/5], Step [9662/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [9664/10336], Loss: 0.0681\n",
      "Epoch [3/5], Step [9666/10336], Loss: 1.5108\n",
      "Epoch [3/5], Step [9668/10336], Loss: 3.1022\n",
      "Epoch [3/5], Step [9670/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [9672/10336], Loss: 0.6406\n",
      "Epoch [3/5], Step [9674/10336], Loss: 0.0249\n",
      "Epoch [3/5], Step [9676/10336], Loss: 0.0977\n",
      "Epoch [3/5], Step [9678/10336], Loss: 0.1775\n",
      "Epoch [3/5], Step [9680/10336], Loss: 0.3618\n",
      "Epoch [3/5], Step [9682/10336], Loss: 0.2957\n",
      "Epoch [3/5], Step [9684/10336], Loss: 1.7145\n",
      "Epoch [3/5], Step [9686/10336], Loss: 1.4206\n",
      "Epoch [3/5], Step [9688/10336], Loss: 0.0134\n",
      "Epoch [3/5], Step [9690/10336], Loss: 0.1052\n",
      "Epoch [3/5], Step [9692/10336], Loss: 2.3919\n",
      "Epoch [3/5], Step [9694/10336], Loss: 0.2693\n",
      "Epoch [3/5], Step [9696/10336], Loss: 0.1597\n",
      "Epoch [3/5], Step [9698/10336], Loss: 3.0348\n",
      "Epoch [3/5], Step [9700/10336], Loss: 0.1838\n",
      "Epoch [3/5], Step [9702/10336], Loss: 0.3577\n",
      "Epoch [3/5], Step [9704/10336], Loss: 0.4862\n",
      "Epoch [3/5], Step [9706/10336], Loss: 0.0810\n",
      "Epoch [3/5], Step [9708/10336], Loss: 0.4524\n",
      "Epoch [3/5], Step [9710/10336], Loss: 1.3386\n",
      "Epoch [3/5], Step [9712/10336], Loss: 0.0851\n",
      "Epoch [3/5], Step [9714/10336], Loss: 0.2917\n",
      "Epoch [3/5], Step [9716/10336], Loss: 0.0404\n",
      "Epoch [3/5], Step [9718/10336], Loss: 0.3362\n",
      "Epoch [3/5], Step [9720/10336], Loss: 1.7884\n",
      "Epoch [3/5], Step [9722/10336], Loss: 0.3367\n",
      "Epoch [3/5], Step [9724/10336], Loss: 0.5395\n",
      "Epoch [3/5], Step [9726/10336], Loss: 1.1106\n",
      "Epoch [3/5], Step [9728/10336], Loss: 1.9378\n",
      "Epoch [3/5], Step [9730/10336], Loss: 3.7290\n",
      "Epoch [3/5], Step [9732/10336], Loss: 0.4813\n",
      "Epoch [3/5], Step [9734/10336], Loss: 0.3927\n",
      "Epoch [3/5], Step [9736/10336], Loss: 1.7678\n",
      "Epoch [3/5], Step [9738/10336], Loss: 0.0894\n",
      "Epoch [3/5], Step [9740/10336], Loss: 0.1856\n",
      "Epoch [3/5], Step [9742/10336], Loss: 0.2997\n",
      "Epoch [3/5], Step [9744/10336], Loss: 0.2484\n",
      "Epoch [3/5], Step [9746/10336], Loss: 0.3235\n",
      "Epoch [3/5], Step [9748/10336], Loss: 0.3539\n",
      "Epoch [3/5], Step [9750/10336], Loss: 0.5472\n",
      "Epoch [3/5], Step [9752/10336], Loss: 0.0028\n",
      "Epoch [3/5], Step [9754/10336], Loss: 0.3272\n",
      "Epoch [3/5], Step [9756/10336], Loss: 0.0160\n",
      "Epoch [3/5], Step [9758/10336], Loss: 1.4316\n",
      "Epoch [3/5], Step [9760/10336], Loss: 0.3445\n",
      "Epoch [3/5], Step [9762/10336], Loss: 0.0499\n",
      "Epoch [3/5], Step [9764/10336], Loss: 1.4779\n",
      "Epoch [3/5], Step [9766/10336], Loss: 2.0020\n",
      "Epoch [3/5], Step [9768/10336], Loss: 1.7359\n",
      "Epoch [3/5], Step [9770/10336], Loss: 0.0620\n",
      "Epoch [3/5], Step [9772/10336], Loss: 0.6806\n",
      "Epoch [3/5], Step [9774/10336], Loss: 1.3187\n",
      "Epoch [3/5], Step [9776/10336], Loss: 0.4067\n",
      "Epoch [3/5], Step [9778/10336], Loss: 1.7414\n",
      "Epoch [3/5], Step [9780/10336], Loss: 0.0683\n",
      "Epoch [3/5], Step [9782/10336], Loss: 0.0474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [9784/10336], Loss: 0.1253\n",
      "Epoch [3/5], Step [9786/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [9788/10336], Loss: 0.3383\n",
      "Epoch [3/5], Step [9790/10336], Loss: 0.0852\n",
      "Epoch [3/5], Step [9792/10336], Loss: 0.6781\n",
      "Epoch [3/5], Step [9794/10336], Loss: 1.0058\n",
      "Epoch [3/5], Step [9796/10336], Loss: 0.2397\n",
      "Epoch [3/5], Step [9798/10336], Loss: 0.0002\n",
      "Epoch [3/5], Step [9800/10336], Loss: 0.0827\n",
      "Epoch [3/5], Step [9802/10336], Loss: 0.0024\n",
      "Epoch [3/5], Step [9804/10336], Loss: 0.1724\n",
      "Epoch [3/5], Step [9806/10336], Loss: 0.0857\n",
      "Epoch [3/5], Step [9808/10336], Loss: 0.1845\n",
      "Epoch [3/5], Step [9810/10336], Loss: 0.7692\n",
      "Epoch [3/5], Step [9812/10336], Loss: 0.0095\n",
      "Epoch [3/5], Step [9814/10336], Loss: 0.8225\n",
      "Epoch [3/5], Step [9816/10336], Loss: 0.2219\n",
      "Epoch [3/5], Step [9818/10336], Loss: 0.1094\n",
      "Epoch [3/5], Step [9820/10336], Loss: 0.0152\n",
      "Epoch [3/5], Step [9822/10336], Loss: 0.6367\n",
      "Epoch [3/5], Step [9824/10336], Loss: 0.2429\n",
      "Epoch [3/5], Step [9826/10336], Loss: 0.0020\n",
      "Epoch [3/5], Step [9828/10336], Loss: 0.0510\n",
      "Epoch [3/5], Step [9830/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [9832/10336], Loss: 0.1313\n",
      "Epoch [3/5], Step [9834/10336], Loss: 0.1340\n",
      "Epoch [3/5], Step [9836/10336], Loss: 0.1769\n",
      "Epoch [3/5], Step [9838/10336], Loss: 0.4625\n",
      "Epoch [3/5], Step [9840/10336], Loss: 0.1603\n",
      "Epoch [3/5], Step [9842/10336], Loss: 0.7685\n",
      "Epoch [3/5], Step [9844/10336], Loss: 2.1179\n",
      "Epoch [3/5], Step [9846/10336], Loss: 0.2340\n",
      "Epoch [3/5], Step [9848/10336], Loss: 0.7193\n",
      "Epoch [3/5], Step [9850/10336], Loss: 0.0505\n",
      "Epoch [3/5], Step [9852/10336], Loss: 0.2251\n",
      "Epoch [3/5], Step [9854/10336], Loss: 0.2923\n",
      "Epoch [3/5], Step [9856/10336], Loss: 0.2662\n",
      "Epoch [3/5], Step [9858/10336], Loss: 0.0419\n",
      "Epoch [3/5], Step [9860/10336], Loss: 0.2049\n",
      "Epoch [3/5], Step [9862/10336], Loss: 0.0043\n",
      "Epoch [3/5], Step [9864/10336], Loss: 0.0537\n",
      "Epoch [3/5], Step [9866/10336], Loss: 0.2749\n",
      "Epoch [3/5], Step [9868/10336], Loss: 0.1905\n",
      "Epoch [3/5], Step [9870/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [9872/10336], Loss: 2.6214\n",
      "Epoch [3/5], Step [9874/10336], Loss: 1.5567\n",
      "Epoch [3/5], Step [9876/10336], Loss: 0.6271\n",
      "Epoch [3/5], Step [9878/10336], Loss: 0.1071\n",
      "Epoch [3/5], Step [9880/10336], Loss: 0.3124\n",
      "Epoch [3/5], Step [9882/10336], Loss: 0.0548\n",
      "Epoch [3/5], Step [9884/10336], Loss: 0.2622\n",
      "Epoch [3/5], Step [9886/10336], Loss: 0.4721\n",
      "Epoch [3/5], Step [9888/10336], Loss: 0.9400\n",
      "Epoch [3/5], Step [9890/10336], Loss: 0.5310\n",
      "Epoch [3/5], Step [9892/10336], Loss: 1.4194\n",
      "Epoch [3/5], Step [9894/10336], Loss: 2.2318\n",
      "Epoch [3/5], Step [9896/10336], Loss: 0.2848\n",
      "Epoch [3/5], Step [9898/10336], Loss: 1.2628\n",
      "Epoch [3/5], Step [9900/10336], Loss: 0.6532\n",
      "Epoch [3/5], Step [9902/10336], Loss: 1.5890\n",
      "Epoch [3/5], Step [9904/10336], Loss: 0.2073\n",
      "Epoch [3/5], Step [9906/10336], Loss: 0.5093\n",
      "Epoch [3/5], Step [9908/10336], Loss: 0.0632\n",
      "Epoch [3/5], Step [9910/10336], Loss: 0.1869\n",
      "Epoch [3/5], Step [9912/10336], Loss: 0.1575\n",
      "Epoch [3/5], Step [9914/10336], Loss: 1.2857\n",
      "Epoch [3/5], Step [9916/10336], Loss: 0.1701\n",
      "Epoch [3/5], Step [9918/10336], Loss: 0.4648\n",
      "Epoch [3/5], Step [9920/10336], Loss: 0.7009\n",
      "Epoch [3/5], Step [9922/10336], Loss: 0.0139\n",
      "Epoch [3/5], Step [9924/10336], Loss: 0.9135\n",
      "Epoch [3/5], Step [9926/10336], Loss: 0.4623\n",
      "Epoch [3/5], Step [9928/10336], Loss: 0.3280\n",
      "Epoch [3/5], Step [9930/10336], Loss: 1.3328\n",
      "Epoch [3/5], Step [9932/10336], Loss: 0.0488\n",
      "Epoch [3/5], Step [9934/10336], Loss: 0.9894\n",
      "Epoch [3/5], Step [9936/10336], Loss: 0.2657\n",
      "Epoch [3/5], Step [9938/10336], Loss: 0.0059\n",
      "Epoch [3/5], Step [9940/10336], Loss: 4.8907\n",
      "Epoch [3/5], Step [9942/10336], Loss: 4.4818\n",
      "Epoch [3/5], Step [9944/10336], Loss: 3.2395\n",
      "Epoch [3/5], Step [9946/10336], Loss: 1.3676\n",
      "Epoch [3/5], Step [9948/10336], Loss: 0.0074\n",
      "Epoch [3/5], Step [9950/10336], Loss: 0.3738\n",
      "Epoch [3/5], Step [9952/10336], Loss: 2.7548\n",
      "Epoch [3/5], Step [9954/10336], Loss: 1.0435\n",
      "Epoch [3/5], Step [9956/10336], Loss: 0.0780\n",
      "Epoch [3/5], Step [9958/10336], Loss: 0.1957\n",
      "Epoch [3/5], Step [9960/10336], Loss: 0.2033\n",
      "Epoch [3/5], Step [9962/10336], Loss: 0.2361\n",
      "Epoch [3/5], Step [9964/10336], Loss: 0.2492\n",
      "Epoch [3/5], Step [9966/10336], Loss: 0.0927\n",
      "Epoch [3/5], Step [9968/10336], Loss: 0.0202\n",
      "Epoch [3/5], Step [9970/10336], Loss: 0.0321\n",
      "Epoch [3/5], Step [9972/10336], Loss: 1.9432\n",
      "Epoch [3/5], Step [9974/10336], Loss: 0.4866\n",
      "Epoch [3/5], Step [9976/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [9978/10336], Loss: 0.4174\n",
      "Epoch [3/5], Step [9980/10336], Loss: 1.4001\n",
      "Epoch [3/5], Step [9982/10336], Loss: 0.1184\n",
      "Epoch [3/5], Step [9984/10336], Loss: 0.0940\n",
      "Epoch [3/5], Step [9986/10336], Loss: 0.0842\n",
      "Epoch [3/5], Step [9988/10336], Loss: 5.4730\n",
      "Epoch [3/5], Step [9990/10336], Loss: 0.0684\n",
      "Epoch [3/5], Step [9992/10336], Loss: 0.9456\n",
      "Epoch [3/5], Step [9994/10336], Loss: 0.9559\n",
      "Epoch [3/5], Step [9996/10336], Loss: 0.5269\n",
      "Epoch [3/5], Step [9998/10336], Loss: 0.0112\n",
      "Epoch [3/5], Step [10000/10336], Loss: 1.7958\n",
      "Epoch [3/5], Step [10002/10336], Loss: 0.8645\n",
      "Epoch [3/5], Step [10004/10336], Loss: 0.0165\n",
      "Epoch [3/5], Step [10006/10336], Loss: 0.4343\n",
      "Epoch [3/5], Step [10008/10336], Loss: 0.2475\n",
      "Epoch [3/5], Step [10010/10336], Loss: 0.8727\n",
      "Epoch [3/5], Step [10012/10336], Loss: 0.1458\n",
      "Epoch [3/5], Step [10014/10336], Loss: 0.0279\n",
      "Epoch [3/5], Step [10016/10336], Loss: 1.0220\n",
      "Epoch [3/5], Step [10018/10336], Loss: 0.1036\n",
      "Epoch [3/5], Step [10020/10336], Loss: 0.8371\n",
      "Epoch [3/5], Step [10022/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [10024/10336], Loss: 1.3703\n",
      "Epoch [3/5], Step [10026/10336], Loss: 0.8033\n",
      "Epoch [3/5], Step [10028/10336], Loss: 0.0656\n",
      "Epoch [3/5], Step [10030/10336], Loss: 0.5089\n",
      "Epoch [3/5], Step [10032/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [10034/10336], Loss: 0.0829\n",
      "Epoch [3/5], Step [10036/10336], Loss: 0.0048\n",
      "Epoch [3/5], Step [10038/10336], Loss: 0.3791\n",
      "Epoch [3/5], Step [10040/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [10042/10336], Loss: 0.1304\n",
      "Epoch [3/5], Step [10044/10336], Loss: 0.4718\n",
      "Epoch [3/5], Step [10046/10336], Loss: 0.0196\n",
      "Epoch [3/5], Step [10048/10336], Loss: 0.2027\n",
      "Epoch [3/5], Step [10050/10336], Loss: 2.3692\n",
      "Epoch [3/5], Step [10052/10336], Loss: 1.0292\n",
      "Epoch [3/5], Step [10054/10336], Loss: 3.7156\n",
      "Epoch [3/5], Step [10056/10336], Loss: 0.4724\n",
      "Epoch [3/5], Step [10058/10336], Loss: 0.1894\n",
      "Epoch [3/5], Step [10060/10336], Loss: 3.3360\n",
      "Epoch [3/5], Step [10062/10336], Loss: 0.1006\n",
      "Epoch [3/5], Step [10064/10336], Loss: 1.2600\n",
      "Epoch [3/5], Step [10066/10336], Loss: 1.1120\n",
      "Epoch [3/5], Step [10068/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [10070/10336], Loss: 0.1466\n",
      "Epoch [3/5], Step [10072/10336], Loss: 0.3552\n",
      "Epoch [3/5], Step [10074/10336], Loss: 0.6768\n",
      "Epoch [3/5], Step [10076/10336], Loss: 0.2703\n",
      "Epoch [3/5], Step [10078/10336], Loss: 0.0119\n",
      "Epoch [3/5], Step [10080/10336], Loss: 0.8139\n",
      "Epoch [3/5], Step [10082/10336], Loss: 0.1160\n",
      "Epoch [3/5], Step [10084/10336], Loss: 0.9159\n",
      "Epoch [3/5], Step [10086/10336], Loss: 0.4349\n",
      "Epoch [3/5], Step [10088/10336], Loss: 0.0782\n",
      "Epoch [3/5], Step [10090/10336], Loss: 0.1147\n",
      "Epoch [3/5], Step [10092/10336], Loss: 0.0318\n",
      "Epoch [3/5], Step [10094/10336], Loss: 0.3053\n",
      "Epoch [3/5], Step [10096/10336], Loss: 0.6727\n",
      "Epoch [3/5], Step [10098/10336], Loss: 0.9994\n",
      "Epoch [3/5], Step [10100/10336], Loss: 0.1363\n",
      "Epoch [3/5], Step [10102/10336], Loss: 0.5632\n",
      "Epoch [3/5], Step [10104/10336], Loss: 0.0496\n",
      "Epoch [3/5], Step [10106/10336], Loss: 0.5309\n",
      "Epoch [3/5], Step [10108/10336], Loss: 0.5048\n",
      "Epoch [3/5], Step [10110/10336], Loss: 0.2203\n",
      "Epoch [3/5], Step [10112/10336], Loss: 0.0257\n",
      "Epoch [3/5], Step [10114/10336], Loss: 0.1071\n",
      "Epoch [3/5], Step [10116/10336], Loss: 0.0695\n",
      "Epoch [3/5], Step [10118/10336], Loss: 0.5475\n",
      "Epoch [3/5], Step [10120/10336], Loss: 0.1411\n",
      "Epoch [3/5], Step [10122/10336], Loss: 0.1775\n",
      "Epoch [3/5], Step [10124/10336], Loss: 0.2894\n",
      "Epoch [3/5], Step [10126/10336], Loss: 1.1943\n",
      "Epoch [3/5], Step [10128/10336], Loss: 0.0056\n",
      "Epoch [3/5], Step [10130/10336], Loss: 0.0784\n",
      "Epoch [3/5], Step [10132/10336], Loss: 1.1095\n",
      "Epoch [3/5], Step [10134/10336], Loss: 4.2279\n",
      "Epoch [3/5], Step [10136/10336], Loss: 0.7593\n",
      "Epoch [3/5], Step [10138/10336], Loss: 0.8733\n",
      "Epoch [3/5], Step [10140/10336], Loss: 0.1693\n",
      "Epoch [3/5], Step [10142/10336], Loss: 0.0244\n",
      "Epoch [3/5], Step [10144/10336], Loss: 0.1172\n",
      "Epoch [3/5], Step [10146/10336], Loss: 0.1897\n",
      "Epoch [3/5], Step [10148/10336], Loss: 3.1836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [10150/10336], Loss: 0.2148\n",
      "Epoch [3/5], Step [10152/10336], Loss: 0.0530\n",
      "Epoch [3/5], Step [10154/10336], Loss: 0.4345\n",
      "Epoch [3/5], Step [10156/10336], Loss: 0.0316\n",
      "Epoch [3/5], Step [10158/10336], Loss: 0.4323\n",
      "Epoch [3/5], Step [10160/10336], Loss: 0.0450\n",
      "Epoch [3/5], Step [10162/10336], Loss: 0.2998\n",
      "Epoch [3/5], Step [10164/10336], Loss: 0.5564\n",
      "Epoch [3/5], Step [10166/10336], Loss: 3.5019\n",
      "Epoch [3/5], Step [10168/10336], Loss: 0.4663\n",
      "Epoch [3/5], Step [10170/10336], Loss: 0.0097\n",
      "Epoch [3/5], Step [10172/10336], Loss: 0.0745\n",
      "Epoch [3/5], Step [10174/10336], Loss: 0.0378\n",
      "Epoch [3/5], Step [10176/10336], Loss: 3.7866\n",
      "Epoch [3/5], Step [10178/10336], Loss: 0.0652\n",
      "Epoch [3/5], Step [10180/10336], Loss: 0.0173\n",
      "Epoch [3/5], Step [10182/10336], Loss: 0.1294\n",
      "Epoch [3/5], Step [10184/10336], Loss: 0.3431\n",
      "Epoch [3/5], Step [10186/10336], Loss: 0.9908\n",
      "Epoch [3/5], Step [10188/10336], Loss: 0.0164\n",
      "Epoch [3/5], Step [10190/10336], Loss: 1.8730\n",
      "Epoch [3/5], Step [10192/10336], Loss: 0.4688\n",
      "Epoch [3/5], Step [10194/10336], Loss: 1.7330\n",
      "Epoch [3/5], Step [10196/10336], Loss: 1.4039\n",
      "Epoch [3/5], Step [10198/10336], Loss: 0.0761\n",
      "Epoch [3/5], Step [10200/10336], Loss: 0.0038\n",
      "Epoch [3/5], Step [10202/10336], Loss: 0.2518\n",
      "Epoch [3/5], Step [10204/10336], Loss: 0.1392\n",
      "Epoch [3/5], Step [10206/10336], Loss: 0.0102\n",
      "Epoch [3/5], Step [10208/10336], Loss: 0.0408\n",
      "Epoch [3/5], Step [10210/10336], Loss: 0.0366\n",
      "Epoch [3/5], Step [10212/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [10214/10336], Loss: 4.0350\n",
      "Epoch [3/5], Step [10216/10336], Loss: 0.3952\n",
      "Epoch [3/5], Step [10218/10336], Loss: 0.1914\n",
      "Epoch [3/5], Step [10220/10336], Loss: 0.3731\n",
      "Epoch [3/5], Step [10222/10336], Loss: 0.4533\n",
      "Epoch [3/5], Step [10224/10336], Loss: 0.4594\n",
      "Epoch [3/5], Step [10226/10336], Loss: 0.0696\n",
      "Epoch [3/5], Step [10228/10336], Loss: 0.0895\n",
      "Epoch [3/5], Step [10230/10336], Loss: 1.6446\n",
      "Epoch [3/5], Step [10232/10336], Loss: 1.0163\n",
      "Epoch [3/5], Step [10234/10336], Loss: 0.0061\n",
      "Epoch [3/5], Step [10236/10336], Loss: 0.0144\n",
      "Epoch [3/5], Step [10238/10336], Loss: 0.6945\n",
      "Epoch [3/5], Step [10240/10336], Loss: 1.6830\n",
      "Epoch [3/5], Step [10242/10336], Loss: 3.0299\n",
      "Epoch [3/5], Step [10244/10336], Loss: 0.0318\n",
      "Epoch [3/5], Step [10246/10336], Loss: 0.0121\n",
      "Epoch [3/5], Step [10248/10336], Loss: 0.1999\n",
      "Epoch [3/5], Step [10250/10336], Loss: 1.5477\n",
      "Epoch [3/5], Step [10252/10336], Loss: 2.0512\n",
      "Epoch [3/5], Step [10254/10336], Loss: 0.0853\n",
      "Epoch [3/5], Step [10256/10336], Loss: 0.0768\n",
      "Epoch [3/5], Step [10258/10336], Loss: 2.5981\n",
      "Epoch [3/5], Step [10260/10336], Loss: 0.4360\n",
      "Epoch [3/5], Step [10262/10336], Loss: 0.0574\n",
      "Epoch [3/5], Step [10264/10336], Loss: 0.1594\n",
      "Epoch [3/5], Step [10266/10336], Loss: 0.0165\n",
      "Epoch [3/5], Step [10268/10336], Loss: 0.2357\n",
      "Epoch [3/5], Step [10270/10336], Loss: 0.0318\n",
      "Epoch [3/5], Step [10272/10336], Loss: 0.3449\n",
      "Epoch [3/5], Step [10274/10336], Loss: 0.2473\n",
      "Epoch [3/5], Step [10276/10336], Loss: 0.3804\n",
      "Epoch [3/5], Step [10278/10336], Loss: 0.0587\n",
      "Epoch [3/5], Step [10280/10336], Loss: 0.2291\n",
      "Epoch [3/5], Step [10282/10336], Loss: 0.6941\n",
      "Epoch [3/5], Step [10284/10336], Loss: 0.0127\n",
      "Epoch [3/5], Step [10286/10336], Loss: 0.3170\n",
      "Epoch [3/5], Step [10288/10336], Loss: 2.6728\n",
      "Epoch [3/5], Step [10290/10336], Loss: 0.0175\n",
      "Epoch [3/5], Step [10292/10336], Loss: 0.1720\n",
      "Epoch [3/5], Step [10294/10336], Loss: 0.2955\n",
      "Epoch [3/5], Step [10296/10336], Loss: 1.9393\n",
      "Epoch [3/5], Step [10298/10336], Loss: 0.2106\n",
      "Epoch [3/5], Step [10300/10336], Loss: 1.6829\n",
      "Epoch [3/5], Step [10302/10336], Loss: 2.8325\n",
      "Epoch [3/5], Step [10304/10336], Loss: 0.4497\n",
      "Epoch [3/5], Step [10306/10336], Loss: 3.6095\n",
      "Epoch [3/5], Step [10308/10336], Loss: 0.1841\n",
      "Epoch [3/5], Step [10310/10336], Loss: 0.2210\n",
      "Epoch [3/5], Step [10312/10336], Loss: 0.2370\n",
      "Epoch [3/5], Step [10314/10336], Loss: 0.1425\n",
      "Epoch [3/5], Step [10316/10336], Loss: 0.1785\n",
      "Epoch [3/5], Step [10318/10336], Loss: 0.5625\n",
      "Epoch [3/5], Step [10320/10336], Loss: 0.6232\n",
      "Epoch [3/5], Step [10322/10336], Loss: 4.2759\n",
      "Epoch [3/5], Step [10324/10336], Loss: 0.1182\n",
      "Epoch [3/5], Step [10326/10336], Loss: 0.0103\n",
      "Epoch [3/5], Step [10328/10336], Loss: 0.6154\n",
      "Epoch [3/5], Step [10330/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [10332/10336], Loss: 0.0821\n",
      "Epoch [3/5], Step [10334/10336], Loss: 0.0963\n",
      "Epoch [3/5], Step [10336/10336], Loss: 1.0300\n",
      "Epoch [4/5], Step [2/10336], Loss: 0.2077\n",
      "Epoch [4/5], Step [4/10336], Loss: 0.0768\n",
      "Epoch [4/5], Step [6/10336], Loss: 1.4323\n",
      "Epoch [4/5], Step [8/10336], Loss: 0.1332\n",
      "Epoch [4/5], Step [10/10336], Loss: 0.0081\n",
      "Epoch [4/5], Step [12/10336], Loss: 0.0099\n",
      "Epoch [4/5], Step [14/10336], Loss: 0.0695\n",
      "Epoch [4/5], Step [16/10336], Loss: 0.3070\n",
      "Epoch [4/5], Step [18/10336], Loss: 0.4234\n",
      "Epoch [4/5], Step [20/10336], Loss: 0.0259\n",
      "Epoch [4/5], Step [22/10336], Loss: 2.3658\n",
      "Epoch [4/5], Step [24/10336], Loss: 0.0810\n",
      "Epoch [4/5], Step [26/10336], Loss: 0.1164\n",
      "Epoch [4/5], Step [28/10336], Loss: 2.3604\n",
      "Epoch [4/5], Step [30/10336], Loss: 1.8797\n",
      "Epoch [4/5], Step [32/10336], Loss: 1.2859\n",
      "Epoch [4/5], Step [34/10336], Loss: 0.2782\n",
      "Epoch [4/5], Step [36/10336], Loss: 0.0413\n",
      "Epoch [4/5], Step [38/10336], Loss: 1.0104\n",
      "Epoch [4/5], Step [40/10336], Loss: 0.3505\n",
      "Epoch [4/5], Step [42/10336], Loss: 0.0056\n",
      "Epoch [4/5], Step [44/10336], Loss: 4.5594\n",
      "Epoch [4/5], Step [46/10336], Loss: 0.0271\n",
      "Epoch [4/5], Step [48/10336], Loss: 0.2973\n",
      "Epoch [4/5], Step [50/10336], Loss: 0.5419\n",
      "Epoch [4/5], Step [52/10336], Loss: 0.0132\n",
      "Epoch [4/5], Step [54/10336], Loss: 0.7517\n",
      "Epoch [4/5], Step [56/10336], Loss: 0.2170\n",
      "Epoch [4/5], Step [58/10336], Loss: 0.0445\n",
      "Epoch [4/5], Step [60/10336], Loss: 1.1412\n",
      "Epoch [4/5], Step [62/10336], Loss: 0.2609\n",
      "Epoch [4/5], Step [64/10336], Loss: 0.6789\n",
      "Epoch [4/5], Step [66/10336], Loss: 0.0083\n",
      "Epoch [4/5], Step [68/10336], Loss: 0.8904\n",
      "Epoch [4/5], Step [70/10336], Loss: 0.2878\n",
      "Epoch [4/5], Step [72/10336], Loss: 0.3525\n",
      "Epoch [4/5], Step [74/10336], Loss: 0.0085\n",
      "Epoch [4/5], Step [76/10336], Loss: 0.9288\n",
      "Epoch [4/5], Step [78/10336], Loss: 0.1260\n",
      "Epoch [4/5], Step [80/10336], Loss: 1.2502\n",
      "Epoch [4/5], Step [82/10336], Loss: 0.4108\n",
      "Epoch [4/5], Step [84/10336], Loss: 0.0258\n",
      "Epoch [4/5], Step [86/10336], Loss: 0.0082\n",
      "Epoch [4/5], Step [88/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [90/10336], Loss: 0.0137\n",
      "Epoch [4/5], Step [92/10336], Loss: 0.1087\n",
      "Epoch [4/5], Step [94/10336], Loss: 0.0740\n",
      "Epoch [4/5], Step [96/10336], Loss: 0.9646\n",
      "Epoch [4/5], Step [98/10336], Loss: 0.7446\n",
      "Epoch [4/5], Step [100/10336], Loss: 0.1462\n",
      "Epoch [4/5], Step [102/10336], Loss: 0.0298\n",
      "Epoch [4/5], Step [104/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [106/10336], Loss: 0.3909\n",
      "Epoch [4/5], Step [108/10336], Loss: 0.1259\n",
      "Epoch [4/5], Step [110/10336], Loss: 0.0176\n",
      "Epoch [4/5], Step [112/10336], Loss: 0.4794\n",
      "Epoch [4/5], Step [114/10336], Loss: 0.1681\n",
      "Epoch [4/5], Step [116/10336], Loss: 1.9579\n",
      "Epoch [4/5], Step [118/10336], Loss: 0.5641\n",
      "Epoch [4/5], Step [120/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [122/10336], Loss: 1.2582\n",
      "Epoch [4/5], Step [124/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [126/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [128/10336], Loss: 0.3406\n",
      "Epoch [4/5], Step [130/10336], Loss: 3.8731\n",
      "Epoch [4/5], Step [132/10336], Loss: 0.0955\n",
      "Epoch [4/5], Step [134/10336], Loss: 0.1965\n",
      "Epoch [4/5], Step [136/10336], Loss: 0.2005\n",
      "Epoch [4/5], Step [138/10336], Loss: 2.4402\n",
      "Epoch [4/5], Step [140/10336], Loss: 0.0171\n",
      "Epoch [4/5], Step [142/10336], Loss: 0.4216\n",
      "Epoch [4/5], Step [144/10336], Loss: 0.9890\n",
      "Epoch [4/5], Step [146/10336], Loss: 0.9018\n",
      "Epoch [4/5], Step [148/10336], Loss: 0.0474\n",
      "Epoch [4/5], Step [150/10336], Loss: 0.2937\n",
      "Epoch [4/5], Step [152/10336], Loss: 0.4092\n",
      "Epoch [4/5], Step [154/10336], Loss: 0.0651\n",
      "Epoch [4/5], Step [156/10336], Loss: 0.3557\n",
      "Epoch [4/5], Step [158/10336], Loss: 0.0213\n",
      "Epoch [4/5], Step [160/10336], Loss: 0.0114\n",
      "Epoch [4/5], Step [162/10336], Loss: 3.1660\n",
      "Epoch [4/5], Step [164/10336], Loss: 0.6836\n",
      "Epoch [4/5], Step [166/10336], Loss: 0.5771\n",
      "Epoch [4/5], Step [168/10336], Loss: 0.3991\n",
      "Epoch [4/5], Step [170/10336], Loss: 0.0266\n",
      "Epoch [4/5], Step [172/10336], Loss: 0.1626\n",
      "Epoch [4/5], Step [174/10336], Loss: 0.2470\n",
      "Epoch [4/5], Step [176/10336], Loss: 1.0658\n",
      "Epoch [4/5], Step [178/10336], Loss: 0.6270\n",
      "Epoch [4/5], Step [180/10336], Loss: 0.0377\n",
      "Epoch [4/5], Step [182/10336], Loss: 0.3973\n",
      "Epoch [4/5], Step [184/10336], Loss: 0.4105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [186/10336], Loss: 0.0261\n",
      "Epoch [4/5], Step [188/10336], Loss: 0.0283\n",
      "Epoch [4/5], Step [190/10336], Loss: 0.5298\n",
      "Epoch [4/5], Step [192/10336], Loss: 0.8880\n",
      "Epoch [4/5], Step [194/10336], Loss: 0.0805\n",
      "Epoch [4/5], Step [196/10336], Loss: 0.2343\n",
      "Epoch [4/5], Step [198/10336], Loss: 0.2748\n",
      "Epoch [4/5], Step [200/10336], Loss: 1.8803\n",
      "Epoch [4/5], Step [202/10336], Loss: 2.7407\n",
      "Epoch [4/5], Step [204/10336], Loss: 0.7746\n",
      "Epoch [4/5], Step [206/10336], Loss: 0.0154\n",
      "Epoch [4/5], Step [208/10336], Loss: 0.3955\n",
      "Epoch [4/5], Step [210/10336], Loss: 0.2333\n",
      "Epoch [4/5], Step [212/10336], Loss: 4.7160\n",
      "Epoch [4/5], Step [214/10336], Loss: 0.0738\n",
      "Epoch [4/5], Step [216/10336], Loss: 0.4756\n",
      "Epoch [4/5], Step [218/10336], Loss: 0.5598\n",
      "Epoch [4/5], Step [220/10336], Loss: 2.4157\n",
      "Epoch [4/5], Step [222/10336], Loss: 0.2622\n",
      "Epoch [4/5], Step [224/10336], Loss: 0.2093\n",
      "Epoch [4/5], Step [226/10336], Loss: 0.0313\n",
      "Epoch [4/5], Step [228/10336], Loss: 2.2609\n",
      "Epoch [4/5], Step [230/10336], Loss: 0.0082\n",
      "Epoch [4/5], Step [232/10336], Loss: 0.2400\n",
      "Epoch [4/5], Step [234/10336], Loss: 0.3736\n",
      "Epoch [4/5], Step [236/10336], Loss: 0.1178\n",
      "Epoch [4/5], Step [238/10336], Loss: 0.0714\n",
      "Epoch [4/5], Step [240/10336], Loss: 0.5818\n",
      "Epoch [4/5], Step [242/10336], Loss: 0.2064\n",
      "Epoch [4/5], Step [244/10336], Loss: 0.2561\n",
      "Epoch [4/5], Step [246/10336], Loss: 2.7347\n",
      "Epoch [4/5], Step [248/10336], Loss: 0.1513\n",
      "Epoch [4/5], Step [250/10336], Loss: 0.4995\n",
      "Epoch [4/5], Step [252/10336], Loss: 0.0249\n",
      "Epoch [4/5], Step [254/10336], Loss: 0.6186\n",
      "Epoch [4/5], Step [256/10336], Loss: 0.2696\n",
      "Epoch [4/5], Step [258/10336], Loss: 0.4094\n",
      "Epoch [4/5], Step [260/10336], Loss: 0.5446\n",
      "Epoch [4/5], Step [262/10336], Loss: 0.2783\n",
      "Epoch [4/5], Step [264/10336], Loss: 0.0534\n",
      "Epoch [4/5], Step [266/10336], Loss: 1.5439\n",
      "Epoch [4/5], Step [268/10336], Loss: 1.5052\n",
      "Epoch [4/5], Step [270/10336], Loss: 0.2176\n",
      "Epoch [4/5], Step [272/10336], Loss: 0.7189\n",
      "Epoch [4/5], Step [274/10336], Loss: 0.0606\n",
      "Epoch [4/5], Step [276/10336], Loss: 0.2816\n",
      "Epoch [4/5], Step [278/10336], Loss: 1.8779\n",
      "Epoch [4/5], Step [280/10336], Loss: 3.5123\n",
      "Epoch [4/5], Step [282/10336], Loss: 0.6527\n",
      "Epoch [4/5], Step [284/10336], Loss: 0.0838\n",
      "Epoch [4/5], Step [286/10336], Loss: 0.2714\n",
      "Epoch [4/5], Step [288/10336], Loss: 1.6560\n",
      "Epoch [4/5], Step [290/10336], Loss: 0.1534\n",
      "Epoch [4/5], Step [292/10336], Loss: 0.0390\n",
      "Epoch [4/5], Step [294/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [296/10336], Loss: 0.1592\n",
      "Epoch [4/5], Step [298/10336], Loss: 1.3802\n",
      "Epoch [4/5], Step [300/10336], Loss: 0.0388\n",
      "Epoch [4/5], Step [302/10336], Loss: 0.0405\n",
      "Epoch [4/5], Step [304/10336], Loss: 0.6216\n",
      "Epoch [4/5], Step [306/10336], Loss: 0.0060\n",
      "Epoch [4/5], Step [308/10336], Loss: 0.0845\n",
      "Epoch [4/5], Step [310/10336], Loss: 0.1470\n",
      "Epoch [4/5], Step [312/10336], Loss: 4.2429\n",
      "Epoch [4/5], Step [314/10336], Loss: 0.0991\n",
      "Epoch [4/5], Step [316/10336], Loss: 0.1366\n",
      "Epoch [4/5], Step [318/10336], Loss: 2.5810\n",
      "Epoch [4/5], Step [320/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [322/10336], Loss: 0.0318\n",
      "Epoch [4/5], Step [324/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [326/10336], Loss: 3.0172\n",
      "Epoch [4/5], Step [328/10336], Loss: 0.2652\n",
      "Epoch [4/5], Step [330/10336], Loss: 0.2952\n",
      "Epoch [4/5], Step [332/10336], Loss: 1.3302\n",
      "Epoch [4/5], Step [334/10336], Loss: 0.2117\n",
      "Epoch [4/5], Step [336/10336], Loss: 0.0117\n",
      "Epoch [4/5], Step [338/10336], Loss: 0.6063\n",
      "Epoch [4/5], Step [340/10336], Loss: 0.0128\n",
      "Epoch [4/5], Step [342/10336], Loss: 3.4671\n",
      "Epoch [4/5], Step [344/10336], Loss: 0.4120\n",
      "Epoch [4/5], Step [346/10336], Loss: 0.1375\n",
      "Epoch [4/5], Step [348/10336], Loss: 0.9387\n",
      "Epoch [4/5], Step [350/10336], Loss: 0.1701\n",
      "Epoch [4/5], Step [352/10336], Loss: 1.9376\n",
      "Epoch [4/5], Step [354/10336], Loss: 2.4351\n",
      "Epoch [4/5], Step [356/10336], Loss: 1.3617\n",
      "Epoch [4/5], Step [358/10336], Loss: 0.0312\n",
      "Epoch [4/5], Step [360/10336], Loss: 0.0386\n",
      "Epoch [4/5], Step [362/10336], Loss: 0.2806\n",
      "Epoch [4/5], Step [364/10336], Loss: 0.0309\n",
      "Epoch [4/5], Step [366/10336], Loss: 0.2000\n",
      "Epoch [4/5], Step [368/10336], Loss: 0.2478\n",
      "Epoch [4/5], Step [370/10336], Loss: 3.5728\n",
      "Epoch [4/5], Step [372/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [374/10336], Loss: 0.2755\n",
      "Epoch [4/5], Step [376/10336], Loss: 0.1058\n",
      "Epoch [4/5], Step [378/10336], Loss: 0.0582\n",
      "Epoch [4/5], Step [380/10336], Loss: 0.2146\n",
      "Epoch [4/5], Step [382/10336], Loss: 0.1889\n",
      "Epoch [4/5], Step [384/10336], Loss: 0.2886\n",
      "Epoch [4/5], Step [386/10336], Loss: 0.3952\n",
      "Epoch [4/5], Step [388/10336], Loss: 0.0313\n",
      "Epoch [4/5], Step [390/10336], Loss: 0.1857\n",
      "Epoch [4/5], Step [392/10336], Loss: 0.0983\n",
      "Epoch [4/5], Step [394/10336], Loss: 0.2571\n",
      "Epoch [4/5], Step [396/10336], Loss: 2.1074\n",
      "Epoch [4/5], Step [398/10336], Loss: 1.5240\n",
      "Epoch [4/5], Step [400/10336], Loss: 0.2590\n",
      "Epoch [4/5], Step [402/10336], Loss: 1.6394\n",
      "Epoch [4/5], Step [404/10336], Loss: 0.1382\n",
      "Epoch [4/5], Step [406/10336], Loss: 0.5076\n",
      "Epoch [4/5], Step [408/10336], Loss: 0.0881\n",
      "Epoch [4/5], Step [410/10336], Loss: 0.1237\n",
      "Epoch [4/5], Step [412/10336], Loss: 0.0172\n",
      "Epoch [4/5], Step [414/10336], Loss: 1.2723\n",
      "Epoch [4/5], Step [416/10336], Loss: 0.1573\n",
      "Epoch [4/5], Step [418/10336], Loss: 0.3381\n",
      "Epoch [4/5], Step [420/10336], Loss: 0.0125\n",
      "Epoch [4/5], Step [422/10336], Loss: 0.2008\n",
      "Epoch [4/5], Step [424/10336], Loss: 0.0131\n",
      "Epoch [4/5], Step [426/10336], Loss: 0.0983\n",
      "Epoch [4/5], Step [428/10336], Loss: 0.2947\n",
      "Epoch [4/5], Step [430/10336], Loss: 0.0972\n",
      "Epoch [4/5], Step [432/10336], Loss: 0.0084\n",
      "Epoch [4/5], Step [434/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [436/10336], Loss: 0.0291\n",
      "Epoch [4/5], Step [438/10336], Loss: 0.0780\n",
      "Epoch [4/5], Step [440/10336], Loss: 0.8709\n",
      "Epoch [4/5], Step [442/10336], Loss: 0.3133\n",
      "Epoch [4/5], Step [444/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [446/10336], Loss: 4.9283\n",
      "Epoch [4/5], Step [448/10336], Loss: 0.2028\n",
      "Epoch [4/5], Step [450/10336], Loss: 0.3510\n",
      "Epoch [4/5], Step [452/10336], Loss: 0.0359\n",
      "Epoch [4/5], Step [454/10336], Loss: 2.8199\n",
      "Epoch [4/5], Step [456/10336], Loss: 0.0843\n",
      "Epoch [4/5], Step [458/10336], Loss: 0.3536\n",
      "Epoch [4/5], Step [460/10336], Loss: 0.7328\n",
      "Epoch [4/5], Step [462/10336], Loss: 0.1791\n",
      "Epoch [4/5], Step [464/10336], Loss: 0.1137\n",
      "Epoch [4/5], Step [466/10336], Loss: 0.1531\n",
      "Epoch [4/5], Step [468/10336], Loss: 0.4328\n",
      "Epoch [4/5], Step [470/10336], Loss: 0.7189\n",
      "Epoch [4/5], Step [472/10336], Loss: 0.2008\n",
      "Epoch [4/5], Step [474/10336], Loss: 0.1265\n",
      "Epoch [4/5], Step [476/10336], Loss: 4.3601\n",
      "Epoch [4/5], Step [478/10336], Loss: 1.6460\n",
      "Epoch [4/5], Step [480/10336], Loss: 0.3506\n",
      "Epoch [4/5], Step [482/10336], Loss: 0.9289\n",
      "Epoch [4/5], Step [484/10336], Loss: 0.0949\n",
      "Epoch [4/5], Step [486/10336], Loss: 0.1531\n",
      "Epoch [4/5], Step [488/10336], Loss: 0.3507\n",
      "Epoch [4/5], Step [490/10336], Loss: 2.3233\n",
      "Epoch [4/5], Step [492/10336], Loss: 0.0045\n",
      "Epoch [4/5], Step [494/10336], Loss: 0.2815\n",
      "Epoch [4/5], Step [496/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [498/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [500/10336], Loss: 0.1873\n",
      "Epoch [4/5], Step [502/10336], Loss: 1.3413\n",
      "Epoch [4/5], Step [504/10336], Loss: 0.7183\n",
      "Epoch [4/5], Step [506/10336], Loss: 0.0766\n",
      "Epoch [4/5], Step [508/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [510/10336], Loss: 0.0930\n",
      "Epoch [4/5], Step [512/10336], Loss: 0.4320\n",
      "Epoch [4/5], Step [514/10336], Loss: 0.2205\n",
      "Epoch [4/5], Step [516/10336], Loss: 0.2737\n",
      "Epoch [4/5], Step [518/10336], Loss: 0.7850\n",
      "Epoch [4/5], Step [520/10336], Loss: 0.2231\n",
      "Epoch [4/5], Step [522/10336], Loss: 1.1958\n",
      "Epoch [4/5], Step [524/10336], Loss: 0.0231\n",
      "Epoch [4/5], Step [526/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [528/10336], Loss: 1.1453\n",
      "Epoch [4/5], Step [530/10336], Loss: 1.0486\n",
      "Epoch [4/5], Step [532/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [534/10336], Loss: 1.8172\n",
      "Epoch [4/5], Step [536/10336], Loss: 0.2528\n",
      "Epoch [4/5], Step [538/10336], Loss: 0.0209\n",
      "Epoch [4/5], Step [540/10336], Loss: 0.0928\n",
      "Epoch [4/5], Step [542/10336], Loss: 0.0217\n",
      "Epoch [4/5], Step [544/10336], Loss: 0.0562\n",
      "Epoch [4/5], Step [546/10336], Loss: 0.1155\n",
      "Epoch [4/5], Step [548/10336], Loss: 0.3064\n",
      "Epoch [4/5], Step [550/10336], Loss: 0.9822\n",
      "Epoch [4/5], Step [552/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [554/10336], Loss: 0.3981\n",
      "Epoch [4/5], Step [556/10336], Loss: 0.1976\n",
      "Epoch [4/5], Step [558/10336], Loss: 0.2473\n",
      "Epoch [4/5], Step [560/10336], Loss: 0.0605\n",
      "Epoch [4/5], Step [562/10336], Loss: 1.9579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [564/10336], Loss: 0.4000\n",
      "Epoch [4/5], Step [566/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [568/10336], Loss: 3.8997\n",
      "Epoch [4/5], Step [570/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [572/10336], Loss: 2.0253\n",
      "Epoch [4/5], Step [574/10336], Loss: 0.3535\n",
      "Epoch [4/5], Step [576/10336], Loss: 0.7618\n",
      "Epoch [4/5], Step [578/10336], Loss: 0.0384\n",
      "Epoch [4/5], Step [580/10336], Loss: 0.6838\n",
      "Epoch [4/5], Step [582/10336], Loss: 0.0153\n",
      "Epoch [4/5], Step [584/10336], Loss: 1.9776\n",
      "Epoch [4/5], Step [586/10336], Loss: 0.0332\n",
      "Epoch [4/5], Step [588/10336], Loss: 0.3907\n",
      "Epoch [4/5], Step [590/10336], Loss: 0.6059\n",
      "Epoch [4/5], Step [592/10336], Loss: 0.1093\n",
      "Epoch [4/5], Step [594/10336], Loss: 0.2480\n",
      "Epoch [4/5], Step [596/10336], Loss: 0.3625\n",
      "Epoch [4/5], Step [598/10336], Loss: 0.0063\n",
      "Epoch [4/5], Step [600/10336], Loss: 0.2017\n",
      "Epoch [4/5], Step [602/10336], Loss: 1.8629\n",
      "Epoch [4/5], Step [604/10336], Loss: 0.0388\n",
      "Epoch [4/5], Step [606/10336], Loss: 0.2559\n",
      "Epoch [4/5], Step [608/10336], Loss: 0.0272\n",
      "Epoch [4/5], Step [610/10336], Loss: 0.3478\n",
      "Epoch [4/5], Step [612/10336], Loss: 0.0704\n",
      "Epoch [4/5], Step [614/10336], Loss: 1.9274\n",
      "Epoch [4/5], Step [616/10336], Loss: 3.1816\n",
      "Epoch [4/5], Step [618/10336], Loss: 0.5233\n",
      "Epoch [4/5], Step [620/10336], Loss: 0.4300\n",
      "Epoch [4/5], Step [622/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [624/10336], Loss: 1.3126\n",
      "Epoch [4/5], Step [626/10336], Loss: 0.3679\n",
      "Epoch [4/5], Step [628/10336], Loss: 2.8219\n",
      "Epoch [4/5], Step [630/10336], Loss: 0.0699\n",
      "Epoch [4/5], Step [632/10336], Loss: 0.4699\n",
      "Epoch [4/5], Step [634/10336], Loss: 0.0813\n",
      "Epoch [4/5], Step [636/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [638/10336], Loss: 0.0530\n",
      "Epoch [4/5], Step [640/10336], Loss: 0.7890\n",
      "Epoch [4/5], Step [642/10336], Loss: 0.2787\n",
      "Epoch [4/5], Step [644/10336], Loss: 0.0450\n",
      "Epoch [4/5], Step [646/10336], Loss: 0.5889\n",
      "Epoch [4/5], Step [648/10336], Loss: 0.8990\n",
      "Epoch [4/5], Step [650/10336], Loss: 0.5631\n",
      "Epoch [4/5], Step [652/10336], Loss: 0.6958\n",
      "Epoch [4/5], Step [654/10336], Loss: 0.0306\n",
      "Epoch [4/5], Step [656/10336], Loss: 0.0437\n",
      "Epoch [4/5], Step [658/10336], Loss: 0.2339\n",
      "Epoch [4/5], Step [660/10336], Loss: 1.1212\n",
      "Epoch [4/5], Step [662/10336], Loss: 0.3501\n",
      "Epoch [4/5], Step [664/10336], Loss: 2.1380\n",
      "Epoch [4/5], Step [666/10336], Loss: 0.0869\n",
      "Epoch [4/5], Step [668/10336], Loss: 0.2023\n",
      "Epoch [4/5], Step [670/10336], Loss: 0.9937\n",
      "Epoch [4/5], Step [672/10336], Loss: 2.5642\n",
      "Epoch [4/5], Step [674/10336], Loss: 1.1460\n",
      "Epoch [4/5], Step [676/10336], Loss: 0.2745\n",
      "Epoch [4/5], Step [678/10336], Loss: 1.0737\n",
      "Epoch [4/5], Step [680/10336], Loss: 0.1553\n",
      "Epoch [4/5], Step [682/10336], Loss: 1.3795\n",
      "Epoch [4/5], Step [684/10336], Loss: 0.2115\n",
      "Epoch [4/5], Step [686/10336], Loss: 0.0731\n",
      "Epoch [4/5], Step [688/10336], Loss: 1.1138\n",
      "Epoch [4/5], Step [690/10336], Loss: 0.1299\n",
      "Epoch [4/5], Step [692/10336], Loss: 0.9916\n",
      "Epoch [4/5], Step [694/10336], Loss: 2.6346\n",
      "Epoch [4/5], Step [696/10336], Loss: 1.3642\n",
      "Epoch [4/5], Step [698/10336], Loss: 0.5959\n",
      "Epoch [4/5], Step [700/10336], Loss: 2.3042\n",
      "Epoch [4/5], Step [702/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [704/10336], Loss: 0.4090\n",
      "Epoch [4/5], Step [706/10336], Loss: 0.9676\n",
      "Epoch [4/5], Step [708/10336], Loss: 1.0542\n",
      "Epoch [4/5], Step [710/10336], Loss: 0.5638\n",
      "Epoch [4/5], Step [712/10336], Loss: 1.1490\n",
      "Epoch [4/5], Step [714/10336], Loss: 0.0829\n",
      "Epoch [4/5], Step [716/10336], Loss: 4.6335\n",
      "Epoch [4/5], Step [718/10336], Loss: 0.0395\n",
      "Epoch [4/5], Step [720/10336], Loss: 0.5822\n",
      "Epoch [4/5], Step [722/10336], Loss: 0.2527\n",
      "Epoch [4/5], Step [724/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [726/10336], Loss: 0.2605\n",
      "Epoch [4/5], Step [728/10336], Loss: 0.0180\n",
      "Epoch [4/5], Step [730/10336], Loss: 0.0177\n",
      "Epoch [4/5], Step [732/10336], Loss: 0.0289\n",
      "Epoch [4/5], Step [734/10336], Loss: 1.2343\n",
      "Epoch [4/5], Step [736/10336], Loss: 0.6088\n",
      "Epoch [4/5], Step [738/10336], Loss: 0.1438\n",
      "Epoch [4/5], Step [740/10336], Loss: 0.2944\n",
      "Epoch [4/5], Step [742/10336], Loss: 0.5507\n",
      "Epoch [4/5], Step [744/10336], Loss: 1.5607\n",
      "Epoch [4/5], Step [746/10336], Loss: 0.1429\n",
      "Epoch [4/5], Step [748/10336], Loss: 0.0663\n",
      "Epoch [4/5], Step [750/10336], Loss: 0.1153\n",
      "Epoch [4/5], Step [752/10336], Loss: 0.4157\n",
      "Epoch [4/5], Step [754/10336], Loss: 0.2151\n",
      "Epoch [4/5], Step [756/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [758/10336], Loss: 0.7900\n",
      "Epoch [4/5], Step [760/10336], Loss: 0.0043\n",
      "Epoch [4/5], Step [762/10336], Loss: 0.8484\n",
      "Epoch [4/5], Step [764/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [766/10336], Loss: 1.4146\n",
      "Epoch [4/5], Step [768/10336], Loss: 0.2946\n",
      "Epoch [4/5], Step [770/10336], Loss: 0.2351\n",
      "Epoch [4/5], Step [772/10336], Loss: 1.0105\n",
      "Epoch [4/5], Step [774/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [776/10336], Loss: 0.0055\n",
      "Epoch [4/5], Step [778/10336], Loss: 0.0409\n",
      "Epoch [4/5], Step [780/10336], Loss: 0.7273\n",
      "Epoch [4/5], Step [782/10336], Loss: 0.1702\n",
      "Epoch [4/5], Step [784/10336], Loss: 0.2412\n",
      "Epoch [4/5], Step [786/10336], Loss: 0.1079\n",
      "Epoch [4/5], Step [788/10336], Loss: 2.6652\n",
      "Epoch [4/5], Step [790/10336], Loss: 0.1821\n",
      "Epoch [4/5], Step [792/10336], Loss: 0.1101\n",
      "Epoch [4/5], Step [794/10336], Loss: 0.0930\n",
      "Epoch [4/5], Step [796/10336], Loss: 0.1769\n",
      "Epoch [4/5], Step [798/10336], Loss: 0.0807\n",
      "Epoch [4/5], Step [800/10336], Loss: 0.2862\n",
      "Epoch [4/5], Step [802/10336], Loss: 0.3902\n",
      "Epoch [4/5], Step [804/10336], Loss: 0.1708\n",
      "Epoch [4/5], Step [806/10336], Loss: 3.0894\n",
      "Epoch [4/5], Step [808/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [810/10336], Loss: 2.2530\n",
      "Epoch [4/5], Step [812/10336], Loss: 0.0337\n",
      "Epoch [4/5], Step [814/10336], Loss: 0.2191\n",
      "Epoch [4/5], Step [816/10336], Loss: 0.0364\n",
      "Epoch [4/5], Step [818/10336], Loss: 0.1502\n",
      "Epoch [4/5], Step [820/10336], Loss: 2.4326\n",
      "Epoch [4/5], Step [822/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [824/10336], Loss: 0.3698\n",
      "Epoch [4/5], Step [826/10336], Loss: 3.5133\n",
      "Epoch [4/5], Step [828/10336], Loss: 2.5342\n",
      "Epoch [4/5], Step [830/10336], Loss: 0.8819\n",
      "Epoch [4/5], Step [832/10336], Loss: 0.0239\n",
      "Epoch [4/5], Step [834/10336], Loss: 0.3764\n",
      "Epoch [4/5], Step [836/10336], Loss: 1.7424\n",
      "Epoch [4/5], Step [838/10336], Loss: 1.0320\n",
      "Epoch [4/5], Step [840/10336], Loss: 0.8668\n",
      "Epoch [4/5], Step [842/10336], Loss: 2.8475\n",
      "Epoch [4/5], Step [844/10336], Loss: 0.4433\n",
      "Epoch [4/5], Step [846/10336], Loss: 0.8821\n",
      "Epoch [4/5], Step [848/10336], Loss: 0.2045\n",
      "Epoch [4/5], Step [850/10336], Loss: 2.0501\n",
      "Epoch [4/5], Step [852/10336], Loss: 0.2975\n",
      "Epoch [4/5], Step [854/10336], Loss: 0.0465\n",
      "Epoch [4/5], Step [856/10336], Loss: 0.2662\n",
      "Epoch [4/5], Step [858/10336], Loss: 0.0180\n",
      "Epoch [4/5], Step [860/10336], Loss: 0.2944\n",
      "Epoch [4/5], Step [862/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [864/10336], Loss: 0.0194\n",
      "Epoch [4/5], Step [866/10336], Loss: 0.0259\n",
      "Epoch [4/5], Step [868/10336], Loss: 0.6989\n",
      "Epoch [4/5], Step [870/10336], Loss: 1.0058\n",
      "Epoch [4/5], Step [872/10336], Loss: 0.5234\n",
      "Epoch [4/5], Step [874/10336], Loss: 0.1557\n",
      "Epoch [4/5], Step [876/10336], Loss: 0.8230\n",
      "Epoch [4/5], Step [878/10336], Loss: 0.4197\n",
      "Epoch [4/5], Step [880/10336], Loss: 0.0187\n",
      "Epoch [4/5], Step [882/10336], Loss: 0.2470\n",
      "Epoch [4/5], Step [884/10336], Loss: 0.0125\n",
      "Epoch [4/5], Step [886/10336], Loss: 0.6867\n",
      "Epoch [4/5], Step [888/10336], Loss: 0.1380\n",
      "Epoch [4/5], Step [890/10336], Loss: 0.0797\n",
      "Epoch [4/5], Step [892/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [894/10336], Loss: 0.0292\n",
      "Epoch [4/5], Step [896/10336], Loss: 0.0162\n",
      "Epoch [4/5], Step [898/10336], Loss: 3.7704\n",
      "Epoch [4/5], Step [900/10336], Loss: 2.3938\n",
      "Epoch [4/5], Step [902/10336], Loss: 0.1416\n",
      "Epoch [4/5], Step [904/10336], Loss: 1.7678\n",
      "Epoch [4/5], Step [906/10336], Loss: 0.0524\n",
      "Epoch [4/5], Step [908/10336], Loss: 0.0644\n",
      "Epoch [4/5], Step [910/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [912/10336], Loss: 0.6822\n",
      "Epoch [4/5], Step [914/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [916/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [918/10336], Loss: 0.1769\n",
      "Epoch [4/5], Step [920/10336], Loss: 0.1202\n",
      "Epoch [4/5], Step [922/10336], Loss: 2.8055\n",
      "Epoch [4/5], Step [924/10336], Loss: 6.2625\n",
      "Epoch [4/5], Step [926/10336], Loss: 2.9377\n",
      "Epoch [4/5], Step [928/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [930/10336], Loss: 0.6917\n",
      "Epoch [4/5], Step [932/10336], Loss: 0.2374\n",
      "Epoch [4/5], Step [934/10336], Loss: 0.3317\n",
      "Epoch [4/5], Step [936/10336], Loss: 0.0961\n",
      "Epoch [4/5], Step [938/10336], Loss: 0.3602\n",
      "Epoch [4/5], Step [940/10336], Loss: 0.3622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [942/10336], Loss: 1.1080\n",
      "Epoch [4/5], Step [944/10336], Loss: 0.2348\n",
      "Epoch [4/5], Step [946/10336], Loss: 1.7863\n",
      "Epoch [4/5], Step [948/10336], Loss: 1.4618\n",
      "Epoch [4/5], Step [950/10336], Loss: 4.0540\n",
      "Epoch [4/5], Step [952/10336], Loss: 3.3667\n",
      "Epoch [4/5], Step [954/10336], Loss: 0.2072\n",
      "Epoch [4/5], Step [956/10336], Loss: 2.6059\n",
      "Epoch [4/5], Step [958/10336], Loss: 1.3156\n",
      "Epoch [4/5], Step [960/10336], Loss: 0.4643\n",
      "Epoch [4/5], Step [962/10336], Loss: 1.3930\n",
      "Epoch [4/5], Step [964/10336], Loss: 0.1287\n",
      "Epoch [4/5], Step [966/10336], Loss: 0.3564\n",
      "Epoch [4/5], Step [968/10336], Loss: 1.4625\n",
      "Epoch [4/5], Step [970/10336], Loss: 2.0184\n",
      "Epoch [4/5], Step [972/10336], Loss: 0.1407\n",
      "Epoch [4/5], Step [974/10336], Loss: 0.5295\n",
      "Epoch [4/5], Step [976/10336], Loss: 0.3475\n",
      "Epoch [4/5], Step [978/10336], Loss: 0.5558\n",
      "Epoch [4/5], Step [980/10336], Loss: 0.3886\n",
      "Epoch [4/5], Step [982/10336], Loss: 0.1949\n",
      "Epoch [4/5], Step [984/10336], Loss: 2.8812\n",
      "Epoch [4/5], Step [986/10336], Loss: 0.1339\n",
      "Epoch [4/5], Step [988/10336], Loss: 0.3771\n",
      "Epoch [4/5], Step [990/10336], Loss: 1.3867\n",
      "Epoch [4/5], Step [992/10336], Loss: 0.7365\n",
      "Epoch [4/5], Step [994/10336], Loss: 0.4418\n",
      "Epoch [4/5], Step [996/10336], Loss: 0.0985\n",
      "Epoch [4/5], Step [998/10336], Loss: 0.1250\n",
      "Epoch [4/5], Step [1000/10336], Loss: 1.7271\n",
      "Epoch [4/5], Step [1002/10336], Loss: 0.0669\n",
      "Epoch [4/5], Step [1004/10336], Loss: 0.2644\n",
      "Epoch [4/5], Step [1006/10336], Loss: 0.3154\n",
      "Epoch [4/5], Step [1008/10336], Loss: 0.1661\n",
      "Epoch [4/5], Step [1010/10336], Loss: 0.5636\n",
      "Epoch [4/5], Step [1012/10336], Loss: 0.0282\n",
      "Epoch [4/5], Step [1014/10336], Loss: 0.0198\n",
      "Epoch [4/5], Step [1016/10336], Loss: 0.3520\n",
      "Epoch [4/5], Step [1018/10336], Loss: 0.3135\n",
      "Epoch [4/5], Step [1020/10336], Loss: 0.2108\n",
      "Epoch [4/5], Step [1022/10336], Loss: 0.2603\n",
      "Epoch [4/5], Step [1024/10336], Loss: 0.0212\n",
      "Epoch [4/5], Step [1026/10336], Loss: 0.0245\n",
      "Epoch [4/5], Step [1028/10336], Loss: 0.0235\n",
      "Epoch [4/5], Step [1030/10336], Loss: 0.0168\n",
      "Epoch [4/5], Step [1032/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [1034/10336], Loss: 0.0998\n",
      "Epoch [4/5], Step [1036/10336], Loss: 0.1301\n",
      "Epoch [4/5], Step [1038/10336], Loss: 4.2452\n",
      "Epoch [4/5], Step [1040/10336], Loss: 1.0562\n",
      "Epoch [4/5], Step [1042/10336], Loss: 0.9974\n",
      "Epoch [4/5], Step [1044/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [1046/10336], Loss: 0.4702\n",
      "Epoch [4/5], Step [1048/10336], Loss: 0.3390\n",
      "Epoch [4/5], Step [1050/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [1052/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [1054/10336], Loss: 0.0085\n",
      "Epoch [4/5], Step [1056/10336], Loss: 2.1309\n",
      "Epoch [4/5], Step [1058/10336], Loss: 0.5016\n",
      "Epoch [4/5], Step [1060/10336], Loss: 0.2164\n",
      "Epoch [4/5], Step [1062/10336], Loss: 0.0058\n",
      "Epoch [4/5], Step [1064/10336], Loss: 0.0113\n",
      "Epoch [4/5], Step [1066/10336], Loss: 3.3227\n",
      "Epoch [4/5], Step [1068/10336], Loss: 0.2057\n",
      "Epoch [4/5], Step [1070/10336], Loss: 0.0305\n",
      "Epoch [4/5], Step [1072/10336], Loss: 0.1848\n",
      "Epoch [4/5], Step [1074/10336], Loss: 0.5871\n",
      "Epoch [4/5], Step [1076/10336], Loss: 0.0120\n",
      "Epoch [4/5], Step [1078/10336], Loss: 1.1255\n",
      "Epoch [4/5], Step [1080/10336], Loss: 0.0694\n",
      "Epoch [4/5], Step [1082/10336], Loss: 0.1677\n",
      "Epoch [4/5], Step [1084/10336], Loss: 0.0096\n",
      "Epoch [4/5], Step [1086/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [1088/10336], Loss: 2.8613\n",
      "Epoch [4/5], Step [1090/10336], Loss: 0.2242\n",
      "Epoch [4/5], Step [1092/10336], Loss: 1.7251\n",
      "Epoch [4/5], Step [1094/10336], Loss: 1.9531\n",
      "Epoch [4/5], Step [1096/10336], Loss: 0.4696\n",
      "Epoch [4/5], Step [1098/10336], Loss: 0.9511\n",
      "Epoch [4/5], Step [1100/10336], Loss: 0.4142\n",
      "Epoch [4/5], Step [1102/10336], Loss: 0.7331\n",
      "Epoch [4/5], Step [1104/10336], Loss: 0.4312\n",
      "Epoch [4/5], Step [1106/10336], Loss: 0.1006\n",
      "Epoch [4/5], Step [1108/10336], Loss: 0.1020\n",
      "Epoch [4/5], Step [1110/10336], Loss: 0.1591\n",
      "Epoch [4/5], Step [1112/10336], Loss: 0.2775\n",
      "Epoch [4/5], Step [1114/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [1116/10336], Loss: 0.0971\n",
      "Epoch [4/5], Step [1118/10336], Loss: 0.1013\n",
      "Epoch [4/5], Step [1120/10336], Loss: 1.0040\n",
      "Epoch [4/5], Step [1122/10336], Loss: 0.6325\n",
      "Epoch [4/5], Step [1124/10336], Loss: 0.0066\n",
      "Epoch [4/5], Step [1126/10336], Loss: 4.8727\n",
      "Epoch [4/5], Step [1128/10336], Loss: 7.1463\n",
      "Epoch [4/5], Step [1130/10336], Loss: 0.6476\n",
      "Epoch [4/5], Step [1132/10336], Loss: 0.6060\n",
      "Epoch [4/5], Step [1134/10336], Loss: 0.3239\n",
      "Epoch [4/5], Step [1136/10336], Loss: 0.7805\n",
      "Epoch [4/5], Step [1138/10336], Loss: 0.1472\n",
      "Epoch [4/5], Step [1140/10336], Loss: 0.2748\n",
      "Epoch [4/5], Step [1142/10336], Loss: 0.4098\n",
      "Epoch [4/5], Step [1144/10336], Loss: 0.0828\n",
      "Epoch [4/5], Step [1146/10336], Loss: 1.7552\n",
      "Epoch [4/5], Step [1148/10336], Loss: 0.1543\n",
      "Epoch [4/5], Step [1150/10336], Loss: 0.1665\n",
      "Epoch [4/5], Step [1152/10336], Loss: 0.0700\n",
      "Epoch [4/5], Step [1154/10336], Loss: 0.5776\n",
      "Epoch [4/5], Step [1156/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [1158/10336], Loss: 0.3557\n",
      "Epoch [4/5], Step [1160/10336], Loss: 3.7470\n",
      "Epoch [4/5], Step [1162/10336], Loss: 0.1597\n",
      "Epoch [4/5], Step [1164/10336], Loss: 0.2640\n",
      "Epoch [4/5], Step [1166/10336], Loss: 0.1051\n",
      "Epoch [4/5], Step [1168/10336], Loss: 0.3277\n",
      "Epoch [4/5], Step [1170/10336], Loss: 0.9758\n",
      "Epoch [4/5], Step [1172/10336], Loss: 0.0064\n",
      "Epoch [4/5], Step [1174/10336], Loss: 0.2910\n",
      "Epoch [4/5], Step [1176/10336], Loss: 0.2450\n",
      "Epoch [4/5], Step [1178/10336], Loss: 0.1584\n",
      "Epoch [4/5], Step [1180/10336], Loss: 1.0351\n",
      "Epoch [4/5], Step [1182/10336], Loss: 0.0901\n",
      "Epoch [4/5], Step [1184/10336], Loss: 3.3377\n",
      "Epoch [4/5], Step [1186/10336], Loss: 0.0568\n",
      "Epoch [4/5], Step [1188/10336], Loss: 0.0069\n",
      "Epoch [4/5], Step [1190/10336], Loss: 1.2696\n",
      "Epoch [4/5], Step [1192/10336], Loss: 0.1762\n",
      "Epoch [4/5], Step [1194/10336], Loss: 1.5528\n",
      "Epoch [4/5], Step [1196/10336], Loss: 0.3256\n",
      "Epoch [4/5], Step [1198/10336], Loss: 0.0712\n",
      "Epoch [4/5], Step [1200/10336], Loss: 0.2729\n",
      "Epoch [4/5], Step [1202/10336], Loss: 0.1482\n",
      "Epoch [4/5], Step [1204/10336], Loss: 0.0679\n",
      "Epoch [4/5], Step [1206/10336], Loss: 0.5818\n",
      "Epoch [4/5], Step [1208/10336], Loss: 0.2597\n",
      "Epoch [4/5], Step [1210/10336], Loss: 0.8976\n",
      "Epoch [4/5], Step [1212/10336], Loss: 0.4073\n",
      "Epoch [4/5], Step [1214/10336], Loss: 0.0910\n",
      "Epoch [4/5], Step [1216/10336], Loss: 0.4828\n",
      "Epoch [4/5], Step [1218/10336], Loss: 0.0584\n",
      "Epoch [4/5], Step [1220/10336], Loss: 2.1044\n",
      "Epoch [4/5], Step [1222/10336], Loss: 0.3097\n",
      "Epoch [4/5], Step [1224/10336], Loss: 0.0278\n",
      "Epoch [4/5], Step [1226/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [1228/10336], Loss: 0.0366\n",
      "Epoch [4/5], Step [1230/10336], Loss: 0.1552\n",
      "Epoch [4/5], Step [1232/10336], Loss: 3.5219\n",
      "Epoch [4/5], Step [1234/10336], Loss: 1.1471\n",
      "Epoch [4/5], Step [1236/10336], Loss: 0.0848\n",
      "Epoch [4/5], Step [1238/10336], Loss: 0.0965\n",
      "Epoch [4/5], Step [1240/10336], Loss: 0.5171\n",
      "Epoch [4/5], Step [1242/10336], Loss: 0.0621\n",
      "Epoch [4/5], Step [1244/10336], Loss: 2.6261\n",
      "Epoch [4/5], Step [1246/10336], Loss: 3.4660\n",
      "Epoch [4/5], Step [1248/10336], Loss: 0.0410\n",
      "Epoch [4/5], Step [1250/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [1252/10336], Loss: 0.2253\n",
      "Epoch [4/5], Step [1254/10336], Loss: 1.2487\n",
      "Epoch [4/5], Step [1256/10336], Loss: 0.0309\n",
      "Epoch [4/5], Step [1258/10336], Loss: 0.2783\n",
      "Epoch [4/5], Step [1260/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [1262/10336], Loss: 0.1346\n",
      "Epoch [4/5], Step [1264/10336], Loss: 0.3347\n",
      "Epoch [4/5], Step [1266/10336], Loss: 0.4827\n",
      "Epoch [4/5], Step [1268/10336], Loss: 0.0755\n",
      "Epoch [4/5], Step [1270/10336], Loss: 0.0226\n",
      "Epoch [4/5], Step [1272/10336], Loss: 0.1779\n",
      "Epoch [4/5], Step [1274/10336], Loss: 0.0261\n",
      "Epoch [4/5], Step [1276/10336], Loss: 0.2088\n",
      "Epoch [4/5], Step [1278/10336], Loss: 0.8914\n",
      "Epoch [4/5], Step [1280/10336], Loss: 0.0468\n",
      "Epoch [4/5], Step [1282/10336], Loss: 0.0347\n",
      "Epoch [4/5], Step [1284/10336], Loss: 2.0833\n",
      "Epoch [4/5], Step [1286/10336], Loss: 0.0860\n",
      "Epoch [4/5], Step [1288/10336], Loss: 0.5039\n",
      "Epoch [4/5], Step [1290/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [1292/10336], Loss: 0.0140\n",
      "Epoch [4/5], Step [1294/10336], Loss: 0.0970\n",
      "Epoch [4/5], Step [1296/10336], Loss: 0.1777\n",
      "Epoch [4/5], Step [1298/10336], Loss: 0.5278\n",
      "Epoch [4/5], Step [1300/10336], Loss: 0.0794\n",
      "Epoch [4/5], Step [1302/10336], Loss: 0.1805\n",
      "Epoch [4/5], Step [1304/10336], Loss: 3.6340\n",
      "Epoch [4/5], Step [1306/10336], Loss: 0.0299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [1308/10336], Loss: 0.0123\n",
      "Epoch [4/5], Step [1310/10336], Loss: 6.4879\n",
      "Epoch [4/5], Step [1312/10336], Loss: 0.2722\n",
      "Epoch [4/5], Step [1314/10336], Loss: 1.1379\n",
      "Epoch [4/5], Step [1316/10336], Loss: 0.3817\n",
      "Epoch [4/5], Step [1318/10336], Loss: 0.5524\n",
      "Epoch [4/5], Step [1320/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [1322/10336], Loss: 1.5142\n",
      "Epoch [4/5], Step [1324/10336], Loss: 0.2016\n",
      "Epoch [4/5], Step [1326/10336], Loss: 0.5429\n",
      "Epoch [4/5], Step [1328/10336], Loss: 0.3761\n",
      "Epoch [4/5], Step [1330/10336], Loss: 3.6080\n",
      "Epoch [4/5], Step [1332/10336], Loss: 0.0170\n",
      "Epoch [4/5], Step [1334/10336], Loss: 0.0073\n",
      "Epoch [4/5], Step [1336/10336], Loss: 1.2572\n",
      "Epoch [4/5], Step [1338/10336], Loss: 0.7197\n",
      "Epoch [4/5], Step [1340/10336], Loss: 0.0237\n",
      "Epoch [4/5], Step [1342/10336], Loss: 0.8014\n",
      "Epoch [4/5], Step [1344/10336], Loss: 0.7131\n",
      "Epoch [4/5], Step [1346/10336], Loss: 0.3771\n",
      "Epoch [4/5], Step [1348/10336], Loss: 0.0643\n",
      "Epoch [4/5], Step [1350/10336], Loss: 0.3175\n",
      "Epoch [4/5], Step [1352/10336], Loss: 0.5980\n",
      "Epoch [4/5], Step [1354/10336], Loss: 0.6315\n",
      "Epoch [4/5], Step [1356/10336], Loss: 0.4247\n",
      "Epoch [4/5], Step [1358/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [1360/10336], Loss: 1.3595\n",
      "Epoch [4/5], Step [1362/10336], Loss: 0.0327\n",
      "Epoch [4/5], Step [1364/10336], Loss: 0.6892\n",
      "Epoch [4/5], Step [1366/10336], Loss: 1.4352\n",
      "Epoch [4/5], Step [1368/10336], Loss: 0.8169\n",
      "Epoch [4/5], Step [1370/10336], Loss: 0.1868\n",
      "Epoch [4/5], Step [1372/10336], Loss: 0.0193\n",
      "Epoch [4/5], Step [1374/10336], Loss: 0.5600\n",
      "Epoch [4/5], Step [1376/10336], Loss: 0.1328\n",
      "Epoch [4/5], Step [1378/10336], Loss: 0.0166\n",
      "Epoch [4/5], Step [1380/10336], Loss: 0.0078\n",
      "Epoch [4/5], Step [1382/10336], Loss: 0.8494\n",
      "Epoch [4/5], Step [1384/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [1386/10336], Loss: 0.0048\n",
      "Epoch [4/5], Step [1388/10336], Loss: 0.9517\n",
      "Epoch [4/5], Step [1390/10336], Loss: 0.2317\n",
      "Epoch [4/5], Step [1392/10336], Loss: 0.2824\n",
      "Epoch [4/5], Step [1394/10336], Loss: 0.1685\n",
      "Epoch [4/5], Step [1396/10336], Loss: 0.0391\n",
      "Epoch [4/5], Step [1398/10336], Loss: 0.4686\n",
      "Epoch [4/5], Step [1400/10336], Loss: 4.1766\n",
      "Epoch [4/5], Step [1402/10336], Loss: 1.0406\n",
      "Epoch [4/5], Step [1404/10336], Loss: 0.2193\n",
      "Epoch [4/5], Step [1406/10336], Loss: 0.0620\n",
      "Epoch [4/5], Step [1408/10336], Loss: 0.6392\n",
      "Epoch [4/5], Step [1410/10336], Loss: 0.5108\n",
      "Epoch [4/5], Step [1412/10336], Loss: 0.2417\n",
      "Epoch [4/5], Step [1414/10336], Loss: 0.4389\n",
      "Epoch [4/5], Step [1416/10336], Loss: 0.7758\n",
      "Epoch [4/5], Step [1418/10336], Loss: 0.0123\n",
      "Epoch [4/5], Step [1420/10336], Loss: 0.4243\n",
      "Epoch [4/5], Step [1422/10336], Loss: 0.2045\n",
      "Epoch [4/5], Step [1424/10336], Loss: 1.0733\n",
      "Epoch [4/5], Step [1426/10336], Loss: 0.0773\n",
      "Epoch [4/5], Step [1428/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [1430/10336], Loss: 0.1687\n",
      "Epoch [4/5], Step [1432/10336], Loss: 0.2699\n",
      "Epoch [4/5], Step [1434/10336], Loss: 0.1279\n",
      "Epoch [4/5], Step [1436/10336], Loss: 2.4697\n",
      "Epoch [4/5], Step [1438/10336], Loss: 0.2228\n",
      "Epoch [4/5], Step [1440/10336], Loss: 0.6334\n",
      "Epoch [4/5], Step [1442/10336], Loss: 0.3553\n",
      "Epoch [4/5], Step [1444/10336], Loss: 0.0030\n",
      "Epoch [4/5], Step [1446/10336], Loss: 0.1777\n",
      "Epoch [4/5], Step [1448/10336], Loss: 0.3257\n",
      "Epoch [4/5], Step [1450/10336], Loss: 1.8540\n",
      "Epoch [4/5], Step [1452/10336], Loss: 0.0184\n",
      "Epoch [4/5], Step [1454/10336], Loss: 0.3748\n",
      "Epoch [4/5], Step [1456/10336], Loss: 0.4927\n",
      "Epoch [4/5], Step [1458/10336], Loss: 0.2502\n",
      "Epoch [4/5], Step [1460/10336], Loss: 0.1034\n",
      "Epoch [4/5], Step [1462/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [1464/10336], Loss: 0.0549\n",
      "Epoch [4/5], Step [1466/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [1468/10336], Loss: 0.0379\n",
      "Epoch [4/5], Step [1470/10336], Loss: 0.0159\n",
      "Epoch [4/5], Step [1472/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [1474/10336], Loss: 0.3442\n",
      "Epoch [4/5], Step [1476/10336], Loss: 0.2471\n",
      "Epoch [4/5], Step [1478/10336], Loss: 0.0661\n",
      "Epoch [4/5], Step [1480/10336], Loss: 0.0284\n",
      "Epoch [4/5], Step [1482/10336], Loss: 0.6468\n",
      "Epoch [4/5], Step [1484/10336], Loss: 0.0594\n",
      "Epoch [4/5], Step [1486/10336], Loss: 0.0464\n",
      "Epoch [4/5], Step [1488/10336], Loss: 2.1787\n",
      "Epoch [4/5], Step [1490/10336], Loss: 1.8055\n",
      "Epoch [4/5], Step [1492/10336], Loss: 0.0031\n",
      "Epoch [4/5], Step [1494/10336], Loss: 0.4932\n",
      "Epoch [4/5], Step [1496/10336], Loss: 0.3932\n",
      "Epoch [4/5], Step [1498/10336], Loss: 0.1183\n",
      "Epoch [4/5], Step [1500/10336], Loss: 0.5553\n",
      "Epoch [4/5], Step [1502/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [1504/10336], Loss: 1.1826\n",
      "Epoch [4/5], Step [1506/10336], Loss: 0.6808\n",
      "Epoch [4/5], Step [1508/10336], Loss: 0.0451\n",
      "Epoch [4/5], Step [1510/10336], Loss: 0.2347\n",
      "Epoch [4/5], Step [1512/10336], Loss: 2.4800\n",
      "Epoch [4/5], Step [1514/10336], Loss: 0.2806\n",
      "Epoch [4/5], Step [1516/10336], Loss: 0.2254\n",
      "Epoch [4/5], Step [1518/10336], Loss: 0.0514\n",
      "Epoch [4/5], Step [1520/10336], Loss: 0.2036\n",
      "Epoch [4/5], Step [1522/10336], Loss: 0.3612\n",
      "Epoch [4/5], Step [1524/10336], Loss: 1.5523\n",
      "Epoch [4/5], Step [1526/10336], Loss: 0.1369\n",
      "Epoch [4/5], Step [1528/10336], Loss: 1.1647\n",
      "Epoch [4/5], Step [1530/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [1532/10336], Loss: 0.6796\n",
      "Epoch [4/5], Step [1534/10336], Loss: 0.0084\n",
      "Epoch [4/5], Step [1536/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [1538/10336], Loss: 0.9460\n",
      "Epoch [4/5], Step [1540/10336], Loss: 0.0889\n",
      "Epoch [4/5], Step [1542/10336], Loss: 3.7006\n",
      "Epoch [4/5], Step [1544/10336], Loss: 1.0996\n",
      "Epoch [4/5], Step [1546/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [1548/10336], Loss: 2.7565\n",
      "Epoch [4/5], Step [1550/10336], Loss: 0.1031\n",
      "Epoch [4/5], Step [1552/10336], Loss: 2.4343\n",
      "Epoch [4/5], Step [1554/10336], Loss: 0.4467\n",
      "Epoch [4/5], Step [1556/10336], Loss: 0.0706\n",
      "Epoch [4/5], Step [1558/10336], Loss: 0.1619\n",
      "Epoch [4/5], Step [1560/10336], Loss: 0.0307\n",
      "Epoch [4/5], Step [1562/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [1564/10336], Loss: 0.7402\n",
      "Epoch [4/5], Step [1566/10336], Loss: 0.4050\n",
      "Epoch [4/5], Step [1568/10336], Loss: 0.0073\n",
      "Epoch [4/5], Step [1570/10336], Loss: 0.0735\n",
      "Epoch [4/5], Step [1572/10336], Loss: 0.5429\n",
      "Epoch [4/5], Step [1574/10336], Loss: 1.6178\n",
      "Epoch [4/5], Step [1576/10336], Loss: 2.2772\n",
      "Epoch [4/5], Step [1578/10336], Loss: 0.5929\n",
      "Epoch [4/5], Step [1580/10336], Loss: 1.5853\n",
      "Epoch [4/5], Step [1582/10336], Loss: 1.0474\n",
      "Epoch [4/5], Step [1584/10336], Loss: 0.1528\n",
      "Epoch [4/5], Step [1586/10336], Loss: 0.1097\n",
      "Epoch [4/5], Step [1588/10336], Loss: 0.0585\n",
      "Epoch [4/5], Step [1590/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [1592/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [1594/10336], Loss: 2.9628\n",
      "Epoch [4/5], Step [1596/10336], Loss: 0.6653\n",
      "Epoch [4/5], Step [1598/10336], Loss: 3.3365\n",
      "Epoch [4/5], Step [1600/10336], Loss: 0.8972\n",
      "Epoch [4/5], Step [1602/10336], Loss: 0.5187\n",
      "Epoch [4/5], Step [1604/10336], Loss: 0.2736\n",
      "Epoch [4/5], Step [1606/10336], Loss: 0.4462\n",
      "Epoch [4/5], Step [1608/10336], Loss: 0.2021\n",
      "Epoch [4/5], Step [1610/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [1612/10336], Loss: 0.6115\n",
      "Epoch [4/5], Step [1614/10336], Loss: 0.4603\n",
      "Epoch [4/5], Step [1616/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [1618/10336], Loss: 0.8026\n",
      "Epoch [4/5], Step [1620/10336], Loss: 0.0257\n",
      "Epoch [4/5], Step [1622/10336], Loss: 0.1341\n",
      "Epoch [4/5], Step [1624/10336], Loss: 1.6613\n",
      "Epoch [4/5], Step [1626/10336], Loss: 0.1274\n",
      "Epoch [4/5], Step [1628/10336], Loss: 0.2136\n",
      "Epoch [4/5], Step [1630/10336], Loss: 0.2702\n",
      "Epoch [4/5], Step [1632/10336], Loss: 1.4459\n",
      "Epoch [4/5], Step [1634/10336], Loss: 0.7574\n",
      "Epoch [4/5], Step [1636/10336], Loss: 0.3454\n",
      "Epoch [4/5], Step [1638/10336], Loss: 0.3869\n",
      "Epoch [4/5], Step [1640/10336], Loss: 0.2316\n",
      "Epoch [4/5], Step [1642/10336], Loss: 1.1801\n",
      "Epoch [4/5], Step [1644/10336], Loss: 0.3582\n",
      "Epoch [4/5], Step [1646/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [1648/10336], Loss: 0.4149\n",
      "Epoch [4/5], Step [1650/10336], Loss: 0.3026\n",
      "Epoch [4/5], Step [1652/10336], Loss: 1.7428\n",
      "Epoch [4/5], Step [1654/10336], Loss: 2.3849\n",
      "Epoch [4/5], Step [1656/10336], Loss: 0.1650\n",
      "Epoch [4/5], Step [1658/10336], Loss: 0.1893\n",
      "Epoch [4/5], Step [1660/10336], Loss: 0.0895\n",
      "Epoch [4/5], Step [1662/10336], Loss: 0.5035\n",
      "Epoch [4/5], Step [1664/10336], Loss: 0.0056\n",
      "Epoch [4/5], Step [1666/10336], Loss: 0.0510\n",
      "Epoch [4/5], Step [1668/10336], Loss: 0.0116\n",
      "Epoch [4/5], Step [1670/10336], Loss: 0.2530\n",
      "Epoch [4/5], Step [1672/10336], Loss: 0.5146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [1674/10336], Loss: 0.5609\n",
      "Epoch [4/5], Step [1676/10336], Loss: 0.9072\n",
      "Epoch [4/5], Step [1678/10336], Loss: 1.0977\n",
      "Epoch [4/5], Step [1680/10336], Loss: 1.8492\n",
      "Epoch [4/5], Step [1682/10336], Loss: 0.1471\n",
      "Epoch [4/5], Step [1684/10336], Loss: 0.1925\n",
      "Epoch [4/5], Step [1686/10336], Loss: 0.3856\n",
      "Epoch [4/5], Step [1688/10336], Loss: 0.0048\n",
      "Epoch [4/5], Step [1690/10336], Loss: 0.1916\n",
      "Epoch [4/5], Step [1692/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [1694/10336], Loss: 0.0248\n",
      "Epoch [4/5], Step [1696/10336], Loss: 0.1800\n",
      "Epoch [4/5], Step [1698/10336], Loss: 1.4734\n",
      "Epoch [4/5], Step [1700/10336], Loss: 1.3606\n",
      "Epoch [4/5], Step [1702/10336], Loss: 0.1220\n",
      "Epoch [4/5], Step [1704/10336], Loss: 2.7263\n",
      "Epoch [4/5], Step [1706/10336], Loss: 0.0140\n",
      "Epoch [4/5], Step [1708/10336], Loss: 0.3539\n",
      "Epoch [4/5], Step [1710/10336], Loss: 0.7721\n",
      "Epoch [4/5], Step [1712/10336], Loss: 0.1236\n",
      "Epoch [4/5], Step [1714/10336], Loss: 0.0951\n",
      "Epoch [4/5], Step [1716/10336], Loss: 1.2062\n",
      "Epoch [4/5], Step [1718/10336], Loss: 0.0901\n",
      "Epoch [4/5], Step [1720/10336], Loss: 0.3113\n",
      "Epoch [4/5], Step [1722/10336], Loss: 0.3318\n",
      "Epoch [4/5], Step [1724/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [1726/10336], Loss: 2.7675\n",
      "Epoch [4/5], Step [1728/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [1730/10336], Loss: 0.1979\n",
      "Epoch [4/5], Step [1732/10336], Loss: 0.2118\n",
      "Epoch [4/5], Step [1734/10336], Loss: 0.8911\n",
      "Epoch [4/5], Step [1736/10336], Loss: 0.1101\n",
      "Epoch [4/5], Step [1738/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [1740/10336], Loss: 0.0661\n",
      "Epoch [4/5], Step [1742/10336], Loss: 0.3423\n",
      "Epoch [4/5], Step [1744/10336], Loss: 0.7599\n",
      "Epoch [4/5], Step [1746/10336], Loss: 0.0358\n",
      "Epoch [4/5], Step [1748/10336], Loss: 1.0253\n",
      "Epoch [4/5], Step [1750/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [1752/10336], Loss: 1.0460\n",
      "Epoch [4/5], Step [1754/10336], Loss: 0.6967\n",
      "Epoch [4/5], Step [1756/10336], Loss: 0.1786\n",
      "Epoch [4/5], Step [1758/10336], Loss: 0.0909\n",
      "Epoch [4/5], Step [1760/10336], Loss: 2.7183\n",
      "Epoch [4/5], Step [1762/10336], Loss: 1.5899\n",
      "Epoch [4/5], Step [1764/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [1766/10336], Loss: 2.7936\n",
      "Epoch [4/5], Step [1768/10336], Loss: 0.0547\n",
      "Epoch [4/5], Step [1770/10336], Loss: 5.7050\n",
      "Epoch [4/5], Step [1772/10336], Loss: 1.5319\n",
      "Epoch [4/5], Step [1774/10336], Loss: 2.2749\n",
      "Epoch [4/5], Step [1776/10336], Loss: 1.2163\n",
      "Epoch [4/5], Step [1778/10336], Loss: 0.1666\n",
      "Epoch [4/5], Step [1780/10336], Loss: 3.1423\n",
      "Epoch [4/5], Step [1782/10336], Loss: 0.0687\n",
      "Epoch [4/5], Step [1784/10336], Loss: 1.1719\n",
      "Epoch [4/5], Step [1786/10336], Loss: 0.2158\n",
      "Epoch [4/5], Step [1788/10336], Loss: 0.7338\n",
      "Epoch [4/5], Step [1790/10336], Loss: 0.0985\n",
      "Epoch [4/5], Step [1792/10336], Loss: 1.2226\n",
      "Epoch [4/5], Step [1794/10336], Loss: 0.6308\n",
      "Epoch [4/5], Step [1796/10336], Loss: 3.3751\n",
      "Epoch [4/5], Step [1798/10336], Loss: 0.1007\n",
      "Epoch [4/5], Step [1800/10336], Loss: 1.6220\n",
      "Epoch [4/5], Step [1802/10336], Loss: 0.2571\n",
      "Epoch [4/5], Step [1804/10336], Loss: 0.2425\n",
      "Epoch [4/5], Step [1806/10336], Loss: 0.2439\n",
      "Epoch [4/5], Step [1808/10336], Loss: 0.3453\n",
      "Epoch [4/5], Step [1810/10336], Loss: 0.6567\n",
      "Epoch [4/5], Step [1812/10336], Loss: 0.0973\n",
      "Epoch [4/5], Step [1814/10336], Loss: 0.0939\n",
      "Epoch [4/5], Step [1816/10336], Loss: 0.0259\n",
      "Epoch [4/5], Step [1818/10336], Loss: 1.4082\n",
      "Epoch [4/5], Step [1820/10336], Loss: 0.2753\n",
      "Epoch [4/5], Step [1822/10336], Loss: 0.8242\n",
      "Epoch [4/5], Step [1824/10336], Loss: 0.0105\n",
      "Epoch [4/5], Step [1826/10336], Loss: 0.0418\n",
      "Epoch [4/5], Step [1828/10336], Loss: 0.9862\n",
      "Epoch [4/5], Step [1830/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [1832/10336], Loss: 2.6153\n",
      "Epoch [4/5], Step [1834/10336], Loss: 0.2470\n",
      "Epoch [4/5], Step [1836/10336], Loss: 0.3479\n",
      "Epoch [4/5], Step [1838/10336], Loss: 0.0684\n",
      "Epoch [4/5], Step [1840/10336], Loss: 0.0372\n",
      "Epoch [4/5], Step [1842/10336], Loss: 0.0364\n",
      "Epoch [4/5], Step [1844/10336], Loss: 0.3058\n",
      "Epoch [4/5], Step [1846/10336], Loss: 0.3479\n",
      "Epoch [4/5], Step [1848/10336], Loss: 0.4341\n",
      "Epoch [4/5], Step [1850/10336], Loss: 0.0683\n",
      "Epoch [4/5], Step [1852/10336], Loss: 0.0497\n",
      "Epoch [4/5], Step [1854/10336], Loss: 2.9107\n",
      "Epoch [4/5], Step [1856/10336], Loss: 0.4042\n",
      "Epoch [4/5], Step [1858/10336], Loss: 0.6329\n",
      "Epoch [4/5], Step [1860/10336], Loss: 0.3809\n",
      "Epoch [4/5], Step [1862/10336], Loss: 0.3364\n",
      "Epoch [4/5], Step [1864/10336], Loss: 3.3945\n",
      "Epoch [4/5], Step [1866/10336], Loss: 0.0789\n",
      "Epoch [4/5], Step [1868/10336], Loss: 0.2591\n",
      "Epoch [4/5], Step [1870/10336], Loss: 0.5105\n",
      "Epoch [4/5], Step [1872/10336], Loss: 0.6155\n",
      "Epoch [4/5], Step [1874/10336], Loss: 1.0288\n",
      "Epoch [4/5], Step [1876/10336], Loss: 1.3366\n",
      "Epoch [4/5], Step [1878/10336], Loss: 0.6771\n",
      "Epoch [4/5], Step [1880/10336], Loss: 0.2551\n",
      "Epoch [4/5], Step [1882/10336], Loss: 1.5441\n",
      "Epoch [4/5], Step [1884/10336], Loss: 0.4612\n",
      "Epoch [4/5], Step [1886/10336], Loss: 2.6675\n",
      "Epoch [4/5], Step [1888/10336], Loss: 0.0687\n",
      "Epoch [4/5], Step [1890/10336], Loss: 0.3640\n",
      "Epoch [4/5], Step [1892/10336], Loss: 0.1232\n",
      "Epoch [4/5], Step [1894/10336], Loss: 0.6640\n",
      "Epoch [4/5], Step [1896/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [1898/10336], Loss: 0.5115\n",
      "Epoch [4/5], Step [1900/10336], Loss: 0.0786\n",
      "Epoch [4/5], Step [1902/10336], Loss: 0.3001\n",
      "Epoch [4/5], Step [1904/10336], Loss: 1.4385\n",
      "Epoch [4/5], Step [1906/10336], Loss: 2.3028\n",
      "Epoch [4/5], Step [1908/10336], Loss: 0.0645\n",
      "Epoch [4/5], Step [1910/10336], Loss: 0.0543\n",
      "Epoch [4/5], Step [1912/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [1914/10336], Loss: 0.2510\n",
      "Epoch [4/5], Step [1916/10336], Loss: 0.0943\n",
      "Epoch [4/5], Step [1918/10336], Loss: 0.0382\n",
      "Epoch [4/5], Step [1920/10336], Loss: 2.1989\n",
      "Epoch [4/5], Step [1922/10336], Loss: 0.1878\n",
      "Epoch [4/5], Step [1924/10336], Loss: 0.2902\n",
      "Epoch [4/5], Step [1926/10336], Loss: 0.4429\n",
      "Epoch [4/5], Step [1928/10336], Loss: 0.1809\n",
      "Epoch [4/5], Step [1930/10336], Loss: 0.2357\n",
      "Epoch [4/5], Step [1932/10336], Loss: 0.8467\n",
      "Epoch [4/5], Step [1934/10336], Loss: 0.1339\n",
      "Epoch [4/5], Step [1936/10336], Loss: 0.0098\n",
      "Epoch [4/5], Step [1938/10336], Loss: 0.3227\n",
      "Epoch [4/5], Step [1940/10336], Loss: 0.4506\n",
      "Epoch [4/5], Step [1942/10336], Loss: 0.2446\n",
      "Epoch [4/5], Step [1944/10336], Loss: 2.0185\n",
      "Epoch [4/5], Step [1946/10336], Loss: 0.2328\n",
      "Epoch [4/5], Step [1948/10336], Loss: 0.0096\n",
      "Epoch [4/5], Step [1950/10336], Loss: 0.0241\n",
      "Epoch [4/5], Step [1952/10336], Loss: 0.0191\n",
      "Epoch [4/5], Step [1954/10336], Loss: 0.6829\n",
      "Epoch [4/5], Step [1956/10336], Loss: 0.2147\n",
      "Epoch [4/5], Step [1958/10336], Loss: 0.0078\n",
      "Epoch [4/5], Step [1960/10336], Loss: 1.1981\n",
      "Epoch [4/5], Step [1962/10336], Loss: 0.0196\n",
      "Epoch [4/5], Step [1964/10336], Loss: 0.0831\n",
      "Epoch [4/5], Step [1966/10336], Loss: 0.0220\n",
      "Epoch [4/5], Step [1968/10336], Loss: 0.0759\n",
      "Epoch [4/5], Step [1970/10336], Loss: 0.1897\n",
      "Epoch [4/5], Step [1972/10336], Loss: 0.0300\n",
      "Epoch [4/5], Step [1974/10336], Loss: 0.2109\n",
      "Epoch [4/5], Step [1976/10336], Loss: 0.0092\n",
      "Epoch [4/5], Step [1978/10336], Loss: 0.0526\n",
      "Epoch [4/5], Step [1980/10336], Loss: 0.0066\n",
      "Epoch [4/5], Step [1982/10336], Loss: 0.3754\n",
      "Epoch [4/5], Step [1984/10336], Loss: 0.0406\n",
      "Epoch [4/5], Step [1986/10336], Loss: 0.2230\n",
      "Epoch [4/5], Step [1988/10336], Loss: 0.2670\n",
      "Epoch [4/5], Step [1990/10336], Loss: 0.0688\n",
      "Epoch [4/5], Step [1992/10336], Loss: 0.1257\n",
      "Epoch [4/5], Step [1994/10336], Loss: 0.0486\n",
      "Epoch [4/5], Step [1996/10336], Loss: 0.0103\n",
      "Epoch [4/5], Step [1998/10336], Loss: 0.2400\n",
      "Epoch [4/5], Step [2000/10336], Loss: 0.6543\n",
      "Epoch [4/5], Step [2002/10336], Loss: 0.1757\n",
      "Epoch [4/5], Step [2004/10336], Loss: 0.2664\n",
      "Epoch [4/5], Step [2006/10336], Loss: 0.2987\n",
      "Epoch [4/5], Step [2008/10336], Loss: 0.2093\n",
      "Epoch [4/5], Step [2010/10336], Loss: 0.0175\n",
      "Epoch [4/5], Step [2012/10336], Loss: 0.0330\n",
      "Epoch [4/5], Step [2014/10336], Loss: 2.1842\n",
      "Epoch [4/5], Step [2016/10336], Loss: 2.9970\n",
      "Epoch [4/5], Step [2018/10336], Loss: 0.0579\n",
      "Epoch [4/5], Step [2020/10336], Loss: 0.7997\n",
      "Epoch [4/5], Step [2022/10336], Loss: 0.0265\n",
      "Epoch [4/5], Step [2024/10336], Loss: 0.0813\n",
      "Epoch [4/5], Step [2026/10336], Loss: 0.0309\n",
      "Epoch [4/5], Step [2028/10336], Loss: 0.3515\n",
      "Epoch [4/5], Step [2030/10336], Loss: 0.1474\n",
      "Epoch [4/5], Step [2032/10336], Loss: 2.2187\n",
      "Epoch [4/5], Step [2034/10336], Loss: 0.0108\n",
      "Epoch [4/5], Step [2036/10336], Loss: 0.2240\n",
      "Epoch [4/5], Step [2038/10336], Loss: 1.6309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [2040/10336], Loss: 0.2886\n",
      "Epoch [4/5], Step [2042/10336], Loss: 1.1727\n",
      "Epoch [4/5], Step [2044/10336], Loss: 0.1585\n",
      "Epoch [4/5], Step [2046/10336], Loss: 0.0605\n",
      "Epoch [4/5], Step [2048/10336], Loss: 0.1579\n",
      "Epoch [4/5], Step [2050/10336], Loss: 0.5286\n",
      "Epoch [4/5], Step [2052/10336], Loss: 1.2331\n",
      "Epoch [4/5], Step [2054/10336], Loss: 0.1431\n",
      "Epoch [4/5], Step [2056/10336], Loss: 0.1466\n",
      "Epoch [4/5], Step [2058/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [2060/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [2062/10336], Loss: 0.2385\n",
      "Epoch [4/5], Step [2064/10336], Loss: 0.1028\n",
      "Epoch [4/5], Step [2066/10336], Loss: 0.4121\n",
      "Epoch [4/5], Step [2068/10336], Loss: 0.0464\n",
      "Epoch [4/5], Step [2070/10336], Loss: 1.7634\n",
      "Epoch [4/5], Step [2072/10336], Loss: 0.4787\n",
      "Epoch [4/5], Step [2074/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [2076/10336], Loss: 0.0952\n",
      "Epoch [4/5], Step [2078/10336], Loss: 0.0195\n",
      "Epoch [4/5], Step [2080/10336], Loss: 0.2938\n",
      "Epoch [4/5], Step [2082/10336], Loss: 0.4076\n",
      "Epoch [4/5], Step [2084/10336], Loss: 0.0247\n",
      "Epoch [4/5], Step [2086/10336], Loss: 3.9142\n",
      "Epoch [4/5], Step [2088/10336], Loss: 0.1767\n",
      "Epoch [4/5], Step [2090/10336], Loss: 0.0087\n",
      "Epoch [4/5], Step [2092/10336], Loss: 0.0129\n",
      "Epoch [4/5], Step [2094/10336], Loss: 7.3512\n",
      "Epoch [4/5], Step [2096/10336], Loss: 0.1372\n",
      "Epoch [4/5], Step [2098/10336], Loss: 0.1597\n",
      "Epoch [4/5], Step [2100/10336], Loss: 2.7059\n",
      "Epoch [4/5], Step [2102/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [2104/10336], Loss: 1.7065\n",
      "Epoch [4/5], Step [2106/10336], Loss: 0.2105\n",
      "Epoch [4/5], Step [2108/10336], Loss: 1.1863\n",
      "Epoch [4/5], Step [2110/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [2112/10336], Loss: 0.2957\n",
      "Epoch [4/5], Step [2114/10336], Loss: 0.0192\n",
      "Epoch [4/5], Step [2116/10336], Loss: 0.4945\n",
      "Epoch [4/5], Step [2118/10336], Loss: 0.4433\n",
      "Epoch [4/5], Step [2120/10336], Loss: 0.1437\n",
      "Epoch [4/5], Step [2122/10336], Loss: 1.7157\n",
      "Epoch [4/5], Step [2124/10336], Loss: 1.1504\n",
      "Epoch [4/5], Step [2126/10336], Loss: 1.3237\n",
      "Epoch [4/5], Step [2128/10336], Loss: 0.3814\n",
      "Epoch [4/5], Step [2130/10336], Loss: 4.7336\n",
      "Epoch [4/5], Step [2132/10336], Loss: 0.0580\n",
      "Epoch [4/5], Step [2134/10336], Loss: 0.1073\n",
      "Epoch [4/5], Step [2136/10336], Loss: 0.0153\n",
      "Epoch [4/5], Step [2138/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [2140/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [2142/10336], Loss: 4.6853\n",
      "Epoch [4/5], Step [2144/10336], Loss: 0.5611\n",
      "Epoch [4/5], Step [2146/10336], Loss: 0.0643\n",
      "Epoch [4/5], Step [2148/10336], Loss: 0.0587\n",
      "Epoch [4/5], Step [2150/10336], Loss: 0.0079\n",
      "Epoch [4/5], Step [2152/10336], Loss: 0.0043\n",
      "Epoch [4/5], Step [2154/10336], Loss: 0.2996\n",
      "Epoch [4/5], Step [2156/10336], Loss: 0.0507\n",
      "Epoch [4/5], Step [2158/10336], Loss: 0.0288\n",
      "Epoch [4/5], Step [2160/10336], Loss: 0.0366\n",
      "Epoch [4/5], Step [2162/10336], Loss: 0.1644\n",
      "Epoch [4/5], Step [2164/10336], Loss: 0.0813\n",
      "Epoch [4/5], Step [2166/10336], Loss: 0.0179\n",
      "Epoch [4/5], Step [2168/10336], Loss: 1.7584\n",
      "Epoch [4/5], Step [2170/10336], Loss: 0.2611\n",
      "Epoch [4/5], Step [2172/10336], Loss: 1.2510\n",
      "Epoch [4/5], Step [2174/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [2176/10336], Loss: 0.0255\n",
      "Epoch [4/5], Step [2178/10336], Loss: 0.0211\n",
      "Epoch [4/5], Step [2180/10336], Loss: 0.0642\n",
      "Epoch [4/5], Step [2182/10336], Loss: 0.6822\n",
      "Epoch [4/5], Step [2184/10336], Loss: 2.7945\n",
      "Epoch [4/5], Step [2186/10336], Loss: 0.4402\n",
      "Epoch [4/5], Step [2188/10336], Loss: 0.0255\n",
      "Epoch [4/5], Step [2190/10336], Loss: 0.1594\n",
      "Epoch [4/5], Step [2192/10336], Loss: 0.0543\n",
      "Epoch [4/5], Step [2194/10336], Loss: 0.0688\n",
      "Epoch [4/5], Step [2196/10336], Loss: 0.0038\n",
      "Epoch [4/5], Step [2198/10336], Loss: 0.7445\n",
      "Epoch [4/5], Step [2200/10336], Loss: 0.2908\n",
      "Epoch [4/5], Step [2202/10336], Loss: 0.5358\n",
      "Epoch [4/5], Step [2204/10336], Loss: 3.1282\n",
      "Epoch [4/5], Step [2206/10336], Loss: 0.2929\n",
      "Epoch [4/5], Step [2208/10336], Loss: 2.6760\n",
      "Epoch [4/5], Step [2210/10336], Loss: 0.0069\n",
      "Epoch [4/5], Step [2212/10336], Loss: 0.8497\n",
      "Epoch [4/5], Step [2214/10336], Loss: 0.0030\n",
      "Epoch [4/5], Step [2216/10336], Loss: 0.2745\n",
      "Epoch [4/5], Step [2218/10336], Loss: 0.7739\n",
      "Epoch [4/5], Step [2220/10336], Loss: 0.1848\n",
      "Epoch [4/5], Step [2222/10336], Loss: 0.1035\n",
      "Epoch [4/5], Step [2224/10336], Loss: 0.0518\n",
      "Epoch [4/5], Step [2226/10336], Loss: 2.9303\n",
      "Epoch [4/5], Step [2228/10336], Loss: 0.1786\n",
      "Epoch [4/5], Step [2230/10336], Loss: 0.1503\n",
      "Epoch [4/5], Step [2232/10336], Loss: 0.9581\n",
      "Epoch [4/5], Step [2234/10336], Loss: 1.7596\n",
      "Epoch [4/5], Step [2236/10336], Loss: 0.0035\n",
      "Epoch [4/5], Step [2238/10336], Loss: 2.3006\n",
      "Epoch [4/5], Step [2240/10336], Loss: 2.8656\n",
      "Epoch [4/5], Step [2242/10336], Loss: 0.1184\n",
      "Epoch [4/5], Step [2244/10336], Loss: 0.0464\n",
      "Epoch [4/5], Step [2246/10336], Loss: 1.1721\n",
      "Epoch [4/5], Step [2248/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [2250/10336], Loss: 1.2528\n",
      "Epoch [4/5], Step [2252/10336], Loss: 0.2099\n",
      "Epoch [4/5], Step [2254/10336], Loss: 0.0179\n",
      "Epoch [4/5], Step [2256/10336], Loss: 0.4542\n",
      "Epoch [4/5], Step [2258/10336], Loss: 2.6368\n",
      "Epoch [4/5], Step [2260/10336], Loss: 0.0322\n",
      "Epoch [4/5], Step [2262/10336], Loss: 0.0174\n",
      "Epoch [4/5], Step [2264/10336], Loss: 0.0690\n",
      "Epoch [4/5], Step [2266/10336], Loss: 0.0972\n",
      "Epoch [4/5], Step [2268/10336], Loss: 1.0970\n",
      "Epoch [4/5], Step [2270/10336], Loss: 0.1361\n",
      "Epoch [4/5], Step [2272/10336], Loss: 0.5207\n",
      "Epoch [4/5], Step [2274/10336], Loss: 0.2209\n",
      "Epoch [4/5], Step [2276/10336], Loss: 0.1530\n",
      "Epoch [4/5], Step [2278/10336], Loss: 0.3862\n",
      "Epoch [4/5], Step [2280/10336], Loss: 0.1564\n",
      "Epoch [4/5], Step [2282/10336], Loss: 0.0053\n",
      "Epoch [4/5], Step [2284/10336], Loss: 0.0690\n",
      "Epoch [4/5], Step [2286/10336], Loss: 0.0176\n",
      "Epoch [4/5], Step [2288/10336], Loss: 0.2913\n",
      "Epoch [4/5], Step [2290/10336], Loss: 4.0083\n",
      "Epoch [4/5], Step [2292/10336], Loss: 0.0608\n",
      "Epoch [4/5], Step [2294/10336], Loss: 0.6577\n",
      "Epoch [4/5], Step [2296/10336], Loss: 2.5071\n",
      "Epoch [4/5], Step [2298/10336], Loss: 0.0837\n",
      "Epoch [4/5], Step [2300/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [2302/10336], Loss: 0.0481\n",
      "Epoch [4/5], Step [2304/10336], Loss: 4.9395\n",
      "Epoch [4/5], Step [2306/10336], Loss: 0.3864\n",
      "Epoch [4/5], Step [2308/10336], Loss: 0.3124\n",
      "Epoch [4/5], Step [2310/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [2312/10336], Loss: 0.0191\n",
      "Epoch [4/5], Step [2314/10336], Loss: 1.1513\n",
      "Epoch [4/5], Step [2316/10336], Loss: 0.2482\n",
      "Epoch [4/5], Step [2318/10336], Loss: 0.0164\n",
      "Epoch [4/5], Step [2320/10336], Loss: 0.1961\n",
      "Epoch [4/5], Step [2322/10336], Loss: 1.0731\n",
      "Epoch [4/5], Step [2324/10336], Loss: 0.1014\n",
      "Epoch [4/5], Step [2326/10336], Loss: 0.2124\n",
      "Epoch [4/5], Step [2328/10336], Loss: 0.0157\n",
      "Epoch [4/5], Step [2330/10336], Loss: 0.7835\n",
      "Epoch [4/5], Step [2332/10336], Loss: 0.0846\n",
      "Epoch [4/5], Step [2334/10336], Loss: 0.1756\n",
      "Epoch [4/5], Step [2336/10336], Loss: 0.3581\n",
      "Epoch [4/5], Step [2338/10336], Loss: 0.3924\n",
      "Epoch [4/5], Step [2340/10336], Loss: 1.0371\n",
      "Epoch [4/5], Step [2342/10336], Loss: 2.9306\n",
      "Epoch [4/5], Step [2344/10336], Loss: 4.1549\n",
      "Epoch [4/5], Step [2346/10336], Loss: 0.2504\n",
      "Epoch [4/5], Step [2348/10336], Loss: 0.1613\n",
      "Epoch [4/5], Step [2350/10336], Loss: 0.4483\n",
      "Epoch [4/5], Step [2352/10336], Loss: 0.0081\n",
      "Epoch [4/5], Step [2354/10336], Loss: 0.5598\n",
      "Epoch [4/5], Step [2356/10336], Loss: 2.1178\n",
      "Epoch [4/5], Step [2358/10336], Loss: 0.1636\n",
      "Epoch [4/5], Step [2360/10336], Loss: 1.1613\n",
      "Epoch [4/5], Step [2362/10336], Loss: 0.1145\n",
      "Epoch [4/5], Step [2364/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [2366/10336], Loss: 0.1986\n",
      "Epoch [4/5], Step [2368/10336], Loss: 1.9643\n",
      "Epoch [4/5], Step [2370/10336], Loss: 0.2841\n",
      "Epoch [4/5], Step [2372/10336], Loss: 0.0731\n",
      "Epoch [4/5], Step [2374/10336], Loss: 0.0270\n",
      "Epoch [4/5], Step [2376/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [2378/10336], Loss: 0.4531\n",
      "Epoch [4/5], Step [2380/10336], Loss: 0.5135\n",
      "Epoch [4/5], Step [2382/10336], Loss: 1.0208\n",
      "Epoch [4/5], Step [2384/10336], Loss: 1.8373\n",
      "Epoch [4/5], Step [2386/10336], Loss: 0.1297\n",
      "Epoch [4/5], Step [2388/10336], Loss: 0.0271\n",
      "Epoch [4/5], Step [2390/10336], Loss: 0.2850\n",
      "Epoch [4/5], Step [2392/10336], Loss: 0.1905\n",
      "Epoch [4/5], Step [2394/10336], Loss: 2.9435\n",
      "Epoch [4/5], Step [2396/10336], Loss: 0.0518\n",
      "Epoch [4/5], Step [2398/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [2400/10336], Loss: 0.0196\n",
      "Epoch [4/5], Step [2402/10336], Loss: 0.0069\n",
      "Epoch [4/5], Step [2404/10336], Loss: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [2406/10336], Loss: 1.2256\n",
      "Epoch [4/5], Step [2408/10336], Loss: 0.0687\n",
      "Epoch [4/5], Step [2410/10336], Loss: 1.7852\n",
      "Epoch [4/5], Step [2412/10336], Loss: 0.2549\n",
      "Epoch [4/5], Step [2414/10336], Loss: 2.5874\n",
      "Epoch [4/5], Step [2416/10336], Loss: 1.1612\n",
      "Epoch [4/5], Step [2418/10336], Loss: 4.7843\n",
      "Epoch [4/5], Step [2420/10336], Loss: 0.2868\n",
      "Epoch [4/5], Step [2422/10336], Loss: 0.0610\n",
      "Epoch [4/5], Step [2424/10336], Loss: 1.5263\n",
      "Epoch [4/5], Step [2426/10336], Loss: 0.2358\n",
      "Epoch [4/5], Step [2428/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [2430/10336], Loss: 0.6624\n",
      "Epoch [4/5], Step [2432/10336], Loss: 0.3604\n",
      "Epoch [4/5], Step [2434/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [2436/10336], Loss: 0.0617\n",
      "Epoch [4/5], Step [2438/10336], Loss: 0.2146\n",
      "Epoch [4/5], Step [2440/10336], Loss: 1.4216\n",
      "Epoch [4/5], Step [2442/10336], Loss: 0.4393\n",
      "Epoch [4/5], Step [2444/10336], Loss: 0.1024\n",
      "Epoch [4/5], Step [2446/10336], Loss: 0.2237\n",
      "Epoch [4/5], Step [2448/10336], Loss: 0.0561\n",
      "Epoch [4/5], Step [2450/10336], Loss: 2.2118\n",
      "Epoch [4/5], Step [2452/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [2454/10336], Loss: 0.9349\n",
      "Epoch [4/5], Step [2456/10336], Loss: 0.0779\n",
      "Epoch [4/5], Step [2458/10336], Loss: 0.0105\n",
      "Epoch [4/5], Step [2460/10336], Loss: 0.0773\n",
      "Epoch [4/5], Step [2462/10336], Loss: 0.1674\n",
      "Epoch [4/5], Step [2464/10336], Loss: 0.4078\n",
      "Epoch [4/5], Step [2466/10336], Loss: 0.1310\n",
      "Epoch [4/5], Step [2468/10336], Loss: 0.1938\n",
      "Epoch [4/5], Step [2470/10336], Loss: 1.6161\n",
      "Epoch [4/5], Step [2472/10336], Loss: 0.0534\n",
      "Epoch [4/5], Step [2474/10336], Loss: 0.1676\n",
      "Epoch [4/5], Step [2476/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [2478/10336], Loss: 0.3367\n",
      "Epoch [4/5], Step [2480/10336], Loss: 2.2361\n",
      "Epoch [4/5], Step [2482/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [2484/10336], Loss: 1.0347\n",
      "Epoch [4/5], Step [2486/10336], Loss: 0.0598\n",
      "Epoch [4/5], Step [2488/10336], Loss: 0.0823\n",
      "Epoch [4/5], Step [2490/10336], Loss: 0.2213\n",
      "Epoch [4/5], Step [2492/10336], Loss: 0.1492\n",
      "Epoch [4/5], Step [2494/10336], Loss: 0.0088\n",
      "Epoch [4/5], Step [2496/10336], Loss: 0.3038\n",
      "Epoch [4/5], Step [2498/10336], Loss: 0.2319\n",
      "Epoch [4/5], Step [2500/10336], Loss: 0.2332\n",
      "Epoch [4/5], Step [2502/10336], Loss: 0.2675\n",
      "Epoch [4/5], Step [2504/10336], Loss: 0.5498\n",
      "Epoch [4/5], Step [2506/10336], Loss: 0.0776\n",
      "Epoch [4/5], Step [2508/10336], Loss: 1.8284\n",
      "Epoch [4/5], Step [2510/10336], Loss: 0.1179\n",
      "Epoch [4/5], Step [2512/10336], Loss: 3.0914\n",
      "Epoch [4/5], Step [2514/10336], Loss: 1.4431\n",
      "Epoch [4/5], Step [2516/10336], Loss: 0.2345\n",
      "Epoch [4/5], Step [2518/10336], Loss: 1.3273\n",
      "Epoch [4/5], Step [2520/10336], Loss: 0.4949\n",
      "Epoch [4/5], Step [2522/10336], Loss: 0.1538\n",
      "Epoch [4/5], Step [2524/10336], Loss: 0.4333\n",
      "Epoch [4/5], Step [2526/10336], Loss: 1.4261\n",
      "Epoch [4/5], Step [2528/10336], Loss: 0.0149\n",
      "Epoch [4/5], Step [2530/10336], Loss: 0.3653\n",
      "Epoch [4/5], Step [2532/10336], Loss: 4.2255\n",
      "Epoch [4/5], Step [2534/10336], Loss: 0.5223\n",
      "Epoch [4/5], Step [2536/10336], Loss: 0.9664\n",
      "Epoch [4/5], Step [2538/10336], Loss: 0.1049\n",
      "Epoch [4/5], Step [2540/10336], Loss: 0.2926\n",
      "Epoch [4/5], Step [2542/10336], Loss: 0.8349\n",
      "Epoch [4/5], Step [2544/10336], Loss: 0.5938\n",
      "Epoch [4/5], Step [2546/10336], Loss: 0.3298\n",
      "Epoch [4/5], Step [2548/10336], Loss: 0.2946\n",
      "Epoch [4/5], Step [2550/10336], Loss: 0.7334\n",
      "Epoch [4/5], Step [2552/10336], Loss: 0.0093\n",
      "Epoch [4/5], Step [2554/10336], Loss: 0.0908\n",
      "Epoch [4/5], Step [2556/10336], Loss: 0.4791\n",
      "Epoch [4/5], Step [2558/10336], Loss: 0.0518\n",
      "Epoch [4/5], Step [2560/10336], Loss: 0.3315\n",
      "Epoch [4/5], Step [2562/10336], Loss: 1.7162\n",
      "Epoch [4/5], Step [2564/10336], Loss: 0.1935\n",
      "Epoch [4/5], Step [2566/10336], Loss: 0.0029\n",
      "Epoch [4/5], Step [2568/10336], Loss: 0.1084\n",
      "Epoch [4/5], Step [2570/10336], Loss: 0.3141\n",
      "Epoch [4/5], Step [2572/10336], Loss: 0.0535\n",
      "Epoch [4/5], Step [2574/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [2576/10336], Loss: 0.3404\n",
      "Epoch [4/5], Step [2578/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [2580/10336], Loss: 0.4293\n",
      "Epoch [4/5], Step [2582/10336], Loss: 0.0548\n",
      "Epoch [4/5], Step [2584/10336], Loss: 1.2108\n",
      "Epoch [4/5], Step [2586/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [2588/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [2590/10336], Loss: 0.0894\n",
      "Epoch [4/5], Step [2592/10336], Loss: 1.4733\n",
      "Epoch [4/5], Step [2594/10336], Loss: 4.8569\n",
      "Epoch [4/5], Step [2596/10336], Loss: 0.8656\n",
      "Epoch [4/5], Step [2598/10336], Loss: 0.2465\n",
      "Epoch [4/5], Step [2600/10336], Loss: 0.3938\n",
      "Epoch [4/5], Step [2602/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [2604/10336], Loss: 0.1580\n",
      "Epoch [4/5], Step [2606/10336], Loss: 1.7429\n",
      "Epoch [4/5], Step [2608/10336], Loss: 1.8548\n",
      "Epoch [4/5], Step [2610/10336], Loss: 1.4004\n",
      "Epoch [4/5], Step [2612/10336], Loss: 1.9286\n",
      "Epoch [4/5], Step [2614/10336], Loss: 0.1356\n",
      "Epoch [4/5], Step [2616/10336], Loss: 0.1133\n",
      "Epoch [4/5], Step [2618/10336], Loss: 0.7921\n",
      "Epoch [4/5], Step [2620/10336], Loss: 0.0536\n",
      "Epoch [4/5], Step [2622/10336], Loss: 0.3032\n",
      "Epoch [4/5], Step [2624/10336], Loss: 0.4809\n",
      "Epoch [4/5], Step [2626/10336], Loss: 2.0304\n",
      "Epoch [4/5], Step [2628/10336], Loss: 0.3990\n",
      "Epoch [4/5], Step [2630/10336], Loss: 0.0043\n",
      "Epoch [4/5], Step [2632/10336], Loss: 0.1024\n",
      "Epoch [4/5], Step [2634/10336], Loss: 0.6839\n",
      "Epoch [4/5], Step [2636/10336], Loss: 0.0687\n",
      "Epoch [4/5], Step [2638/10336], Loss: 0.0363\n",
      "Epoch [4/5], Step [2640/10336], Loss: 0.0097\n",
      "Epoch [4/5], Step [2642/10336], Loss: 1.2362\n",
      "Epoch [4/5], Step [2644/10336], Loss: 0.1827\n",
      "Epoch [4/5], Step [2646/10336], Loss: 0.0102\n",
      "Epoch [4/5], Step [2648/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [2650/10336], Loss: 1.3139\n",
      "Epoch [4/5], Step [2652/10336], Loss: 0.1688\n",
      "Epoch [4/5], Step [2654/10336], Loss: 0.0212\n",
      "Epoch [4/5], Step [2656/10336], Loss: 0.3504\n",
      "Epoch [4/5], Step [2658/10336], Loss: 0.3644\n",
      "Epoch [4/5], Step [2660/10336], Loss: 0.1501\n",
      "Epoch [4/5], Step [2662/10336], Loss: 0.1438\n",
      "Epoch [4/5], Step [2664/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [2666/10336], Loss: 1.1086\n",
      "Epoch [4/5], Step [2668/10336], Loss: 0.0845\n",
      "Epoch [4/5], Step [2670/10336], Loss: 3.0545\n",
      "Epoch [4/5], Step [2672/10336], Loss: 1.0789\n",
      "Epoch [4/5], Step [2674/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [2676/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [2678/10336], Loss: 0.0131\n",
      "Epoch [4/5], Step [2680/10336], Loss: 0.6221\n",
      "Epoch [4/5], Step [2682/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [2684/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [2686/10336], Loss: 0.1264\n",
      "Epoch [4/5], Step [2688/10336], Loss: 0.4906\n",
      "Epoch [4/5], Step [2690/10336], Loss: 0.2519\n",
      "Epoch [4/5], Step [2692/10336], Loss: 0.1065\n",
      "Epoch [4/5], Step [2694/10336], Loss: 0.0810\n",
      "Epoch [4/5], Step [2696/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [2698/10336], Loss: 0.2175\n",
      "Epoch [4/5], Step [2700/10336], Loss: 3.4702\n",
      "Epoch [4/5], Step [2702/10336], Loss: 0.3072\n",
      "Epoch [4/5], Step [2704/10336], Loss: 0.0582\n",
      "Epoch [4/5], Step [2706/10336], Loss: 0.0699\n",
      "Epoch [4/5], Step [2708/10336], Loss: 0.8364\n",
      "Epoch [4/5], Step [2710/10336], Loss: 0.1954\n",
      "Epoch [4/5], Step [2712/10336], Loss: 0.1689\n",
      "Epoch [4/5], Step [2714/10336], Loss: 0.6122\n",
      "Epoch [4/5], Step [2716/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [2718/10336], Loss: 0.2609\n",
      "Epoch [4/5], Step [2720/10336], Loss: 0.0474\n",
      "Epoch [4/5], Step [2722/10336], Loss: 0.3521\n",
      "Epoch [4/5], Step [2724/10336], Loss: 3.1634\n",
      "Epoch [4/5], Step [2726/10336], Loss: 0.2292\n",
      "Epoch [4/5], Step [2728/10336], Loss: 0.0479\n",
      "Epoch [4/5], Step [2730/10336], Loss: 0.4815\n",
      "Epoch [4/5], Step [2732/10336], Loss: 0.4624\n",
      "Epoch [4/5], Step [2734/10336], Loss: 0.2033\n",
      "Epoch [4/5], Step [2736/10336], Loss: 3.7582\n",
      "Epoch [4/5], Step [2738/10336], Loss: 3.6334\n",
      "Epoch [4/5], Step [2740/10336], Loss: 0.2015\n",
      "Epoch [4/5], Step [2742/10336], Loss: 0.3158\n",
      "Epoch [4/5], Step [2744/10336], Loss: 1.3736\n",
      "Epoch [4/5], Step [2746/10336], Loss: 0.2330\n",
      "Epoch [4/5], Step [2748/10336], Loss: 1.5188\n",
      "Epoch [4/5], Step [2750/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [2752/10336], Loss: 0.4646\n",
      "Epoch [4/5], Step [2754/10336], Loss: 0.8113\n",
      "Epoch [4/5], Step [2756/10336], Loss: 0.0179\n",
      "Epoch [4/5], Step [2758/10336], Loss: 0.0320\n",
      "Epoch [4/5], Step [2760/10336], Loss: 0.0681\n",
      "Epoch [4/5], Step [2762/10336], Loss: 0.0938\n",
      "Epoch [4/5], Step [2764/10336], Loss: 1.9882\n",
      "Epoch [4/5], Step [2766/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [2768/10336], Loss: 0.0106\n",
      "Epoch [4/5], Step [2770/10336], Loss: 7.6908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [2772/10336], Loss: 0.3766\n",
      "Epoch [4/5], Step [2774/10336], Loss: 3.3221\n",
      "Epoch [4/5], Step [2776/10336], Loss: 0.0198\n",
      "Epoch [4/5], Step [2778/10336], Loss: 0.0073\n",
      "Epoch [4/5], Step [2780/10336], Loss: 0.0491\n",
      "Epoch [4/5], Step [2782/10336], Loss: 4.5716\n",
      "Epoch [4/5], Step [2784/10336], Loss: 0.8896\n",
      "Epoch [4/5], Step [2786/10336], Loss: 0.0978\n",
      "Epoch [4/5], Step [2788/10336], Loss: 0.3574\n",
      "Epoch [4/5], Step [2790/10336], Loss: 0.0227\n",
      "Epoch [4/5], Step [2792/10336], Loss: 0.0092\n",
      "Epoch [4/5], Step [2794/10336], Loss: 0.4697\n",
      "Epoch [4/5], Step [2796/10336], Loss: 0.5380\n",
      "Epoch [4/5], Step [2798/10336], Loss: 0.0252\n",
      "Epoch [4/5], Step [2800/10336], Loss: 0.3180\n",
      "Epoch [4/5], Step [2802/10336], Loss: 1.8265\n",
      "Epoch [4/5], Step [2804/10336], Loss: 0.8878\n",
      "Epoch [4/5], Step [2806/10336], Loss: 3.3694\n",
      "Epoch [4/5], Step [2808/10336], Loss: 1.5617\n",
      "Epoch [4/5], Step [2810/10336], Loss: 0.5010\n",
      "Epoch [4/5], Step [2812/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [2814/10336], Loss: 0.6652\n",
      "Epoch [4/5], Step [2816/10336], Loss: 0.1215\n",
      "Epoch [4/5], Step [2818/10336], Loss: 0.0200\n",
      "Epoch [4/5], Step [2820/10336], Loss: 0.0086\n",
      "Epoch [4/5], Step [2822/10336], Loss: 0.0607\n",
      "Epoch [4/5], Step [2824/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [2826/10336], Loss: 0.4084\n",
      "Epoch [4/5], Step [2828/10336], Loss: 0.4579\n",
      "Epoch [4/5], Step [2830/10336], Loss: 0.1256\n",
      "Epoch [4/5], Step [2832/10336], Loss: 0.4680\n",
      "Epoch [4/5], Step [2834/10336], Loss: 0.5126\n",
      "Epoch [4/5], Step [2836/10336], Loss: 0.3072\n",
      "Epoch [4/5], Step [2838/10336], Loss: 0.5220\n",
      "Epoch [4/5], Step [2840/10336], Loss: 0.0197\n",
      "Epoch [4/5], Step [2842/10336], Loss: 0.8563\n",
      "Epoch [4/5], Step [2844/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [2846/10336], Loss: 1.8740\n",
      "Epoch [4/5], Step [2848/10336], Loss: 0.2230\n",
      "Epoch [4/5], Step [2850/10336], Loss: 0.0547\n",
      "Epoch [4/5], Step [2852/10336], Loss: 0.1756\n",
      "Epoch [4/5], Step [2854/10336], Loss: 0.0048\n",
      "Epoch [4/5], Step [2856/10336], Loss: 0.0324\n",
      "Epoch [4/5], Step [2858/10336], Loss: 0.4164\n",
      "Epoch [4/5], Step [2860/10336], Loss: 2.1566\n",
      "Epoch [4/5], Step [2862/10336], Loss: 2.0722\n",
      "Epoch [4/5], Step [2864/10336], Loss: 0.6718\n",
      "Epoch [4/5], Step [2866/10336], Loss: 1.0237\n",
      "Epoch [4/5], Step [2868/10336], Loss: 0.2583\n",
      "Epoch [4/5], Step [2870/10336], Loss: 0.0463\n",
      "Epoch [4/5], Step [2872/10336], Loss: 0.1507\n",
      "Epoch [4/5], Step [2874/10336], Loss: 0.1839\n",
      "Epoch [4/5], Step [2876/10336], Loss: 0.2841\n",
      "Epoch [4/5], Step [2878/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [2880/10336], Loss: 2.0694\n",
      "Epoch [4/5], Step [2882/10336], Loss: 0.4763\n",
      "Epoch [4/5], Step [2884/10336], Loss: 0.1317\n",
      "Epoch [4/5], Step [2886/10336], Loss: 0.2583\n",
      "Epoch [4/5], Step [2888/10336], Loss: 0.0078\n",
      "Epoch [4/5], Step [2890/10336], Loss: 0.2464\n",
      "Epoch [4/5], Step [2892/10336], Loss: 0.1493\n",
      "Epoch [4/5], Step [2894/10336], Loss: 0.1825\n",
      "Epoch [4/5], Step [2896/10336], Loss: 1.3667\n",
      "Epoch [4/5], Step [2898/10336], Loss: 0.2339\n",
      "Epoch [4/5], Step [2900/10336], Loss: 0.2699\n",
      "Epoch [4/5], Step [2902/10336], Loss: 0.6298\n",
      "Epoch [4/5], Step [2904/10336], Loss: 0.0194\n",
      "Epoch [4/5], Step [2906/10336], Loss: 0.7816\n",
      "Epoch [4/5], Step [2908/10336], Loss: 0.4702\n",
      "Epoch [4/5], Step [2910/10336], Loss: 0.0322\n",
      "Epoch [4/5], Step [2912/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [2914/10336], Loss: 3.7030\n",
      "Epoch [4/5], Step [2916/10336], Loss: 1.1602\n",
      "Epoch [4/5], Step [2918/10336], Loss: 0.1244\n",
      "Epoch [4/5], Step [2920/10336], Loss: 0.0830\n",
      "Epoch [4/5], Step [2922/10336], Loss: 0.4287\n",
      "Epoch [4/5], Step [2924/10336], Loss: 0.1634\n",
      "Epoch [4/5], Step [2926/10336], Loss: 0.2096\n",
      "Epoch [4/5], Step [2928/10336], Loss: 0.2733\n",
      "Epoch [4/5], Step [2930/10336], Loss: 1.3814\n",
      "Epoch [4/5], Step [2932/10336], Loss: 0.0681\n",
      "Epoch [4/5], Step [2934/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [2936/10336], Loss: 0.9493\n",
      "Epoch [4/5], Step [2938/10336], Loss: 0.1234\n",
      "Epoch [4/5], Step [2940/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [2942/10336], Loss: 2.9174\n",
      "Epoch [4/5], Step [2944/10336], Loss: 0.2541\n",
      "Epoch [4/5], Step [2946/10336], Loss: 0.2090\n",
      "Epoch [4/5], Step [2948/10336], Loss: 1.1016\n",
      "Epoch [4/5], Step [2950/10336], Loss: 0.0561\n",
      "Epoch [4/5], Step [2952/10336], Loss: 0.3878\n",
      "Epoch [4/5], Step [2954/10336], Loss: 1.6419\n",
      "Epoch [4/5], Step [2956/10336], Loss: 0.0845\n",
      "Epoch [4/5], Step [2958/10336], Loss: 0.2089\n",
      "Epoch [4/5], Step [2960/10336], Loss: 1.8733\n",
      "Epoch [4/5], Step [2962/10336], Loss: 0.0384\n",
      "Epoch [4/5], Step [2964/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [2966/10336], Loss: 0.2252\n",
      "Epoch [4/5], Step [2968/10336], Loss: 0.1574\n",
      "Epoch [4/5], Step [2970/10336], Loss: 0.5236\n",
      "Epoch [4/5], Step [2972/10336], Loss: 0.1303\n",
      "Epoch [4/5], Step [2974/10336], Loss: 3.1675\n",
      "Epoch [4/5], Step [2976/10336], Loss: 0.0969\n",
      "Epoch [4/5], Step [2978/10336], Loss: 0.0101\n",
      "Epoch [4/5], Step [2980/10336], Loss: 0.0095\n",
      "Epoch [4/5], Step [2982/10336], Loss: 0.1314\n",
      "Epoch [4/5], Step [2984/10336], Loss: 0.8104\n",
      "Epoch [4/5], Step [2986/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [2988/10336], Loss: 0.3842\n",
      "Epoch [4/5], Step [2990/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [2992/10336], Loss: 0.0123\n",
      "Epoch [4/5], Step [2994/10336], Loss: 0.0174\n",
      "Epoch [4/5], Step [2996/10336], Loss: 0.6417\n",
      "Epoch [4/5], Step [2998/10336], Loss: 0.2216\n",
      "Epoch [4/5], Step [3000/10336], Loss: 1.4823\n",
      "Epoch [4/5], Step [3002/10336], Loss: 0.1938\n",
      "Epoch [4/5], Step [3004/10336], Loss: 3.0207\n",
      "Epoch [4/5], Step [3006/10336], Loss: 0.0317\n",
      "Epoch [4/5], Step [3008/10336], Loss: 0.4463\n",
      "Epoch [4/5], Step [3010/10336], Loss: 0.4504\n",
      "Epoch [4/5], Step [3012/10336], Loss: 0.0405\n",
      "Epoch [4/5], Step [3014/10336], Loss: 1.6413\n",
      "Epoch [4/5], Step [3016/10336], Loss: 1.0281\n",
      "Epoch [4/5], Step [3018/10336], Loss: 2.1224\n",
      "Epoch [4/5], Step [3020/10336], Loss: 0.2111\n",
      "Epoch [4/5], Step [3022/10336], Loss: 2.8240\n",
      "Epoch [4/5], Step [3024/10336], Loss: 2.0834\n",
      "Epoch [4/5], Step [3026/10336], Loss: 3.9967\n",
      "Epoch [4/5], Step [3028/10336], Loss: 0.6235\n",
      "Epoch [4/5], Step [3030/10336], Loss: 0.1548\n",
      "Epoch [4/5], Step [3032/10336], Loss: 0.0189\n",
      "Epoch [4/5], Step [3034/10336], Loss: 0.0202\n",
      "Epoch [4/5], Step [3036/10336], Loss: 1.5971\n",
      "Epoch [4/5], Step [3038/10336], Loss: 0.1302\n",
      "Epoch [4/5], Step [3040/10336], Loss: 0.0360\n",
      "Epoch [4/5], Step [3042/10336], Loss: 0.6934\n",
      "Epoch [4/5], Step [3044/10336], Loss: 0.0996\n",
      "Epoch [4/5], Step [3046/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [3048/10336], Loss: 0.0231\n",
      "Epoch [4/5], Step [3050/10336], Loss: 0.1836\n",
      "Epoch [4/5], Step [3052/10336], Loss: 4.2576\n",
      "Epoch [4/5], Step [3054/10336], Loss: 0.0760\n",
      "Epoch [4/5], Step [3056/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [3058/10336], Loss: 1.5863\n",
      "Epoch [4/5], Step [3060/10336], Loss: 3.8659\n",
      "Epoch [4/5], Step [3062/10336], Loss: 0.7195\n",
      "Epoch [4/5], Step [3064/10336], Loss: 1.7996\n",
      "Epoch [4/5], Step [3066/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [3068/10336], Loss: 0.8821\n",
      "Epoch [4/5], Step [3070/10336], Loss: 0.0364\n",
      "Epoch [4/5], Step [3072/10336], Loss: 0.1984\n",
      "Epoch [4/5], Step [3074/10336], Loss: 2.3409\n",
      "Epoch [4/5], Step [3076/10336], Loss: 2.7993\n",
      "Epoch [4/5], Step [3078/10336], Loss: 0.3696\n",
      "Epoch [4/5], Step [3080/10336], Loss: 0.2127\n",
      "Epoch [4/5], Step [3082/10336], Loss: 2.2560\n",
      "Epoch [4/5], Step [3084/10336], Loss: 1.0910\n",
      "Epoch [4/5], Step [3086/10336], Loss: 0.2666\n",
      "Epoch [4/5], Step [3088/10336], Loss: 1.3982\n",
      "Epoch [4/5], Step [3090/10336], Loss: 0.0282\n",
      "Epoch [4/5], Step [3092/10336], Loss: 0.0712\n",
      "Epoch [4/5], Step [3094/10336], Loss: 0.2666\n",
      "Epoch [4/5], Step [3096/10336], Loss: 0.0163\n",
      "Epoch [4/5], Step [3098/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [3100/10336], Loss: 0.0302\n",
      "Epoch [4/5], Step [3102/10336], Loss: 0.0511\n",
      "Epoch [4/5], Step [3104/10336], Loss: 2.3756\n",
      "Epoch [4/5], Step [3106/10336], Loss: 2.0532\n",
      "Epoch [4/5], Step [3108/10336], Loss: 0.2869\n",
      "Epoch [4/5], Step [3110/10336], Loss: 0.1283\n",
      "Epoch [4/5], Step [3112/10336], Loss: 0.1895\n",
      "Epoch [4/5], Step [3114/10336], Loss: 0.1720\n",
      "Epoch [4/5], Step [3116/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [3118/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [3120/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [3122/10336], Loss: 0.4065\n",
      "Epoch [4/5], Step [3124/10336], Loss: 0.1789\n",
      "Epoch [4/5], Step [3126/10336], Loss: 3.6137\n",
      "Epoch [4/5], Step [3128/10336], Loss: 0.0226\n",
      "Epoch [4/5], Step [3130/10336], Loss: 1.1545\n",
      "Epoch [4/5], Step [3132/10336], Loss: 0.7909\n",
      "Epoch [4/5], Step [3134/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [3136/10336], Loss: 1.0037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [3138/10336], Loss: 3.8022\n",
      "Epoch [4/5], Step [3140/10336], Loss: 0.0429\n",
      "Epoch [4/5], Step [3142/10336], Loss: 1.7620\n",
      "Epoch [4/5], Step [3144/10336], Loss: 0.4302\n",
      "Epoch [4/5], Step [3146/10336], Loss: 0.7294\n",
      "Epoch [4/5], Step [3148/10336], Loss: 0.3198\n",
      "Epoch [4/5], Step [3150/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [3152/10336], Loss: 0.0254\n",
      "Epoch [4/5], Step [3154/10336], Loss: 0.5326\n",
      "Epoch [4/5], Step [3156/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [3158/10336], Loss: 0.7170\n",
      "Epoch [4/5], Step [3160/10336], Loss: 0.0158\n",
      "Epoch [4/5], Step [3162/10336], Loss: 1.9851\n",
      "Epoch [4/5], Step [3164/10336], Loss: 0.1464\n",
      "Epoch [4/5], Step [3166/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [3168/10336], Loss: 0.1207\n",
      "Epoch [4/5], Step [3170/10336], Loss: 0.5525\n",
      "Epoch [4/5], Step [3172/10336], Loss: 0.2410\n",
      "Epoch [4/5], Step [3174/10336], Loss: 0.2122\n",
      "Epoch [4/5], Step [3176/10336], Loss: 2.3145\n",
      "Epoch [4/5], Step [3178/10336], Loss: 0.1813\n",
      "Epoch [4/5], Step [3180/10336], Loss: 0.0585\n",
      "Epoch [4/5], Step [3182/10336], Loss: 0.5809\n",
      "Epoch [4/5], Step [3184/10336], Loss: 0.1684\n",
      "Epoch [4/5], Step [3186/10336], Loss: 0.2736\n",
      "Epoch [4/5], Step [3188/10336], Loss: 0.0896\n",
      "Epoch [4/5], Step [3190/10336], Loss: 0.1350\n",
      "Epoch [4/5], Step [3192/10336], Loss: 0.0923\n",
      "Epoch [4/5], Step [3194/10336], Loss: 0.0184\n",
      "Epoch [4/5], Step [3196/10336], Loss: 0.2868\n",
      "Epoch [4/5], Step [3198/10336], Loss: 0.0275\n",
      "Epoch [4/5], Step [3200/10336], Loss: 0.0064\n",
      "Epoch [4/5], Step [3202/10336], Loss: 0.5159\n",
      "Epoch [4/5], Step [3204/10336], Loss: 1.4392\n",
      "Epoch [4/5], Step [3206/10336], Loss: 0.7897\n",
      "Epoch [4/5], Step [3208/10336], Loss: 0.0511\n",
      "Epoch [4/5], Step [3210/10336], Loss: 0.1894\n",
      "Epoch [4/5], Step [3212/10336], Loss: 0.7819\n",
      "Epoch [4/5], Step [3214/10336], Loss: 3.0314\n",
      "Epoch [4/5], Step [3216/10336], Loss: 0.0814\n",
      "Epoch [4/5], Step [3218/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [3220/10336], Loss: 0.0317\n",
      "Epoch [4/5], Step [3222/10336], Loss: 0.1120\n",
      "Epoch [4/5], Step [3224/10336], Loss: 0.4586\n",
      "Epoch [4/5], Step [3226/10336], Loss: 1.0289\n",
      "Epoch [4/5], Step [3228/10336], Loss: 5.2119\n",
      "Epoch [4/5], Step [3230/10336], Loss: 0.1803\n",
      "Epoch [4/5], Step [3232/10336], Loss: 0.3047\n",
      "Epoch [4/5], Step [3234/10336], Loss: 0.3289\n",
      "Epoch [4/5], Step [3236/10336], Loss: 0.2232\n",
      "Epoch [4/5], Step [3238/10336], Loss: 1.6599\n",
      "Epoch [4/5], Step [3240/10336], Loss: 1.7394\n",
      "Epoch [4/5], Step [3242/10336], Loss: 0.5469\n",
      "Epoch [4/5], Step [3244/10336], Loss: 0.5215\n",
      "Epoch [4/5], Step [3246/10336], Loss: 0.0319\n",
      "Epoch [4/5], Step [3248/10336], Loss: 4.9486\n",
      "Epoch [4/5], Step [3250/10336], Loss: 0.5378\n",
      "Epoch [4/5], Step [3252/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [3254/10336], Loss: 0.0659\n",
      "Epoch [4/5], Step [3256/10336], Loss: 0.3132\n",
      "Epoch [4/5], Step [3258/10336], Loss: 0.9102\n",
      "Epoch [4/5], Step [3260/10336], Loss: 0.2022\n",
      "Epoch [4/5], Step [3262/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [3264/10336], Loss: 0.0808\n",
      "Epoch [4/5], Step [3266/10336], Loss: 0.3861\n",
      "Epoch [4/5], Step [3268/10336], Loss: 0.3826\n",
      "Epoch [4/5], Step [3270/10336], Loss: 0.0548\n",
      "Epoch [4/5], Step [3272/10336], Loss: 1.6117\n",
      "Epoch [4/5], Step [3274/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [3276/10336], Loss: 0.0856\n",
      "Epoch [4/5], Step [3278/10336], Loss: 0.1602\n",
      "Epoch [4/5], Step [3280/10336], Loss: 0.1166\n",
      "Epoch [4/5], Step [3282/10336], Loss: 0.2940\n",
      "Epoch [4/5], Step [3284/10336], Loss: 0.0387\n",
      "Epoch [4/5], Step [3286/10336], Loss: 0.1281\n",
      "Epoch [4/5], Step [3288/10336], Loss: 0.0713\n",
      "Epoch [4/5], Step [3290/10336], Loss: 0.4465\n",
      "Epoch [4/5], Step [3292/10336], Loss: 0.3464\n",
      "Epoch [4/5], Step [3294/10336], Loss: 0.3895\n",
      "Epoch [4/5], Step [3296/10336], Loss: 0.1155\n",
      "Epoch [4/5], Step [3298/10336], Loss: 0.0683\n",
      "Epoch [4/5], Step [3300/10336], Loss: 0.0594\n",
      "Epoch [4/5], Step [3302/10336], Loss: 0.6083\n",
      "Epoch [4/5], Step [3304/10336], Loss: 0.0080\n",
      "Epoch [4/5], Step [3306/10336], Loss: 0.0052\n",
      "Epoch [4/5], Step [3308/10336], Loss: 0.0298\n",
      "Epoch [4/5], Step [3310/10336], Loss: 2.9707\n",
      "Epoch [4/5], Step [3312/10336], Loss: 1.2173\n",
      "Epoch [4/5], Step [3314/10336], Loss: 0.1224\n",
      "Epoch [4/5], Step [3316/10336], Loss: 0.0339\n",
      "Epoch [4/5], Step [3318/10336], Loss: 0.3080\n",
      "Epoch [4/5], Step [3320/10336], Loss: 0.2802\n",
      "Epoch [4/5], Step [3322/10336], Loss: 0.0308\n",
      "Epoch [4/5], Step [3324/10336], Loss: 0.2307\n",
      "Epoch [4/5], Step [3326/10336], Loss: 0.4586\n",
      "Epoch [4/5], Step [3328/10336], Loss: 0.0101\n",
      "Epoch [4/5], Step [3330/10336], Loss: 0.8944\n",
      "Epoch [4/5], Step [3332/10336], Loss: 0.7537\n",
      "Epoch [4/5], Step [3334/10336], Loss: 0.5762\n",
      "Epoch [4/5], Step [3336/10336], Loss: 0.1315\n",
      "Epoch [4/5], Step [3338/10336], Loss: 0.6445\n",
      "Epoch [4/5], Step [3340/10336], Loss: 0.0390\n",
      "Epoch [4/5], Step [3342/10336], Loss: 0.6236\n",
      "Epoch [4/5], Step [3344/10336], Loss: 0.0846\n",
      "Epoch [4/5], Step [3346/10336], Loss: 1.5818\n",
      "Epoch [4/5], Step [3348/10336], Loss: 4.1771\n",
      "Epoch [4/5], Step [3350/10336], Loss: 4.2148\n",
      "Epoch [4/5], Step [3352/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [3354/10336], Loss: 0.9010\n",
      "Epoch [4/5], Step [3356/10336], Loss: 0.3502\n",
      "Epoch [4/5], Step [3358/10336], Loss: 0.0862\n",
      "Epoch [4/5], Step [3360/10336], Loss: 0.3111\n",
      "Epoch [4/5], Step [3362/10336], Loss: 0.0378\n",
      "Epoch [4/5], Step [3364/10336], Loss: 0.4700\n",
      "Epoch [4/5], Step [3366/10336], Loss: 3.7089\n",
      "Epoch [4/5], Step [3368/10336], Loss: 0.8219\n",
      "Epoch [4/5], Step [3370/10336], Loss: 2.1853\n",
      "Epoch [4/5], Step [3372/10336], Loss: 5.2495\n",
      "Epoch [4/5], Step [3374/10336], Loss: 0.0052\n",
      "Epoch [4/5], Step [3376/10336], Loss: 0.0310\n",
      "Epoch [4/5], Step [3378/10336], Loss: 0.0027\n",
      "Epoch [4/5], Step [3380/10336], Loss: 0.4388\n",
      "Epoch [4/5], Step [3382/10336], Loss: 0.3160\n",
      "Epoch [4/5], Step [3384/10336], Loss: 0.1836\n",
      "Epoch [4/5], Step [3386/10336], Loss: 0.5626\n",
      "Epoch [4/5], Step [3388/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [3390/10336], Loss: 4.0494\n",
      "Epoch [4/5], Step [3392/10336], Loss: 0.2063\n",
      "Epoch [4/5], Step [3394/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [3396/10336], Loss: 0.0199\n",
      "Epoch [4/5], Step [3398/10336], Loss: 1.2885\n",
      "Epoch [4/5], Step [3400/10336], Loss: 0.4325\n",
      "Epoch [4/5], Step [3402/10336], Loss: 0.0475\n",
      "Epoch [4/5], Step [3404/10336], Loss: 0.5406\n",
      "Epoch [4/5], Step [3406/10336], Loss: 0.6802\n",
      "Epoch [4/5], Step [3408/10336], Loss: 3.4858\n",
      "Epoch [4/5], Step [3410/10336], Loss: 0.2415\n",
      "Epoch [4/5], Step [3412/10336], Loss: 0.4391\n",
      "Epoch [4/5], Step [3414/10336], Loss: 0.0595\n",
      "Epoch [4/5], Step [3416/10336], Loss: 0.0800\n",
      "Epoch [4/5], Step [3418/10336], Loss: 0.3229\n",
      "Epoch [4/5], Step [3420/10336], Loss: 1.0835\n",
      "Epoch [4/5], Step [3422/10336], Loss: 0.2957\n",
      "Epoch [4/5], Step [3424/10336], Loss: 0.3052\n",
      "Epoch [4/5], Step [3426/10336], Loss: 0.2159\n",
      "Epoch [4/5], Step [3428/10336], Loss: 0.3772\n",
      "Epoch [4/5], Step [3430/10336], Loss: 0.8379\n",
      "Epoch [4/5], Step [3432/10336], Loss: 0.0153\n",
      "Epoch [4/5], Step [3434/10336], Loss: 0.1237\n",
      "Epoch [4/5], Step [3436/10336], Loss: 0.5682\n",
      "Epoch [4/5], Step [3438/10336], Loss: 0.0493\n",
      "Epoch [4/5], Step [3440/10336], Loss: 2.2095\n",
      "Epoch [4/5], Step [3442/10336], Loss: 0.6968\n",
      "Epoch [4/5], Step [3444/10336], Loss: 3.3697\n",
      "Epoch [4/5], Step [3446/10336], Loss: 0.2846\n",
      "Epoch [4/5], Step [3448/10336], Loss: 0.3942\n",
      "Epoch [4/5], Step [3450/10336], Loss: 1.1758\n",
      "Epoch [4/5], Step [3452/10336], Loss: 2.6922\n",
      "Epoch [4/5], Step [3454/10336], Loss: 0.3443\n",
      "Epoch [4/5], Step [3456/10336], Loss: 0.1245\n",
      "Epoch [4/5], Step [3458/10336], Loss: 0.5045\n",
      "Epoch [4/5], Step [3460/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [3462/10336], Loss: 1.4636\n",
      "Epoch [4/5], Step [3464/10336], Loss: 2.2316\n",
      "Epoch [4/5], Step [3466/10336], Loss: 0.5421\n",
      "Epoch [4/5], Step [3468/10336], Loss: 1.2422\n",
      "Epoch [4/5], Step [3470/10336], Loss: 1.5281\n",
      "Epoch [4/5], Step [3472/10336], Loss: 0.1947\n",
      "Epoch [4/5], Step [3474/10336], Loss: 0.6911\n",
      "Epoch [4/5], Step [3476/10336], Loss: 0.2187\n",
      "Epoch [4/5], Step [3478/10336], Loss: 3.8483\n",
      "Epoch [4/5], Step [3480/10336], Loss: 0.3707\n",
      "Epoch [4/5], Step [3482/10336], Loss: 0.3752\n",
      "Epoch [4/5], Step [3484/10336], Loss: 0.0258\n",
      "Epoch [4/5], Step [3486/10336], Loss: 0.0921\n",
      "Epoch [4/5], Step [3488/10336], Loss: 0.0880\n",
      "Epoch [4/5], Step [3490/10336], Loss: 0.5300\n",
      "Epoch [4/5], Step [3492/10336], Loss: 0.2465\n",
      "Epoch [4/5], Step [3494/10336], Loss: 0.0052\n",
      "Epoch [4/5], Step [3496/10336], Loss: 0.0440\n",
      "Epoch [4/5], Step [3498/10336], Loss: 0.3215\n",
      "Epoch [4/5], Step [3500/10336], Loss: 0.0516\n",
      "Epoch [4/5], Step [3502/10336], Loss: 0.1040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [3504/10336], Loss: 0.0657\n",
      "Epoch [4/5], Step [3506/10336], Loss: 0.0091\n",
      "Epoch [4/5], Step [3508/10336], Loss: 0.0635\n",
      "Epoch [4/5], Step [3510/10336], Loss: 1.4681\n",
      "Epoch [4/5], Step [3512/10336], Loss: 0.9329\n",
      "Epoch [4/5], Step [3514/10336], Loss: 0.0639\n",
      "Epoch [4/5], Step [3516/10336], Loss: 0.0103\n",
      "Epoch [4/5], Step [3518/10336], Loss: 1.0055\n",
      "Epoch [4/5], Step [3520/10336], Loss: 0.0288\n",
      "Epoch [4/5], Step [3522/10336], Loss: 0.2144\n",
      "Epoch [4/5], Step [3524/10336], Loss: 0.3447\n",
      "Epoch [4/5], Step [3526/10336], Loss: 0.1489\n",
      "Epoch [4/5], Step [3528/10336], Loss: 0.1584\n",
      "Epoch [4/5], Step [3530/10336], Loss: 0.2839\n",
      "Epoch [4/5], Step [3532/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [3534/10336], Loss: 0.4043\n",
      "Epoch [4/5], Step [3536/10336], Loss: 0.6517\n",
      "Epoch [4/5], Step [3538/10336], Loss: 0.3078\n",
      "Epoch [4/5], Step [3540/10336], Loss: 0.7647\n",
      "Epoch [4/5], Step [3542/10336], Loss: 0.4473\n",
      "Epoch [4/5], Step [3544/10336], Loss: 0.0454\n",
      "Epoch [4/5], Step [3546/10336], Loss: 0.5811\n",
      "Epoch [4/5], Step [3548/10336], Loss: 0.2019\n",
      "Epoch [4/5], Step [3550/10336], Loss: 0.0091\n",
      "Epoch [4/5], Step [3552/10336], Loss: 0.0845\n",
      "Epoch [4/5], Step [3554/10336], Loss: 1.5883\n",
      "Epoch [4/5], Step [3556/10336], Loss: 0.0478\n",
      "Epoch [4/5], Step [3558/10336], Loss: 0.0557\n",
      "Epoch [4/5], Step [3560/10336], Loss: 0.3215\n",
      "Epoch [4/5], Step [3562/10336], Loss: 0.7756\n",
      "Epoch [4/5], Step [3564/10336], Loss: 1.0926\n",
      "Epoch [4/5], Step [3566/10336], Loss: 0.1387\n",
      "Epoch [4/5], Step [3568/10336], Loss: 1.1516\n",
      "Epoch [4/5], Step [3570/10336], Loss: 3.2007\n",
      "Epoch [4/5], Step [3572/10336], Loss: 2.9049\n",
      "Epoch [4/5], Step [3574/10336], Loss: 2.1074\n",
      "Epoch [4/5], Step [3576/10336], Loss: 0.1945\n",
      "Epoch [4/5], Step [3578/10336], Loss: 0.3315\n",
      "Epoch [4/5], Step [3580/10336], Loss: 1.0831\n",
      "Epoch [4/5], Step [3582/10336], Loss: 0.3614\n",
      "Epoch [4/5], Step [3584/10336], Loss: 0.4799\n",
      "Epoch [4/5], Step [3586/10336], Loss: 1.4469\n",
      "Epoch [4/5], Step [3588/10336], Loss: 0.2684\n",
      "Epoch [4/5], Step [3590/10336], Loss: 0.0359\n",
      "Epoch [4/5], Step [3592/10336], Loss: 2.8147\n",
      "Epoch [4/5], Step [3594/10336], Loss: 0.3405\n",
      "Epoch [4/5], Step [3596/10336], Loss: 0.1371\n",
      "Epoch [4/5], Step [3598/10336], Loss: 0.0450\n",
      "Epoch [4/5], Step [3600/10336], Loss: 0.6385\n",
      "Epoch [4/5], Step [3602/10336], Loss: 2.2960\n",
      "Epoch [4/5], Step [3604/10336], Loss: 2.8017\n",
      "Epoch [4/5], Step [3606/10336], Loss: 0.5693\n",
      "Epoch [4/5], Step [3608/10336], Loss: 0.0497\n",
      "Epoch [4/5], Step [3610/10336], Loss: 0.1156\n",
      "Epoch [4/5], Step [3612/10336], Loss: 0.0651\n",
      "Epoch [4/5], Step [3614/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [3616/10336], Loss: 0.1405\n",
      "Epoch [4/5], Step [3618/10336], Loss: 0.0173\n",
      "Epoch [4/5], Step [3620/10336], Loss: 0.0699\n",
      "Epoch [4/5], Step [3622/10336], Loss: 0.0757\n",
      "Epoch [4/5], Step [3624/10336], Loss: 0.3329\n",
      "Epoch [4/5], Step [3626/10336], Loss: 0.6644\n",
      "Epoch [4/5], Step [3628/10336], Loss: 0.1427\n",
      "Epoch [4/5], Step [3630/10336], Loss: 0.0270\n",
      "Epoch [4/5], Step [3632/10336], Loss: 0.0060\n",
      "Epoch [4/5], Step [3634/10336], Loss: 0.2381\n",
      "Epoch [4/5], Step [3636/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [3638/10336], Loss: 3.5192\n",
      "Epoch [4/5], Step [3640/10336], Loss: 0.9983\n",
      "Epoch [4/5], Step [3642/10336], Loss: 0.1749\n",
      "Epoch [4/5], Step [3644/10336], Loss: 0.2678\n",
      "Epoch [4/5], Step [3646/10336], Loss: 0.5267\n",
      "Epoch [4/5], Step [3648/10336], Loss: 0.4942\n",
      "Epoch [4/5], Step [3650/10336], Loss: 1.0969\n",
      "Epoch [4/5], Step [3652/10336], Loss: 0.1681\n",
      "Epoch [4/5], Step [3654/10336], Loss: 0.0209\n",
      "Epoch [4/5], Step [3656/10336], Loss: 0.8415\n",
      "Epoch [4/5], Step [3658/10336], Loss: 0.0074\n",
      "Epoch [4/5], Step [3660/10336], Loss: 0.0432\n",
      "Epoch [4/5], Step [3662/10336], Loss: 0.0914\n",
      "Epoch [4/5], Step [3664/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [3666/10336], Loss: 2.7839\n",
      "Epoch [4/5], Step [3668/10336], Loss: 0.0949\n",
      "Epoch [4/5], Step [3670/10336], Loss: 0.0465\n",
      "Epoch [4/5], Step [3672/10336], Loss: 0.7129\n",
      "Epoch [4/5], Step [3674/10336], Loss: 0.5843\n",
      "Epoch [4/5], Step [3676/10336], Loss: 0.0552\n",
      "Epoch [4/5], Step [3678/10336], Loss: 1.9915\n",
      "Epoch [4/5], Step [3680/10336], Loss: 0.1328\n",
      "Epoch [4/5], Step [3682/10336], Loss: 0.1151\n",
      "Epoch [4/5], Step [3684/10336], Loss: 0.7730\n",
      "Epoch [4/5], Step [3686/10336], Loss: 3.4303\n",
      "Epoch [4/5], Step [3688/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [3690/10336], Loss: 5.1216\n",
      "Epoch [4/5], Step [3692/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [3694/10336], Loss: 0.0089\n",
      "Epoch [4/5], Step [3696/10336], Loss: 0.2123\n",
      "Epoch [4/5], Step [3698/10336], Loss: 0.7310\n",
      "Epoch [4/5], Step [3700/10336], Loss: 0.5081\n",
      "Epoch [4/5], Step [3702/10336], Loss: 0.0094\n",
      "Epoch [4/5], Step [3704/10336], Loss: 0.3310\n",
      "Epoch [4/5], Step [3706/10336], Loss: 0.9499\n",
      "Epoch [4/5], Step [3708/10336], Loss: 0.7688\n",
      "Epoch [4/5], Step [3710/10336], Loss: 0.1382\n",
      "Epoch [4/5], Step [3712/10336], Loss: 0.0489\n",
      "Epoch [4/5], Step [3714/10336], Loss: 2.5292\n",
      "Epoch [4/5], Step [3716/10336], Loss: 0.0712\n",
      "Epoch [4/5], Step [3718/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [3720/10336], Loss: 0.4285\n",
      "Epoch [4/5], Step [3722/10336], Loss: 0.3078\n",
      "Epoch [4/5], Step [3724/10336], Loss: 0.6965\n",
      "Epoch [4/5], Step [3726/10336], Loss: 0.7949\n",
      "Epoch [4/5], Step [3728/10336], Loss: 0.0727\n",
      "Epoch [4/5], Step [3730/10336], Loss: 0.5639\n",
      "Epoch [4/5], Step [3732/10336], Loss: 0.0353\n",
      "Epoch [4/5], Step [3734/10336], Loss: 0.1703\n",
      "Epoch [4/5], Step [3736/10336], Loss: 0.1347\n",
      "Epoch [4/5], Step [3738/10336], Loss: 0.0959\n",
      "Epoch [4/5], Step [3740/10336], Loss: 0.1604\n",
      "Epoch [4/5], Step [3742/10336], Loss: 0.2256\n",
      "Epoch [4/5], Step [3744/10336], Loss: 0.1160\n",
      "Epoch [4/5], Step [3746/10336], Loss: 0.0520\n",
      "Epoch [4/5], Step [3748/10336], Loss: 0.1018\n",
      "Epoch [4/5], Step [3750/10336], Loss: 0.0056\n",
      "Epoch [4/5], Step [3752/10336], Loss: 0.3169\n",
      "Epoch [4/5], Step [3754/10336], Loss: 0.0489\n",
      "Epoch [4/5], Step [3756/10336], Loss: 0.2151\n",
      "Epoch [4/5], Step [3758/10336], Loss: 0.1580\n",
      "Epoch [4/5], Step [3760/10336], Loss: 0.1209\n",
      "Epoch [4/5], Step [3762/10336], Loss: 0.0261\n",
      "Epoch [4/5], Step [3764/10336], Loss: 0.6323\n",
      "Epoch [4/5], Step [3766/10336], Loss: 0.0570\n",
      "Epoch [4/5], Step [3768/10336], Loss: 0.1702\n",
      "Epoch [4/5], Step [3770/10336], Loss: 0.1778\n",
      "Epoch [4/5], Step [3772/10336], Loss: 0.2868\n",
      "Epoch [4/5], Step [3774/10336], Loss: 0.0474\n",
      "Epoch [4/5], Step [3776/10336], Loss: 0.1126\n",
      "Epoch [4/5], Step [3778/10336], Loss: 2.9712\n",
      "Epoch [4/5], Step [3780/10336], Loss: 0.2150\n",
      "Epoch [4/5], Step [3782/10336], Loss: 0.1265\n",
      "Epoch [4/5], Step [3784/10336], Loss: 1.3379\n",
      "Epoch [4/5], Step [3786/10336], Loss: 0.3157\n",
      "Epoch [4/5], Step [3788/10336], Loss: 1.5654\n",
      "Epoch [4/5], Step [3790/10336], Loss: 0.0666\n",
      "Epoch [4/5], Step [3792/10336], Loss: 0.0329\n",
      "Epoch [4/5], Step [3794/10336], Loss: 0.0513\n",
      "Epoch [4/5], Step [3796/10336], Loss: 0.4454\n",
      "Epoch [4/5], Step [3798/10336], Loss: 0.1263\n",
      "Epoch [4/5], Step [3800/10336], Loss: 0.1071\n",
      "Epoch [4/5], Step [3802/10336], Loss: 1.2711\n",
      "Epoch [4/5], Step [3804/10336], Loss: 0.0802\n",
      "Epoch [4/5], Step [3806/10336], Loss: 0.3432\n",
      "Epoch [4/5], Step [3808/10336], Loss: 0.0143\n",
      "Epoch [4/5], Step [3810/10336], Loss: 0.0149\n",
      "Epoch [4/5], Step [3812/10336], Loss: 0.0380\n",
      "Epoch [4/5], Step [3814/10336], Loss: 0.0125\n",
      "Epoch [4/5], Step [3816/10336], Loss: 0.0737\n",
      "Epoch [4/5], Step [3818/10336], Loss: 0.1528\n",
      "Epoch [4/5], Step [3820/10336], Loss: 1.5706\n",
      "Epoch [4/5], Step [3822/10336], Loss: 0.1696\n",
      "Epoch [4/5], Step [3824/10336], Loss: 1.5870\n",
      "Epoch [4/5], Step [3826/10336], Loss: 0.0240\n",
      "Epoch [4/5], Step [3828/10336], Loss: 0.1363\n",
      "Epoch [4/5], Step [3830/10336], Loss: 0.3482\n",
      "Epoch [4/5], Step [3832/10336], Loss: 0.4525\n",
      "Epoch [4/5], Step [3834/10336], Loss: 0.0851\n",
      "Epoch [4/5], Step [3836/10336], Loss: 0.6709\n",
      "Epoch [4/5], Step [3838/10336], Loss: 0.3995\n",
      "Epoch [4/5], Step [3840/10336], Loss: 0.7458\n",
      "Epoch [4/5], Step [3842/10336], Loss: 0.0454\n",
      "Epoch [4/5], Step [3844/10336], Loss: 0.3094\n",
      "Epoch [4/5], Step [3846/10336], Loss: 0.0222\n",
      "Epoch [4/5], Step [3848/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [3850/10336], Loss: 1.0384\n",
      "Epoch [4/5], Step [3852/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [3854/10336], Loss: 0.2866\n",
      "Epoch [4/5], Step [3856/10336], Loss: 0.1638\n",
      "Epoch [4/5], Step [3858/10336], Loss: 1.1575\n",
      "Epoch [4/5], Step [3860/10336], Loss: 3.1430\n",
      "Epoch [4/5], Step [3862/10336], Loss: 0.3321\n",
      "Epoch [4/5], Step [3864/10336], Loss: 0.0877\n",
      "Epoch [4/5], Step [3866/10336], Loss: 1.5393\n",
      "Epoch [4/5], Step [3868/10336], Loss: 2.9734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [3870/10336], Loss: 0.2986\n",
      "Epoch [4/5], Step [3872/10336], Loss: 0.0078\n",
      "Epoch [4/5], Step [3874/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [3876/10336], Loss: 0.0244\n",
      "Epoch [4/5], Step [3878/10336], Loss: 0.3028\n",
      "Epoch [4/5], Step [3880/10336], Loss: 0.2446\n",
      "Epoch [4/5], Step [3882/10336], Loss: 0.0641\n",
      "Epoch [4/5], Step [3884/10336], Loss: 0.0110\n",
      "Epoch [4/5], Step [3886/10336], Loss: 1.5745\n",
      "Epoch [4/5], Step [3888/10336], Loss: 0.3240\n",
      "Epoch [4/5], Step [3890/10336], Loss: 0.2113\n",
      "Epoch [4/5], Step [3892/10336], Loss: 1.7337\n",
      "Epoch [4/5], Step [3894/10336], Loss: 0.2971\n",
      "Epoch [4/5], Step [3896/10336], Loss: 0.5000\n",
      "Epoch [4/5], Step [3898/10336], Loss: 0.7258\n",
      "Epoch [4/5], Step [3900/10336], Loss: 0.1086\n",
      "Epoch [4/5], Step [3902/10336], Loss: 0.0891\n",
      "Epoch [4/5], Step [3904/10336], Loss: 0.2256\n",
      "Epoch [4/5], Step [3906/10336], Loss: 0.1624\n",
      "Epoch [4/5], Step [3908/10336], Loss: 0.0075\n",
      "Epoch [4/5], Step [3910/10336], Loss: 0.7063\n",
      "Epoch [4/5], Step [3912/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [3914/10336], Loss: 0.3851\n",
      "Epoch [4/5], Step [3916/10336], Loss: 0.0295\n",
      "Epoch [4/5], Step [3918/10336], Loss: 0.2747\n",
      "Epoch [4/5], Step [3920/10336], Loss: 0.3263\n",
      "Epoch [4/5], Step [3922/10336], Loss: 0.0420\n",
      "Epoch [4/5], Step [3924/10336], Loss: 0.4966\n",
      "Epoch [4/5], Step [3926/10336], Loss: 0.0613\n",
      "Epoch [4/5], Step [3928/10336], Loss: 2.1401\n",
      "Epoch [4/5], Step [3930/10336], Loss: 0.1494\n",
      "Epoch [4/5], Step [3932/10336], Loss: 0.4982\n",
      "Epoch [4/5], Step [3934/10336], Loss: 0.0583\n",
      "Epoch [4/5], Step [3936/10336], Loss: 3.0248\n",
      "Epoch [4/5], Step [3938/10336], Loss: 0.8539\n",
      "Epoch [4/5], Step [3940/10336], Loss: 0.0189\n",
      "Epoch [4/5], Step [3942/10336], Loss: 1.9072\n",
      "Epoch [4/5], Step [3944/10336], Loss: 0.0919\n",
      "Epoch [4/5], Step [3946/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [3948/10336], Loss: 2.2570\n",
      "Epoch [4/5], Step [3950/10336], Loss: 1.3108\n",
      "Epoch [4/5], Step [3952/10336], Loss: 6.6296\n",
      "Epoch [4/5], Step [3954/10336], Loss: 0.1735\n",
      "Epoch [4/5], Step [3956/10336], Loss: 0.0516\n",
      "Epoch [4/5], Step [3958/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [3960/10336], Loss: 0.1235\n",
      "Epoch [4/5], Step [3962/10336], Loss: 0.2242\n",
      "Epoch [4/5], Step [3964/10336], Loss: 0.0871\n",
      "Epoch [4/5], Step [3966/10336], Loss: 0.1047\n",
      "Epoch [4/5], Step [3968/10336], Loss: 0.0082\n",
      "Epoch [4/5], Step [3970/10336], Loss: 0.0333\n",
      "Epoch [4/5], Step [3972/10336], Loss: 0.1198\n",
      "Epoch [4/5], Step [3974/10336], Loss: 0.0103\n",
      "Epoch [4/5], Step [3976/10336], Loss: 0.8740\n",
      "Epoch [4/5], Step [3978/10336], Loss: 0.0668\n",
      "Epoch [4/5], Step [3980/10336], Loss: 0.2250\n",
      "Epoch [4/5], Step [3982/10336], Loss: 3.4410\n",
      "Epoch [4/5], Step [3984/10336], Loss: 0.0101\n",
      "Epoch [4/5], Step [3986/10336], Loss: 0.0904\n",
      "Epoch [4/5], Step [3988/10336], Loss: 0.0485\n",
      "Epoch [4/5], Step [3990/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [3992/10336], Loss: 0.4311\n",
      "Epoch [4/5], Step [3994/10336], Loss: 0.0431\n",
      "Epoch [4/5], Step [3996/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [3998/10336], Loss: 0.0697\n",
      "Epoch [4/5], Step [4000/10336], Loss: 0.0374\n",
      "Epoch [4/5], Step [4002/10336], Loss: 2.5942\n",
      "Epoch [4/5], Step [4004/10336], Loss: 0.7834\n",
      "Epoch [4/5], Step [4006/10336], Loss: 0.0210\n",
      "Epoch [4/5], Step [4008/10336], Loss: 1.9331\n",
      "Epoch [4/5], Step [4010/10336], Loss: 0.0741\n",
      "Epoch [4/5], Step [4012/10336], Loss: 0.0254\n",
      "Epoch [4/5], Step [4014/10336], Loss: 1.0314\n",
      "Epoch [4/5], Step [4016/10336], Loss: 0.0489\n",
      "Epoch [4/5], Step [4018/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [4020/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [4022/10336], Loss: 0.0341\n",
      "Epoch [4/5], Step [4024/10336], Loss: 0.3920\n",
      "Epoch [4/5], Step [4026/10336], Loss: 1.3201\n",
      "Epoch [4/5], Step [4028/10336], Loss: 0.4342\n",
      "Epoch [4/5], Step [4030/10336], Loss: 0.0032\n",
      "Epoch [4/5], Step [4032/10336], Loss: 0.4498\n",
      "Epoch [4/5], Step [4034/10336], Loss: 0.0323\n",
      "Epoch [4/5], Step [4036/10336], Loss: 0.1286\n",
      "Epoch [4/5], Step [4038/10336], Loss: 0.0102\n",
      "Epoch [4/5], Step [4040/10336], Loss: 1.4331\n",
      "Epoch [4/5], Step [4042/10336], Loss: 0.0161\n",
      "Epoch [4/5], Step [4044/10336], Loss: 0.0132\n",
      "Epoch [4/5], Step [4046/10336], Loss: 0.2237\n",
      "Epoch [4/5], Step [4048/10336], Loss: 0.6116\n",
      "Epoch [4/5], Step [4050/10336], Loss: 3.1128\n",
      "Epoch [4/5], Step [4052/10336], Loss: 0.0562\n",
      "Epoch [4/5], Step [4054/10336], Loss: 0.1026\n",
      "Epoch [4/5], Step [4056/10336], Loss: 0.1275\n",
      "Epoch [4/5], Step [4058/10336], Loss: 0.1348\n",
      "Epoch [4/5], Step [4060/10336], Loss: 0.1418\n",
      "Epoch [4/5], Step [4062/10336], Loss: 0.5063\n",
      "Epoch [4/5], Step [4064/10336], Loss: 0.6972\n",
      "Epoch [4/5], Step [4066/10336], Loss: 1.4558\n",
      "Epoch [4/5], Step [4068/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [4070/10336], Loss: 0.1615\n",
      "Epoch [4/5], Step [4072/10336], Loss: 0.1646\n",
      "Epoch [4/5], Step [4074/10336], Loss: 1.7573\n",
      "Epoch [4/5], Step [4076/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [4078/10336], Loss: 0.1403\n",
      "Epoch [4/5], Step [4080/10336], Loss: 0.4103\n",
      "Epoch [4/5], Step [4082/10336], Loss: 0.0137\n",
      "Epoch [4/5], Step [4084/10336], Loss: 0.0031\n",
      "Epoch [4/5], Step [4086/10336], Loss: 0.4765\n",
      "Epoch [4/5], Step [4088/10336], Loss: 0.0984\n",
      "Epoch [4/5], Step [4090/10336], Loss: 0.1600\n",
      "Epoch [4/5], Step [4092/10336], Loss: 0.1059\n",
      "Epoch [4/5], Step [4094/10336], Loss: 0.1205\n",
      "Epoch [4/5], Step [4096/10336], Loss: 1.5893\n",
      "Epoch [4/5], Step [4098/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [4100/10336], Loss: 1.4850\n",
      "Epoch [4/5], Step [4102/10336], Loss: 0.2121\n",
      "Epoch [4/5], Step [4104/10336], Loss: 2.4085\n",
      "Epoch [4/5], Step [4106/10336], Loss: 0.1737\n",
      "Epoch [4/5], Step [4108/10336], Loss: 0.4931\n",
      "Epoch [4/5], Step [4110/10336], Loss: 0.0391\n",
      "Epoch [4/5], Step [4112/10336], Loss: 0.2967\n",
      "Epoch [4/5], Step [4114/10336], Loss: 0.0293\n",
      "Epoch [4/5], Step [4116/10336], Loss: 0.0222\n",
      "Epoch [4/5], Step [4118/10336], Loss: 0.5617\n",
      "Epoch [4/5], Step [4120/10336], Loss: 1.7084\n",
      "Epoch [4/5], Step [4122/10336], Loss: 0.0459\n",
      "Epoch [4/5], Step [4124/10336], Loss: 0.0707\n",
      "Epoch [4/5], Step [4126/10336], Loss: 1.5574\n",
      "Epoch [4/5], Step [4128/10336], Loss: 0.2737\n",
      "Epoch [4/5], Step [4130/10336], Loss: 0.1661\n",
      "Epoch [4/5], Step [4132/10336], Loss: 3.7771\n",
      "Epoch [4/5], Step [4134/10336], Loss: 3.1719\n",
      "Epoch [4/5], Step [4136/10336], Loss: 0.2265\n",
      "Epoch [4/5], Step [4138/10336], Loss: 1.4309\n",
      "Epoch [4/5], Step [4140/10336], Loss: 0.1142\n",
      "Epoch [4/5], Step [4142/10336], Loss: 0.4691\n",
      "Epoch [4/5], Step [4144/10336], Loss: 0.0869\n",
      "Epoch [4/5], Step [4146/10336], Loss: 1.2158\n",
      "Epoch [4/5], Step [4148/10336], Loss: 0.4471\n",
      "Epoch [4/5], Step [4150/10336], Loss: 1.0409\n",
      "Epoch [4/5], Step [4152/10336], Loss: 2.9485\n",
      "Epoch [4/5], Step [4154/10336], Loss: 0.2853\n",
      "Epoch [4/5], Step [4156/10336], Loss: 2.4819\n",
      "Epoch [4/5], Step [4158/10336], Loss: 0.6756\n",
      "Epoch [4/5], Step [4160/10336], Loss: 6.1417\n",
      "Epoch [4/5], Step [4162/10336], Loss: 0.8147\n",
      "Epoch [4/5], Step [4164/10336], Loss: 0.2201\n",
      "Epoch [4/5], Step [4166/10336], Loss: 0.1421\n",
      "Epoch [4/5], Step [4168/10336], Loss: 0.9518\n",
      "Epoch [4/5], Step [4170/10336], Loss: 0.0043\n",
      "Epoch [4/5], Step [4172/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [4174/10336], Loss: 0.3697\n",
      "Epoch [4/5], Step [4176/10336], Loss: 0.1237\n",
      "Epoch [4/5], Step [4178/10336], Loss: 0.0162\n",
      "Epoch [4/5], Step [4180/10336], Loss: 0.1500\n",
      "Epoch [4/5], Step [4182/10336], Loss: 0.1547\n",
      "Epoch [4/5], Step [4184/10336], Loss: 0.2256\n",
      "Epoch [4/5], Step [4186/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [4188/10336], Loss: 0.1002\n",
      "Epoch [4/5], Step [4190/10336], Loss: 0.1009\n",
      "Epoch [4/5], Step [4192/10336], Loss: 0.3405\n",
      "Epoch [4/5], Step [4194/10336], Loss: 1.0363\n",
      "Epoch [4/5], Step [4196/10336], Loss: 0.0163\n",
      "Epoch [4/5], Step [4198/10336], Loss: 0.0264\n",
      "Epoch [4/5], Step [4200/10336], Loss: 3.7008\n",
      "Epoch [4/5], Step [4202/10336], Loss: 0.1956\n",
      "Epoch [4/5], Step [4204/10336], Loss: 1.6785\n",
      "Epoch [4/5], Step [4206/10336], Loss: 0.1337\n",
      "Epoch [4/5], Step [4208/10336], Loss: 3.4669\n",
      "Epoch [4/5], Step [4210/10336], Loss: 2.9810\n",
      "Epoch [4/5], Step [4212/10336], Loss: 0.0413\n",
      "Epoch [4/5], Step [4214/10336], Loss: 0.0035\n",
      "Epoch [4/5], Step [4216/10336], Loss: 1.2471\n",
      "Epoch [4/5], Step [4218/10336], Loss: 0.4778\n",
      "Epoch [4/5], Step [4220/10336], Loss: 0.2282\n",
      "Epoch [4/5], Step [4222/10336], Loss: 1.6409\n",
      "Epoch [4/5], Step [4224/10336], Loss: 0.0443\n",
      "Epoch [4/5], Step [4226/10336], Loss: 0.0179\n",
      "Epoch [4/5], Step [4228/10336], Loss: 0.0090\n",
      "Epoch [4/5], Step [4230/10336], Loss: 0.6868\n",
      "Epoch [4/5], Step [4232/10336], Loss: 0.3174\n",
      "Epoch [4/5], Step [4234/10336], Loss: 0.0170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [4236/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [4238/10336], Loss: 0.1492\n",
      "Epoch [4/5], Step [4240/10336], Loss: 0.2438\n",
      "Epoch [4/5], Step [4242/10336], Loss: 2.2006\n",
      "Epoch [4/5], Step [4244/10336], Loss: 0.3692\n",
      "Epoch [4/5], Step [4246/10336], Loss: 5.2918\n",
      "Epoch [4/5], Step [4248/10336], Loss: 1.9353\n",
      "Epoch [4/5], Step [4250/10336], Loss: 0.4399\n",
      "Epoch [4/5], Step [4252/10336], Loss: 0.1009\n",
      "Epoch [4/5], Step [4254/10336], Loss: 0.0946\n",
      "Epoch [4/5], Step [4256/10336], Loss: 0.0874\n",
      "Epoch [4/5], Step [4258/10336], Loss: 0.0172\n",
      "Epoch [4/5], Step [4260/10336], Loss: 0.0089\n",
      "Epoch [4/5], Step [4262/10336], Loss: 0.2815\n",
      "Epoch [4/5], Step [4264/10336], Loss: 0.1422\n",
      "Epoch [4/5], Step [4266/10336], Loss: 0.5903\n",
      "Epoch [4/5], Step [4268/10336], Loss: 0.0484\n",
      "Epoch [4/5], Step [4270/10336], Loss: 2.4475\n",
      "Epoch [4/5], Step [4272/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [4274/10336], Loss: 0.3752\n",
      "Epoch [4/5], Step [4276/10336], Loss: 0.0433\n",
      "Epoch [4/5], Step [4278/10336], Loss: 0.0142\n",
      "Epoch [4/5], Step [4280/10336], Loss: 1.3648\n",
      "Epoch [4/5], Step [4282/10336], Loss: 0.1665\n",
      "Epoch [4/5], Step [4284/10336], Loss: 0.6864\n",
      "Epoch [4/5], Step [4286/10336], Loss: 0.0336\n",
      "Epoch [4/5], Step [4288/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [4290/10336], Loss: 0.0074\n",
      "Epoch [4/5], Step [4292/10336], Loss: 0.6131\n",
      "Epoch [4/5], Step [4294/10336], Loss: 0.0300\n",
      "Epoch [4/5], Step [4296/10336], Loss: 0.9925\n",
      "Epoch [4/5], Step [4298/10336], Loss: 0.9457\n",
      "Epoch [4/5], Step [4300/10336], Loss: 1.8410\n",
      "Epoch [4/5], Step [4302/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [4304/10336], Loss: 0.2298\n",
      "Epoch [4/5], Step [4306/10336], Loss: 1.0290\n",
      "Epoch [4/5], Step [4308/10336], Loss: 2.3361\n",
      "Epoch [4/5], Step [4310/10336], Loss: 0.1389\n",
      "Epoch [4/5], Step [4312/10336], Loss: 1.0399\n",
      "Epoch [4/5], Step [4314/10336], Loss: 0.7071\n",
      "Epoch [4/5], Step [4316/10336], Loss: 1.2552\n",
      "Epoch [4/5], Step [4318/10336], Loss: 0.8402\n",
      "Epoch [4/5], Step [4320/10336], Loss: 0.6804\n",
      "Epoch [4/5], Step [4322/10336], Loss: 1.2445\n",
      "Epoch [4/5], Step [4324/10336], Loss: 0.5439\n",
      "Epoch [4/5], Step [4326/10336], Loss: 0.2211\n",
      "Epoch [4/5], Step [4328/10336], Loss: 0.3906\n",
      "Epoch [4/5], Step [4330/10336], Loss: 0.7351\n",
      "Epoch [4/5], Step [4332/10336], Loss: 0.2782\n",
      "Epoch [4/5], Step [4334/10336], Loss: 0.0578\n",
      "Epoch [4/5], Step [4336/10336], Loss: 0.1207\n",
      "Epoch [4/5], Step [4338/10336], Loss: 0.2266\n",
      "Epoch [4/5], Step [4340/10336], Loss: 0.1644\n",
      "Epoch [4/5], Step [4342/10336], Loss: 0.0031\n",
      "Epoch [4/5], Step [4344/10336], Loss: 0.1020\n",
      "Epoch [4/5], Step [4346/10336], Loss: 0.5590\n",
      "Epoch [4/5], Step [4348/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [4350/10336], Loss: 0.1423\n",
      "Epoch [4/5], Step [4352/10336], Loss: 0.0183\n",
      "Epoch [4/5], Step [4354/10336], Loss: 0.6099\n",
      "Epoch [4/5], Step [4356/10336], Loss: 0.3294\n",
      "Epoch [4/5], Step [4358/10336], Loss: 0.7880\n",
      "Epoch [4/5], Step [4360/10336], Loss: 0.0370\n",
      "Epoch [4/5], Step [4362/10336], Loss: 0.2837\n",
      "Epoch [4/5], Step [4364/10336], Loss: 0.1814\n",
      "Epoch [4/5], Step [4366/10336], Loss: 0.1892\n",
      "Epoch [4/5], Step [4368/10336], Loss: 0.3587\n",
      "Epoch [4/5], Step [4370/10336], Loss: 2.3354\n",
      "Epoch [4/5], Step [4372/10336], Loss: 0.1755\n",
      "Epoch [4/5], Step [4374/10336], Loss: 0.3075\n",
      "Epoch [4/5], Step [4376/10336], Loss: 1.1269\n",
      "Epoch [4/5], Step [4378/10336], Loss: 2.7854\n",
      "Epoch [4/5], Step [4380/10336], Loss: 0.2244\n",
      "Epoch [4/5], Step [4382/10336], Loss: 3.6096\n",
      "Epoch [4/5], Step [4384/10336], Loss: 0.0425\n",
      "Epoch [4/5], Step [4386/10336], Loss: 0.0270\n",
      "Epoch [4/5], Step [4388/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [4390/10336], Loss: 1.2998\n",
      "Epoch [4/5], Step [4392/10336], Loss: 0.0154\n",
      "Epoch [4/5], Step [4394/10336], Loss: 0.7603\n",
      "Epoch [4/5], Step [4396/10336], Loss: 0.5781\n",
      "Epoch [4/5], Step [4398/10336], Loss: 0.0185\n",
      "Epoch [4/5], Step [4400/10336], Loss: 0.0182\n",
      "Epoch [4/5], Step [4402/10336], Loss: 0.0987\n",
      "Epoch [4/5], Step [4404/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [4406/10336], Loss: 0.5243\n",
      "Epoch [4/5], Step [4408/10336], Loss: 0.1996\n",
      "Epoch [4/5], Step [4410/10336], Loss: 0.6553\n",
      "Epoch [4/5], Step [4412/10336], Loss: 0.4294\n",
      "Epoch [4/5], Step [4414/10336], Loss: 0.0095\n",
      "Epoch [4/5], Step [4416/10336], Loss: 0.0397\n",
      "Epoch [4/5], Step [4418/10336], Loss: 4.1911\n",
      "Epoch [4/5], Step [4420/10336], Loss: 0.0115\n",
      "Epoch [4/5], Step [4422/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [4424/10336], Loss: 0.0577\n",
      "Epoch [4/5], Step [4426/10336], Loss: 0.1839\n",
      "Epoch [4/5], Step [4428/10336], Loss: 0.2019\n",
      "Epoch [4/5], Step [4430/10336], Loss: 0.0418\n",
      "Epoch [4/5], Step [4432/10336], Loss: 2.4089\n",
      "Epoch [4/5], Step [4434/10336], Loss: 2.8927\n",
      "Epoch [4/5], Step [4436/10336], Loss: 2.7831\n",
      "Epoch [4/5], Step [4438/10336], Loss: 0.0395\n",
      "Epoch [4/5], Step [4440/10336], Loss: 0.5234\n",
      "Epoch [4/5], Step [4442/10336], Loss: 2.1545\n",
      "Epoch [4/5], Step [4444/10336], Loss: 0.7936\n",
      "Epoch [4/5], Step [4446/10336], Loss: 0.1701\n",
      "Epoch [4/5], Step [4448/10336], Loss: 0.0265\n",
      "Epoch [4/5], Step [4450/10336], Loss: 0.1942\n",
      "Epoch [4/5], Step [4452/10336], Loss: 0.1732\n",
      "Epoch [4/5], Step [4454/10336], Loss: 0.0688\n",
      "Epoch [4/5], Step [4456/10336], Loss: 0.3009\n",
      "Epoch [4/5], Step [4458/10336], Loss: 0.2109\n",
      "Epoch [4/5], Step [4460/10336], Loss: 2.4182\n",
      "Epoch [4/5], Step [4462/10336], Loss: 0.7660\n",
      "Epoch [4/5], Step [4464/10336], Loss: 0.7666\n",
      "Epoch [4/5], Step [4466/10336], Loss: 0.8123\n",
      "Epoch [4/5], Step [4468/10336], Loss: 0.4572\n",
      "Epoch [4/5], Step [4470/10336], Loss: 0.1118\n",
      "Epoch [4/5], Step [4472/10336], Loss: 0.1311\n",
      "Epoch [4/5], Step [4474/10336], Loss: 0.2767\n",
      "Epoch [4/5], Step [4476/10336], Loss: 1.9281\n",
      "Epoch [4/5], Step [4478/10336], Loss: 0.2229\n",
      "Epoch [4/5], Step [4480/10336], Loss: 0.6604\n",
      "Epoch [4/5], Step [4482/10336], Loss: 3.4354\n",
      "Epoch [4/5], Step [4484/10336], Loss: 0.0176\n",
      "Epoch [4/5], Step [4486/10336], Loss: 0.6306\n",
      "Epoch [4/5], Step [4488/10336], Loss: 0.0095\n",
      "Epoch [4/5], Step [4490/10336], Loss: 0.3046\n",
      "Epoch [4/5], Step [4492/10336], Loss: 1.4704\n",
      "Epoch [4/5], Step [4494/10336], Loss: 0.0455\n",
      "Epoch [4/5], Step [4496/10336], Loss: 0.2240\n",
      "Epoch [4/5], Step [4498/10336], Loss: 0.0569\n",
      "Epoch [4/5], Step [4500/10336], Loss: 0.1279\n",
      "Epoch [4/5], Step [4502/10336], Loss: 0.2662\n",
      "Epoch [4/5], Step [4504/10336], Loss: 0.1025\n",
      "Epoch [4/5], Step [4506/10336], Loss: 0.1405\n",
      "Epoch [4/5], Step [4508/10336], Loss: 0.5139\n",
      "Epoch [4/5], Step [4510/10336], Loss: 0.2483\n",
      "Epoch [4/5], Step [4512/10336], Loss: 0.0986\n",
      "Epoch [4/5], Step [4514/10336], Loss: 0.7694\n",
      "Epoch [4/5], Step [4516/10336], Loss: 0.0622\n",
      "Epoch [4/5], Step [4518/10336], Loss: 0.0543\n",
      "Epoch [4/5], Step [4520/10336], Loss: 0.0175\n",
      "Epoch [4/5], Step [4522/10336], Loss: 0.0089\n",
      "Epoch [4/5], Step [4524/10336], Loss: 0.2125\n",
      "Epoch [4/5], Step [4526/10336], Loss: 0.2254\n",
      "Epoch [4/5], Step [4528/10336], Loss: 2.3466\n",
      "Epoch [4/5], Step [4530/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [4532/10336], Loss: 0.9232\n",
      "Epoch [4/5], Step [4534/10336], Loss: 0.0543\n",
      "Epoch [4/5], Step [4536/10336], Loss: 0.0542\n",
      "Epoch [4/5], Step [4538/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [4540/10336], Loss: 0.0209\n",
      "Epoch [4/5], Step [4542/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [4544/10336], Loss: 0.1371\n",
      "Epoch [4/5], Step [4546/10336], Loss: 0.7746\n",
      "Epoch [4/5], Step [4548/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [4550/10336], Loss: 0.0713\n",
      "Epoch [4/5], Step [4552/10336], Loss: 0.1393\n",
      "Epoch [4/5], Step [4554/10336], Loss: 0.8221\n",
      "Epoch [4/5], Step [4556/10336], Loss: 0.1133\n",
      "Epoch [4/5], Step [4558/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [4560/10336], Loss: 0.0489\n",
      "Epoch [4/5], Step [4562/10336], Loss: 0.2368\n",
      "Epoch [4/5], Step [4564/10336], Loss: 6.0125\n",
      "Epoch [4/5], Step [4566/10336], Loss: 0.2191\n",
      "Epoch [4/5], Step [4568/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [4570/10336], Loss: 1.8161\n",
      "Epoch [4/5], Step [4572/10336], Loss: 0.2359\n",
      "Epoch [4/5], Step [4574/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [4576/10336], Loss: 4.6096\n",
      "Epoch [4/5], Step [4578/10336], Loss: 0.0530\n",
      "Epoch [4/5], Step [4580/10336], Loss: 0.0411\n",
      "Epoch [4/5], Step [4582/10336], Loss: 0.1297\n",
      "Epoch [4/5], Step [4584/10336], Loss: 0.4691\n",
      "Epoch [4/5], Step [4586/10336], Loss: 0.0185\n",
      "Epoch [4/5], Step [4588/10336], Loss: 1.0677\n",
      "Epoch [4/5], Step [4590/10336], Loss: 0.7396\n",
      "Epoch [4/5], Step [4592/10336], Loss: 0.2876\n",
      "Epoch [4/5], Step [4594/10336], Loss: 0.3460\n",
      "Epoch [4/5], Step [4596/10336], Loss: 0.7350\n",
      "Epoch [4/5], Step [4598/10336], Loss: 0.1527\n",
      "Epoch [4/5], Step [4600/10336], Loss: 0.0966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [4602/10336], Loss: 0.7221\n",
      "Epoch [4/5], Step [4604/10336], Loss: 0.5224\n",
      "Epoch [4/5], Step [4606/10336], Loss: 0.0087\n",
      "Epoch [4/5], Step [4608/10336], Loss: 1.2234\n",
      "Epoch [4/5], Step [4610/10336], Loss: 0.3157\n",
      "Epoch [4/5], Step [4612/10336], Loss: 2.2594\n",
      "Epoch [4/5], Step [4614/10336], Loss: 0.3481\n",
      "Epoch [4/5], Step [4616/10336], Loss: 1.5326\n",
      "Epoch [4/5], Step [4618/10336], Loss: 0.0964\n",
      "Epoch [4/5], Step [4620/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [4622/10336], Loss: 0.0846\n",
      "Epoch [4/5], Step [4624/10336], Loss: 5.8572\n",
      "Epoch [4/5], Step [4626/10336], Loss: 0.1756\n",
      "Epoch [4/5], Step [4628/10336], Loss: 0.1166\n",
      "Epoch [4/5], Step [4630/10336], Loss: 0.1767\n",
      "Epoch [4/5], Step [4632/10336], Loss: 1.5213\n",
      "Epoch [4/5], Step [4634/10336], Loss: 0.1885\n",
      "Epoch [4/5], Step [4636/10336], Loss: 2.4110\n",
      "Epoch [4/5], Step [4638/10336], Loss: 0.4618\n",
      "Epoch [4/5], Step [4640/10336], Loss: 0.0101\n",
      "Epoch [4/5], Step [4642/10336], Loss: 0.9495\n",
      "Epoch [4/5], Step [4644/10336], Loss: 0.2665\n",
      "Epoch [4/5], Step [4646/10336], Loss: 0.1648\n",
      "Epoch [4/5], Step [4648/10336], Loss: 3.4039\n",
      "Epoch [4/5], Step [4650/10336], Loss: 1.2237\n",
      "Epoch [4/5], Step [4652/10336], Loss: 0.4224\n",
      "Epoch [4/5], Step [4654/10336], Loss: 2.9094\n",
      "Epoch [4/5], Step [4656/10336], Loss: 0.3366\n",
      "Epoch [4/5], Step [4658/10336], Loss: 0.1440\n",
      "Epoch [4/5], Step [4660/10336], Loss: 0.5130\n",
      "Epoch [4/5], Step [4662/10336], Loss: 0.4014\n",
      "Epoch [4/5], Step [4664/10336], Loss: 0.0637\n",
      "Epoch [4/5], Step [4666/10336], Loss: 0.1717\n",
      "Epoch [4/5], Step [4668/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [4670/10336], Loss: 0.4002\n",
      "Epoch [4/5], Step [4672/10336], Loss: 0.7099\n",
      "Epoch [4/5], Step [4674/10336], Loss: 1.7810\n",
      "Epoch [4/5], Step [4676/10336], Loss: 0.2082\n",
      "Epoch [4/5], Step [4678/10336], Loss: 1.0976\n",
      "Epoch [4/5], Step [4680/10336], Loss: 0.5525\n",
      "Epoch [4/5], Step [4682/10336], Loss: 0.3089\n",
      "Epoch [4/5], Step [4684/10336], Loss: 0.9688\n",
      "Epoch [4/5], Step [4686/10336], Loss: 0.2641\n",
      "Epoch [4/5], Step [4688/10336], Loss: 0.2009\n",
      "Epoch [4/5], Step [4690/10336], Loss: 0.0509\n",
      "Epoch [4/5], Step [4692/10336], Loss: 0.0215\n",
      "Epoch [4/5], Step [4694/10336], Loss: 0.4262\n",
      "Epoch [4/5], Step [4696/10336], Loss: 0.7501\n",
      "Epoch [4/5], Step [4698/10336], Loss: 0.2292\n",
      "Epoch [4/5], Step [4700/10336], Loss: 0.0677\n",
      "Epoch [4/5], Step [4702/10336], Loss: 0.4472\n",
      "Epoch [4/5], Step [4704/10336], Loss: 3.2905\n",
      "Epoch [4/5], Step [4706/10336], Loss: 0.0476\n",
      "Epoch [4/5], Step [4708/10336], Loss: 4.1911\n",
      "Epoch [4/5], Step [4710/10336], Loss: 0.8047\n",
      "Epoch [4/5], Step [4712/10336], Loss: 0.0128\n",
      "Epoch [4/5], Step [4714/10336], Loss: 0.0718\n",
      "Epoch [4/5], Step [4716/10336], Loss: 0.3222\n",
      "Epoch [4/5], Step [4718/10336], Loss: 0.1580\n",
      "Epoch [4/5], Step [4720/10336], Loss: 0.0344\n",
      "Epoch [4/5], Step [4722/10336], Loss: 0.2286\n",
      "Epoch [4/5], Step [4724/10336], Loss: 0.2426\n",
      "Epoch [4/5], Step [4726/10336], Loss: 0.7895\n",
      "Epoch [4/5], Step [4728/10336], Loss: 0.6204\n",
      "Epoch [4/5], Step [4730/10336], Loss: 0.0713\n",
      "Epoch [4/5], Step [4732/10336], Loss: 0.3677\n",
      "Epoch [4/5], Step [4734/10336], Loss: 0.3999\n",
      "Epoch [4/5], Step [4736/10336], Loss: 0.2023\n",
      "Epoch [4/5], Step [4738/10336], Loss: 0.3436\n",
      "Epoch [4/5], Step [4740/10336], Loss: 0.0379\n",
      "Epoch [4/5], Step [4742/10336], Loss: 0.2124\n",
      "Epoch [4/5], Step [4744/10336], Loss: 0.1634\n",
      "Epoch [4/5], Step [4746/10336], Loss: 1.0182\n",
      "Epoch [4/5], Step [4748/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [4750/10336], Loss: 0.1873\n",
      "Epoch [4/5], Step [4752/10336], Loss: 0.2176\n",
      "Epoch [4/5], Step [4754/10336], Loss: 1.8834\n",
      "Epoch [4/5], Step [4756/10336], Loss: 0.7198\n",
      "Epoch [4/5], Step [4758/10336], Loss: 0.6448\n",
      "Epoch [4/5], Step [4760/10336], Loss: 2.9450\n",
      "Epoch [4/5], Step [4762/10336], Loss: 0.1329\n",
      "Epoch [4/5], Step [4764/10336], Loss: 0.2247\n",
      "Epoch [4/5], Step [4766/10336], Loss: 0.0977\n",
      "Epoch [4/5], Step [4768/10336], Loss: 0.0585\n",
      "Epoch [4/5], Step [4770/10336], Loss: 5.4903\n",
      "Epoch [4/5], Step [4772/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [4774/10336], Loss: 0.1585\n",
      "Epoch [4/5], Step [4776/10336], Loss: 1.5952\n",
      "Epoch [4/5], Step [4778/10336], Loss: 0.4666\n",
      "Epoch [4/5], Step [4780/10336], Loss: 0.1435\n",
      "Epoch [4/5], Step [4782/10336], Loss: 0.4683\n",
      "Epoch [4/5], Step [4784/10336], Loss: 0.0879\n",
      "Epoch [4/5], Step [4786/10336], Loss: 0.0514\n",
      "Epoch [4/5], Step [4788/10336], Loss: 0.6884\n",
      "Epoch [4/5], Step [4790/10336], Loss: 0.0962\n",
      "Epoch [4/5], Step [4792/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [4794/10336], Loss: 0.2861\n",
      "Epoch [4/5], Step [4796/10336], Loss: 0.3187\n",
      "Epoch [4/5], Step [4798/10336], Loss: 0.3726\n",
      "Epoch [4/5], Step [4800/10336], Loss: 0.0058\n",
      "Epoch [4/5], Step [4802/10336], Loss: 0.2011\n",
      "Epoch [4/5], Step [4804/10336], Loss: 0.2371\n",
      "Epoch [4/5], Step [4806/10336], Loss: 0.5541\n",
      "Epoch [4/5], Step [4808/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [4810/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [4812/10336], Loss: 3.0704\n",
      "Epoch [4/5], Step [4814/10336], Loss: 0.0988\n",
      "Epoch [4/5], Step [4816/10336], Loss: 0.0143\n",
      "Epoch [4/5], Step [4818/10336], Loss: 0.2152\n",
      "Epoch [4/5], Step [4820/10336], Loss: 2.6529\n",
      "Epoch [4/5], Step [4822/10336], Loss: 0.1430\n",
      "Epoch [4/5], Step [4824/10336], Loss: 2.8507\n",
      "Epoch [4/5], Step [4826/10336], Loss: 0.3414\n",
      "Epoch [4/5], Step [4828/10336], Loss: 0.1650\n",
      "Epoch [4/5], Step [4830/10336], Loss: 0.1296\n",
      "Epoch [4/5], Step [4832/10336], Loss: 0.4295\n",
      "Epoch [4/5], Step [4834/10336], Loss: 0.2511\n",
      "Epoch [4/5], Step [4836/10336], Loss: 0.2945\n",
      "Epoch [4/5], Step [4838/10336], Loss: 1.6675\n",
      "Epoch [4/5], Step [4840/10336], Loss: 0.7856\n",
      "Epoch [4/5], Step [4842/10336], Loss: 0.2436\n",
      "Epoch [4/5], Step [4844/10336], Loss: 1.0506\n",
      "Epoch [4/5], Step [4846/10336], Loss: 3.4040\n",
      "Epoch [4/5], Step [4848/10336], Loss: 0.4853\n",
      "Epoch [4/5], Step [4850/10336], Loss: 4.2580\n",
      "Epoch [4/5], Step [4852/10336], Loss: 0.1194\n",
      "Epoch [4/5], Step [4854/10336], Loss: 0.2716\n",
      "Epoch [4/5], Step [4856/10336], Loss: 0.1257\n",
      "Epoch [4/5], Step [4858/10336], Loss: 0.4146\n",
      "Epoch [4/5], Step [4860/10336], Loss: 0.2313\n",
      "Epoch [4/5], Step [4862/10336], Loss: 0.3110\n",
      "Epoch [4/5], Step [4864/10336], Loss: 0.2148\n",
      "Epoch [4/5], Step [4866/10336], Loss: 0.0357\n",
      "Epoch [4/5], Step [4868/10336], Loss: 0.0032\n",
      "Epoch [4/5], Step [4870/10336], Loss: 0.1049\n",
      "Epoch [4/5], Step [4872/10336], Loss: 0.0150\n",
      "Epoch [4/5], Step [4874/10336], Loss: 0.0038\n",
      "Epoch [4/5], Step [4876/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [4878/10336], Loss: 0.3255\n",
      "Epoch [4/5], Step [4880/10336], Loss: 0.2005\n",
      "Epoch [4/5], Step [4882/10336], Loss: 0.2560\n",
      "Epoch [4/5], Step [4884/10336], Loss: 0.0400\n",
      "Epoch [4/5], Step [4886/10336], Loss: 1.5284\n",
      "Epoch [4/5], Step [4888/10336], Loss: 3.9563\n",
      "Epoch [4/5], Step [4890/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [4892/10336], Loss: 0.8172\n",
      "Epoch [4/5], Step [4894/10336], Loss: 0.1224\n",
      "Epoch [4/5], Step [4896/10336], Loss: 0.5478\n",
      "Epoch [4/5], Step [4898/10336], Loss: 3.5607\n",
      "Epoch [4/5], Step [4900/10336], Loss: 0.0693\n",
      "Epoch [4/5], Step [4902/10336], Loss: 2.3618\n",
      "Epoch [4/5], Step [4904/10336], Loss: 1.0939\n",
      "Epoch [4/5], Step [4906/10336], Loss: 0.1404\n",
      "Epoch [4/5], Step [4908/10336], Loss: 0.3241\n",
      "Epoch [4/5], Step [4910/10336], Loss: 1.2996\n",
      "Epoch [4/5], Step [4912/10336], Loss: 0.0614\n",
      "Epoch [4/5], Step [4914/10336], Loss: 0.4883\n",
      "Epoch [4/5], Step [4916/10336], Loss: 0.2193\n",
      "Epoch [4/5], Step [4918/10336], Loss: 0.3118\n",
      "Epoch [4/5], Step [4920/10336], Loss: 0.0323\n",
      "Epoch [4/5], Step [4922/10336], Loss: 0.0118\n",
      "Epoch [4/5], Step [4924/10336], Loss: 1.2679\n",
      "Epoch [4/5], Step [4926/10336], Loss: 0.2628\n",
      "Epoch [4/5], Step [4928/10336], Loss: 0.0150\n",
      "Epoch [4/5], Step [4930/10336], Loss: 1.7773\n",
      "Epoch [4/5], Step [4932/10336], Loss: 1.4961\n",
      "Epoch [4/5], Step [4934/10336], Loss: 0.0048\n",
      "Epoch [4/5], Step [4936/10336], Loss: 0.0205\n",
      "Epoch [4/5], Step [4938/10336], Loss: 0.0084\n",
      "Epoch [4/5], Step [4940/10336], Loss: 0.2223\n",
      "Epoch [4/5], Step [4942/10336], Loss: 0.1849\n",
      "Epoch [4/5], Step [4944/10336], Loss: 0.2476\n",
      "Epoch [4/5], Step [4946/10336], Loss: 0.1108\n",
      "Epoch [4/5], Step [4948/10336], Loss: 0.4955\n",
      "Epoch [4/5], Step [4950/10336], Loss: 0.0172\n",
      "Epoch [4/5], Step [4952/10336], Loss: 0.0598\n",
      "Epoch [4/5], Step [4954/10336], Loss: 3.1281\n",
      "Epoch [4/5], Step [4956/10336], Loss: 0.2718\n",
      "Epoch [4/5], Step [4958/10336], Loss: 1.3433\n",
      "Epoch [4/5], Step [4960/10336], Loss: 2.6675\n",
      "Epoch [4/5], Step [4962/10336], Loss: 3.1505\n",
      "Epoch [4/5], Step [4964/10336], Loss: 1.4371\n",
      "Epoch [4/5], Step [4966/10336], Loss: 0.7686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [4968/10336], Loss: 0.1358\n",
      "Epoch [4/5], Step [4970/10336], Loss: 0.2737\n",
      "Epoch [4/5], Step [4972/10336], Loss: 0.2481\n",
      "Epoch [4/5], Step [4974/10336], Loss: 0.3760\n",
      "Epoch [4/5], Step [4976/10336], Loss: 4.2916\n",
      "Epoch [4/5], Step [4978/10336], Loss: 1.8581\n",
      "Epoch [4/5], Step [4980/10336], Loss: 0.2194\n",
      "Epoch [4/5], Step [4982/10336], Loss: 1.9578\n",
      "Epoch [4/5], Step [4984/10336], Loss: 0.3241\n",
      "Epoch [4/5], Step [4986/10336], Loss: 1.5835\n",
      "Epoch [4/5], Step [4988/10336], Loss: 0.5762\n",
      "Epoch [4/5], Step [4990/10336], Loss: 1.5230\n",
      "Epoch [4/5], Step [4992/10336], Loss: 1.6105\n",
      "Epoch [4/5], Step [4994/10336], Loss: 0.0881\n",
      "Epoch [4/5], Step [4996/10336], Loss: 0.7391\n",
      "Epoch [4/5], Step [4998/10336], Loss: 0.8473\n",
      "Epoch [4/5], Step [5000/10336], Loss: 0.9793\n",
      "Epoch [4/5], Step [5002/10336], Loss: 0.3302\n",
      "Epoch [4/5], Step [5004/10336], Loss: 0.1202\n",
      "Epoch [4/5], Step [5006/10336], Loss: 0.3976\n",
      "Epoch [4/5], Step [5008/10336], Loss: 0.1887\n",
      "Epoch [4/5], Step [5010/10336], Loss: 0.1545\n",
      "Epoch [4/5], Step [5012/10336], Loss: 0.5846\n",
      "Epoch [4/5], Step [5014/10336], Loss: 0.3405\n",
      "Epoch [4/5], Step [5016/10336], Loss: 1.2163\n",
      "Epoch [4/5], Step [5018/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [5020/10336], Loss: 0.0680\n",
      "Epoch [4/5], Step [5022/10336], Loss: 0.1372\n",
      "Epoch [4/5], Step [5024/10336], Loss: 0.1808\n",
      "Epoch [4/5], Step [5026/10336], Loss: 0.0295\n",
      "Epoch [4/5], Step [5028/10336], Loss: 0.2025\n",
      "Epoch [4/5], Step [5030/10336], Loss: 0.4356\n",
      "Epoch [4/5], Step [5032/10336], Loss: 0.5075\n",
      "Epoch [4/5], Step [5034/10336], Loss: 1.7533\n",
      "Epoch [4/5], Step [5036/10336], Loss: 0.0063\n",
      "Epoch [4/5], Step [5038/10336], Loss: 0.1524\n",
      "Epoch [4/5], Step [5040/10336], Loss: 0.6451\n",
      "Epoch [4/5], Step [5042/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [5044/10336], Loss: 0.0386\n",
      "Epoch [4/5], Step [5046/10336], Loss: 2.4354\n",
      "Epoch [4/5], Step [5048/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [5050/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [5052/10336], Loss: 5.0671\n",
      "Epoch [4/5], Step [5054/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [5056/10336], Loss: 0.1084\n",
      "Epoch [4/5], Step [5058/10336], Loss: 0.5386\n",
      "Epoch [4/5], Step [5060/10336], Loss: 0.7412\n",
      "Epoch [4/5], Step [5062/10336], Loss: 0.2135\n",
      "Epoch [4/5], Step [5064/10336], Loss: 0.2758\n",
      "Epoch [4/5], Step [5066/10336], Loss: 0.0674\n",
      "Epoch [4/5], Step [5068/10336], Loss: 0.2143\n",
      "Epoch [4/5], Step [5070/10336], Loss: 0.8045\n",
      "Epoch [4/5], Step [5072/10336], Loss: 0.1835\n",
      "Epoch [4/5], Step [5074/10336], Loss: 3.2199\n",
      "Epoch [4/5], Step [5076/10336], Loss: 1.2966\n",
      "Epoch [4/5], Step [5078/10336], Loss: 0.0195\n",
      "Epoch [4/5], Step [5080/10336], Loss: 0.3930\n",
      "Epoch [4/5], Step [5082/10336], Loss: 0.1527\n",
      "Epoch [4/5], Step [5084/10336], Loss: 0.1258\n",
      "Epoch [4/5], Step [5086/10336], Loss: 0.0584\n",
      "Epoch [4/5], Step [5088/10336], Loss: 1.2810\n",
      "Epoch [4/5], Step [5090/10336], Loss: 1.1121\n",
      "Epoch [4/5], Step [5092/10336], Loss: 0.2010\n",
      "Epoch [4/5], Step [5094/10336], Loss: 1.2232\n",
      "Epoch [4/5], Step [5096/10336], Loss: 0.5164\n",
      "Epoch [4/5], Step [5098/10336], Loss: 0.6965\n",
      "Epoch [4/5], Step [5100/10336], Loss: 0.0106\n",
      "Epoch [4/5], Step [5102/10336], Loss: 0.0162\n",
      "Epoch [4/5], Step [5104/10336], Loss: 0.3497\n",
      "Epoch [4/5], Step [5106/10336], Loss: 1.6442\n",
      "Epoch [4/5], Step [5108/10336], Loss: 0.0195\n",
      "Epoch [4/5], Step [5110/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [5112/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [5114/10336], Loss: 0.3263\n",
      "Epoch [4/5], Step [5116/10336], Loss: 2.4538\n",
      "Epoch [4/5], Step [5118/10336], Loss: 0.2867\n",
      "Epoch [4/5], Step [5120/10336], Loss: 0.0906\n",
      "Epoch [4/5], Step [5122/10336], Loss: 0.6848\n",
      "Epoch [4/5], Step [5124/10336], Loss: 3.3519\n",
      "Epoch [4/5], Step [5126/10336], Loss: 0.0031\n",
      "Epoch [4/5], Step [5128/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [5130/10336], Loss: 0.0118\n",
      "Epoch [4/5], Step [5132/10336], Loss: 0.3436\n",
      "Epoch [4/5], Step [5134/10336], Loss: 0.0447\n",
      "Epoch [4/5], Step [5136/10336], Loss: 0.1388\n",
      "Epoch [4/5], Step [5138/10336], Loss: 0.0648\n",
      "Epoch [4/5], Step [5140/10336], Loss: 0.0213\n",
      "Epoch [4/5], Step [5142/10336], Loss: 0.3706\n",
      "Epoch [4/5], Step [5144/10336], Loss: 1.5267\n",
      "Epoch [4/5], Step [5146/10336], Loss: 0.3732\n",
      "Epoch [4/5], Step [5148/10336], Loss: 0.0985\n",
      "Epoch [4/5], Step [5150/10336], Loss: 0.0564\n",
      "Epoch [4/5], Step [5152/10336], Loss: 2.4212\n",
      "Epoch [4/5], Step [5154/10336], Loss: 0.0075\n",
      "Epoch [4/5], Step [5156/10336], Loss: 0.3757\n",
      "Epoch [4/5], Step [5158/10336], Loss: 3.8442\n",
      "Epoch [4/5], Step [5160/10336], Loss: 0.3066\n",
      "Epoch [4/5], Step [5162/10336], Loss: 0.1982\n",
      "Epoch [4/5], Step [5164/10336], Loss: 1.0789\n",
      "Epoch [4/5], Step [5166/10336], Loss: 0.2364\n",
      "Epoch [4/5], Step [5168/10336], Loss: 0.6281\n",
      "Epoch [4/5], Step [5170/10336], Loss: 0.1314\n",
      "Epoch [4/5], Step [5172/10336], Loss: 0.1875\n",
      "Epoch [4/5], Step [5174/10336], Loss: 4.3086\n",
      "Epoch [4/5], Step [5176/10336], Loss: 0.2976\n",
      "Epoch [4/5], Step [5178/10336], Loss: 2.3146\n",
      "Epoch [4/5], Step [5180/10336], Loss: 0.1657\n",
      "Epoch [4/5], Step [5182/10336], Loss: 0.0453\n",
      "Epoch [4/5], Step [5184/10336], Loss: 0.5967\n",
      "Epoch [4/5], Step [5186/10336], Loss: 0.5949\n",
      "Epoch [4/5], Step [5188/10336], Loss: 2.0303\n",
      "Epoch [4/5], Step [5190/10336], Loss: 0.2718\n",
      "Epoch [4/5], Step [5192/10336], Loss: 1.9029\n",
      "Epoch [4/5], Step [5194/10336], Loss: 2.2150\n",
      "Epoch [4/5], Step [5196/10336], Loss: 0.4570\n",
      "Epoch [4/5], Step [5198/10336], Loss: 3.1582\n",
      "Epoch [4/5], Step [5200/10336], Loss: 0.6561\n",
      "Epoch [4/5], Step [5202/10336], Loss: 2.3003\n",
      "Epoch [4/5], Step [5204/10336], Loss: 1.4506\n",
      "Epoch [4/5], Step [5206/10336], Loss: 0.6923\n",
      "Epoch [4/5], Step [5208/10336], Loss: 0.4176\n",
      "Epoch [4/5], Step [5210/10336], Loss: 0.0175\n",
      "Epoch [4/5], Step [5212/10336], Loss: 0.9365\n",
      "Epoch [4/5], Step [5214/10336], Loss: 0.3364\n",
      "Epoch [4/5], Step [5216/10336], Loss: 0.3004\n",
      "Epoch [4/5], Step [5218/10336], Loss: 1.8989\n",
      "Epoch [4/5], Step [5220/10336], Loss: 1.5589\n",
      "Epoch [4/5], Step [5222/10336], Loss: 5.1439\n",
      "Epoch [4/5], Step [5224/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [5226/10336], Loss: 0.0053\n",
      "Epoch [4/5], Step [5228/10336], Loss: 0.6414\n",
      "Epoch [4/5], Step [5230/10336], Loss: 0.1526\n",
      "Epoch [4/5], Step [5232/10336], Loss: 0.5745\n",
      "Epoch [4/5], Step [5234/10336], Loss: 0.0471\n",
      "Epoch [4/5], Step [5236/10336], Loss: 0.0847\n",
      "Epoch [4/5], Step [5238/10336], Loss: 0.0346\n",
      "Epoch [4/5], Step [5240/10336], Loss: 0.0922\n",
      "Epoch [4/5], Step [5242/10336], Loss: 2.6389\n",
      "Epoch [4/5], Step [5244/10336], Loss: 0.3437\n",
      "Epoch [4/5], Step [5246/10336], Loss: 0.1349\n",
      "Epoch [4/5], Step [5248/10336], Loss: 0.1684\n",
      "Epoch [4/5], Step [5250/10336], Loss: 0.3716\n",
      "Epoch [4/5], Step [5252/10336], Loss: 0.5150\n",
      "Epoch [4/5], Step [5254/10336], Loss: 0.0106\n",
      "Epoch [4/5], Step [5256/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [5258/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [5260/10336], Loss: 4.2539\n",
      "Epoch [4/5], Step [5262/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [5264/10336], Loss: 0.5191\n",
      "Epoch [4/5], Step [5266/10336], Loss: 0.1093\n",
      "Epoch [4/5], Step [5268/10336], Loss: 1.8922\n",
      "Epoch [4/5], Step [5270/10336], Loss: 0.3198\n",
      "Epoch [4/5], Step [5272/10336], Loss: 0.0733\n",
      "Epoch [4/5], Step [5274/10336], Loss: 0.3066\n",
      "Epoch [4/5], Step [5276/10336], Loss: 0.3687\n",
      "Epoch [4/5], Step [5278/10336], Loss: 2.1263\n",
      "Epoch [4/5], Step [5280/10336], Loss: 2.7437\n",
      "Epoch [4/5], Step [5282/10336], Loss: 0.0868\n",
      "Epoch [4/5], Step [5284/10336], Loss: 0.0871\n",
      "Epoch [4/5], Step [5286/10336], Loss: 0.0251\n",
      "Epoch [4/5], Step [5288/10336], Loss: 0.3891\n",
      "Epoch [4/5], Step [5290/10336], Loss: 0.0667\n",
      "Epoch [4/5], Step [5292/10336], Loss: 0.3256\n",
      "Epoch [4/5], Step [5294/10336], Loss: 0.9158\n",
      "Epoch [4/5], Step [5296/10336], Loss: 0.2545\n",
      "Epoch [4/5], Step [5298/10336], Loss: 0.5000\n",
      "Epoch [4/5], Step [5300/10336], Loss: 0.4804\n",
      "Epoch [4/5], Step [5302/10336], Loss: 0.0117\n",
      "Epoch [4/5], Step [5304/10336], Loss: 2.6913\n",
      "Epoch [4/5], Step [5306/10336], Loss: 0.2519\n",
      "Epoch [4/5], Step [5308/10336], Loss: 0.1931\n",
      "Epoch [4/5], Step [5310/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [5312/10336], Loss: 0.0483\n",
      "Epoch [4/5], Step [5314/10336], Loss: 0.3893\n",
      "Epoch [4/5], Step [5316/10336], Loss: 0.2271\n",
      "Epoch [4/5], Step [5318/10336], Loss: 0.6620\n",
      "Epoch [4/5], Step [5320/10336], Loss: 0.0293\n",
      "Epoch [4/5], Step [5322/10336], Loss: 0.0842\n",
      "Epoch [4/5], Step [5324/10336], Loss: 0.1171\n",
      "Epoch [4/5], Step [5326/10336], Loss: 0.7824\n",
      "Epoch [4/5], Step [5328/10336], Loss: 1.1996\n",
      "Epoch [4/5], Step [5330/10336], Loss: 0.5663\n",
      "Epoch [4/5], Step [5332/10336], Loss: 0.2751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [5334/10336], Loss: 0.0342\n",
      "Epoch [4/5], Step [5336/10336], Loss: 0.2997\n",
      "Epoch [4/5], Step [5338/10336], Loss: 2.4323\n",
      "Epoch [4/5], Step [5340/10336], Loss: 0.2026\n",
      "Epoch [4/5], Step [5342/10336], Loss: 0.2685\n",
      "Epoch [4/5], Step [5344/10336], Loss: 0.0085\n",
      "Epoch [4/5], Step [5346/10336], Loss: 0.1450\n",
      "Epoch [4/5], Step [5348/10336], Loss: 0.2131\n",
      "Epoch [4/5], Step [5350/10336], Loss: 0.2929\n",
      "Epoch [4/5], Step [5352/10336], Loss: 0.0200\n",
      "Epoch [4/5], Step [5354/10336], Loss: 0.2855\n",
      "Epoch [4/5], Step [5356/10336], Loss: 3.0440\n",
      "Epoch [4/5], Step [5358/10336], Loss: 0.2847\n",
      "Epoch [4/5], Step [5360/10336], Loss: 1.1738\n",
      "Epoch [4/5], Step [5362/10336], Loss: 1.7320\n",
      "Epoch [4/5], Step [5364/10336], Loss: 1.6836\n",
      "Epoch [4/5], Step [5366/10336], Loss: 1.8019\n",
      "Epoch [4/5], Step [5368/10336], Loss: 0.1508\n",
      "Epoch [4/5], Step [5370/10336], Loss: 0.3637\n",
      "Epoch [4/5], Step [5372/10336], Loss: 0.4002\n",
      "Epoch [4/5], Step [5374/10336], Loss: 2.2399\n",
      "Epoch [4/5], Step [5376/10336], Loss: 1.3248\n",
      "Epoch [4/5], Step [5378/10336], Loss: 2.2944\n",
      "Epoch [4/5], Step [5380/10336], Loss: 0.6508\n",
      "Epoch [4/5], Step [5382/10336], Loss: 0.2017\n",
      "Epoch [4/5], Step [5384/10336], Loss: 0.1744\n",
      "Epoch [4/5], Step [5386/10336], Loss: 1.9422\n",
      "Epoch [4/5], Step [5388/10336], Loss: 0.0286\n",
      "Epoch [4/5], Step [5390/10336], Loss: 0.7142\n",
      "Epoch [4/5], Step [5392/10336], Loss: 0.4237\n",
      "Epoch [4/5], Step [5394/10336], Loss: 0.7177\n",
      "Epoch [4/5], Step [5396/10336], Loss: 0.4863\n",
      "Epoch [4/5], Step [5398/10336], Loss: 1.6531\n",
      "Epoch [4/5], Step [5400/10336], Loss: 0.0986\n",
      "Epoch [4/5], Step [5402/10336], Loss: 0.0838\n",
      "Epoch [4/5], Step [5404/10336], Loss: 0.1336\n",
      "Epoch [4/5], Step [5406/10336], Loss: 0.3330\n",
      "Epoch [4/5], Step [5408/10336], Loss: 0.2601\n",
      "Epoch [4/5], Step [5410/10336], Loss: 0.1804\n",
      "Epoch [4/5], Step [5412/10336], Loss: 0.5816\n",
      "Epoch [4/5], Step [5414/10336], Loss: 1.9626\n",
      "Epoch [4/5], Step [5416/10336], Loss: 0.0348\n",
      "Epoch [4/5], Step [5418/10336], Loss: 0.2540\n",
      "Epoch [4/5], Step [5420/10336], Loss: 0.3815\n",
      "Epoch [4/5], Step [5422/10336], Loss: 0.0918\n",
      "Epoch [4/5], Step [5424/10336], Loss: 0.4200\n",
      "Epoch [4/5], Step [5426/10336], Loss: 0.0173\n",
      "Epoch [4/5], Step [5428/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [5430/10336], Loss: 0.5209\n",
      "Epoch [4/5], Step [5432/10336], Loss: 0.2816\n",
      "Epoch [4/5], Step [5434/10336], Loss: 0.0644\n",
      "Epoch [4/5], Step [5436/10336], Loss: 0.4227\n",
      "Epoch [4/5], Step [5438/10336], Loss: 0.0706\n",
      "Epoch [4/5], Step [5440/10336], Loss: 2.6922\n",
      "Epoch [4/5], Step [5442/10336], Loss: 0.4320\n",
      "Epoch [4/5], Step [5444/10336], Loss: 0.4548\n",
      "Epoch [4/5], Step [5446/10336], Loss: 1.6756\n",
      "Epoch [4/5], Step [5448/10336], Loss: 0.3686\n",
      "Epoch [4/5], Step [5450/10336], Loss: 0.2555\n",
      "Epoch [4/5], Step [5452/10336], Loss: 0.0397\n",
      "Epoch [4/5], Step [5454/10336], Loss: 0.2839\n",
      "Epoch [4/5], Step [5456/10336], Loss: 0.2208\n",
      "Epoch [4/5], Step [5458/10336], Loss: 0.3608\n",
      "Epoch [4/5], Step [5460/10336], Loss: 3.8496\n",
      "Epoch [4/5], Step [5462/10336], Loss: 0.0318\n",
      "Epoch [4/5], Step [5464/10336], Loss: 0.4476\n",
      "Epoch [4/5], Step [5466/10336], Loss: 0.7050\n",
      "Epoch [4/5], Step [5468/10336], Loss: 0.1862\n",
      "Epoch [4/5], Step [5470/10336], Loss: 0.1021\n",
      "Epoch [4/5], Step [5472/10336], Loss: 1.0742\n",
      "Epoch [4/5], Step [5474/10336], Loss: 0.0282\n",
      "Epoch [4/5], Step [5476/10336], Loss: 2.5796\n",
      "Epoch [4/5], Step [5478/10336], Loss: 0.0493\n",
      "Epoch [4/5], Step [5480/10336], Loss: 0.3663\n",
      "Epoch [4/5], Step [5482/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [5484/10336], Loss: 0.1518\n",
      "Epoch [4/5], Step [5486/10336], Loss: 0.0375\n",
      "Epoch [4/5], Step [5488/10336], Loss: 2.4970\n",
      "Epoch [4/5], Step [5490/10336], Loss: 1.3352\n",
      "Epoch [4/5], Step [5492/10336], Loss: 0.0508\n",
      "Epoch [4/5], Step [5494/10336], Loss: 0.7108\n",
      "Epoch [4/5], Step [5496/10336], Loss: 0.6445\n",
      "Epoch [4/5], Step [5498/10336], Loss: 0.6016\n",
      "Epoch [4/5], Step [5500/10336], Loss: 0.0387\n",
      "Epoch [4/5], Step [5502/10336], Loss: 0.1485\n",
      "Epoch [4/5], Step [5504/10336], Loss: 0.1357\n",
      "Epoch [4/5], Step [5506/10336], Loss: 0.8618\n",
      "Epoch [4/5], Step [5508/10336], Loss: 0.0104\n",
      "Epoch [4/5], Step [5510/10336], Loss: 0.1560\n",
      "Epoch [4/5], Step [5512/10336], Loss: 1.0446\n",
      "Epoch [4/5], Step [5514/10336], Loss: 0.0224\n",
      "Epoch [4/5], Step [5516/10336], Loss: 2.3019\n",
      "Epoch [4/5], Step [5518/10336], Loss: 0.8641\n",
      "Epoch [4/5], Step [5520/10336], Loss: 0.0395\n",
      "Epoch [4/5], Step [5522/10336], Loss: 0.6043\n",
      "Epoch [4/5], Step [5524/10336], Loss: 1.0151\n",
      "Epoch [4/5], Step [5526/10336], Loss: 3.8764\n",
      "Epoch [4/5], Step [5528/10336], Loss: 0.0030\n",
      "Epoch [4/5], Step [5530/10336], Loss: 4.3070\n",
      "Epoch [4/5], Step [5532/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [5534/10336], Loss: 0.5379\n",
      "Epoch [4/5], Step [5536/10336], Loss: 0.4682\n",
      "Epoch [4/5], Step [5538/10336], Loss: 0.8800\n",
      "Epoch [4/5], Step [5540/10336], Loss: 0.7925\n",
      "Epoch [4/5], Step [5542/10336], Loss: 0.3457\n",
      "Epoch [4/5], Step [5544/10336], Loss: 0.5253\n",
      "Epoch [4/5], Step [5546/10336], Loss: 0.6328\n",
      "Epoch [4/5], Step [5548/10336], Loss: 0.0475\n",
      "Epoch [4/5], Step [5550/10336], Loss: 0.5390\n",
      "Epoch [4/5], Step [5552/10336], Loss: 0.3349\n",
      "Epoch [4/5], Step [5554/10336], Loss: 0.2491\n",
      "Epoch [4/5], Step [5556/10336], Loss: 0.0641\n",
      "Epoch [4/5], Step [5558/10336], Loss: 1.2569\n",
      "Epoch [4/5], Step [5560/10336], Loss: 0.0529\n",
      "Epoch [4/5], Step [5562/10336], Loss: 1.8392\n",
      "Epoch [4/5], Step [5564/10336], Loss: 1.5049\n",
      "Epoch [4/5], Step [5566/10336], Loss: 0.7608\n",
      "Epoch [4/5], Step [5568/10336], Loss: 0.0103\n",
      "Epoch [4/5], Step [5570/10336], Loss: 0.3341\n",
      "Epoch [4/5], Step [5572/10336], Loss: 0.1275\n",
      "Epoch [4/5], Step [5574/10336], Loss: 0.5547\n",
      "Epoch [4/5], Step [5576/10336], Loss: 0.8665\n",
      "Epoch [4/5], Step [5578/10336], Loss: 0.1393\n",
      "Epoch [4/5], Step [5580/10336], Loss: 0.4289\n",
      "Epoch [4/5], Step [5582/10336], Loss: 0.0884\n",
      "Epoch [4/5], Step [5584/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [5586/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [5588/10336], Loss: 0.0150\n",
      "Epoch [4/5], Step [5590/10336], Loss: 0.0975\n",
      "Epoch [4/5], Step [5592/10336], Loss: 0.3614\n",
      "Epoch [4/5], Step [5594/10336], Loss: 0.1217\n",
      "Epoch [4/5], Step [5596/10336], Loss: 0.3178\n",
      "Epoch [4/5], Step [5598/10336], Loss: 0.3822\n",
      "Epoch [4/5], Step [5600/10336], Loss: 0.0102\n",
      "Epoch [4/5], Step [5602/10336], Loss: 2.1426\n",
      "Epoch [4/5], Step [5604/10336], Loss: 0.0340\n",
      "Epoch [4/5], Step [5606/10336], Loss: 0.0176\n",
      "Epoch [4/5], Step [5608/10336], Loss: 0.1389\n",
      "Epoch [4/5], Step [5610/10336], Loss: 3.6002\n",
      "Epoch [4/5], Step [5612/10336], Loss: 1.0226\n",
      "Epoch [4/5], Step [5614/10336], Loss: 0.2205\n",
      "Epoch [4/5], Step [5616/10336], Loss: 0.0339\n",
      "Epoch [4/5], Step [5618/10336], Loss: 0.7028\n",
      "Epoch [4/5], Step [5620/10336], Loss: 0.8926\n",
      "Epoch [4/5], Step [5622/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [5624/10336], Loss: 1.7424\n",
      "Epoch [4/5], Step [5626/10336], Loss: 0.0111\n",
      "Epoch [4/5], Step [5628/10336], Loss: 0.2053\n",
      "Epoch [4/5], Step [5630/10336], Loss: 1.0440\n",
      "Epoch [4/5], Step [5632/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [5634/10336], Loss: 0.2384\n",
      "Epoch [4/5], Step [5636/10336], Loss: 0.0571\n",
      "Epoch [4/5], Step [5638/10336], Loss: 0.0355\n",
      "Epoch [4/5], Step [5640/10336], Loss: 0.5689\n",
      "Epoch [4/5], Step [5642/10336], Loss: 2.3572\n",
      "Epoch [4/5], Step [5644/10336], Loss: 0.0436\n",
      "Epoch [4/5], Step [5646/10336], Loss: 0.1572\n",
      "Epoch [4/5], Step [5648/10336], Loss: 1.3206\n",
      "Epoch [4/5], Step [5650/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [5652/10336], Loss: 0.2032\n",
      "Epoch [4/5], Step [5654/10336], Loss: 0.1086\n",
      "Epoch [4/5], Step [5656/10336], Loss: 0.1854\n",
      "Epoch [4/5], Step [5658/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [5660/10336], Loss: 0.1061\n",
      "Epoch [4/5], Step [5662/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [5664/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [5666/10336], Loss: 0.2570\n",
      "Epoch [4/5], Step [5668/10336], Loss: 0.0855\n",
      "Epoch [4/5], Step [5670/10336], Loss: 0.2340\n",
      "Epoch [4/5], Step [5672/10336], Loss: 1.4194\n",
      "Epoch [4/5], Step [5674/10336], Loss: 0.4339\n",
      "Epoch [4/5], Step [5676/10336], Loss: 0.8936\n",
      "Epoch [4/5], Step [5678/10336], Loss: 1.5343\n",
      "Epoch [4/5], Step [5680/10336], Loss: 0.5222\n",
      "Epoch [4/5], Step [5682/10336], Loss: 0.1775\n",
      "Epoch [4/5], Step [5684/10336], Loss: 0.0311\n",
      "Epoch [4/5], Step [5686/10336], Loss: 0.1057\n",
      "Epoch [4/5], Step [5688/10336], Loss: 1.5230\n",
      "Epoch [4/5], Step [5690/10336], Loss: 0.1132\n",
      "Epoch [4/5], Step [5692/10336], Loss: 0.3861\n",
      "Epoch [4/5], Step [5694/10336], Loss: 0.0896\n",
      "Epoch [4/5], Step [5696/10336], Loss: 2.2398\n",
      "Epoch [4/5], Step [5698/10336], Loss: 1.2652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [5700/10336], Loss: 0.6185\n",
      "Epoch [4/5], Step [5702/10336], Loss: 0.2530\n",
      "Epoch [4/5], Step [5704/10336], Loss: 1.1248\n",
      "Epoch [4/5], Step [5706/10336], Loss: 0.0041\n",
      "Epoch [4/5], Step [5708/10336], Loss: 1.1091\n",
      "Epoch [4/5], Step [5710/10336], Loss: 3.8741\n",
      "Epoch [4/5], Step [5712/10336], Loss: 0.2084\n",
      "Epoch [4/5], Step [5714/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [5716/10336], Loss: 0.9412\n",
      "Epoch [4/5], Step [5718/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [5720/10336], Loss: 0.0320\n",
      "Epoch [4/5], Step [5722/10336], Loss: 0.8659\n",
      "Epoch [4/5], Step [5724/10336], Loss: 3.5875\n",
      "Epoch [4/5], Step [5726/10336], Loss: 0.3930\n",
      "Epoch [4/5], Step [5728/10336], Loss: 0.0660\n",
      "Epoch [4/5], Step [5730/10336], Loss: 0.2374\n",
      "Epoch [4/5], Step [5732/10336], Loss: 0.3283\n",
      "Epoch [4/5], Step [5734/10336], Loss: 0.3846\n",
      "Epoch [4/5], Step [5736/10336], Loss: 0.0786\n",
      "Epoch [4/5], Step [5738/10336], Loss: 0.2311\n",
      "Epoch [4/5], Step [5740/10336], Loss: 0.6987\n",
      "Epoch [4/5], Step [5742/10336], Loss: 0.8288\n",
      "Epoch [4/5], Step [5744/10336], Loss: 1.8890\n",
      "Epoch [4/5], Step [5746/10336], Loss: 0.1501\n",
      "Epoch [4/5], Step [5748/10336], Loss: 1.0546\n",
      "Epoch [4/5], Step [5750/10336], Loss: 1.4621\n",
      "Epoch [4/5], Step [5752/10336], Loss: 1.0268\n",
      "Epoch [4/5], Step [5754/10336], Loss: 0.0515\n",
      "Epoch [4/5], Step [5756/10336], Loss: 0.0285\n",
      "Epoch [4/5], Step [5758/10336], Loss: 0.0172\n",
      "Epoch [4/5], Step [5760/10336], Loss: 0.3692\n",
      "Epoch [4/5], Step [5762/10336], Loss: 0.0244\n",
      "Epoch [4/5], Step [5764/10336], Loss: 0.3489\n",
      "Epoch [4/5], Step [5766/10336], Loss: 0.0327\n",
      "Epoch [4/5], Step [5768/10336], Loss: 0.1533\n",
      "Epoch [4/5], Step [5770/10336], Loss: 0.2673\n",
      "Epoch [4/5], Step [5772/10336], Loss: 0.0409\n",
      "Epoch [4/5], Step [5774/10336], Loss: 0.4367\n",
      "Epoch [4/5], Step [5776/10336], Loss: 1.6979\n",
      "Epoch [4/5], Step [5778/10336], Loss: 0.2060\n",
      "Epoch [4/5], Step [5780/10336], Loss: 0.1776\n",
      "Epoch [4/5], Step [5782/10336], Loss: 0.0205\n",
      "Epoch [4/5], Step [5784/10336], Loss: 0.1445\n",
      "Epoch [4/5], Step [5786/10336], Loss: 0.4581\n",
      "Epoch [4/5], Step [5788/10336], Loss: 1.3019\n",
      "Epoch [4/5], Step [5790/10336], Loss: 0.2254\n",
      "Epoch [4/5], Step [5792/10336], Loss: 3.8032\n",
      "Epoch [4/5], Step [5794/10336], Loss: 0.0045\n",
      "Epoch [4/5], Step [5796/10336], Loss: 0.2442\n",
      "Epoch [4/5], Step [5798/10336], Loss: 0.3967\n",
      "Epoch [4/5], Step [5800/10336], Loss: 0.2166\n",
      "Epoch [4/5], Step [5802/10336], Loss: 0.2834\n",
      "Epoch [4/5], Step [5804/10336], Loss: 0.0673\n",
      "Epoch [4/5], Step [5806/10336], Loss: 0.0172\n",
      "Epoch [4/5], Step [5808/10336], Loss: 0.8025\n",
      "Epoch [4/5], Step [5810/10336], Loss: 0.1302\n",
      "Epoch [4/5], Step [5812/10336], Loss: 1.6382\n",
      "Epoch [4/5], Step [5814/10336], Loss: 0.0912\n",
      "Epoch [4/5], Step [5816/10336], Loss: 2.0714\n",
      "Epoch [4/5], Step [5818/10336], Loss: 0.0317\n",
      "Epoch [4/5], Step [5820/10336], Loss: 0.0119\n",
      "Epoch [4/5], Step [5822/10336], Loss: 1.5010\n",
      "Epoch [4/5], Step [5824/10336], Loss: 0.0100\n",
      "Epoch [4/5], Step [5826/10336], Loss: 0.1378\n",
      "Epoch [4/5], Step [5828/10336], Loss: 3.2558\n",
      "Epoch [4/5], Step [5830/10336], Loss: 0.2566\n",
      "Epoch [4/5], Step [5832/10336], Loss: 0.0406\n",
      "Epoch [4/5], Step [5834/10336], Loss: 0.0822\n",
      "Epoch [4/5], Step [5836/10336], Loss: 0.0180\n",
      "Epoch [4/5], Step [5838/10336], Loss: 0.2345\n",
      "Epoch [4/5], Step [5840/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [5842/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [5844/10336], Loss: 0.2663\n",
      "Epoch [4/5], Step [5846/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [5848/10336], Loss: 0.0613\n",
      "Epoch [4/5], Step [5850/10336], Loss: 0.0250\n",
      "Epoch [4/5], Step [5852/10336], Loss: 0.3057\n",
      "Epoch [4/5], Step [5854/10336], Loss: 1.3955\n",
      "Epoch [4/5], Step [5856/10336], Loss: 0.3037\n",
      "Epoch [4/5], Step [5858/10336], Loss: 0.2001\n",
      "Epoch [4/5], Step [5860/10336], Loss: 0.1251\n",
      "Epoch [4/5], Step [5862/10336], Loss: 0.0343\n",
      "Epoch [4/5], Step [5864/10336], Loss: 0.7304\n",
      "Epoch [4/5], Step [5866/10336], Loss: 0.0132\n",
      "Epoch [4/5], Step [5868/10336], Loss: 1.6593\n",
      "Epoch [4/5], Step [5870/10336], Loss: 0.9032\n",
      "Epoch [4/5], Step [5872/10336], Loss: 0.1178\n",
      "Epoch [4/5], Step [5874/10336], Loss: 0.3014\n",
      "Epoch [4/5], Step [5876/10336], Loss: 1.3989\n",
      "Epoch [4/5], Step [5878/10336], Loss: 0.1215\n",
      "Epoch [4/5], Step [5880/10336], Loss: 0.0603\n",
      "Epoch [4/5], Step [5882/10336], Loss: 0.0219\n",
      "Epoch [4/5], Step [5884/10336], Loss: 0.0052\n",
      "Epoch [4/5], Step [5886/10336], Loss: 0.1794\n",
      "Epoch [4/5], Step [5888/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [5890/10336], Loss: 0.0769\n",
      "Epoch [4/5], Step [5892/10336], Loss: 0.2630\n",
      "Epoch [4/5], Step [5894/10336], Loss: 0.3032\n",
      "Epoch [4/5], Step [5896/10336], Loss: 2.4712\n",
      "Epoch [4/5], Step [5898/10336], Loss: 0.0305\n",
      "Epoch [4/5], Step [5900/10336], Loss: 0.1974\n",
      "Epoch [4/5], Step [5902/10336], Loss: 0.0135\n",
      "Epoch [4/5], Step [5904/10336], Loss: 0.6005\n",
      "Epoch [4/5], Step [5906/10336], Loss: 0.4795\n",
      "Epoch [4/5], Step [5908/10336], Loss: 0.1648\n",
      "Epoch [4/5], Step [5910/10336], Loss: 0.3841\n",
      "Epoch [4/5], Step [5912/10336], Loss: 2.0022\n",
      "Epoch [4/5], Step [5914/10336], Loss: 0.0789\n",
      "Epoch [4/5], Step [5916/10336], Loss: 0.3237\n",
      "Epoch [4/5], Step [5918/10336], Loss: 0.0574\n",
      "Epoch [4/5], Step [5920/10336], Loss: 0.6103\n",
      "Epoch [4/5], Step [5922/10336], Loss: 0.0238\n",
      "Epoch [4/5], Step [5924/10336], Loss: 0.0072\n",
      "Epoch [4/5], Step [5926/10336], Loss: 0.4214\n",
      "Epoch [4/5], Step [5928/10336], Loss: 0.8589\n",
      "Epoch [4/5], Step [5930/10336], Loss: 0.0344\n",
      "Epoch [4/5], Step [5932/10336], Loss: 0.6066\n",
      "Epoch [4/5], Step [5934/10336], Loss: 0.2589\n",
      "Epoch [4/5], Step [5936/10336], Loss: 4.0505\n",
      "Epoch [4/5], Step [5938/10336], Loss: 0.1415\n",
      "Epoch [4/5], Step [5940/10336], Loss: 1.6476\n",
      "Epoch [4/5], Step [5942/10336], Loss: 0.7076\n",
      "Epoch [4/5], Step [5944/10336], Loss: 0.0168\n",
      "Epoch [4/5], Step [5946/10336], Loss: 0.1508\n",
      "Epoch [4/5], Step [5948/10336], Loss: 0.0702\n",
      "Epoch [4/5], Step [5950/10336], Loss: 0.7145\n",
      "Epoch [4/5], Step [5952/10336], Loss: 0.0373\n",
      "Epoch [4/5], Step [5954/10336], Loss: 0.1415\n",
      "Epoch [4/5], Step [5956/10336], Loss: 0.8040\n",
      "Epoch [4/5], Step [5958/10336], Loss: 0.0069\n",
      "Epoch [4/5], Step [5960/10336], Loss: 0.1805\n",
      "Epoch [4/5], Step [5962/10336], Loss: 0.0379\n",
      "Epoch [4/5], Step [5964/10336], Loss: 0.2815\n",
      "Epoch [4/5], Step [5966/10336], Loss: 1.7883\n",
      "Epoch [4/5], Step [5968/10336], Loss: 4.0919\n",
      "Epoch [4/5], Step [5970/10336], Loss: 0.1523\n",
      "Epoch [4/5], Step [5972/10336], Loss: 0.0887\n",
      "Epoch [4/5], Step [5974/10336], Loss: 0.2488\n",
      "Epoch [4/5], Step [5976/10336], Loss: 0.6824\n",
      "Epoch [4/5], Step [5978/10336], Loss: 1.2946\n",
      "Epoch [4/5], Step [5980/10336], Loss: 0.4683\n",
      "Epoch [4/5], Step [5982/10336], Loss: 0.2502\n",
      "Epoch [4/5], Step [5984/10336], Loss: 1.5205\n",
      "Epoch [4/5], Step [5986/10336], Loss: 0.1402\n",
      "Epoch [4/5], Step [5988/10336], Loss: 1.2100\n",
      "Epoch [4/5], Step [5990/10336], Loss: 0.0107\n",
      "Epoch [4/5], Step [5992/10336], Loss: 0.4038\n",
      "Epoch [4/5], Step [5994/10336], Loss: 0.0277\n",
      "Epoch [4/5], Step [5996/10336], Loss: 0.1959\n",
      "Epoch [4/5], Step [5998/10336], Loss: 0.0052\n",
      "Epoch [4/5], Step [6000/10336], Loss: 1.9479\n",
      "Epoch [4/5], Step [6002/10336], Loss: 1.9151\n",
      "Epoch [4/5], Step [6004/10336], Loss: 0.2155\n",
      "Epoch [4/5], Step [6006/10336], Loss: 0.1634\n",
      "Epoch [4/5], Step [6008/10336], Loss: 2.5145\n",
      "Epoch [4/5], Step [6010/10336], Loss: 0.9142\n",
      "Epoch [4/5], Step [6012/10336], Loss: 0.1086\n",
      "Epoch [4/5], Step [6014/10336], Loss: 0.0777\n",
      "Epoch [4/5], Step [6016/10336], Loss: 0.7273\n",
      "Epoch [4/5], Step [6018/10336], Loss: 0.0525\n",
      "Epoch [4/5], Step [6020/10336], Loss: 0.2222\n",
      "Epoch [4/5], Step [6022/10336], Loss: 0.2156\n",
      "Epoch [4/5], Step [6024/10336], Loss: 0.2052\n",
      "Epoch [4/5], Step [6026/10336], Loss: 2.1784\n",
      "Epoch [4/5], Step [6028/10336], Loss: 0.2462\n",
      "Epoch [4/5], Step [6030/10336], Loss: 4.2111\n",
      "Epoch [4/5], Step [6032/10336], Loss: 0.1723\n",
      "Epoch [4/5], Step [6034/10336], Loss: 0.0259\n",
      "Epoch [4/5], Step [6036/10336], Loss: 1.2974\n",
      "Epoch [4/5], Step [6038/10336], Loss: 0.6674\n",
      "Epoch [4/5], Step [6040/10336], Loss: 3.4533\n",
      "Epoch [4/5], Step [6042/10336], Loss: 0.0599\n",
      "Epoch [4/5], Step [6044/10336], Loss: 0.4553\n",
      "Epoch [4/5], Step [6046/10336], Loss: 0.2053\n",
      "Epoch [4/5], Step [6048/10336], Loss: 0.0157\n",
      "Epoch [4/5], Step [6050/10336], Loss: 2.0501\n",
      "Epoch [4/5], Step [6052/10336], Loss: 0.1182\n",
      "Epoch [4/5], Step [6054/10336], Loss: 0.4759\n",
      "Epoch [4/5], Step [6056/10336], Loss: 0.7855\n",
      "Epoch [4/5], Step [6058/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [6060/10336], Loss: 0.0648\n",
      "Epoch [4/5], Step [6062/10336], Loss: 0.3324\n",
      "Epoch [4/5], Step [6064/10336], Loss: 0.1908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [6066/10336], Loss: 1.0166\n",
      "Epoch [4/5], Step [6068/10336], Loss: 0.1628\n",
      "Epoch [4/5], Step [6070/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [6072/10336], Loss: 0.0537\n",
      "Epoch [4/5], Step [6074/10336], Loss: 0.2611\n",
      "Epoch [4/5], Step [6076/10336], Loss: 0.7550\n",
      "Epoch [4/5], Step [6078/10336], Loss: 0.2810\n",
      "Epoch [4/5], Step [6080/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [6082/10336], Loss: 0.4616\n",
      "Epoch [4/5], Step [6084/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [6086/10336], Loss: 0.0172\n",
      "Epoch [4/5], Step [6088/10336], Loss: 0.4772\n",
      "Epoch [4/5], Step [6090/10336], Loss: 0.5113\n",
      "Epoch [4/5], Step [6092/10336], Loss: 0.4743\n",
      "Epoch [4/5], Step [6094/10336], Loss: 0.3234\n",
      "Epoch [4/5], Step [6096/10336], Loss: 0.0029\n",
      "Epoch [4/5], Step [6098/10336], Loss: 0.0029\n",
      "Epoch [4/5], Step [6100/10336], Loss: 0.0197\n",
      "Epoch [4/5], Step [6102/10336], Loss: 0.9130\n",
      "Epoch [4/5], Step [6104/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [6106/10336], Loss: 0.1326\n",
      "Epoch [4/5], Step [6108/10336], Loss: 0.0498\n",
      "Epoch [4/5], Step [6110/10336], Loss: 0.2160\n",
      "Epoch [4/5], Step [6112/10336], Loss: 0.1055\n",
      "Epoch [4/5], Step [6114/10336], Loss: 0.0072\n",
      "Epoch [4/5], Step [6116/10336], Loss: 0.0632\n",
      "Epoch [4/5], Step [6118/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [6120/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [6122/10336], Loss: 0.9875\n",
      "Epoch [4/5], Step [6124/10336], Loss: 0.1572\n",
      "Epoch [4/5], Step [6126/10336], Loss: 0.2247\n",
      "Epoch [4/5], Step [6128/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [6130/10336], Loss: 0.2386\n",
      "Epoch [4/5], Step [6132/10336], Loss: 0.1341\n",
      "Epoch [4/5], Step [6134/10336], Loss: 0.1535\n",
      "Epoch [4/5], Step [6136/10336], Loss: 2.6066\n",
      "Epoch [4/5], Step [6138/10336], Loss: 2.8146\n",
      "Epoch [4/5], Step [6140/10336], Loss: 0.9654\n",
      "Epoch [4/5], Step [6142/10336], Loss: 0.3454\n",
      "Epoch [4/5], Step [6144/10336], Loss: 0.0178\n",
      "Epoch [4/5], Step [6146/10336], Loss: 2.9492\n",
      "Epoch [4/5], Step [6148/10336], Loss: 0.0787\n",
      "Epoch [4/5], Step [6150/10336], Loss: 0.2325\n",
      "Epoch [4/5], Step [6152/10336], Loss: 2.1912\n",
      "Epoch [4/5], Step [6154/10336], Loss: 0.0555\n",
      "Epoch [4/5], Step [6156/10336], Loss: 0.2390\n",
      "Epoch [4/5], Step [6158/10336], Loss: 0.0147\n",
      "Epoch [4/5], Step [6160/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [6162/10336], Loss: 0.0379\n",
      "Epoch [4/5], Step [6164/10336], Loss: 0.3321\n",
      "Epoch [4/5], Step [6166/10336], Loss: 1.3340\n",
      "Epoch [4/5], Step [6168/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [6170/10336], Loss: 2.5000\n",
      "Epoch [4/5], Step [6172/10336], Loss: 1.6611\n",
      "Epoch [4/5], Step [6174/10336], Loss: 0.0493\n",
      "Epoch [4/5], Step [6176/10336], Loss: 0.0795\n",
      "Epoch [4/5], Step [6178/10336], Loss: 0.3006\n",
      "Epoch [4/5], Step [6180/10336], Loss: 0.9043\n",
      "Epoch [4/5], Step [6182/10336], Loss: 0.9037\n",
      "Epoch [4/5], Step [6184/10336], Loss: 0.4627\n",
      "Epoch [4/5], Step [6186/10336], Loss: 0.0271\n",
      "Epoch [4/5], Step [6188/10336], Loss: 0.7097\n",
      "Epoch [4/5], Step [6190/10336], Loss: 0.2540\n",
      "Epoch [4/5], Step [6192/10336], Loss: 0.0099\n",
      "Epoch [4/5], Step [6194/10336], Loss: 2.8681\n",
      "Epoch [4/5], Step [6196/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [6198/10336], Loss: 0.0337\n",
      "Epoch [4/5], Step [6200/10336], Loss: 3.0260\n",
      "Epoch [4/5], Step [6202/10336], Loss: 0.1988\n",
      "Epoch [4/5], Step [6204/10336], Loss: 0.4155\n",
      "Epoch [4/5], Step [6206/10336], Loss: 0.0429\n",
      "Epoch [4/5], Step [6208/10336], Loss: 0.2972\n",
      "Epoch [4/5], Step [6210/10336], Loss: 0.7537\n",
      "Epoch [4/5], Step [6212/10336], Loss: 1.4150\n",
      "Epoch [4/5], Step [6214/10336], Loss: 2.9737\n",
      "Epoch [4/5], Step [6216/10336], Loss: 0.2428\n",
      "Epoch [4/5], Step [6218/10336], Loss: 0.1041\n",
      "Epoch [4/5], Step [6220/10336], Loss: 0.5004\n",
      "Epoch [4/5], Step [6222/10336], Loss: 0.0151\n",
      "Epoch [4/5], Step [6224/10336], Loss: 0.0116\n",
      "Epoch [4/5], Step [6226/10336], Loss: 0.4160\n",
      "Epoch [4/5], Step [6228/10336], Loss: 3.3586\n",
      "Epoch [4/5], Step [6230/10336], Loss: 0.6749\n",
      "Epoch [4/5], Step [6232/10336], Loss: 0.0174\n",
      "Epoch [4/5], Step [6234/10336], Loss: 0.6274\n",
      "Epoch [4/5], Step [6236/10336], Loss: 1.2792\n",
      "Epoch [4/5], Step [6238/10336], Loss: 2.5137\n",
      "Epoch [4/5], Step [6240/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [6242/10336], Loss: 0.0311\n",
      "Epoch [4/5], Step [6244/10336], Loss: 0.6613\n",
      "Epoch [4/5], Step [6246/10336], Loss: 1.3275\n",
      "Epoch [4/5], Step [6248/10336], Loss: 0.2152\n",
      "Epoch [4/5], Step [6250/10336], Loss: 1.1023\n",
      "Epoch [4/5], Step [6252/10336], Loss: 0.1808\n",
      "Epoch [4/5], Step [6254/10336], Loss: 0.1646\n",
      "Epoch [4/5], Step [6256/10336], Loss: 0.0043\n",
      "Epoch [4/5], Step [6258/10336], Loss: 0.6330\n",
      "Epoch [4/5], Step [6260/10336], Loss: 0.4190\n",
      "Epoch [4/5], Step [6262/10336], Loss: 0.0055\n",
      "Epoch [4/5], Step [6264/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [6266/10336], Loss: 0.1716\n",
      "Epoch [4/5], Step [6268/10336], Loss: 0.1008\n",
      "Epoch [4/5], Step [6270/10336], Loss: 0.2876\n",
      "Epoch [4/5], Step [6272/10336], Loss: 0.9654\n",
      "Epoch [4/5], Step [6274/10336], Loss: 0.6524\n",
      "Epoch [4/5], Step [6276/10336], Loss: 0.8329\n",
      "Epoch [4/5], Step [6278/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [6280/10336], Loss: 0.6843\n",
      "Epoch [4/5], Step [6282/10336], Loss: 0.3091\n",
      "Epoch [4/5], Step [6284/10336], Loss: 0.2034\n",
      "Epoch [4/5], Step [6286/10336], Loss: 0.1655\n",
      "Epoch [4/5], Step [6288/10336], Loss: 0.0844\n",
      "Epoch [4/5], Step [6290/10336], Loss: 0.0856\n",
      "Epoch [4/5], Step [6292/10336], Loss: 0.0913\n",
      "Epoch [4/5], Step [6294/10336], Loss: 2.6144\n",
      "Epoch [4/5], Step [6296/10336], Loss: 0.1846\n",
      "Epoch [4/5], Step [6298/10336], Loss: 1.4667\n",
      "Epoch [4/5], Step [6300/10336], Loss: 0.0087\n",
      "Epoch [4/5], Step [6302/10336], Loss: 0.0606\n",
      "Epoch [4/5], Step [6304/10336], Loss: 1.3152\n",
      "Epoch [4/5], Step [6306/10336], Loss: 1.0885\n",
      "Epoch [4/5], Step [6308/10336], Loss: 0.0153\n",
      "Epoch [4/5], Step [6310/10336], Loss: 2.9526\n",
      "Epoch [4/5], Step [6312/10336], Loss: 0.2335\n",
      "Epoch [4/5], Step [6314/10336], Loss: 0.4732\n",
      "Epoch [4/5], Step [6316/10336], Loss: 0.0449\n",
      "Epoch [4/5], Step [6318/10336], Loss: 3.1902\n",
      "Epoch [4/5], Step [6320/10336], Loss: 0.0513\n",
      "Epoch [4/5], Step [6322/10336], Loss: 0.0086\n",
      "Epoch [4/5], Step [6324/10336], Loss: 2.5218\n",
      "Epoch [4/5], Step [6326/10336], Loss: 0.1915\n",
      "Epoch [4/5], Step [6328/10336], Loss: 0.3071\n",
      "Epoch [4/5], Step [6330/10336], Loss: 1.0409\n",
      "Epoch [4/5], Step [6332/10336], Loss: 1.1491\n",
      "Epoch [4/5], Step [6334/10336], Loss: 1.7391\n",
      "Epoch [4/5], Step [6336/10336], Loss: 0.3067\n",
      "Epoch [4/5], Step [6338/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [6340/10336], Loss: 0.4495\n",
      "Epoch [4/5], Step [6342/10336], Loss: 2.1660\n",
      "Epoch [4/5], Step [6344/10336], Loss: 1.4851\n",
      "Epoch [4/5], Step [6346/10336], Loss: 0.0376\n",
      "Epoch [4/5], Step [6348/10336], Loss: 0.0119\n",
      "Epoch [4/5], Step [6350/10336], Loss: 0.4798\n",
      "Epoch [4/5], Step [6352/10336], Loss: 0.0381\n",
      "Epoch [4/5], Step [6354/10336], Loss: 0.0398\n",
      "Epoch [4/5], Step [6356/10336], Loss: 0.2706\n",
      "Epoch [4/5], Step [6358/10336], Loss: 0.1174\n",
      "Epoch [4/5], Step [6360/10336], Loss: 0.0885\n",
      "Epoch [4/5], Step [6362/10336], Loss: 0.3612\n",
      "Epoch [4/5], Step [6364/10336], Loss: 0.1894\n",
      "Epoch [4/5], Step [6366/10336], Loss: 0.6183\n",
      "Epoch [4/5], Step [6368/10336], Loss: 0.0135\n",
      "Epoch [4/5], Step [6370/10336], Loss: 0.1502\n",
      "Epoch [4/5], Step [6372/10336], Loss: 0.2643\n",
      "Epoch [4/5], Step [6374/10336], Loss: 0.4969\n",
      "Epoch [4/5], Step [6376/10336], Loss: 0.6775\n",
      "Epoch [4/5], Step [6378/10336], Loss: 0.0593\n",
      "Epoch [4/5], Step [6380/10336], Loss: 0.1643\n",
      "Epoch [4/5], Step [6382/10336], Loss: 1.1006\n",
      "Epoch [4/5], Step [6384/10336], Loss: 0.0064\n",
      "Epoch [4/5], Step [6386/10336], Loss: 0.0562\n",
      "Epoch [4/5], Step [6388/10336], Loss: 0.4318\n",
      "Epoch [4/5], Step [6390/10336], Loss: 1.0927\n",
      "Epoch [4/5], Step [6392/10336], Loss: 1.3006\n",
      "Epoch [4/5], Step [6394/10336], Loss: 0.3264\n",
      "Epoch [4/5], Step [6396/10336], Loss: 0.0708\n",
      "Epoch [4/5], Step [6398/10336], Loss: 0.1051\n",
      "Epoch [4/5], Step [6400/10336], Loss: 0.3402\n",
      "Epoch [4/5], Step [6402/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [6404/10336], Loss: 0.0454\n",
      "Epoch [4/5], Step [6406/10336], Loss: 2.0353\n",
      "Epoch [4/5], Step [6408/10336], Loss: 0.0296\n",
      "Epoch [4/5], Step [6410/10336], Loss: 0.1175\n",
      "Epoch [4/5], Step [6412/10336], Loss: 0.3076\n",
      "Epoch [4/5], Step [6414/10336], Loss: 0.1864\n",
      "Epoch [4/5], Step [6416/10336], Loss: 0.0270\n",
      "Epoch [4/5], Step [6418/10336], Loss: 0.1052\n",
      "Epoch [4/5], Step [6420/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [6422/10336], Loss: 0.5084\n",
      "Epoch [4/5], Step [6424/10336], Loss: 0.2386\n",
      "Epoch [4/5], Step [6426/10336], Loss: 0.1273\n",
      "Epoch [4/5], Step [6428/10336], Loss: 0.3603\n",
      "Epoch [4/5], Step [6430/10336], Loss: 0.2658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [6432/10336], Loss: 0.0538\n",
      "Epoch [4/5], Step [6434/10336], Loss: 0.0209\n",
      "Epoch [4/5], Step [6436/10336], Loss: 0.2021\n",
      "Epoch [4/5], Step [6438/10336], Loss: 0.3387\n",
      "Epoch [4/5], Step [6440/10336], Loss: 0.2906\n",
      "Epoch [4/5], Step [6442/10336], Loss: 0.0896\n",
      "Epoch [4/5], Step [6444/10336], Loss: 0.1041\n",
      "Epoch [4/5], Step [6446/10336], Loss: 0.0077\n",
      "Epoch [4/5], Step [6448/10336], Loss: 0.0570\n",
      "Epoch [4/5], Step [6450/10336], Loss: 0.0055\n",
      "Epoch [4/5], Step [6452/10336], Loss: 0.5098\n",
      "Epoch [4/5], Step [6454/10336], Loss: 0.4150\n",
      "Epoch [4/5], Step [6456/10336], Loss: 2.2749\n",
      "Epoch [4/5], Step [6458/10336], Loss: 2.1840\n",
      "Epoch [4/5], Step [6460/10336], Loss: 0.0571\n",
      "Epoch [4/5], Step [6462/10336], Loss: 0.0355\n",
      "Epoch [4/5], Step [6464/10336], Loss: 1.2614\n",
      "Epoch [4/5], Step [6466/10336], Loss: 2.5643\n",
      "Epoch [4/5], Step [6468/10336], Loss: 0.4244\n",
      "Epoch [4/5], Step [6470/10336], Loss: 0.2552\n",
      "Epoch [4/5], Step [6472/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [6474/10336], Loss: 0.0848\n",
      "Epoch [4/5], Step [6476/10336], Loss: 5.9586\n",
      "Epoch [4/5], Step [6478/10336], Loss: 0.6078\n",
      "Epoch [4/5], Step [6480/10336], Loss: 0.3203\n",
      "Epoch [4/5], Step [6482/10336], Loss: 0.5625\n",
      "Epoch [4/5], Step [6484/10336], Loss: 0.2312\n",
      "Epoch [4/5], Step [6486/10336], Loss: 0.2781\n",
      "Epoch [4/5], Step [6488/10336], Loss: 1.1332\n",
      "Epoch [4/5], Step [6490/10336], Loss: 0.1645\n",
      "Epoch [4/5], Step [6492/10336], Loss: 0.0916\n",
      "Epoch [4/5], Step [6494/10336], Loss: 0.1373\n",
      "Epoch [4/5], Step [6496/10336], Loss: 0.1906\n",
      "Epoch [4/5], Step [6498/10336], Loss: 0.0440\n",
      "Epoch [4/5], Step [6500/10336], Loss: 0.1633\n",
      "Epoch [4/5], Step [6502/10336], Loss: 0.0118\n",
      "Epoch [4/5], Step [6504/10336], Loss: 1.8534\n",
      "Epoch [4/5], Step [6506/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [6508/10336], Loss: 5.4893\n",
      "Epoch [4/5], Step [6510/10336], Loss: 0.0298\n",
      "Epoch [4/5], Step [6512/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [6514/10336], Loss: 0.0038\n",
      "Epoch [4/5], Step [6516/10336], Loss: 0.0412\n",
      "Epoch [4/5], Step [6518/10336], Loss: 0.2090\n",
      "Epoch [4/5], Step [6520/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [6522/10336], Loss: 0.7410\n",
      "Epoch [4/5], Step [6524/10336], Loss: 0.9712\n",
      "Epoch [4/5], Step [6526/10336], Loss: 0.0826\n",
      "Epoch [4/5], Step [6528/10336], Loss: 5.1572\n",
      "Epoch [4/5], Step [6530/10336], Loss: 0.1601\n",
      "Epoch [4/5], Step [6532/10336], Loss: 0.1022\n",
      "Epoch [4/5], Step [6534/10336], Loss: 0.0593\n",
      "Epoch [4/5], Step [6536/10336], Loss: 2.0818\n",
      "Epoch [4/5], Step [6538/10336], Loss: 1.0304\n",
      "Epoch [4/5], Step [6540/10336], Loss: 0.0170\n",
      "Epoch [4/5], Step [6542/10336], Loss: 4.7542\n",
      "Epoch [4/5], Step [6544/10336], Loss: 0.0753\n",
      "Epoch [4/5], Step [6546/10336], Loss: 0.4725\n",
      "Epoch [4/5], Step [6548/10336], Loss: 0.2669\n",
      "Epoch [4/5], Step [6550/10336], Loss: 1.7326\n",
      "Epoch [4/5], Step [6552/10336], Loss: 1.5044\n",
      "Epoch [4/5], Step [6554/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [6556/10336], Loss: 1.1085\n",
      "Epoch [4/5], Step [6558/10336], Loss: 1.1605\n",
      "Epoch [4/5], Step [6560/10336], Loss: 0.2552\n",
      "Epoch [4/5], Step [6562/10336], Loss: 0.0235\n",
      "Epoch [4/5], Step [6564/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [6566/10336], Loss: 0.0843\n",
      "Epoch [4/5], Step [6568/10336], Loss: 0.2703\n",
      "Epoch [4/5], Step [6570/10336], Loss: 0.1069\n",
      "Epoch [4/5], Step [6572/10336], Loss: 0.0762\n",
      "Epoch [4/5], Step [6574/10336], Loss: 0.1978\n",
      "Epoch [4/5], Step [6576/10336], Loss: 0.0215\n",
      "Epoch [4/5], Step [6578/10336], Loss: 0.9485\n",
      "Epoch [4/5], Step [6580/10336], Loss: 1.0062\n",
      "Epoch [4/5], Step [6582/10336], Loss: 0.2709\n",
      "Epoch [4/5], Step [6584/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [6586/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [6588/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [6590/10336], Loss: 2.1832\n",
      "Epoch [4/5], Step [6592/10336], Loss: 0.4378\n",
      "Epoch [4/5], Step [6594/10336], Loss: 0.0038\n",
      "Epoch [4/5], Step [6596/10336], Loss: 0.6901\n",
      "Epoch [4/5], Step [6598/10336], Loss: 0.6167\n",
      "Epoch [4/5], Step [6600/10336], Loss: 1.1237\n",
      "Epoch [4/5], Step [6602/10336], Loss: 0.0478\n",
      "Epoch [4/5], Step [6604/10336], Loss: 1.5703\n",
      "Epoch [4/5], Step [6606/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [6608/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [6610/10336], Loss: 0.2060\n",
      "Epoch [4/5], Step [6612/10336], Loss: 0.2147\n",
      "Epoch [4/5], Step [6614/10336], Loss: 1.6347\n",
      "Epoch [4/5], Step [6616/10336], Loss: 0.1220\n",
      "Epoch [4/5], Step [6618/10336], Loss: 1.8339\n",
      "Epoch [4/5], Step [6620/10336], Loss: 1.9141\n",
      "Epoch [4/5], Step [6622/10336], Loss: 0.7073\n",
      "Epoch [4/5], Step [6624/10336], Loss: 1.0540\n",
      "Epoch [4/5], Step [6626/10336], Loss: 3.5594\n",
      "Epoch [4/5], Step [6628/10336], Loss: 0.1788\n",
      "Epoch [4/5], Step [6630/10336], Loss: 0.3004\n",
      "Epoch [4/5], Step [6632/10336], Loss: 0.0045\n",
      "Epoch [4/5], Step [6634/10336], Loss: 0.0301\n",
      "Epoch [4/5], Step [6636/10336], Loss: 0.1095\n",
      "Epoch [4/5], Step [6638/10336], Loss: 0.3748\n",
      "Epoch [4/5], Step [6640/10336], Loss: 0.4137\n",
      "Epoch [4/5], Step [6642/10336], Loss: 0.0415\n",
      "Epoch [4/5], Step [6644/10336], Loss: 0.0256\n",
      "Epoch [4/5], Step [6646/10336], Loss: 0.9873\n",
      "Epoch [4/5], Step [6648/10336], Loss: 1.2270\n",
      "Epoch [4/5], Step [6650/10336], Loss: 0.0072\n",
      "Epoch [4/5], Step [6652/10336], Loss: 0.0869\n",
      "Epoch [4/5], Step [6654/10336], Loss: 0.2680\n",
      "Epoch [4/5], Step [6656/10336], Loss: 0.0148\n",
      "Epoch [4/5], Step [6658/10336], Loss: 3.9914\n",
      "Epoch [4/5], Step [6660/10336], Loss: 0.0401\n",
      "Epoch [4/5], Step [6662/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [6664/10336], Loss: 2.5911\n",
      "Epoch [4/5], Step [6666/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [6668/10336], Loss: 0.0604\n",
      "Epoch [4/5], Step [6670/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [6672/10336], Loss: 0.9782\n",
      "Epoch [4/5], Step [6674/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [6676/10336], Loss: 0.1393\n",
      "Epoch [4/5], Step [6678/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [6680/10336], Loss: 5.3779\n",
      "Epoch [4/5], Step [6682/10336], Loss: 0.1588\n",
      "Epoch [4/5], Step [6684/10336], Loss: 0.0137\n",
      "Epoch [4/5], Step [6686/10336], Loss: 0.0093\n",
      "Epoch [4/5], Step [6688/10336], Loss: 0.3501\n",
      "Epoch [4/5], Step [6690/10336], Loss: 0.6567\n",
      "Epoch [4/5], Step [6692/10336], Loss: 0.0215\n",
      "Epoch [4/5], Step [6694/10336], Loss: 0.1312\n",
      "Epoch [4/5], Step [6696/10336], Loss: 0.4678\n",
      "Epoch [4/5], Step [6698/10336], Loss: 0.3641\n",
      "Epoch [4/5], Step [6700/10336], Loss: 2.1353\n",
      "Epoch [4/5], Step [6702/10336], Loss: 0.0386\n",
      "Epoch [4/5], Step [6704/10336], Loss: 2.5865\n",
      "Epoch [4/5], Step [6706/10336], Loss: 0.1447\n",
      "Epoch [4/5], Step [6708/10336], Loss: 0.3692\n",
      "Epoch [4/5], Step [6710/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [6712/10336], Loss: 0.1731\n",
      "Epoch [4/5], Step [6714/10336], Loss: 0.0128\n",
      "Epoch [4/5], Step [6716/10336], Loss: 3.8572\n",
      "Epoch [4/5], Step [6718/10336], Loss: 1.8633\n",
      "Epoch [4/5], Step [6720/10336], Loss: 0.0567\n",
      "Epoch [4/5], Step [6722/10336], Loss: 1.5763\n",
      "Epoch [4/5], Step [6724/10336], Loss: 0.1382\n",
      "Epoch [4/5], Step [6726/10336], Loss: 0.0140\n",
      "Epoch [4/5], Step [6728/10336], Loss: 1.4237\n",
      "Epoch [4/5], Step [6730/10336], Loss: 0.5282\n",
      "Epoch [4/5], Step [6732/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [6734/10336], Loss: 0.4924\n",
      "Epoch [4/5], Step [6736/10336], Loss: 0.6679\n",
      "Epoch [4/5], Step [6738/10336], Loss: 0.0147\n",
      "Epoch [4/5], Step [6740/10336], Loss: 2.0340\n",
      "Epoch [4/5], Step [6742/10336], Loss: 0.0979\n",
      "Epoch [4/5], Step [6744/10336], Loss: 0.2623\n",
      "Epoch [4/5], Step [6746/10336], Loss: 0.4171\n",
      "Epoch [4/5], Step [6748/10336], Loss: 2.8219\n",
      "Epoch [4/5], Step [6750/10336], Loss: 0.3302\n",
      "Epoch [4/5], Step [6752/10336], Loss: 0.1971\n",
      "Epoch [4/5], Step [6754/10336], Loss: 0.0289\n",
      "Epoch [4/5], Step [6756/10336], Loss: 0.2321\n",
      "Epoch [4/5], Step [6758/10336], Loss: 0.9684\n",
      "Epoch [4/5], Step [6760/10336], Loss: 0.0103\n",
      "Epoch [4/5], Step [6762/10336], Loss: 0.0524\n",
      "Epoch [4/5], Step [6764/10336], Loss: 0.1171\n",
      "Epoch [4/5], Step [6766/10336], Loss: 0.7673\n",
      "Epoch [4/5], Step [6768/10336], Loss: 1.8791\n",
      "Epoch [4/5], Step [6770/10336], Loss: 0.2335\n",
      "Epoch [4/5], Step [6772/10336], Loss: 0.7304\n",
      "Epoch [4/5], Step [6774/10336], Loss: 0.0029\n",
      "Epoch [4/5], Step [6776/10336], Loss: 0.0107\n",
      "Epoch [4/5], Step [6778/10336], Loss: 0.0061\n",
      "Epoch [4/5], Step [6780/10336], Loss: 0.1326\n",
      "Epoch [4/5], Step [6782/10336], Loss: 0.3369\n",
      "Epoch [4/5], Step [6784/10336], Loss: 0.4528\n",
      "Epoch [4/5], Step [6786/10336], Loss: 0.0472\n",
      "Epoch [4/5], Step [6788/10336], Loss: 1.3339\n",
      "Epoch [4/5], Step [6790/10336], Loss: 0.2344\n",
      "Epoch [4/5], Step [6792/10336], Loss: 0.1378\n",
      "Epoch [4/5], Step [6794/10336], Loss: 1.3414\n",
      "Epoch [4/5], Step [6796/10336], Loss: 0.2730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [6798/10336], Loss: 0.0305\n",
      "Epoch [4/5], Step [6800/10336], Loss: 0.0084\n",
      "Epoch [4/5], Step [6802/10336], Loss: 0.0764\n",
      "Epoch [4/5], Step [6804/10336], Loss: 2.0046\n",
      "Epoch [4/5], Step [6806/10336], Loss: 0.2338\n",
      "Epoch [4/5], Step [6808/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [6810/10336], Loss: 0.0159\n",
      "Epoch [4/5], Step [6812/10336], Loss: 0.8957\n",
      "Epoch [4/5], Step [6814/10336], Loss: 0.0705\n",
      "Epoch [4/5], Step [6816/10336], Loss: 0.1781\n",
      "Epoch [4/5], Step [6818/10336], Loss: 0.4920\n",
      "Epoch [4/5], Step [6820/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [6822/10336], Loss: 0.0082\n",
      "Epoch [4/5], Step [6824/10336], Loss: 0.0080\n",
      "Epoch [4/5], Step [6826/10336], Loss: 0.0445\n",
      "Epoch [4/5], Step [6828/10336], Loss: 0.0143\n",
      "Epoch [4/5], Step [6830/10336], Loss: 0.0447\n",
      "Epoch [4/5], Step [6832/10336], Loss: 0.1888\n",
      "Epoch [4/5], Step [6834/10336], Loss: 0.1016\n",
      "Epoch [4/5], Step [6836/10336], Loss: 0.0411\n",
      "Epoch [4/5], Step [6838/10336], Loss: 0.1930\n",
      "Epoch [4/5], Step [6840/10336], Loss: 0.2686\n",
      "Epoch [4/5], Step [6842/10336], Loss: 0.1119\n",
      "Epoch [4/5], Step [6844/10336], Loss: 0.6879\n",
      "Epoch [4/5], Step [6846/10336], Loss: 0.3588\n",
      "Epoch [4/5], Step [6848/10336], Loss: 0.1993\n",
      "Epoch [4/5], Step [6850/10336], Loss: 0.1328\n",
      "Epoch [4/5], Step [6852/10336], Loss: 0.3635\n",
      "Epoch [4/5], Step [6854/10336], Loss: 0.1459\n",
      "Epoch [4/5], Step [6856/10336], Loss: 0.3321\n",
      "Epoch [4/5], Step [6858/10336], Loss: 1.9261\n",
      "Epoch [4/5], Step [6860/10336], Loss: 0.0096\n",
      "Epoch [4/5], Step [6862/10336], Loss: 0.1521\n",
      "Epoch [4/5], Step [6864/10336], Loss: 3.5687\n",
      "Epoch [4/5], Step [6866/10336], Loss: 2.6613\n",
      "Epoch [4/5], Step [6868/10336], Loss: 0.1494\n",
      "Epoch [4/5], Step [6870/10336], Loss: 1.5022\n",
      "Epoch [4/5], Step [6872/10336], Loss: 0.1356\n",
      "Epoch [4/5], Step [6874/10336], Loss: 0.8148\n",
      "Epoch [4/5], Step [6876/10336], Loss: 0.1352\n",
      "Epoch [4/5], Step [6878/10336], Loss: 0.1065\n",
      "Epoch [4/5], Step [6880/10336], Loss: 0.7012\n",
      "Epoch [4/5], Step [6882/10336], Loss: 0.0129\n",
      "Epoch [4/5], Step [6884/10336], Loss: 0.0129\n",
      "Epoch [4/5], Step [6886/10336], Loss: 1.6606\n",
      "Epoch [4/5], Step [6888/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [6890/10336], Loss: 0.4280\n",
      "Epoch [4/5], Step [6892/10336], Loss: 3.6900\n",
      "Epoch [4/5], Step [6894/10336], Loss: 3.3179\n",
      "Epoch [4/5], Step [6896/10336], Loss: 0.5482\n",
      "Epoch [4/5], Step [6898/10336], Loss: 0.1590\n",
      "Epoch [4/5], Step [6900/10336], Loss: 0.1441\n",
      "Epoch [4/5], Step [6902/10336], Loss: 0.3511\n",
      "Epoch [4/5], Step [6904/10336], Loss: 0.3155\n",
      "Epoch [4/5], Step [6906/10336], Loss: 0.6210\n",
      "Epoch [4/5], Step [6908/10336], Loss: 1.0444\n",
      "Epoch [4/5], Step [6910/10336], Loss: 0.0841\n",
      "Epoch [4/5], Step [6912/10336], Loss: 0.1158\n",
      "Epoch [4/5], Step [6914/10336], Loss: 0.0265\n",
      "Epoch [4/5], Step [6916/10336], Loss: 0.0248\n",
      "Epoch [4/5], Step [6918/10336], Loss: 0.0756\n",
      "Epoch [4/5], Step [6920/10336], Loss: 0.3855\n",
      "Epoch [4/5], Step [6922/10336], Loss: 0.2341\n",
      "Epoch [4/5], Step [6924/10336], Loss: 0.7313\n",
      "Epoch [4/5], Step [6926/10336], Loss: 0.7632\n",
      "Epoch [4/5], Step [6928/10336], Loss: 0.1270\n",
      "Epoch [4/5], Step [6930/10336], Loss: 0.0985\n",
      "Epoch [4/5], Step [6932/10336], Loss: 2.3719\n",
      "Epoch [4/5], Step [6934/10336], Loss: 0.4764\n",
      "Epoch [4/5], Step [6936/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [6938/10336], Loss: 2.0333\n",
      "Epoch [4/5], Step [6940/10336], Loss: 0.1134\n",
      "Epoch [4/5], Step [6942/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [6944/10336], Loss: 0.0175\n",
      "Epoch [4/5], Step [6946/10336], Loss: 0.1461\n",
      "Epoch [4/5], Step [6948/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [6950/10336], Loss: 0.0406\n",
      "Epoch [4/5], Step [6952/10336], Loss: 0.0081\n",
      "Epoch [4/5], Step [6954/10336], Loss: 0.6646\n",
      "Epoch [4/5], Step [6956/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [6958/10336], Loss: 0.3238\n",
      "Epoch [4/5], Step [6960/10336], Loss: 1.4137\n",
      "Epoch [4/5], Step [6962/10336], Loss: 0.1518\n",
      "Epoch [4/5], Step [6964/10336], Loss: 2.1977\n",
      "Epoch [4/5], Step [6966/10336], Loss: 0.0181\n",
      "Epoch [4/5], Step [6968/10336], Loss: 3.9096\n",
      "Epoch [4/5], Step [6970/10336], Loss: 0.0725\n",
      "Epoch [4/5], Step [6972/10336], Loss: 0.9175\n",
      "Epoch [4/5], Step [6974/10336], Loss: 0.2191\n",
      "Epoch [4/5], Step [6976/10336], Loss: 0.3937\n",
      "Epoch [4/5], Step [6978/10336], Loss: 0.0273\n",
      "Epoch [4/5], Step [6980/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [6982/10336], Loss: 1.2701\n",
      "Epoch [4/5], Step [6984/10336], Loss: 0.1396\n",
      "Epoch [4/5], Step [6986/10336], Loss: 0.2407\n",
      "Epoch [4/5], Step [6988/10336], Loss: 0.4821\n",
      "Epoch [4/5], Step [6990/10336], Loss: 0.1267\n",
      "Epoch [4/5], Step [6992/10336], Loss: 0.0888\n",
      "Epoch [4/5], Step [6994/10336], Loss: 0.9863\n",
      "Epoch [4/5], Step [6996/10336], Loss: 0.1765\n",
      "Epoch [4/5], Step [6998/10336], Loss: 0.0098\n",
      "Epoch [4/5], Step [7000/10336], Loss: 0.0661\n",
      "Epoch [4/5], Step [7002/10336], Loss: 0.1814\n",
      "Epoch [4/5], Step [7004/10336], Loss: 0.0533\n",
      "Epoch [4/5], Step [7006/10336], Loss: 0.4989\n",
      "Epoch [4/5], Step [7008/10336], Loss: 0.2787\n",
      "Epoch [4/5], Step [7010/10336], Loss: 0.6047\n",
      "Epoch [4/5], Step [7012/10336], Loss: 0.1415\n",
      "Epoch [4/5], Step [7014/10336], Loss: 0.0241\n",
      "Epoch [4/5], Step [7016/10336], Loss: 0.4811\n",
      "Epoch [4/5], Step [7018/10336], Loss: 0.9843\n",
      "Epoch [4/5], Step [7020/10336], Loss: 0.2490\n",
      "Epoch [4/5], Step [7022/10336], Loss: 0.2504\n",
      "Epoch [4/5], Step [7024/10336], Loss: 0.9564\n",
      "Epoch [4/5], Step [7026/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [7028/10336], Loss: 0.0090\n",
      "Epoch [4/5], Step [7030/10336], Loss: 0.0141\n",
      "Epoch [4/5], Step [7032/10336], Loss: 0.2451\n",
      "Epoch [4/5], Step [7034/10336], Loss: 0.9300\n",
      "Epoch [4/5], Step [7036/10336], Loss: 0.1448\n",
      "Epoch [4/5], Step [7038/10336], Loss: 0.1174\n",
      "Epoch [4/5], Step [7040/10336], Loss: 0.1351\n",
      "Epoch [4/5], Step [7042/10336], Loss: 0.4540\n",
      "Epoch [4/5], Step [7044/10336], Loss: 0.6004\n",
      "Epoch [4/5], Step [7046/10336], Loss: 0.3050\n",
      "Epoch [4/5], Step [7048/10336], Loss: 0.0812\n",
      "Epoch [4/5], Step [7050/10336], Loss: 1.0480\n",
      "Epoch [4/5], Step [7052/10336], Loss: 1.4781\n",
      "Epoch [4/5], Step [7054/10336], Loss: 0.5750\n",
      "Epoch [4/5], Step [7056/10336], Loss: 0.3606\n",
      "Epoch [4/5], Step [7058/10336], Loss: 0.3601\n",
      "Epoch [4/5], Step [7060/10336], Loss: 0.3818\n",
      "Epoch [4/5], Step [7062/10336], Loss: 0.4358\n",
      "Epoch [4/5], Step [7064/10336], Loss: 0.2347\n",
      "Epoch [4/5], Step [7066/10336], Loss: 0.4421\n",
      "Epoch [4/5], Step [7068/10336], Loss: 0.1472\n",
      "Epoch [4/5], Step [7070/10336], Loss: 0.2023\n",
      "Epoch [4/5], Step [7072/10336], Loss: 0.0263\n",
      "Epoch [4/5], Step [7074/10336], Loss: 0.1017\n",
      "Epoch [4/5], Step [7076/10336], Loss: 0.3145\n",
      "Epoch [4/5], Step [7078/10336], Loss: 0.0735\n",
      "Epoch [4/5], Step [7080/10336], Loss: 0.0063\n",
      "Epoch [4/5], Step [7082/10336], Loss: 0.2248\n",
      "Epoch [4/5], Step [7084/10336], Loss: 4.5890\n",
      "Epoch [4/5], Step [7086/10336], Loss: 0.0255\n",
      "Epoch [4/5], Step [7088/10336], Loss: 2.3842\n",
      "Epoch [4/5], Step [7090/10336], Loss: 0.1943\n",
      "Epoch [4/5], Step [7092/10336], Loss: 0.1144\n",
      "Epoch [4/5], Step [7094/10336], Loss: 0.1064\n",
      "Epoch [4/5], Step [7096/10336], Loss: 0.0600\n",
      "Epoch [4/5], Step [7098/10336], Loss: 1.7545\n",
      "Epoch [4/5], Step [7100/10336], Loss: 1.6637\n",
      "Epoch [4/5], Step [7102/10336], Loss: 0.2530\n",
      "Epoch [4/5], Step [7104/10336], Loss: 1.1519\n",
      "Epoch [4/5], Step [7106/10336], Loss: 0.3724\n",
      "Epoch [4/5], Step [7108/10336], Loss: 0.1252\n",
      "Epoch [4/5], Step [7110/10336], Loss: 0.1672\n",
      "Epoch [4/5], Step [7112/10336], Loss: 0.7241\n",
      "Epoch [4/5], Step [7114/10336], Loss: 0.3180\n",
      "Epoch [4/5], Step [7116/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [7118/10336], Loss: 0.0758\n",
      "Epoch [4/5], Step [7120/10336], Loss: 0.2014\n",
      "Epoch [4/5], Step [7122/10336], Loss: 0.2955\n",
      "Epoch [4/5], Step [7124/10336], Loss: 0.0793\n",
      "Epoch [4/5], Step [7126/10336], Loss: 0.3347\n",
      "Epoch [4/5], Step [7128/10336], Loss: 0.0235\n",
      "Epoch [4/5], Step [7130/10336], Loss: 0.3319\n",
      "Epoch [4/5], Step [7132/10336], Loss: 3.0944\n",
      "Epoch [4/5], Step [7134/10336], Loss: 0.6378\n",
      "Epoch [4/5], Step [7136/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [7138/10336], Loss: 0.1741\n",
      "Epoch [4/5], Step [7140/10336], Loss: 1.5786\n",
      "Epoch [4/5], Step [7142/10336], Loss: 3.2492\n",
      "Epoch [4/5], Step [7144/10336], Loss: 3.1735\n",
      "Epoch [4/5], Step [7146/10336], Loss: 0.0231\n",
      "Epoch [4/5], Step [7148/10336], Loss: 0.1012\n",
      "Epoch [4/5], Step [7150/10336], Loss: 0.3327\n",
      "Epoch [4/5], Step [7152/10336], Loss: 0.0053\n",
      "Epoch [4/5], Step [7154/10336], Loss: 0.0107\n",
      "Epoch [4/5], Step [7156/10336], Loss: 1.5009\n",
      "Epoch [4/5], Step [7158/10336], Loss: 0.3052\n",
      "Epoch [4/5], Step [7160/10336], Loss: 0.2128\n",
      "Epoch [4/5], Step [7162/10336], Loss: 1.2777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [7164/10336], Loss: 0.4346\n",
      "Epoch [4/5], Step [7166/10336], Loss: 0.1212\n",
      "Epoch [4/5], Step [7168/10336], Loss: 0.1192\n",
      "Epoch [4/5], Step [7170/10336], Loss: 0.0775\n",
      "Epoch [4/5], Step [7172/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [7174/10336], Loss: 0.2632\n",
      "Epoch [4/5], Step [7176/10336], Loss: 0.0363\n",
      "Epoch [4/5], Step [7178/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [7180/10336], Loss: 0.1141\n",
      "Epoch [4/5], Step [7182/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [7184/10336], Loss: 1.2598\n",
      "Epoch [4/5], Step [7186/10336], Loss: 0.7921\n",
      "Epoch [4/5], Step [7188/10336], Loss: 0.0065\n",
      "Epoch [4/5], Step [7190/10336], Loss: 0.0072\n",
      "Epoch [4/5], Step [7192/10336], Loss: 0.0191\n",
      "Epoch [4/5], Step [7194/10336], Loss: 0.0355\n",
      "Epoch [4/5], Step [7196/10336], Loss: 3.6795\n",
      "Epoch [4/5], Step [7198/10336], Loss: 1.0037\n",
      "Epoch [4/5], Step [7200/10336], Loss: 0.3410\n",
      "Epoch [4/5], Step [7202/10336], Loss: 0.1318\n",
      "Epoch [4/5], Step [7204/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [7206/10336], Loss: 0.1877\n",
      "Epoch [4/5], Step [7208/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [7210/10336], Loss: 0.0216\n",
      "Epoch [4/5], Step [7212/10336], Loss: 0.2819\n",
      "Epoch [4/5], Step [7214/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [7216/10336], Loss: 1.1728\n",
      "Epoch [4/5], Step [7218/10336], Loss: 0.0662\n",
      "Epoch [4/5], Step [7220/10336], Loss: 0.1419\n",
      "Epoch [4/5], Step [7222/10336], Loss: 0.2218\n",
      "Epoch [4/5], Step [7224/10336], Loss: 0.6838\n",
      "Epoch [4/5], Step [7226/10336], Loss: 0.1955\n",
      "Epoch [4/5], Step [7228/10336], Loss: 0.1533\n",
      "Epoch [4/5], Step [7230/10336], Loss: 0.5102\n",
      "Epoch [4/5], Step [7232/10336], Loss: 0.2841\n",
      "Epoch [4/5], Step [7234/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [7236/10336], Loss: 0.0312\n",
      "Epoch [4/5], Step [7238/10336], Loss: 0.6432\n",
      "Epoch [4/5], Step [7240/10336], Loss: 0.4046\n",
      "Epoch [4/5], Step [7242/10336], Loss: 1.5567\n",
      "Epoch [4/5], Step [7244/10336], Loss: 0.5356\n",
      "Epoch [4/5], Step [7246/10336], Loss: 0.4066\n",
      "Epoch [4/5], Step [7248/10336], Loss: 0.0045\n",
      "Epoch [4/5], Step [7250/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [7252/10336], Loss: 0.0130\n",
      "Epoch [4/5], Step [7254/10336], Loss: 2.3945\n",
      "Epoch [4/5], Step [7256/10336], Loss: 0.4865\n",
      "Epoch [4/5], Step [7258/10336], Loss: 0.4867\n",
      "Epoch [4/5], Step [7260/10336], Loss: 0.7341\n",
      "Epoch [4/5], Step [7262/10336], Loss: 0.9920\n",
      "Epoch [4/5], Step [7264/10336], Loss: 0.0119\n",
      "Epoch [4/5], Step [7266/10336], Loss: 0.2263\n",
      "Epoch [4/5], Step [7268/10336], Loss: 0.3162\n",
      "Epoch [4/5], Step [7270/10336], Loss: 1.5237\n",
      "Epoch [4/5], Step [7272/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [7274/10336], Loss: 0.2483\n",
      "Epoch [4/5], Step [7276/10336], Loss: 0.0096\n",
      "Epoch [4/5], Step [7278/10336], Loss: 0.0116\n",
      "Epoch [4/5], Step [7280/10336], Loss: 0.3055\n",
      "Epoch [4/5], Step [7282/10336], Loss: 0.7698\n",
      "Epoch [4/5], Step [7284/10336], Loss: 0.5213\n",
      "Epoch [4/5], Step [7286/10336], Loss: 0.0038\n",
      "Epoch [4/5], Step [7288/10336], Loss: 0.5973\n",
      "Epoch [4/5], Step [7290/10336], Loss: 0.1484\n",
      "Epoch [4/5], Step [7292/10336], Loss: 0.0611\n",
      "Epoch [4/5], Step [7294/10336], Loss: 0.9348\n",
      "Epoch [4/5], Step [7296/10336], Loss: 0.0041\n",
      "Epoch [4/5], Step [7298/10336], Loss: 0.5188\n",
      "Epoch [4/5], Step [7300/10336], Loss: 0.1571\n",
      "Epoch [4/5], Step [7302/10336], Loss: 2.0856\n",
      "Epoch [4/5], Step [7304/10336], Loss: 3.9163\n",
      "Epoch [4/5], Step [7306/10336], Loss: 0.3600\n",
      "Epoch [4/5], Step [7308/10336], Loss: 0.0086\n",
      "Epoch [4/5], Step [7310/10336], Loss: 0.5278\n",
      "Epoch [4/5], Step [7312/10336], Loss: 0.1516\n",
      "Epoch [4/5], Step [7314/10336], Loss: 0.7429\n",
      "Epoch [4/5], Step [7316/10336], Loss: 0.2593\n",
      "Epoch [4/5], Step [7318/10336], Loss: 0.2491\n",
      "Epoch [4/5], Step [7320/10336], Loss: 1.0135\n",
      "Epoch [4/5], Step [7322/10336], Loss: 0.7531\n",
      "Epoch [4/5], Step [7324/10336], Loss: 1.9383\n",
      "Epoch [4/5], Step [7326/10336], Loss: 0.0394\n",
      "Epoch [4/5], Step [7328/10336], Loss: 2.8229\n",
      "Epoch [4/5], Step [7330/10336], Loss: 0.0063\n",
      "Epoch [4/5], Step [7332/10336], Loss: 0.1955\n",
      "Epoch [4/5], Step [7334/10336], Loss: 2.9765\n",
      "Epoch [4/5], Step [7336/10336], Loss: 0.7830\n",
      "Epoch [4/5], Step [7338/10336], Loss: 3.4656\n",
      "Epoch [4/5], Step [7340/10336], Loss: 2.7493\n",
      "Epoch [4/5], Step [7342/10336], Loss: 0.1084\n",
      "Epoch [4/5], Step [7344/10336], Loss: 0.1896\n",
      "Epoch [4/5], Step [7346/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [7348/10336], Loss: 0.0148\n",
      "Epoch [4/5], Step [7350/10336], Loss: 2.7994\n",
      "Epoch [4/5], Step [7352/10336], Loss: 0.1170\n",
      "Epoch [4/5], Step [7354/10336], Loss: 3.7004\n",
      "Epoch [4/5], Step [7356/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [7358/10336], Loss: 0.4346\n",
      "Epoch [4/5], Step [7360/10336], Loss: 0.2018\n",
      "Epoch [4/5], Step [7362/10336], Loss: 0.8163\n",
      "Epoch [4/5], Step [7364/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [7366/10336], Loss: 0.3471\n",
      "Epoch [4/5], Step [7368/10336], Loss: 0.0282\n",
      "Epoch [4/5], Step [7370/10336], Loss: 0.6637\n",
      "Epoch [4/5], Step [7372/10336], Loss: 0.1052\n",
      "Epoch [4/5], Step [7374/10336], Loss: 0.8449\n",
      "Epoch [4/5], Step [7376/10336], Loss: 0.1382\n",
      "Epoch [4/5], Step [7378/10336], Loss: 0.0535\n",
      "Epoch [4/5], Step [7380/10336], Loss: 3.8726\n",
      "Epoch [4/5], Step [7382/10336], Loss: 0.1276\n",
      "Epoch [4/5], Step [7384/10336], Loss: 0.3423\n",
      "Epoch [4/5], Step [7386/10336], Loss: 0.1689\n",
      "Epoch [4/5], Step [7388/10336], Loss: 2.0221\n",
      "Epoch [4/5], Step [7390/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [7392/10336], Loss: 1.0218\n",
      "Epoch [4/5], Step [7394/10336], Loss: 3.5606\n",
      "Epoch [4/5], Step [7396/10336], Loss: 0.0244\n",
      "Epoch [4/5], Step [7398/10336], Loss: 0.0316\n",
      "Epoch [4/5], Step [7400/10336], Loss: 0.2513\n",
      "Epoch [4/5], Step [7402/10336], Loss: 0.0078\n",
      "Epoch [4/5], Step [7404/10336], Loss: 0.1892\n",
      "Epoch [4/5], Step [7406/10336], Loss: 0.5618\n",
      "Epoch [4/5], Step [7408/10336], Loss: 0.0469\n",
      "Epoch [4/5], Step [7410/10336], Loss: 0.6497\n",
      "Epoch [4/5], Step [7412/10336], Loss: 0.1041\n",
      "Epoch [4/5], Step [7414/10336], Loss: 0.0334\n",
      "Epoch [4/5], Step [7416/10336], Loss: 0.6555\n",
      "Epoch [4/5], Step [7418/10336], Loss: 2.1323\n",
      "Epoch [4/5], Step [7420/10336], Loss: 0.3948\n",
      "Epoch [4/5], Step [7422/10336], Loss: 0.1692\n",
      "Epoch [4/5], Step [7424/10336], Loss: 1.7220\n",
      "Epoch [4/5], Step [7426/10336], Loss: 0.9170\n",
      "Epoch [4/5], Step [7428/10336], Loss: 0.1687\n",
      "Epoch [4/5], Step [7430/10336], Loss: 0.1044\n",
      "Epoch [4/5], Step [7432/10336], Loss: 1.4421\n",
      "Epoch [4/5], Step [7434/10336], Loss: 1.5698\n",
      "Epoch [4/5], Step [7436/10336], Loss: 0.8641\n",
      "Epoch [4/5], Step [7438/10336], Loss: 0.5180\n",
      "Epoch [4/5], Step [7440/10336], Loss: 0.6505\n",
      "Epoch [4/5], Step [7442/10336], Loss: 0.7105\n",
      "Epoch [4/5], Step [7444/10336], Loss: 0.1240\n",
      "Epoch [4/5], Step [7446/10336], Loss: 0.3627\n",
      "Epoch [4/5], Step [7448/10336], Loss: 0.1605\n",
      "Epoch [4/5], Step [7450/10336], Loss: 0.8348\n",
      "Epoch [4/5], Step [7452/10336], Loss: 0.7687\n",
      "Epoch [4/5], Step [7454/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [7456/10336], Loss: 0.0245\n",
      "Epoch [4/5], Step [7458/10336], Loss: 0.1463\n",
      "Epoch [4/5], Step [7460/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [7462/10336], Loss: 0.1211\n",
      "Epoch [4/5], Step [7464/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [7466/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [7468/10336], Loss: 0.4772\n",
      "Epoch [4/5], Step [7470/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [7472/10336], Loss: 0.0148\n",
      "Epoch [4/5], Step [7474/10336], Loss: 0.0741\n",
      "Epoch [4/5], Step [7476/10336], Loss: 0.0379\n",
      "Epoch [4/5], Step [7478/10336], Loss: 1.6157\n",
      "Epoch [4/5], Step [7480/10336], Loss: 0.7862\n",
      "Epoch [4/5], Step [7482/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [7484/10336], Loss: 1.0885\n",
      "Epoch [4/5], Step [7486/10336], Loss: 0.5473\n",
      "Epoch [4/5], Step [7488/10336], Loss: 0.1714\n",
      "Epoch [4/5], Step [7490/10336], Loss: 1.1839\n",
      "Epoch [4/5], Step [7492/10336], Loss: 0.0376\n",
      "Epoch [4/5], Step [7494/10336], Loss: 1.4408\n",
      "Epoch [4/5], Step [7496/10336], Loss: 0.6968\n",
      "Epoch [4/5], Step [7498/10336], Loss: 2.1637\n",
      "Epoch [4/5], Step [7500/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [7502/10336], Loss: 0.2677\n",
      "Epoch [4/5], Step [7504/10336], Loss: 0.0654\n",
      "Epoch [4/5], Step [7506/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [7508/10336], Loss: 0.2281\n",
      "Epoch [4/5], Step [7510/10336], Loss: 0.2129\n",
      "Epoch [4/5], Step [7512/10336], Loss: 4.1751\n",
      "Epoch [4/5], Step [7514/10336], Loss: 0.0000\n",
      "Epoch [4/5], Step [7516/10336], Loss: 0.0097\n",
      "Epoch [4/5], Step [7518/10336], Loss: 3.7392\n",
      "Epoch [4/5], Step [7520/10336], Loss: 0.1841\n",
      "Epoch [4/5], Step [7522/10336], Loss: 0.3811\n",
      "Epoch [4/5], Step [7524/10336], Loss: 0.1172\n",
      "Epoch [4/5], Step [7526/10336], Loss: 0.0388\n",
      "Epoch [4/5], Step [7528/10336], Loss: 0.9824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [7530/10336], Loss: 1.9289\n",
      "Epoch [4/5], Step [7532/10336], Loss: 0.2562\n",
      "Epoch [4/5], Step [7534/10336], Loss: 0.0347\n",
      "Epoch [4/5], Step [7536/10336], Loss: 0.0489\n",
      "Epoch [4/5], Step [7538/10336], Loss: 0.0446\n",
      "Epoch [4/5], Step [7540/10336], Loss: 0.0338\n",
      "Epoch [4/5], Step [7542/10336], Loss: 0.5709\n",
      "Epoch [4/5], Step [7544/10336], Loss: 0.7078\n",
      "Epoch [4/5], Step [7546/10336], Loss: 0.3070\n",
      "Epoch [4/5], Step [7548/10336], Loss: 1.5117\n",
      "Epoch [4/5], Step [7550/10336], Loss: 0.4068\n",
      "Epoch [4/5], Step [7552/10336], Loss: 0.6708\n",
      "Epoch [4/5], Step [7554/10336], Loss: 4.5959\n",
      "Epoch [4/5], Step [7556/10336], Loss: 0.0144\n",
      "Epoch [4/5], Step [7558/10336], Loss: 2.5998\n",
      "Epoch [4/5], Step [7560/10336], Loss: 0.1844\n",
      "Epoch [4/5], Step [7562/10336], Loss: 0.7916\n",
      "Epoch [4/5], Step [7564/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [7566/10336], Loss: 0.0092\n",
      "Epoch [4/5], Step [7568/10336], Loss: 0.9687\n",
      "Epoch [4/5], Step [7570/10336], Loss: 0.1855\n",
      "Epoch [4/5], Step [7572/10336], Loss: 0.0100\n",
      "Epoch [4/5], Step [7574/10336], Loss: 0.0702\n",
      "Epoch [4/5], Step [7576/10336], Loss: 0.9877\n",
      "Epoch [4/5], Step [7578/10336], Loss: 0.4208\n",
      "Epoch [4/5], Step [7580/10336], Loss: 0.2769\n",
      "Epoch [4/5], Step [7582/10336], Loss: 0.9474\n",
      "Epoch [4/5], Step [7584/10336], Loss: 0.0137\n",
      "Epoch [4/5], Step [7586/10336], Loss: 0.0098\n",
      "Epoch [4/5], Step [7588/10336], Loss: 0.1243\n",
      "Epoch [4/5], Step [7590/10336], Loss: 0.2052\n",
      "Epoch [4/5], Step [7592/10336], Loss: 0.9316\n",
      "Epoch [4/5], Step [7594/10336], Loss: 0.2197\n",
      "Epoch [4/5], Step [7596/10336], Loss: 0.4664\n",
      "Epoch [4/5], Step [7598/10336], Loss: 3.0684\n",
      "Epoch [4/5], Step [7600/10336], Loss: 0.2789\n",
      "Epoch [4/5], Step [7602/10336], Loss: 0.0101\n",
      "Epoch [4/5], Step [7604/10336], Loss: 0.2553\n",
      "Epoch [4/5], Step [7606/10336], Loss: 0.0079\n",
      "Epoch [4/5], Step [7608/10336], Loss: 0.1988\n",
      "Epoch [4/5], Step [7610/10336], Loss: 0.4442\n",
      "Epoch [4/5], Step [7612/10336], Loss: 0.0660\n",
      "Epoch [4/5], Step [7614/10336], Loss: 0.3907\n",
      "Epoch [4/5], Step [7616/10336], Loss: 0.0032\n",
      "Epoch [4/5], Step [7618/10336], Loss: 1.9334\n",
      "Epoch [4/5], Step [7620/10336], Loss: 3.9294\n",
      "Epoch [4/5], Step [7622/10336], Loss: 0.0421\n",
      "Epoch [4/5], Step [7624/10336], Loss: 2.6863\n",
      "Epoch [4/5], Step [7626/10336], Loss: 0.1205\n",
      "Epoch [4/5], Step [7628/10336], Loss: 0.1355\n",
      "Epoch [4/5], Step [7630/10336], Loss: 0.3776\n",
      "Epoch [4/5], Step [7632/10336], Loss: 0.2628\n",
      "Epoch [4/5], Step [7634/10336], Loss: 0.4606\n",
      "Epoch [4/5], Step [7636/10336], Loss: 0.0107\n",
      "Epoch [4/5], Step [7638/10336], Loss: 0.1190\n",
      "Epoch [4/5], Step [7640/10336], Loss: 0.1245\n",
      "Epoch [4/5], Step [7642/10336], Loss: 0.4523\n",
      "Epoch [4/5], Step [7644/10336], Loss: 0.3049\n",
      "Epoch [4/5], Step [7646/10336], Loss: 0.2292\n",
      "Epoch [4/5], Step [7648/10336], Loss: 0.3579\n",
      "Epoch [4/5], Step [7650/10336], Loss: 0.4813\n",
      "Epoch [4/5], Step [7652/10336], Loss: 0.1553\n",
      "Epoch [4/5], Step [7654/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [7656/10336], Loss: 0.5141\n",
      "Epoch [4/5], Step [7658/10336], Loss: 1.5544\n",
      "Epoch [4/5], Step [7660/10336], Loss: 0.0990\n",
      "Epoch [4/5], Step [7662/10336], Loss: 2.2221\n",
      "Epoch [4/5], Step [7664/10336], Loss: 0.0572\n",
      "Epoch [4/5], Step [7666/10336], Loss: 0.2823\n",
      "Epoch [4/5], Step [7668/10336], Loss: 0.1893\n",
      "Epoch [4/5], Step [7670/10336], Loss: 0.0670\n",
      "Epoch [4/5], Step [7672/10336], Loss: 0.0289\n",
      "Epoch [4/5], Step [7674/10336], Loss: 0.0171\n",
      "Epoch [4/5], Step [7676/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [7678/10336], Loss: 0.8817\n",
      "Epoch [4/5], Step [7680/10336], Loss: 0.8206\n",
      "Epoch [4/5], Step [7682/10336], Loss: 0.1800\n",
      "Epoch [4/5], Step [7684/10336], Loss: 0.0145\n",
      "Epoch [4/5], Step [7686/10336], Loss: 0.0091\n",
      "Epoch [4/5], Step [7688/10336], Loss: 0.0112\n",
      "Epoch [4/5], Step [7690/10336], Loss: 0.6859\n",
      "Epoch [4/5], Step [7692/10336], Loss: 0.9093\n",
      "Epoch [4/5], Step [7694/10336], Loss: 0.0080\n",
      "Epoch [4/5], Step [7696/10336], Loss: 0.2095\n",
      "Epoch [4/5], Step [7698/10336], Loss: 0.0083\n",
      "Epoch [4/5], Step [7700/10336], Loss: 0.0379\n",
      "Epoch [4/5], Step [7702/10336], Loss: 0.3928\n",
      "Epoch [4/5], Step [7704/10336], Loss: 1.2478\n",
      "Epoch [4/5], Step [7706/10336], Loss: 0.5147\n",
      "Epoch [4/5], Step [7708/10336], Loss: 0.5259\n",
      "Epoch [4/5], Step [7710/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [7712/10336], Loss: 3.8951\n",
      "Epoch [4/5], Step [7714/10336], Loss: 0.2780\n",
      "Epoch [4/5], Step [7716/10336], Loss: 0.1862\n",
      "Epoch [4/5], Step [7718/10336], Loss: 0.2042\n",
      "Epoch [4/5], Step [7720/10336], Loss: 0.1358\n",
      "Epoch [4/5], Step [7722/10336], Loss: 0.2084\n",
      "Epoch [4/5], Step [7724/10336], Loss: 3.3083\n",
      "Epoch [4/5], Step [7726/10336], Loss: 0.9569\n",
      "Epoch [4/5], Step [7728/10336], Loss: 0.1244\n",
      "Epoch [4/5], Step [7730/10336], Loss: 1.6551\n",
      "Epoch [4/5], Step [7732/10336], Loss: 0.0840\n",
      "Epoch [4/5], Step [7734/10336], Loss: 0.2473\n",
      "Epoch [4/5], Step [7736/10336], Loss: 1.5115\n",
      "Epoch [4/5], Step [7738/10336], Loss: 0.3108\n",
      "Epoch [4/5], Step [7740/10336], Loss: 0.1802\n",
      "Epoch [4/5], Step [7742/10336], Loss: 0.0270\n",
      "Epoch [4/5], Step [7744/10336], Loss: 1.5295\n",
      "Epoch [4/5], Step [7746/10336], Loss: 0.2604\n",
      "Epoch [4/5], Step [7748/10336], Loss: 1.5134\n",
      "Epoch [4/5], Step [7750/10336], Loss: 0.1709\n",
      "Epoch [4/5], Step [7752/10336], Loss: 0.0294\n",
      "Epoch [4/5], Step [7754/10336], Loss: 0.0726\n",
      "Epoch [4/5], Step [7756/10336], Loss: 0.4884\n",
      "Epoch [4/5], Step [7758/10336], Loss: 0.1504\n",
      "Epoch [4/5], Step [7760/10336], Loss: 0.3631\n",
      "Epoch [4/5], Step [7762/10336], Loss: 0.2558\n",
      "Epoch [4/5], Step [7764/10336], Loss: 0.0105\n",
      "Epoch [4/5], Step [7766/10336], Loss: 0.3878\n",
      "Epoch [4/5], Step [7768/10336], Loss: 0.2665\n",
      "Epoch [4/5], Step [7770/10336], Loss: 0.5664\n",
      "Epoch [4/5], Step [7772/10336], Loss: 1.0544\n",
      "Epoch [4/5], Step [7774/10336], Loss: 0.3485\n",
      "Epoch [4/5], Step [7776/10336], Loss: 1.3500\n",
      "Epoch [4/5], Step [7778/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [7780/10336], Loss: 0.1809\n",
      "Epoch [4/5], Step [7782/10336], Loss: 0.0473\n",
      "Epoch [4/5], Step [7784/10336], Loss: 1.7748\n",
      "Epoch [4/5], Step [7786/10336], Loss: 1.0595\n",
      "Epoch [4/5], Step [7788/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [7790/10336], Loss: 0.0294\n",
      "Epoch [4/5], Step [7792/10336], Loss: 1.5577\n",
      "Epoch [4/5], Step [7794/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [7796/10336], Loss: 0.0288\n",
      "Epoch [4/5], Step [7798/10336], Loss: 0.1031\n",
      "Epoch [4/5], Step [7800/10336], Loss: 0.0118\n",
      "Epoch [4/5], Step [7802/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [7804/10336], Loss: 1.4611\n",
      "Epoch [4/5], Step [7806/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [7808/10336], Loss: 1.8381\n",
      "Epoch [4/5], Step [7810/10336], Loss: 0.3173\n",
      "Epoch [4/5], Step [7812/10336], Loss: 2.3427\n",
      "Epoch [4/5], Step [7814/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [7816/10336], Loss: 0.1856\n",
      "Epoch [4/5], Step [7818/10336], Loss: 0.3339\n",
      "Epoch [4/5], Step [7820/10336], Loss: 1.1503\n",
      "Epoch [4/5], Step [7822/10336], Loss: 0.4301\n",
      "Epoch [4/5], Step [7824/10336], Loss: 0.0310\n",
      "Epoch [4/5], Step [7826/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [7828/10336], Loss: 0.4129\n",
      "Epoch [4/5], Step [7830/10336], Loss: 1.2066\n",
      "Epoch [4/5], Step [7832/10336], Loss: 1.2070\n",
      "Epoch [4/5], Step [7834/10336], Loss: 0.0253\n",
      "Epoch [4/5], Step [7836/10336], Loss: 1.1211\n",
      "Epoch [4/5], Step [7838/10336], Loss: 2.6587\n",
      "Epoch [4/5], Step [7840/10336], Loss: 0.8907\n",
      "Epoch [4/5], Step [7842/10336], Loss: 0.0794\n",
      "Epoch [4/5], Step [7844/10336], Loss: 4.6502\n",
      "Epoch [4/5], Step [7846/10336], Loss: 0.2693\n",
      "Epoch [4/5], Step [7848/10336], Loss: 0.2212\n",
      "Epoch [4/5], Step [7850/10336], Loss: 0.2535\n",
      "Epoch [4/5], Step [7852/10336], Loss: 1.0682\n",
      "Epoch [4/5], Step [7854/10336], Loss: 0.3255\n",
      "Epoch [4/5], Step [7856/10336], Loss: 2.5593\n",
      "Epoch [4/5], Step [7858/10336], Loss: 0.1797\n",
      "Epoch [4/5], Step [7860/10336], Loss: 0.2301\n",
      "Epoch [4/5], Step [7862/10336], Loss: 0.9549\n",
      "Epoch [4/5], Step [7864/10336], Loss: 0.8700\n",
      "Epoch [4/5], Step [7866/10336], Loss: 0.0367\n",
      "Epoch [4/5], Step [7868/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [7870/10336], Loss: 0.0422\n",
      "Epoch [4/5], Step [7872/10336], Loss: 0.1411\n",
      "Epoch [4/5], Step [7874/10336], Loss: 0.0181\n",
      "Epoch [4/5], Step [7876/10336], Loss: 0.3997\n",
      "Epoch [4/5], Step [7878/10336], Loss: 1.7889\n",
      "Epoch [4/5], Step [7880/10336], Loss: 1.3459\n",
      "Epoch [4/5], Step [7882/10336], Loss: 0.2538\n",
      "Epoch [4/5], Step [7884/10336], Loss: 2.4493\n",
      "Epoch [4/5], Step [7886/10336], Loss: 0.0991\n",
      "Epoch [4/5], Step [7888/10336], Loss: 0.0266\n",
      "Epoch [4/5], Step [7890/10336], Loss: 0.0919\n",
      "Epoch [4/5], Step [7892/10336], Loss: 0.0644\n",
      "Epoch [4/5], Step [7894/10336], Loss: 0.5109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [7896/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [7898/10336], Loss: 0.0247\n",
      "Epoch [4/5], Step [7900/10336], Loss: 0.8710\n",
      "Epoch [4/5], Step [7902/10336], Loss: 0.1830\n",
      "Epoch [4/5], Step [7904/10336], Loss: 1.9097\n",
      "Epoch [4/5], Step [7906/10336], Loss: 0.9820\n",
      "Epoch [4/5], Step [7908/10336], Loss: 0.3422\n",
      "Epoch [4/5], Step [7910/10336], Loss: 0.2304\n",
      "Epoch [4/5], Step [7912/10336], Loss: 0.3173\n",
      "Epoch [4/5], Step [7914/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [7916/10336], Loss: 0.0129\n",
      "Epoch [4/5], Step [7918/10336], Loss: 0.0351\n",
      "Epoch [4/5], Step [7920/10336], Loss: 0.0035\n",
      "Epoch [4/5], Step [7922/10336], Loss: 1.9902\n",
      "Epoch [4/5], Step [7924/10336], Loss: 0.5973\n",
      "Epoch [4/5], Step [7926/10336], Loss: 0.5292\n",
      "Epoch [4/5], Step [7928/10336], Loss: 0.2835\n",
      "Epoch [4/5], Step [7930/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [7932/10336], Loss: 0.1682\n",
      "Epoch [4/5], Step [7934/10336], Loss: 0.0137\n",
      "Epoch [4/5], Step [7936/10336], Loss: 0.0722\n",
      "Epoch [4/5], Step [7938/10336], Loss: 0.0138\n",
      "Epoch [4/5], Step [7940/10336], Loss: 0.4374\n",
      "Epoch [4/5], Step [7942/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [7944/10336], Loss: 4.5669\n",
      "Epoch [4/5], Step [7946/10336], Loss: 0.1597\n",
      "Epoch [4/5], Step [7948/10336], Loss: 0.1425\n",
      "Epoch [4/5], Step [7950/10336], Loss: 0.3129\n",
      "Epoch [4/5], Step [7952/10336], Loss: 0.1777\n",
      "Epoch [4/5], Step [7954/10336], Loss: 0.3387\n",
      "Epoch [4/5], Step [7956/10336], Loss: 0.0949\n",
      "Epoch [4/5], Step [7958/10336], Loss: 0.0409\n",
      "Epoch [4/5], Step [7960/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [7962/10336], Loss: 0.3890\n",
      "Epoch [4/5], Step [7964/10336], Loss: 1.8355\n",
      "Epoch [4/5], Step [7966/10336], Loss: 1.0124\n",
      "Epoch [4/5], Step [7968/10336], Loss: 0.1257\n",
      "Epoch [4/5], Step [7970/10336], Loss: 2.9980\n",
      "Epoch [4/5], Step [7972/10336], Loss: 0.2247\n",
      "Epoch [4/5], Step [7974/10336], Loss: 0.6409\n",
      "Epoch [4/5], Step [7976/10336], Loss: 0.2375\n",
      "Epoch [4/5], Step [7978/10336], Loss: 1.8109\n",
      "Epoch [4/5], Step [7980/10336], Loss: 0.1931\n",
      "Epoch [4/5], Step [7982/10336], Loss: 2.8610\n",
      "Epoch [4/5], Step [7984/10336], Loss: 0.6356\n",
      "Epoch [4/5], Step [7986/10336], Loss: 0.4091\n",
      "Epoch [4/5], Step [7988/10336], Loss: 0.0089\n",
      "Epoch [4/5], Step [7990/10336], Loss: 2.9157\n",
      "Epoch [4/5], Step [7992/10336], Loss: 0.2891\n",
      "Epoch [4/5], Step [7994/10336], Loss: 0.1846\n",
      "Epoch [4/5], Step [7996/10336], Loss: 0.9311\n",
      "Epoch [4/5], Step [7998/10336], Loss: 0.3768\n",
      "Epoch [4/5], Step [8000/10336], Loss: 0.2792\n",
      "Epoch [4/5], Step [8002/10336], Loss: 1.2355\n",
      "Epoch [4/5], Step [8004/10336], Loss: 0.1448\n",
      "Epoch [4/5], Step [8006/10336], Loss: 1.6711\n",
      "Epoch [4/5], Step [8008/10336], Loss: 0.3395\n",
      "Epoch [4/5], Step [8010/10336], Loss: 1.6900\n",
      "Epoch [4/5], Step [8012/10336], Loss: 0.1540\n",
      "Epoch [4/5], Step [8014/10336], Loss: 0.1614\n",
      "Epoch [4/5], Step [8016/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [8018/10336], Loss: 0.0147\n",
      "Epoch [4/5], Step [8020/10336], Loss: 0.1278\n",
      "Epoch [4/5], Step [8022/10336], Loss: 0.0124\n",
      "Epoch [4/5], Step [8024/10336], Loss: 0.2420\n",
      "Epoch [4/5], Step [8026/10336], Loss: 0.1614\n",
      "Epoch [4/5], Step [8028/10336], Loss: 0.0290\n",
      "Epoch [4/5], Step [8030/10336], Loss: 3.4886\n",
      "Epoch [4/5], Step [8032/10336], Loss: 0.1334\n",
      "Epoch [4/5], Step [8034/10336], Loss: 0.2895\n",
      "Epoch [4/5], Step [8036/10336], Loss: 0.1285\n",
      "Epoch [4/5], Step [8038/10336], Loss: 0.5710\n",
      "Epoch [4/5], Step [8040/10336], Loss: 0.4227\n",
      "Epoch [4/5], Step [8042/10336], Loss: 0.0083\n",
      "Epoch [4/5], Step [8044/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [8046/10336], Loss: 0.4358\n",
      "Epoch [4/5], Step [8048/10336], Loss: 0.2611\n",
      "Epoch [4/5], Step [8050/10336], Loss: 1.2313\n",
      "Epoch [4/5], Step [8052/10336], Loss: 2.2358\n",
      "Epoch [4/5], Step [8054/10336], Loss: 2.3706\n",
      "Epoch [4/5], Step [8056/10336], Loss: 0.0781\n",
      "Epoch [4/5], Step [8058/10336], Loss: 0.7583\n",
      "Epoch [4/5], Step [8060/10336], Loss: 0.0738\n",
      "Epoch [4/5], Step [8062/10336], Loss: 3.5135\n",
      "Epoch [4/5], Step [8064/10336], Loss: 0.1436\n",
      "Epoch [4/5], Step [8066/10336], Loss: 1.9874\n",
      "Epoch [4/5], Step [8068/10336], Loss: 0.7557\n",
      "Epoch [4/5], Step [8070/10336], Loss: 0.1290\n",
      "Epoch [4/5], Step [8072/10336], Loss: 0.0038\n",
      "Epoch [4/5], Step [8074/10336], Loss: 0.1283\n",
      "Epoch [4/5], Step [8076/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [8078/10336], Loss: 3.4547\n",
      "Epoch [4/5], Step [8080/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [8082/10336], Loss: 0.9056\n",
      "Epoch [4/5], Step [8084/10336], Loss: 0.2557\n",
      "Epoch [4/5], Step [8086/10336], Loss: 0.4007\n",
      "Epoch [4/5], Step [8088/10336], Loss: 0.0998\n",
      "Epoch [4/5], Step [8090/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [8092/10336], Loss: 0.0453\n",
      "Epoch [4/5], Step [8094/10336], Loss: 0.0238\n",
      "Epoch [4/5], Step [8096/10336], Loss: 2.9402\n",
      "Epoch [4/5], Step [8098/10336], Loss: 1.1936\n",
      "Epoch [4/5], Step [8100/10336], Loss: 2.3178\n",
      "Epoch [4/5], Step [8102/10336], Loss: 0.4452\n",
      "Epoch [4/5], Step [8104/10336], Loss: 0.0818\n",
      "Epoch [4/5], Step [8106/10336], Loss: 0.1038\n",
      "Epoch [4/5], Step [8108/10336], Loss: 0.1715\n",
      "Epoch [4/5], Step [8110/10336], Loss: 0.4556\n",
      "Epoch [4/5], Step [8112/10336], Loss: 0.0053\n",
      "Epoch [4/5], Step [8114/10336], Loss: 0.0810\n",
      "Epoch [4/5], Step [8116/10336], Loss: 0.2300\n",
      "Epoch [4/5], Step [8118/10336], Loss: 0.3469\n",
      "Epoch [4/5], Step [8120/10336], Loss: 0.1138\n",
      "Epoch [4/5], Step [8122/10336], Loss: 0.1106\n",
      "Epoch [4/5], Step [8124/10336], Loss: 1.7966\n",
      "Epoch [4/5], Step [8126/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [8128/10336], Loss: 0.3071\n",
      "Epoch [4/5], Step [8130/10336], Loss: 0.0197\n",
      "Epoch [4/5], Step [8132/10336], Loss: 0.0102\n",
      "Epoch [4/5], Step [8134/10336], Loss: 0.0490\n",
      "Epoch [4/5], Step [8136/10336], Loss: 2.2976\n",
      "Epoch [4/5], Step [8138/10336], Loss: 0.0626\n",
      "Epoch [4/5], Step [8140/10336], Loss: 0.0214\n",
      "Epoch [4/5], Step [8142/10336], Loss: 0.6091\n",
      "Epoch [4/5], Step [8144/10336], Loss: 0.5525\n",
      "Epoch [4/5], Step [8146/10336], Loss: 0.0353\n",
      "Epoch [4/5], Step [8148/10336], Loss: 0.0183\n",
      "Epoch [4/5], Step [8150/10336], Loss: 0.2460\n",
      "Epoch [4/5], Step [8152/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [8154/10336], Loss: 3.1037\n",
      "Epoch [4/5], Step [8156/10336], Loss: 0.5321\n",
      "Epoch [4/5], Step [8158/10336], Loss: 0.2672\n",
      "Epoch [4/5], Step [8160/10336], Loss: 0.0291\n",
      "Epoch [4/5], Step [8162/10336], Loss: 0.0521\n",
      "Epoch [4/5], Step [8164/10336], Loss: 0.0756\n",
      "Epoch [4/5], Step [8166/10336], Loss: 0.1014\n",
      "Epoch [4/5], Step [8168/10336], Loss: 0.0243\n",
      "Epoch [4/5], Step [8170/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [8172/10336], Loss: 0.3374\n",
      "Epoch [4/5], Step [8174/10336], Loss: 0.2126\n",
      "Epoch [4/5], Step [8176/10336], Loss: 0.0428\n",
      "Epoch [4/5], Step [8178/10336], Loss: 0.0173\n",
      "Epoch [4/5], Step [8180/10336], Loss: 1.1715\n",
      "Epoch [4/5], Step [8182/10336], Loss: 0.3255\n",
      "Epoch [4/5], Step [8184/10336], Loss: 2.5221\n",
      "Epoch [4/5], Step [8186/10336], Loss: 0.3400\n",
      "Epoch [4/5], Step [8188/10336], Loss: 0.7163\n",
      "Epoch [4/5], Step [8190/10336], Loss: 0.0143\n",
      "Epoch [4/5], Step [8192/10336], Loss: 0.0700\n",
      "Epoch [4/5], Step [8194/10336], Loss: 0.9675\n",
      "Epoch [4/5], Step [8196/10336], Loss: 0.0090\n",
      "Epoch [4/5], Step [8198/10336], Loss: 0.0311\n",
      "Epoch [4/5], Step [8200/10336], Loss: 0.1486\n",
      "Epoch [4/5], Step [8202/10336], Loss: 0.5225\n",
      "Epoch [4/5], Step [8204/10336], Loss: 0.0529\n",
      "Epoch [4/5], Step [8206/10336], Loss: 0.4119\n",
      "Epoch [4/5], Step [8208/10336], Loss: 1.0150\n",
      "Epoch [4/5], Step [8210/10336], Loss: 0.8304\n",
      "Epoch [4/5], Step [8212/10336], Loss: 0.5031\n",
      "Epoch [4/5], Step [8214/10336], Loss: 0.2469\n",
      "Epoch [4/5], Step [8216/10336], Loss: 1.3528\n",
      "Epoch [4/5], Step [8218/10336], Loss: 2.2438\n",
      "Epoch [4/5], Step [8220/10336], Loss: 0.6994\n",
      "Epoch [4/5], Step [8222/10336], Loss: 0.0239\n",
      "Epoch [4/5], Step [8224/10336], Loss: 0.0498\n",
      "Epoch [4/5], Step [8226/10336], Loss: 0.2438\n",
      "Epoch [4/5], Step [8228/10336], Loss: 0.0928\n",
      "Epoch [4/5], Step [8230/10336], Loss: 0.1326\n",
      "Epoch [4/5], Step [8232/10336], Loss: 0.1197\n",
      "Epoch [4/5], Step [8234/10336], Loss: 0.0245\n",
      "Epoch [4/5], Step [8236/10336], Loss: 0.0168\n",
      "Epoch [4/5], Step [8238/10336], Loss: 0.0027\n",
      "Epoch [4/5], Step [8240/10336], Loss: 0.1817\n",
      "Epoch [4/5], Step [8242/10336], Loss: 0.2799\n",
      "Epoch [4/5], Step [8244/10336], Loss: 0.2145\n",
      "Epoch [4/5], Step [8246/10336], Loss: 0.7638\n",
      "Epoch [4/5], Step [8248/10336], Loss: 0.8556\n",
      "Epoch [4/5], Step [8250/10336], Loss: 0.0309\n",
      "Epoch [4/5], Step [8252/10336], Loss: 0.1580\n",
      "Epoch [4/5], Step [8254/10336], Loss: 0.1024\n",
      "Epoch [4/5], Step [8256/10336], Loss: 0.3611\n",
      "Epoch [4/5], Step [8258/10336], Loss: 0.4038\n",
      "Epoch [4/5], Step [8260/10336], Loss: 2.0453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [8262/10336], Loss: 1.2096\n",
      "Epoch [4/5], Step [8264/10336], Loss: 1.2132\n",
      "Epoch [4/5], Step [8266/10336], Loss: 0.0139\n",
      "Epoch [4/5], Step [8268/10336], Loss: 1.9510\n",
      "Epoch [4/5], Step [8270/10336], Loss: 0.2197\n",
      "Epoch [4/5], Step [8272/10336], Loss: 0.1851\n",
      "Epoch [4/5], Step [8274/10336], Loss: 1.5788\n",
      "Epoch [4/5], Step [8276/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [8278/10336], Loss: 0.0032\n",
      "Epoch [4/5], Step [8280/10336], Loss: 0.0765\n",
      "Epoch [4/5], Step [8282/10336], Loss: 1.4001\n",
      "Epoch [4/5], Step [8284/10336], Loss: 1.2612\n",
      "Epoch [4/5], Step [8286/10336], Loss: 0.4660\n",
      "Epoch [4/5], Step [8288/10336], Loss: 0.3361\n",
      "Epoch [4/5], Step [8290/10336], Loss: 0.2478\n",
      "Epoch [4/5], Step [8292/10336], Loss: 1.9277\n",
      "Epoch [4/5], Step [8294/10336], Loss: 0.5312\n",
      "Epoch [4/5], Step [8296/10336], Loss: 0.2796\n",
      "Epoch [4/5], Step [8298/10336], Loss: 0.8252\n",
      "Epoch [4/5], Step [8300/10336], Loss: 0.1335\n",
      "Epoch [4/5], Step [8302/10336], Loss: 0.8161\n",
      "Epoch [4/5], Step [8304/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [8306/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [8308/10336], Loss: 0.0612\n",
      "Epoch [4/5], Step [8310/10336], Loss: 0.1234\n",
      "Epoch [4/5], Step [8312/10336], Loss: 0.0291\n",
      "Epoch [4/5], Step [8314/10336], Loss: 3.3906\n",
      "Epoch [4/5], Step [8316/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [8318/10336], Loss: 0.2323\n",
      "Epoch [4/5], Step [8320/10336], Loss: 0.4862\n",
      "Epoch [4/5], Step [8322/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [8324/10336], Loss: 0.1862\n",
      "Epoch [4/5], Step [8326/10336], Loss: 0.1283\n",
      "Epoch [4/5], Step [8328/10336], Loss: 0.1037\n",
      "Epoch [4/5], Step [8330/10336], Loss: 0.0518\n",
      "Epoch [4/5], Step [8332/10336], Loss: 0.0261\n",
      "Epoch [4/5], Step [8334/10336], Loss: 0.6943\n",
      "Epoch [4/5], Step [8336/10336], Loss: 0.1909\n",
      "Epoch [4/5], Step [8338/10336], Loss: 2.8967\n",
      "Epoch [4/5], Step [8340/10336], Loss: 0.2946\n",
      "Epoch [4/5], Step [8342/10336], Loss: 0.1829\n",
      "Epoch [4/5], Step [8344/10336], Loss: 0.5170\n",
      "Epoch [4/5], Step [8346/10336], Loss: 0.0063\n",
      "Epoch [4/5], Step [8348/10336], Loss: 1.7051\n",
      "Epoch [4/5], Step [8350/10336], Loss: 0.0205\n",
      "Epoch [4/5], Step [8352/10336], Loss: 1.2885\n",
      "Epoch [4/5], Step [8354/10336], Loss: 0.0132\n",
      "Epoch [4/5], Step [8356/10336], Loss: 3.6854\n",
      "Epoch [4/5], Step [8358/10336], Loss: 0.0454\n",
      "Epoch [4/5], Step [8360/10336], Loss: 0.0744\n",
      "Epoch [4/5], Step [8362/10336], Loss: 0.0399\n",
      "Epoch [4/5], Step [8364/10336], Loss: 0.2840\n",
      "Epoch [4/5], Step [8366/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [8368/10336], Loss: 0.1655\n",
      "Epoch [4/5], Step [8370/10336], Loss: 0.1400\n",
      "Epoch [4/5], Step [8372/10336], Loss: 0.1069\n",
      "Epoch [4/5], Step [8374/10336], Loss: 0.0288\n",
      "Epoch [4/5], Step [8376/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [8378/10336], Loss: 0.2152\n",
      "Epoch [4/5], Step [8380/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [8382/10336], Loss: 0.0893\n",
      "Epoch [4/5], Step [8384/10336], Loss: 0.0657\n",
      "Epoch [4/5], Step [8386/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [8388/10336], Loss: 0.7054\n",
      "Epoch [4/5], Step [8390/10336], Loss: 0.3723\n",
      "Epoch [4/5], Step [8392/10336], Loss: 0.7378\n",
      "Epoch [4/5], Step [8394/10336], Loss: 0.4646\n",
      "Epoch [4/5], Step [8396/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [8398/10336], Loss: 0.3224\n",
      "Epoch [4/5], Step [8400/10336], Loss: 0.5382\n",
      "Epoch [4/5], Step [8402/10336], Loss: 3.2401\n",
      "Epoch [4/5], Step [8404/10336], Loss: 0.0602\n",
      "Epoch [4/5], Step [8406/10336], Loss: 0.1779\n",
      "Epoch [4/5], Step [8408/10336], Loss: 5.2323\n",
      "Epoch [4/5], Step [8410/10336], Loss: 0.4370\n",
      "Epoch [4/5], Step [8412/10336], Loss: 0.3798\n",
      "Epoch [4/5], Step [8414/10336], Loss: 0.0913\n",
      "Epoch [4/5], Step [8416/10336], Loss: 0.0320\n",
      "Epoch [4/5], Step [8418/10336], Loss: 0.5646\n",
      "Epoch [4/5], Step [8420/10336], Loss: 0.0104\n",
      "Epoch [4/5], Step [8422/10336], Loss: 2.2194\n",
      "Epoch [4/5], Step [8424/10336], Loss: 0.9203\n",
      "Epoch [4/5], Step [8426/10336], Loss: 3.2856\n",
      "Epoch [4/5], Step [8428/10336], Loss: 0.1128\n",
      "Epoch [4/5], Step [8430/10336], Loss: 0.3381\n",
      "Epoch [4/5], Step [8432/10336], Loss: 0.0719\n",
      "Epoch [4/5], Step [8434/10336], Loss: 0.8659\n",
      "Epoch [4/5], Step [8436/10336], Loss: 0.9094\n",
      "Epoch [4/5], Step [8438/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [8440/10336], Loss: 0.1787\n",
      "Epoch [4/5], Step [8442/10336], Loss: 0.1269\n",
      "Epoch [4/5], Step [8444/10336], Loss: 0.0186\n",
      "Epoch [4/5], Step [8446/10336], Loss: 0.6042\n",
      "Epoch [4/5], Step [8448/10336], Loss: 0.4623\n",
      "Epoch [4/5], Step [8450/10336], Loss: 0.1359\n",
      "Epoch [4/5], Step [8452/10336], Loss: 0.1096\n",
      "Epoch [4/5], Step [8454/10336], Loss: 0.3883\n",
      "Epoch [4/5], Step [8456/10336], Loss: 0.1624\n",
      "Epoch [4/5], Step [8458/10336], Loss: 0.0148\n",
      "Epoch [4/5], Step [8460/10336], Loss: 0.2317\n",
      "Epoch [4/5], Step [8462/10336], Loss: 0.0832\n",
      "Epoch [4/5], Step [8464/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [8466/10336], Loss: 1.8145\n",
      "Epoch [4/5], Step [8468/10336], Loss: 0.7236\n",
      "Epoch [4/5], Step [8470/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [8472/10336], Loss: 0.2461\n",
      "Epoch [4/5], Step [8474/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [8476/10336], Loss: 0.2339\n",
      "Epoch [4/5], Step [8478/10336], Loss: 1.4401\n",
      "Epoch [4/5], Step [8480/10336], Loss: 0.7892\n",
      "Epoch [4/5], Step [8482/10336], Loss: 0.0295\n",
      "Epoch [4/5], Step [8484/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [8486/10336], Loss: 0.8029\n",
      "Epoch [4/5], Step [8488/10336], Loss: 1.5096\n",
      "Epoch [4/5], Step [8490/10336], Loss: 0.6638\n",
      "Epoch [4/5], Step [8492/10336], Loss: 0.1988\n",
      "Epoch [4/5], Step [8494/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [8496/10336], Loss: 0.0066\n",
      "Epoch [4/5], Step [8498/10336], Loss: 0.5244\n",
      "Epoch [4/5], Step [8500/10336], Loss: 0.0557\n",
      "Epoch [4/5], Step [8502/10336], Loss: 0.0178\n",
      "Epoch [4/5], Step [8504/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [8506/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [8508/10336], Loss: 0.0171\n",
      "Epoch [4/5], Step [8510/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [8512/10336], Loss: 0.3099\n",
      "Epoch [4/5], Step [8514/10336], Loss: 0.2587\n",
      "Epoch [4/5], Step [8516/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [8518/10336], Loss: 0.0063\n",
      "Epoch [4/5], Step [8520/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [8522/10336], Loss: 0.1761\n",
      "Epoch [4/5], Step [8524/10336], Loss: 0.0914\n",
      "Epoch [4/5], Step [8526/10336], Loss: 0.0081\n",
      "Epoch [4/5], Step [8528/10336], Loss: 0.0129\n",
      "Epoch [4/5], Step [8530/10336], Loss: 0.0254\n",
      "Epoch [4/5], Step [8532/10336], Loss: 2.4084\n",
      "Epoch [4/5], Step [8534/10336], Loss: 0.0709\n",
      "Epoch [4/5], Step [8536/10336], Loss: 4.2861\n",
      "Epoch [4/5], Step [8538/10336], Loss: 0.1889\n",
      "Epoch [4/5], Step [8540/10336], Loss: 0.0138\n",
      "Epoch [4/5], Step [8542/10336], Loss: 1.7574\n",
      "Epoch [4/5], Step [8544/10336], Loss: 0.1810\n",
      "Epoch [4/5], Step [8546/10336], Loss: 1.3285\n",
      "Epoch [4/5], Step [8548/10336], Loss: 0.4172\n",
      "Epoch [4/5], Step [8550/10336], Loss: 0.4720\n",
      "Epoch [4/5], Step [8552/10336], Loss: 0.2248\n",
      "Epoch [4/5], Step [8554/10336], Loss: 0.0664\n",
      "Epoch [4/5], Step [8556/10336], Loss: 0.0196\n",
      "Epoch [4/5], Step [8558/10336], Loss: 0.0064\n",
      "Epoch [4/5], Step [8560/10336], Loss: 2.0132\n",
      "Epoch [4/5], Step [8562/10336], Loss: 0.1332\n",
      "Epoch [4/5], Step [8564/10336], Loss: 1.3062\n",
      "Epoch [4/5], Step [8566/10336], Loss: 1.2378\n",
      "Epoch [4/5], Step [8568/10336], Loss: 0.4357\n",
      "Epoch [4/5], Step [8570/10336], Loss: 0.0229\n",
      "Epoch [4/5], Step [8572/10336], Loss: 0.1937\n",
      "Epoch [4/5], Step [8574/10336], Loss: 0.3525\n",
      "Epoch [4/5], Step [8576/10336], Loss: 0.0073\n",
      "Epoch [4/5], Step [8578/10336], Loss: 0.8218\n",
      "Epoch [4/5], Step [8580/10336], Loss: 0.6727\n",
      "Epoch [4/5], Step [8582/10336], Loss: 0.2517\n",
      "Epoch [4/5], Step [8584/10336], Loss: 1.2694\n",
      "Epoch [4/5], Step [8586/10336], Loss: 0.1686\n",
      "Epoch [4/5], Step [8588/10336], Loss: 0.0065\n",
      "Epoch [4/5], Step [8590/10336], Loss: 0.4446\n",
      "Epoch [4/5], Step [8592/10336], Loss: 0.1377\n",
      "Epoch [4/5], Step [8594/10336], Loss: 0.2575\n",
      "Epoch [4/5], Step [8596/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [8598/10336], Loss: 0.0277\n",
      "Epoch [4/5], Step [8600/10336], Loss: 0.1255\n",
      "Epoch [4/5], Step [8602/10336], Loss: 0.3250\n",
      "Epoch [4/5], Step [8604/10336], Loss: 0.3742\n",
      "Epoch [4/5], Step [8606/10336], Loss: 0.2678\n",
      "Epoch [4/5], Step [8608/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [8610/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [8612/10336], Loss: 0.2430\n",
      "Epoch [4/5], Step [8614/10336], Loss: 0.1985\n",
      "Epoch [4/5], Step [8616/10336], Loss: 0.5624\n",
      "Epoch [4/5], Step [8618/10336], Loss: 0.3198\n",
      "Epoch [4/5], Step [8620/10336], Loss: 0.1508\n",
      "Epoch [4/5], Step [8622/10336], Loss: 0.4657\n",
      "Epoch [4/5], Step [8624/10336], Loss: 0.9134\n",
      "Epoch [4/5], Step [8626/10336], Loss: 0.2374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [8628/10336], Loss: 2.0355\n",
      "Epoch [4/5], Step [8630/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [8632/10336], Loss: 0.1985\n",
      "Epoch [4/5], Step [8634/10336], Loss: 0.0204\n",
      "Epoch [4/5], Step [8636/10336], Loss: 0.2401\n",
      "Epoch [4/5], Step [8638/10336], Loss: 2.5229\n",
      "Epoch [4/5], Step [8640/10336], Loss: 0.4362\n",
      "Epoch [4/5], Step [8642/10336], Loss: 0.3206\n",
      "Epoch [4/5], Step [8644/10336], Loss: 0.9043\n",
      "Epoch [4/5], Step [8646/10336], Loss: 1.2469\n",
      "Epoch [4/5], Step [8648/10336], Loss: 0.2863\n",
      "Epoch [4/5], Step [8650/10336], Loss: 0.2182\n",
      "Epoch [4/5], Step [8652/10336], Loss: 0.1459\n",
      "Epoch [4/5], Step [8654/10336], Loss: 0.0522\n",
      "Epoch [4/5], Step [8656/10336], Loss: 0.6320\n",
      "Epoch [4/5], Step [8658/10336], Loss: 0.2386\n",
      "Epoch [4/5], Step [8660/10336], Loss: 0.0123\n",
      "Epoch [4/5], Step [8662/10336], Loss: 1.0045\n",
      "Epoch [4/5], Step [8664/10336], Loss: 0.2336\n",
      "Epoch [4/5], Step [8666/10336], Loss: 0.0031\n",
      "Epoch [4/5], Step [8668/10336], Loss: 0.2131\n",
      "Epoch [4/5], Step [8670/10336], Loss: 0.1853\n",
      "Epoch [4/5], Step [8672/10336], Loss: 1.9868\n",
      "Epoch [4/5], Step [8674/10336], Loss: 1.9509\n",
      "Epoch [4/5], Step [8676/10336], Loss: 3.1096\n",
      "Epoch [4/5], Step [8678/10336], Loss: 1.2355\n",
      "Epoch [4/5], Step [8680/10336], Loss: 0.5480\n",
      "Epoch [4/5], Step [8682/10336], Loss: 1.9835\n",
      "Epoch [4/5], Step [8684/10336], Loss: 0.6111\n",
      "Epoch [4/5], Step [8686/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [8688/10336], Loss: 6.0214\n",
      "Epoch [4/5], Step [8690/10336], Loss: 1.2344\n",
      "Epoch [4/5], Step [8692/10336], Loss: 0.3209\n",
      "Epoch [4/5], Step [8694/10336], Loss: 2.3698\n",
      "Epoch [4/5], Step [8696/10336], Loss: 0.4176\n",
      "Epoch [4/5], Step [8698/10336], Loss: 2.5707\n",
      "Epoch [4/5], Step [8700/10336], Loss: 1.1728\n",
      "Epoch [4/5], Step [8702/10336], Loss: 0.2998\n",
      "Epoch [4/5], Step [8704/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [8706/10336], Loss: 1.5866\n",
      "Epoch [4/5], Step [8708/10336], Loss: 1.3754\n",
      "Epoch [4/5], Step [8710/10336], Loss: 0.0286\n",
      "Epoch [4/5], Step [8712/10336], Loss: 1.7531\n",
      "Epoch [4/5], Step [8714/10336], Loss: 0.0061\n",
      "Epoch [4/5], Step [8716/10336], Loss: 5.1107\n",
      "Epoch [4/5], Step [8718/10336], Loss: 1.4132\n",
      "Epoch [4/5], Step [8720/10336], Loss: 0.1567\n",
      "Epoch [4/5], Step [8722/10336], Loss: 0.0085\n",
      "Epoch [4/5], Step [8724/10336], Loss: 0.1243\n",
      "Epoch [4/5], Step [8726/10336], Loss: 0.0299\n",
      "Epoch [4/5], Step [8728/10336], Loss: 0.4084\n",
      "Epoch [4/5], Step [8730/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [8732/10336], Loss: 0.0740\n",
      "Epoch [4/5], Step [8734/10336], Loss: 0.5345\n",
      "Epoch [4/5], Step [8736/10336], Loss: 0.0280\n",
      "Epoch [4/5], Step [8738/10336], Loss: 0.0095\n",
      "Epoch [4/5], Step [8740/10336], Loss: 0.0408\n",
      "Epoch [4/5], Step [8742/10336], Loss: 0.2967\n",
      "Epoch [4/5], Step [8744/10336], Loss: 0.1874\n",
      "Epoch [4/5], Step [8746/10336], Loss: 0.4886\n",
      "Epoch [4/5], Step [8748/10336], Loss: 0.1107\n",
      "Epoch [4/5], Step [8750/10336], Loss: 0.0286\n",
      "Epoch [4/5], Step [8752/10336], Loss: 0.2361\n",
      "Epoch [4/5], Step [8754/10336], Loss: 0.7937\n",
      "Epoch [4/5], Step [8756/10336], Loss: 0.2131\n",
      "Epoch [4/5], Step [8758/10336], Loss: 0.1388\n",
      "Epoch [4/5], Step [8760/10336], Loss: 1.2923\n",
      "Epoch [4/5], Step [8762/10336], Loss: 0.7808\n",
      "Epoch [4/5], Step [8764/10336], Loss: 1.5569\n",
      "Epoch [4/5], Step [8766/10336], Loss: 0.7165\n",
      "Epoch [4/5], Step [8768/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [8770/10336], Loss: 0.2343\n",
      "Epoch [4/5], Step [8772/10336], Loss: 0.0052\n",
      "Epoch [4/5], Step [8774/10336], Loss: 0.6042\n",
      "Epoch [4/5], Step [8776/10336], Loss: 0.0881\n",
      "Epoch [4/5], Step [8778/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [8780/10336], Loss: 1.2368\n",
      "Epoch [4/5], Step [8782/10336], Loss: 0.0894\n",
      "Epoch [4/5], Step [8784/10336], Loss: 0.3242\n",
      "Epoch [4/5], Step [8786/10336], Loss: 0.4702\n",
      "Epoch [4/5], Step [8788/10336], Loss: 0.2420\n",
      "Epoch [4/5], Step [8790/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [8792/10336], Loss: 0.7033\n",
      "Epoch [4/5], Step [8794/10336], Loss: 0.0633\n",
      "Epoch [4/5], Step [8796/10336], Loss: 0.0312\n",
      "Epoch [4/5], Step [8798/10336], Loss: 0.0512\n",
      "Epoch [4/5], Step [8800/10336], Loss: 0.0873\n",
      "Epoch [4/5], Step [8802/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [8804/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [8806/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [8808/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [8810/10336], Loss: 0.3101\n",
      "Epoch [4/5], Step [8812/10336], Loss: 0.4765\n",
      "Epoch [4/5], Step [8814/10336], Loss: 0.3585\n",
      "Epoch [4/5], Step [8816/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [8818/10336], Loss: 0.1424\n",
      "Epoch [4/5], Step [8820/10336], Loss: 0.0186\n",
      "Epoch [4/5], Step [8822/10336], Loss: 0.4511\n",
      "Epoch [4/5], Step [8824/10336], Loss: 0.9790\n",
      "Epoch [4/5], Step [8826/10336], Loss: 0.0422\n",
      "Epoch [4/5], Step [8828/10336], Loss: 1.3594\n",
      "Epoch [4/5], Step [8830/10336], Loss: 0.1209\n",
      "Epoch [4/5], Step [8832/10336], Loss: 0.1689\n",
      "Epoch [4/5], Step [8834/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [8836/10336], Loss: 0.0230\n",
      "Epoch [4/5], Step [8838/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [8840/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [8842/10336], Loss: 0.0078\n",
      "Epoch [4/5], Step [8844/10336], Loss: 0.1496\n",
      "Epoch [4/5], Step [8846/10336], Loss: 0.0440\n",
      "Epoch [4/5], Step [8848/10336], Loss: 0.7792\n",
      "Epoch [4/5], Step [8850/10336], Loss: 0.0317\n",
      "Epoch [4/5], Step [8852/10336], Loss: 0.5884\n",
      "Epoch [4/5], Step [8854/10336], Loss: 3.0489\n",
      "Epoch [4/5], Step [8856/10336], Loss: 0.1378\n",
      "Epoch [4/5], Step [8858/10336], Loss: 2.8354\n",
      "Epoch [4/5], Step [8860/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [8862/10336], Loss: 0.0818\n",
      "Epoch [4/5], Step [8864/10336], Loss: 0.0381\n",
      "Epoch [4/5], Step [8866/10336], Loss: 0.8989\n",
      "Epoch [4/5], Step [8868/10336], Loss: 0.1152\n",
      "Epoch [4/5], Step [8870/10336], Loss: 0.1161\n",
      "Epoch [4/5], Step [8872/10336], Loss: 0.0158\n",
      "Epoch [4/5], Step [8874/10336], Loss: 0.0971\n",
      "Epoch [4/5], Step [8876/10336], Loss: 0.2075\n",
      "Epoch [4/5], Step [8878/10336], Loss: 1.0305\n",
      "Epoch [4/5], Step [8880/10336], Loss: 0.1353\n",
      "Epoch [4/5], Step [8882/10336], Loss: 0.2734\n",
      "Epoch [4/5], Step [8884/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [8886/10336], Loss: 0.6694\n",
      "Epoch [4/5], Step [8888/10336], Loss: 2.8153\n",
      "Epoch [4/5], Step [8890/10336], Loss: 4.5675\n",
      "Epoch [4/5], Step [8892/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [8894/10336], Loss: 0.0472\n",
      "Epoch [4/5], Step [8896/10336], Loss: 0.0628\n",
      "Epoch [4/5], Step [8898/10336], Loss: 0.0364\n",
      "Epoch [4/5], Step [8900/10336], Loss: 0.1823\n",
      "Epoch [4/5], Step [8902/10336], Loss: 0.3275\n",
      "Epoch [4/5], Step [8904/10336], Loss: 1.2170\n",
      "Epoch [4/5], Step [8906/10336], Loss: 0.3223\n",
      "Epoch [4/5], Step [8908/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [8910/10336], Loss: 0.3584\n",
      "Epoch [4/5], Step [8912/10336], Loss: 0.4054\n",
      "Epoch [4/5], Step [8914/10336], Loss: 0.0735\n",
      "Epoch [4/5], Step [8916/10336], Loss: 0.2575\n",
      "Epoch [4/5], Step [8918/10336], Loss: 0.1972\n",
      "Epoch [4/5], Step [8920/10336], Loss: 1.3612\n",
      "Epoch [4/5], Step [8922/10336], Loss: 1.6475\n",
      "Epoch [4/5], Step [8924/10336], Loss: 0.0679\n",
      "Epoch [4/5], Step [8926/10336], Loss: 1.4862\n",
      "Epoch [4/5], Step [8928/10336], Loss: 0.3352\n",
      "Epoch [4/5], Step [8930/10336], Loss: 0.5782\n",
      "Epoch [4/5], Step [8932/10336], Loss: 0.4544\n",
      "Epoch [4/5], Step [8934/10336], Loss: 0.0032\n",
      "Epoch [4/5], Step [8936/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [8938/10336], Loss: 0.4109\n",
      "Epoch [4/5], Step [8940/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [8942/10336], Loss: 0.1128\n",
      "Epoch [4/5], Step [8944/10336], Loss: 0.4433\n",
      "Epoch [4/5], Step [8946/10336], Loss: 0.0960\n",
      "Epoch [4/5], Step [8948/10336], Loss: 0.2472\n",
      "Epoch [4/5], Step [8950/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [8952/10336], Loss: 0.0196\n",
      "Epoch [4/5], Step [8954/10336], Loss: 0.3662\n",
      "Epoch [4/5], Step [8956/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [8958/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [8960/10336], Loss: 0.1502\n",
      "Epoch [4/5], Step [8962/10336], Loss: 0.2603\n",
      "Epoch [4/5], Step [8964/10336], Loss: 0.1589\n",
      "Epoch [4/5], Step [8966/10336], Loss: 0.6940\n",
      "Epoch [4/5], Step [8968/10336], Loss: 0.3242\n",
      "Epoch [4/5], Step [8970/10336], Loss: 0.2459\n",
      "Epoch [4/5], Step [8972/10336], Loss: 0.4218\n",
      "Epoch [4/5], Step [8974/10336], Loss: 2.1278\n",
      "Epoch [4/5], Step [8976/10336], Loss: 0.0464\n",
      "Epoch [4/5], Step [8978/10336], Loss: 0.1129\n",
      "Epoch [4/5], Step [8980/10336], Loss: 0.3151\n",
      "Epoch [4/5], Step [8982/10336], Loss: 3.8584\n",
      "Epoch [4/5], Step [8984/10336], Loss: 0.4210\n",
      "Epoch [4/5], Step [8986/10336], Loss: 4.3430\n",
      "Epoch [4/5], Step [8988/10336], Loss: 0.4207\n",
      "Epoch [4/5], Step [8990/10336], Loss: 0.0616\n",
      "Epoch [4/5], Step [8992/10336], Loss: 0.2292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [8994/10336], Loss: 0.0278\n",
      "Epoch [4/5], Step [8996/10336], Loss: 0.7324\n",
      "Epoch [4/5], Step [8998/10336], Loss: 0.1703\n",
      "Epoch [4/5], Step [9000/10336], Loss: 1.3114\n",
      "Epoch [4/5], Step [9002/10336], Loss: 0.0954\n",
      "Epoch [4/5], Step [9004/10336], Loss: 0.0549\n",
      "Epoch [4/5], Step [9006/10336], Loss: 0.7650\n",
      "Epoch [4/5], Step [9008/10336], Loss: 0.0088\n",
      "Epoch [4/5], Step [9010/10336], Loss: 2.4777\n",
      "Epoch [4/5], Step [9012/10336], Loss: 0.4069\n",
      "Epoch [4/5], Step [9014/10336], Loss: 0.1993\n",
      "Epoch [4/5], Step [9016/10336], Loss: 1.2949\n",
      "Epoch [4/5], Step [9018/10336], Loss: 0.2106\n",
      "Epoch [4/5], Step [9020/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [9022/10336], Loss: 0.1309\n",
      "Epoch [4/5], Step [9024/10336], Loss: 0.1540\n",
      "Epoch [4/5], Step [9026/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [9028/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [9030/10336], Loss: 0.6220\n",
      "Epoch [4/5], Step [9032/10336], Loss: 0.0114\n",
      "Epoch [4/5], Step [9034/10336], Loss: 0.0069\n",
      "Epoch [4/5], Step [9036/10336], Loss: 0.0528\n",
      "Epoch [4/5], Step [9038/10336], Loss: 0.4180\n",
      "Epoch [4/5], Step [9040/10336], Loss: 0.0665\n",
      "Epoch [4/5], Step [9042/10336], Loss: 0.1463\n",
      "Epoch [4/5], Step [9044/10336], Loss: 0.3143\n",
      "Epoch [4/5], Step [9046/10336], Loss: 0.1938\n",
      "Epoch [4/5], Step [9048/10336], Loss: 0.5899\n",
      "Epoch [4/5], Step [9050/10336], Loss: 0.0753\n",
      "Epoch [4/5], Step [9052/10336], Loss: 0.1256\n",
      "Epoch [4/5], Step [9054/10336], Loss: 0.0804\n",
      "Epoch [4/5], Step [9056/10336], Loss: 0.2281\n",
      "Epoch [4/5], Step [9058/10336], Loss: 0.0089\n",
      "Epoch [4/5], Step [9060/10336], Loss: 0.1548\n",
      "Epoch [4/5], Step [9062/10336], Loss: 0.2432\n",
      "Epoch [4/5], Step [9064/10336], Loss: 4.5342\n",
      "Epoch [4/5], Step [9066/10336], Loss: 6.9691\n",
      "Epoch [4/5], Step [9068/10336], Loss: 2.2663\n",
      "Epoch [4/5], Step [9070/10336], Loss: 0.0814\n",
      "Epoch [4/5], Step [9072/10336], Loss: 4.2759\n",
      "Epoch [4/5], Step [9074/10336], Loss: 0.1315\n",
      "Epoch [4/5], Step [9076/10336], Loss: 2.2922\n",
      "Epoch [4/5], Step [9078/10336], Loss: 0.1176\n",
      "Epoch [4/5], Step [9080/10336], Loss: 0.2609\n",
      "Epoch [4/5], Step [9082/10336], Loss: 0.0094\n",
      "Epoch [4/5], Step [9084/10336], Loss: 1.8163\n",
      "Epoch [4/5], Step [9086/10336], Loss: 0.1106\n",
      "Epoch [4/5], Step [9088/10336], Loss: 0.6779\n",
      "Epoch [4/5], Step [9090/10336], Loss: 0.7558\n",
      "Epoch [4/5], Step [9092/10336], Loss: 0.6038\n",
      "Epoch [4/5], Step [9094/10336], Loss: 1.0921\n",
      "Epoch [4/5], Step [9096/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [9098/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [9100/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [9102/10336], Loss: 0.3301\n",
      "Epoch [4/5], Step [9104/10336], Loss: 2.7493\n",
      "Epoch [4/5], Step [9106/10336], Loss: 0.3656\n",
      "Epoch [4/5], Step [9108/10336], Loss: 0.0922\n",
      "Epoch [4/5], Step [9110/10336], Loss: 0.4291\n",
      "Epoch [4/5], Step [9112/10336], Loss: 1.4918\n",
      "Epoch [4/5], Step [9114/10336], Loss: 0.3018\n",
      "Epoch [4/5], Step [9116/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [9118/10336], Loss: 0.4139\n",
      "Epoch [4/5], Step [9120/10336], Loss: 3.0391\n",
      "Epoch [4/5], Step [9122/10336], Loss: 0.0407\n",
      "Epoch [4/5], Step [9124/10336], Loss: 0.0425\n",
      "Epoch [4/5], Step [9126/10336], Loss: 0.0160\n",
      "Epoch [4/5], Step [9128/10336], Loss: 0.2141\n",
      "Epoch [4/5], Step [9130/10336], Loss: 1.4462\n",
      "Epoch [4/5], Step [9132/10336], Loss: 0.0079\n",
      "Epoch [4/5], Step [9134/10336], Loss: 0.1329\n",
      "Epoch [4/5], Step [9136/10336], Loss: 3.1917\n",
      "Epoch [4/5], Step [9138/10336], Loss: 0.8470\n",
      "Epoch [4/5], Step [9140/10336], Loss: 0.1208\n",
      "Epoch [4/5], Step [9142/10336], Loss: 0.3767\n",
      "Epoch [4/5], Step [9144/10336], Loss: 0.0559\n",
      "Epoch [4/5], Step [9146/10336], Loss: 1.5845\n",
      "Epoch [4/5], Step [9148/10336], Loss: 0.1711\n",
      "Epoch [4/5], Step [9150/10336], Loss: 3.8262\n",
      "Epoch [4/5], Step [9152/10336], Loss: 0.1025\n",
      "Epoch [4/5], Step [9154/10336], Loss: 0.7065\n",
      "Epoch [4/5], Step [9156/10336], Loss: 1.7102\n",
      "Epoch [4/5], Step [9158/10336], Loss: 0.4323\n",
      "Epoch [4/5], Step [9160/10336], Loss: 1.5130\n",
      "Epoch [4/5], Step [9162/10336], Loss: 0.1126\n",
      "Epoch [4/5], Step [9164/10336], Loss: 0.3821\n",
      "Epoch [4/5], Step [9166/10336], Loss: 0.2842\n",
      "Epoch [4/5], Step [9168/10336], Loss: 1.5907\n",
      "Epoch [4/5], Step [9170/10336], Loss: 0.6395\n",
      "Epoch [4/5], Step [9172/10336], Loss: 2.6849\n",
      "Epoch [4/5], Step [9174/10336], Loss: 0.4309\n",
      "Epoch [4/5], Step [9176/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [9178/10336], Loss: 0.0958\n",
      "Epoch [4/5], Step [9180/10336], Loss: 1.7450\n",
      "Epoch [4/5], Step [9182/10336], Loss: 1.7515\n",
      "Epoch [4/5], Step [9184/10336], Loss: 0.0899\n",
      "Epoch [4/5], Step [9186/10336], Loss: 0.9178\n",
      "Epoch [4/5], Step [9188/10336], Loss: 0.1376\n",
      "Epoch [4/5], Step [9190/10336], Loss: 0.2087\n",
      "Epoch [4/5], Step [9192/10336], Loss: 0.2392\n",
      "Epoch [4/5], Step [9194/10336], Loss: 1.2068\n",
      "Epoch [4/5], Step [9196/10336], Loss: 1.3758\n",
      "Epoch [4/5], Step [9198/10336], Loss: 0.2197\n",
      "Epoch [4/5], Step [9200/10336], Loss: 2.7737\n",
      "Epoch [4/5], Step [9202/10336], Loss: 0.0814\n",
      "Epoch [4/5], Step [9204/10336], Loss: 0.3641\n",
      "Epoch [4/5], Step [9206/10336], Loss: 1.3214\n",
      "Epoch [4/5], Step [9208/10336], Loss: 1.4611\n",
      "Epoch [4/5], Step [9210/10336], Loss: 0.0077\n",
      "Epoch [4/5], Step [9212/10336], Loss: 0.0129\n",
      "Epoch [4/5], Step [9214/10336], Loss: 0.9855\n",
      "Epoch [4/5], Step [9216/10336], Loss: 0.5432\n",
      "Epoch [4/5], Step [9218/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [9220/10336], Loss: 0.2029\n",
      "Epoch [4/5], Step [9222/10336], Loss: 0.0862\n",
      "Epoch [4/5], Step [9224/10336], Loss: 0.0052\n",
      "Epoch [4/5], Step [9226/10336], Loss: 0.1537\n",
      "Epoch [4/5], Step [9228/10336], Loss: 2.4121\n",
      "Epoch [4/5], Step [9230/10336], Loss: 0.2016\n",
      "Epoch [4/5], Step [9232/10336], Loss: 0.8969\n",
      "Epoch [4/5], Step [9234/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [9236/10336], Loss: 2.6997\n",
      "Epoch [4/5], Step [9238/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [9240/10336], Loss: 0.0261\n",
      "Epoch [4/5], Step [9242/10336], Loss: 3.2081\n",
      "Epoch [4/5], Step [9244/10336], Loss: 0.5474\n",
      "Epoch [4/5], Step [9246/10336], Loss: 0.3014\n",
      "Epoch [4/5], Step [9248/10336], Loss: 1.4308\n",
      "Epoch [4/5], Step [9250/10336], Loss: 0.1437\n",
      "Epoch [4/5], Step [9252/10336], Loss: 0.9389\n",
      "Epoch [4/5], Step [9254/10336], Loss: 1.1937\n",
      "Epoch [4/5], Step [9256/10336], Loss: 0.2530\n",
      "Epoch [4/5], Step [9258/10336], Loss: 0.6963\n",
      "Epoch [4/5], Step [9260/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [9262/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [9264/10336], Loss: 0.9542\n",
      "Epoch [4/5], Step [9266/10336], Loss: 0.4475\n",
      "Epoch [4/5], Step [9268/10336], Loss: 0.6365\n",
      "Epoch [4/5], Step [9270/10336], Loss: 0.6507\n",
      "Epoch [4/5], Step [9272/10336], Loss: 0.4725\n",
      "Epoch [4/5], Step [9274/10336], Loss: 0.8103\n",
      "Epoch [4/5], Step [9276/10336], Loss: 3.0650\n",
      "Epoch [4/5], Step [9278/10336], Loss: 0.2801\n",
      "Epoch [4/5], Step [9280/10336], Loss: 0.2381\n",
      "Epoch [4/5], Step [9282/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [9284/10336], Loss: 1.1733\n",
      "Epoch [4/5], Step [9286/10336], Loss: 0.0788\n",
      "Epoch [4/5], Step [9288/10336], Loss: 0.0866\n",
      "Epoch [4/5], Step [9290/10336], Loss: 0.0107\n",
      "Epoch [4/5], Step [9292/10336], Loss: 0.0881\n",
      "Epoch [4/5], Step [9294/10336], Loss: 0.6426\n",
      "Epoch [4/5], Step [9296/10336], Loss: 0.0611\n",
      "Epoch [4/5], Step [9298/10336], Loss: 1.7899\n",
      "Epoch [4/5], Step [9300/10336], Loss: 0.0611\n",
      "Epoch [4/5], Step [9302/10336], Loss: 0.0544\n",
      "Epoch [4/5], Step [9304/10336], Loss: 0.3174\n",
      "Epoch [4/5], Step [9306/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [9308/10336], Loss: 0.0305\n",
      "Epoch [4/5], Step [9310/10336], Loss: 0.6240\n",
      "Epoch [4/5], Step [9312/10336], Loss: 0.2258\n",
      "Epoch [4/5], Step [9314/10336], Loss: 0.2991\n",
      "Epoch [4/5], Step [9316/10336], Loss: 0.1276\n",
      "Epoch [4/5], Step [9318/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [9320/10336], Loss: 0.4277\n",
      "Epoch [4/5], Step [9322/10336], Loss: 0.3990\n",
      "Epoch [4/5], Step [9324/10336], Loss: 0.2142\n",
      "Epoch [4/5], Step [9326/10336], Loss: 0.3885\n",
      "Epoch [4/5], Step [9328/10336], Loss: 0.3518\n",
      "Epoch [4/5], Step [9330/10336], Loss: 2.3397\n",
      "Epoch [4/5], Step [9332/10336], Loss: 0.0392\n",
      "Epoch [4/5], Step [9334/10336], Loss: 0.6986\n",
      "Epoch [4/5], Step [9336/10336], Loss: 0.1094\n",
      "Epoch [4/5], Step [9338/10336], Loss: 1.0989\n",
      "Epoch [4/5], Step [9340/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [9342/10336], Loss: 1.6322\n",
      "Epoch [4/5], Step [9344/10336], Loss: 0.1482\n",
      "Epoch [4/5], Step [9346/10336], Loss: 0.1888\n",
      "Epoch [4/5], Step [9348/10336], Loss: 1.1171\n",
      "Epoch [4/5], Step [9350/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [9352/10336], Loss: 0.4808\n",
      "Epoch [4/5], Step [9354/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [9356/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [9358/10336], Loss: 0.0341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [9360/10336], Loss: 0.1706\n",
      "Epoch [4/5], Step [9362/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [9364/10336], Loss: 0.3034\n",
      "Epoch [4/5], Step [9366/10336], Loss: 2.3362\n",
      "Epoch [4/5], Step [9368/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [9370/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [9372/10336], Loss: 0.1275\n",
      "Epoch [4/5], Step [9374/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [9376/10336], Loss: 0.9611\n",
      "Epoch [4/5], Step [9378/10336], Loss: 0.0538\n",
      "Epoch [4/5], Step [9380/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [9382/10336], Loss: 1.3225\n",
      "Epoch [4/5], Step [9384/10336], Loss: 1.2214\n",
      "Epoch [4/5], Step [9386/10336], Loss: 0.1738\n",
      "Epoch [4/5], Step [9388/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [9390/10336], Loss: 0.0090\n",
      "Epoch [4/5], Step [9392/10336], Loss: 0.4186\n",
      "Epoch [4/5], Step [9394/10336], Loss: 0.0844\n",
      "Epoch [4/5], Step [9396/10336], Loss: 0.1679\n",
      "Epoch [4/5], Step [9398/10336], Loss: 0.9040\n",
      "Epoch [4/5], Step [9400/10336], Loss: 0.0227\n",
      "Epoch [4/5], Step [9402/10336], Loss: 0.3007\n",
      "Epoch [4/5], Step [9404/10336], Loss: 0.9405\n",
      "Epoch [4/5], Step [9406/10336], Loss: 0.0849\n",
      "Epoch [4/5], Step [9408/10336], Loss: 0.0358\n",
      "Epoch [4/5], Step [9410/10336], Loss: 0.3076\n",
      "Epoch [4/5], Step [9412/10336], Loss: 0.3312\n",
      "Epoch [4/5], Step [9414/10336], Loss: 0.0064\n",
      "Epoch [4/5], Step [9416/10336], Loss: 0.4401\n",
      "Epoch [4/5], Step [9418/10336], Loss: 0.6359\n",
      "Epoch [4/5], Step [9420/10336], Loss: 0.3435\n",
      "Epoch [4/5], Step [9422/10336], Loss: 0.4093\n",
      "Epoch [4/5], Step [9424/10336], Loss: 0.3258\n",
      "Epoch [4/5], Step [9426/10336], Loss: 3.0538\n",
      "Epoch [4/5], Step [9428/10336], Loss: 1.4802\n",
      "Epoch [4/5], Step [9430/10336], Loss: 0.4454\n",
      "Epoch [4/5], Step [9432/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [9434/10336], Loss: 0.5986\n",
      "Epoch [4/5], Step [9436/10336], Loss: 1.5561\n",
      "Epoch [4/5], Step [9438/10336], Loss: 0.2204\n",
      "Epoch [4/5], Step [9440/10336], Loss: 0.6221\n",
      "Epoch [4/5], Step [9442/10336], Loss: 0.2415\n",
      "Epoch [4/5], Step [9444/10336], Loss: 0.2400\n",
      "Epoch [4/5], Step [9446/10336], Loss: 4.2557\n",
      "Epoch [4/5], Step [9448/10336], Loss: 1.2755\n",
      "Epoch [4/5], Step [9450/10336], Loss: 0.3768\n",
      "Epoch [4/5], Step [9452/10336], Loss: 0.0703\n",
      "Epoch [4/5], Step [9454/10336], Loss: 0.2349\n",
      "Epoch [4/5], Step [9456/10336], Loss: 0.0231\n",
      "Epoch [4/5], Step [9458/10336], Loss: 0.3444\n",
      "Epoch [4/5], Step [9460/10336], Loss: 0.1382\n",
      "Epoch [4/5], Step [9462/10336], Loss: 0.3584\n",
      "Epoch [4/5], Step [9464/10336], Loss: 0.1895\n",
      "Epoch [4/5], Step [9466/10336], Loss: 0.1097\n",
      "Epoch [4/5], Step [9468/10336], Loss: 0.1546\n",
      "Epoch [4/5], Step [9470/10336], Loss: 0.5643\n",
      "Epoch [4/5], Step [9472/10336], Loss: 2.4452\n",
      "Epoch [4/5], Step [9474/10336], Loss: 0.2444\n",
      "Epoch [4/5], Step [9476/10336], Loss: 0.6369\n",
      "Epoch [4/5], Step [9478/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [9480/10336], Loss: 1.6839\n",
      "Epoch [4/5], Step [9482/10336], Loss: 0.1852\n",
      "Epoch [4/5], Step [9484/10336], Loss: 0.5945\n",
      "Epoch [4/5], Step [9486/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [9488/10336], Loss: 0.1778\n",
      "Epoch [4/5], Step [9490/10336], Loss: 1.1334\n",
      "Epoch [4/5], Step [9492/10336], Loss: 0.4007\n",
      "Epoch [4/5], Step [9494/10336], Loss: 0.0255\n",
      "Epoch [4/5], Step [9496/10336], Loss: 0.6942\n",
      "Epoch [4/5], Step [9498/10336], Loss: 0.0446\n",
      "Epoch [4/5], Step [9500/10336], Loss: 0.2472\n",
      "Epoch [4/5], Step [9502/10336], Loss: 0.0323\n",
      "Epoch [4/5], Step [9504/10336], Loss: 0.0500\n",
      "Epoch [4/5], Step [9506/10336], Loss: 0.0895\n",
      "Epoch [4/5], Step [9508/10336], Loss: 0.0775\n",
      "Epoch [4/5], Step [9510/10336], Loss: 1.3065\n",
      "Epoch [4/5], Step [9512/10336], Loss: 0.1049\n",
      "Epoch [4/5], Step [9514/10336], Loss: 1.3670\n",
      "Epoch [4/5], Step [9516/10336], Loss: 0.0929\n",
      "Epoch [4/5], Step [9518/10336], Loss: 1.0130\n",
      "Epoch [4/5], Step [9520/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [9522/10336], Loss: 0.3740\n",
      "Epoch [4/5], Step [9524/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [9526/10336], Loss: 2.8164\n",
      "Epoch [4/5], Step [9528/10336], Loss: 0.2029\n",
      "Epoch [4/5], Step [9530/10336], Loss: 1.4389\n",
      "Epoch [4/5], Step [9532/10336], Loss: 0.0708\n",
      "Epoch [4/5], Step [9534/10336], Loss: 0.2597\n",
      "Epoch [4/5], Step [9536/10336], Loss: 0.5226\n",
      "Epoch [4/5], Step [9538/10336], Loss: 0.4455\n",
      "Epoch [4/5], Step [9540/10336], Loss: 1.9087\n",
      "Epoch [4/5], Step [9542/10336], Loss: 0.9898\n",
      "Epoch [4/5], Step [9544/10336], Loss: 1.7565\n",
      "Epoch [4/5], Step [9546/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [9548/10336], Loss: 0.1968\n",
      "Epoch [4/5], Step [9550/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [9552/10336], Loss: 0.0135\n",
      "Epoch [4/5], Step [9554/10336], Loss: 1.9249\n",
      "Epoch [4/5], Step [9556/10336], Loss: 2.6924\n",
      "Epoch [4/5], Step [9558/10336], Loss: 0.7384\n",
      "Epoch [4/5], Step [9560/10336], Loss: 0.2114\n",
      "Epoch [4/5], Step [9562/10336], Loss: 3.8125\n",
      "Epoch [4/5], Step [9564/10336], Loss: 0.1080\n",
      "Epoch [4/5], Step [9566/10336], Loss: 0.1886\n",
      "Epoch [4/5], Step [9568/10336], Loss: 2.1854\n",
      "Epoch [4/5], Step [9570/10336], Loss: 0.1299\n",
      "Epoch [4/5], Step [9572/10336], Loss: 0.3118\n",
      "Epoch [4/5], Step [9574/10336], Loss: 0.0309\n",
      "Epoch [4/5], Step [9576/10336], Loss: 1.0555\n",
      "Epoch [4/5], Step [9578/10336], Loss: 0.2979\n",
      "Epoch [4/5], Step [9580/10336], Loss: 0.6205\n",
      "Epoch [4/5], Step [9582/10336], Loss: 0.3923\n",
      "Epoch [4/5], Step [9584/10336], Loss: 0.5550\n",
      "Epoch [4/5], Step [9586/10336], Loss: 0.3446\n",
      "Epoch [4/5], Step [9588/10336], Loss: 1.6490\n",
      "Epoch [4/5], Step [9590/10336], Loss: 0.8418\n",
      "Epoch [4/5], Step [9592/10336], Loss: 0.7168\n",
      "Epoch [4/5], Step [9594/10336], Loss: 0.2677\n",
      "Epoch [4/5], Step [9596/10336], Loss: 0.1095\n",
      "Epoch [4/5], Step [9598/10336], Loss: 0.3145\n",
      "Epoch [4/5], Step [9600/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [9602/10336], Loss: 0.0734\n",
      "Epoch [4/5], Step [9604/10336], Loss: 0.0055\n",
      "Epoch [4/5], Step [9606/10336], Loss: 0.9679\n",
      "Epoch [4/5], Step [9608/10336], Loss: 0.3054\n",
      "Epoch [4/5], Step [9610/10336], Loss: 0.8396\n",
      "Epoch [4/5], Step [9612/10336], Loss: 0.6655\n",
      "Epoch [4/5], Step [9614/10336], Loss: 0.0354\n",
      "Epoch [4/5], Step [9616/10336], Loss: 0.2203\n",
      "Epoch [4/5], Step [9618/10336], Loss: 0.4083\n",
      "Epoch [4/5], Step [9620/10336], Loss: 0.0369\n",
      "Epoch [4/5], Step [9622/10336], Loss: 1.7035\n",
      "Epoch [4/5], Step [9624/10336], Loss: 0.2278\n",
      "Epoch [4/5], Step [9626/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [9628/10336], Loss: 0.4192\n",
      "Epoch [4/5], Step [9630/10336], Loss: 0.1084\n",
      "Epoch [4/5], Step [9632/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [9634/10336], Loss: 0.0451\n",
      "Epoch [4/5], Step [9636/10336], Loss: 2.0177\n",
      "Epoch [4/5], Step [9638/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [9640/10336], Loss: 0.0877\n",
      "Epoch [4/5], Step [9642/10336], Loss: 0.4511\n",
      "Epoch [4/5], Step [9644/10336], Loss: 0.1281\n",
      "Epoch [4/5], Step [9646/10336], Loss: 0.0282\n",
      "Epoch [4/5], Step [9648/10336], Loss: 2.9953\n",
      "Epoch [4/5], Step [9650/10336], Loss: 0.5524\n",
      "Epoch [4/5], Step [9652/10336], Loss: 2.9874\n",
      "Epoch [4/5], Step [9654/10336], Loss: 1.1026\n",
      "Epoch [4/5], Step [9656/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [9658/10336], Loss: 0.0233\n",
      "Epoch [4/5], Step [9660/10336], Loss: 0.5321\n",
      "Epoch [4/5], Step [9662/10336], Loss: 0.1119\n",
      "Epoch [4/5], Step [9664/10336], Loss: 0.0232\n",
      "Epoch [4/5], Step [9666/10336], Loss: 0.4245\n",
      "Epoch [4/5], Step [9668/10336], Loss: 0.0187\n",
      "Epoch [4/5], Step [9670/10336], Loss: 0.0219\n",
      "Epoch [4/5], Step [9672/10336], Loss: 3.8582\n",
      "Epoch [4/5], Step [9674/10336], Loss: 0.1577\n",
      "Epoch [4/5], Step [9676/10336], Loss: 0.1859\n",
      "Epoch [4/5], Step [9678/10336], Loss: 0.1052\n",
      "Epoch [4/5], Step [9680/10336], Loss: 1.4512\n",
      "Epoch [4/5], Step [9682/10336], Loss: 0.0075\n",
      "Epoch [4/5], Step [9684/10336], Loss: 0.1000\n",
      "Epoch [4/5], Step [9686/10336], Loss: 0.7758\n",
      "Epoch [4/5], Step [9688/10336], Loss: 0.0206\n",
      "Epoch [4/5], Step [9690/10336], Loss: 0.1709\n",
      "Epoch [4/5], Step [9692/10336], Loss: 3.6616\n",
      "Epoch [4/5], Step [9694/10336], Loss: 3.9726\n",
      "Epoch [4/5], Step [9696/10336], Loss: 0.0384\n",
      "Epoch [4/5], Step [9698/10336], Loss: 0.0546\n",
      "Epoch [4/5], Step [9700/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [9702/10336], Loss: 2.1724\n",
      "Epoch [4/5], Step [9704/10336], Loss: 0.0580\n",
      "Epoch [4/5], Step [9706/10336], Loss: 0.0229\n",
      "Epoch [4/5], Step [9708/10336], Loss: 1.3608\n",
      "Epoch [4/5], Step [9710/10336], Loss: 0.0102\n",
      "Epoch [4/5], Step [9712/10336], Loss: 0.0321\n",
      "Epoch [4/5], Step [9714/10336], Loss: 1.4208\n",
      "Epoch [4/5], Step [9716/10336], Loss: 0.3862\n",
      "Epoch [4/5], Step [9718/10336], Loss: 2.0518\n",
      "Epoch [4/5], Step [9720/10336], Loss: 0.2932\n",
      "Epoch [4/5], Step [9722/10336], Loss: 0.2530\n",
      "Epoch [4/5], Step [9724/10336], Loss: 2.2608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [9726/10336], Loss: 0.9462\n",
      "Epoch [4/5], Step [9728/10336], Loss: 0.1212\n",
      "Epoch [4/5], Step [9730/10336], Loss: 0.0476\n",
      "Epoch [4/5], Step [9732/10336], Loss: 0.0196\n",
      "Epoch [4/5], Step [9734/10336], Loss: 0.1877\n",
      "Epoch [4/5], Step [9736/10336], Loss: 0.1068\n",
      "Epoch [4/5], Step [9738/10336], Loss: 1.1812\n",
      "Epoch [4/5], Step [9740/10336], Loss: 0.0179\n",
      "Epoch [4/5], Step [9742/10336], Loss: 0.0528\n",
      "Epoch [4/5], Step [9744/10336], Loss: 3.2092\n",
      "Epoch [4/5], Step [9746/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [9748/10336], Loss: 0.0166\n",
      "Epoch [4/5], Step [9750/10336], Loss: 2.3284\n",
      "Epoch [4/5], Step [9752/10336], Loss: 0.5283\n",
      "Epoch [4/5], Step [9754/10336], Loss: 0.0184\n",
      "Epoch [4/5], Step [9756/10336], Loss: 0.2429\n",
      "Epoch [4/5], Step [9758/10336], Loss: 0.1543\n",
      "Epoch [4/5], Step [9760/10336], Loss: 0.2899\n",
      "Epoch [4/5], Step [9762/10336], Loss: 0.2009\n",
      "Epoch [4/5], Step [9764/10336], Loss: 0.2008\n",
      "Epoch [4/5], Step [9766/10336], Loss: 0.2628\n",
      "Epoch [4/5], Step [9768/10336], Loss: 0.2297\n",
      "Epoch [4/5], Step [9770/10336], Loss: 0.0160\n",
      "Epoch [4/5], Step [9772/10336], Loss: 0.0124\n",
      "Epoch [4/5], Step [9774/10336], Loss: 1.8992\n",
      "Epoch [4/5], Step [9776/10336], Loss: 0.0529\n",
      "Epoch [4/5], Step [9778/10336], Loss: 1.4947\n",
      "Epoch [4/5], Step [9780/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [9782/10336], Loss: 0.0928\n",
      "Epoch [4/5], Step [9784/10336], Loss: 0.2315\n",
      "Epoch [4/5], Step [9786/10336], Loss: 1.5753\n",
      "Epoch [4/5], Step [9788/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [9790/10336], Loss: 2.9062\n",
      "Epoch [4/5], Step [9792/10336], Loss: 0.9042\n",
      "Epoch [4/5], Step [9794/10336], Loss: 0.4053\n",
      "Epoch [4/5], Step [9796/10336], Loss: 0.1916\n",
      "Epoch [4/5], Step [9798/10336], Loss: 0.0030\n",
      "Epoch [4/5], Step [9800/10336], Loss: 0.0407\n",
      "Epoch [4/5], Step [9802/10336], Loss: 0.0366\n",
      "Epoch [4/5], Step [9804/10336], Loss: 0.5232\n",
      "Epoch [4/5], Step [9806/10336], Loss: 1.6410\n",
      "Epoch [4/5], Step [9808/10336], Loss: 0.0242\n",
      "Epoch [4/5], Step [9810/10336], Loss: 0.0986\n",
      "Epoch [4/5], Step [9812/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [9814/10336], Loss: 0.1708\n",
      "Epoch [4/5], Step [9816/10336], Loss: 0.0449\n",
      "Epoch [4/5], Step [9818/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [9820/10336], Loss: 0.1478\n",
      "Epoch [4/5], Step [9822/10336], Loss: 0.1930\n",
      "Epoch [4/5], Step [9824/10336], Loss: 0.3570\n",
      "Epoch [4/5], Step [9826/10336], Loss: 3.3322\n",
      "Epoch [4/5], Step [9828/10336], Loss: 0.2497\n",
      "Epoch [4/5], Step [9830/10336], Loss: 1.8690\n",
      "Epoch [4/5], Step [9832/10336], Loss: 0.1991\n",
      "Epoch [4/5], Step [9834/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [9836/10336], Loss: 0.1012\n",
      "Epoch [4/5], Step [9838/10336], Loss: 0.6694\n",
      "Epoch [4/5], Step [9840/10336], Loss: 0.3863\n",
      "Epoch [4/5], Step [9842/10336], Loss: 0.0470\n",
      "Epoch [4/5], Step [9844/10336], Loss: 1.2026\n",
      "Epoch [4/5], Step [9846/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [9848/10336], Loss: 0.0211\n",
      "Epoch [4/5], Step [9850/10336], Loss: 0.0204\n",
      "Epoch [4/5], Step [9852/10336], Loss: 0.1277\n",
      "Epoch [4/5], Step [9854/10336], Loss: 3.4303\n",
      "Epoch [4/5], Step [9856/10336], Loss: 0.1935\n",
      "Epoch [4/5], Step [9858/10336], Loss: 0.4919\n",
      "Epoch [4/5], Step [9860/10336], Loss: 0.0056\n",
      "Epoch [4/5], Step [9862/10336], Loss: 0.3877\n",
      "Epoch [4/5], Step [9864/10336], Loss: 0.4762\n",
      "Epoch [4/5], Step [9866/10336], Loss: 0.2036\n",
      "Epoch [4/5], Step [9868/10336], Loss: 0.1094\n",
      "Epoch [4/5], Step [9870/10336], Loss: 0.0300\n",
      "Epoch [4/5], Step [9872/10336], Loss: 0.3706\n",
      "Epoch [4/5], Step [9874/10336], Loss: 0.0568\n",
      "Epoch [4/5], Step [9876/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [9878/10336], Loss: 3.7518\n",
      "Epoch [4/5], Step [9880/10336], Loss: 0.4812\n",
      "Epoch [4/5], Step [9882/10336], Loss: 0.0566\n",
      "Epoch [4/5], Step [9884/10336], Loss: 0.3519\n",
      "Epoch [4/5], Step [9886/10336], Loss: 0.7286\n",
      "Epoch [4/5], Step [9888/10336], Loss: 0.8809\n",
      "Epoch [4/5], Step [9890/10336], Loss: 0.0410\n",
      "Epoch [4/5], Step [9892/10336], Loss: 0.6152\n",
      "Epoch [4/5], Step [9894/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [9896/10336], Loss: 0.1896\n",
      "Epoch [4/5], Step [9898/10336], Loss: 2.2847\n",
      "Epoch [4/5], Step [9900/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [9902/10336], Loss: 0.4907\n",
      "Epoch [4/5], Step [9904/10336], Loss: 0.2650\n",
      "Epoch [4/5], Step [9906/10336], Loss: 0.1781\n",
      "Epoch [4/5], Step [9908/10336], Loss: 0.0660\n",
      "Epoch [4/5], Step [9910/10336], Loss: 0.5294\n",
      "Epoch [4/5], Step [9912/10336], Loss: 1.3180\n",
      "Epoch [4/5], Step [9914/10336], Loss: 0.0090\n",
      "Epoch [4/5], Step [9916/10336], Loss: 0.8415\n",
      "Epoch [4/5], Step [9918/10336], Loss: 0.2946\n",
      "Epoch [4/5], Step [9920/10336], Loss: 0.7796\n",
      "Epoch [4/5], Step [9922/10336], Loss: 0.0085\n",
      "Epoch [4/5], Step [9924/10336], Loss: 0.2262\n",
      "Epoch [4/5], Step [9926/10336], Loss: 0.1517\n",
      "Epoch [4/5], Step [9928/10336], Loss: 0.4032\n",
      "Epoch [4/5], Step [9930/10336], Loss: 3.0299\n",
      "Epoch [4/5], Step [9932/10336], Loss: 0.1186\n",
      "Epoch [4/5], Step [9934/10336], Loss: 0.4201\n",
      "Epoch [4/5], Step [9936/10336], Loss: 0.0237\n",
      "Epoch [4/5], Step [9938/10336], Loss: 1.2182\n",
      "Epoch [4/5], Step [9940/10336], Loss: 0.0608\n",
      "Epoch [4/5], Step [9942/10336], Loss: 0.3089\n",
      "Epoch [4/5], Step [9944/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [9946/10336], Loss: 2.3966\n",
      "Epoch [4/5], Step [9948/10336], Loss: 0.0082\n",
      "Epoch [4/5], Step [9950/10336], Loss: 0.4668\n",
      "Epoch [4/5], Step [9952/10336], Loss: 0.4447\n",
      "Epoch [4/5], Step [9954/10336], Loss: 0.0884\n",
      "Epoch [4/5], Step [9956/10336], Loss: 0.2386\n",
      "Epoch [4/5], Step [9958/10336], Loss: 0.4133\n",
      "Epoch [4/5], Step [9960/10336], Loss: 0.3115\n",
      "Epoch [4/5], Step [9962/10336], Loss: 1.4916\n",
      "Epoch [4/5], Step [9964/10336], Loss: 1.7275\n",
      "Epoch [4/5], Step [9966/10336], Loss: 5.7620\n",
      "Epoch [4/5], Step [9968/10336], Loss: 0.0027\n",
      "Epoch [4/5], Step [9970/10336], Loss: 0.7816\n",
      "Epoch [4/5], Step [9972/10336], Loss: 0.0460\n",
      "Epoch [4/5], Step [9974/10336], Loss: 0.2033\n",
      "Epoch [4/5], Step [9976/10336], Loss: 0.0517\n",
      "Epoch [4/5], Step [9978/10336], Loss: 0.0328\n",
      "Epoch [4/5], Step [9980/10336], Loss: 0.4756\n",
      "Epoch [4/5], Step [9982/10336], Loss: 0.0689\n",
      "Epoch [4/5], Step [9984/10336], Loss: 2.4219\n",
      "Epoch [4/5], Step [9986/10336], Loss: 2.1224\n",
      "Epoch [4/5], Step [9988/10336], Loss: 1.2562\n",
      "Epoch [4/5], Step [9990/10336], Loss: 0.9601\n",
      "Epoch [4/5], Step [9992/10336], Loss: 0.2744\n",
      "Epoch [4/5], Step [9994/10336], Loss: 0.3954\n",
      "Epoch [4/5], Step [9996/10336], Loss: 0.0319\n",
      "Epoch [4/5], Step [9998/10336], Loss: 0.0124\n",
      "Epoch [4/5], Step [10000/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [10002/10336], Loss: 0.3341\n",
      "Epoch [4/5], Step [10004/10336], Loss: 0.0672\n",
      "Epoch [4/5], Step [10006/10336], Loss: 0.2234\n",
      "Epoch [4/5], Step [10008/10336], Loss: 0.3948\n",
      "Epoch [4/5], Step [10010/10336], Loss: 0.4319\n",
      "Epoch [4/5], Step [10012/10336], Loss: 0.8251\n",
      "Epoch [4/5], Step [10014/10336], Loss: 0.2980\n",
      "Epoch [4/5], Step [10016/10336], Loss: 0.1639\n",
      "Epoch [4/5], Step [10018/10336], Loss: 0.7203\n",
      "Epoch [4/5], Step [10020/10336], Loss: 0.1859\n",
      "Epoch [4/5], Step [10022/10336], Loss: 2.1035\n",
      "Epoch [4/5], Step [10024/10336], Loss: 0.0831\n",
      "Epoch [4/5], Step [10026/10336], Loss: 0.1868\n",
      "Epoch [4/5], Step [10028/10336], Loss: 0.0668\n",
      "Epoch [4/5], Step [10030/10336], Loss: 0.7452\n",
      "Epoch [4/5], Step [10032/10336], Loss: 1.7677\n",
      "Epoch [4/5], Step [10034/10336], Loss: 0.0079\n",
      "Epoch [4/5], Step [10036/10336], Loss: 0.3698\n",
      "Epoch [4/5], Step [10038/10336], Loss: 0.1368\n",
      "Epoch [4/5], Step [10040/10336], Loss: 2.2048\n",
      "Epoch [4/5], Step [10042/10336], Loss: 3.0816\n",
      "Epoch [4/5], Step [10044/10336], Loss: 0.3737\n",
      "Epoch [4/5], Step [10046/10336], Loss: 0.2358\n",
      "Epoch [4/5], Step [10048/10336], Loss: 0.5556\n",
      "Epoch [4/5], Step [10050/10336], Loss: 0.3927\n",
      "Epoch [4/5], Step [10052/10336], Loss: 1.7359\n",
      "Epoch [4/5], Step [10054/10336], Loss: 0.2990\n",
      "Epoch [4/5], Step [10056/10336], Loss: 0.6040\n",
      "Epoch [4/5], Step [10058/10336], Loss: 0.1786\n",
      "Epoch [4/5], Step [10060/10336], Loss: 0.1001\n",
      "Epoch [4/5], Step [10062/10336], Loss: 0.5571\n",
      "Epoch [4/5], Step [10064/10336], Loss: 0.0792\n",
      "Epoch [4/5], Step [10066/10336], Loss: 0.4469\n",
      "Epoch [4/5], Step [10068/10336], Loss: 0.3144\n",
      "Epoch [4/5], Step [10070/10336], Loss: 4.3146\n",
      "Epoch [4/5], Step [10072/10336], Loss: 0.5056\n",
      "Epoch [4/5], Step [10074/10336], Loss: 0.0146\n",
      "Epoch [4/5], Step [10076/10336], Loss: 2.0776\n",
      "Epoch [4/5], Step [10078/10336], Loss: 0.2875\n",
      "Epoch [4/5], Step [10080/10336], Loss: 0.0468\n",
      "Epoch [4/5], Step [10082/10336], Loss: 3.6463\n",
      "Epoch [4/5], Step [10084/10336], Loss: 1.9047\n",
      "Epoch [4/5], Step [10086/10336], Loss: 0.1291\n",
      "Epoch [4/5], Step [10088/10336], Loss: 0.0575\n",
      "Epoch [4/5], Step [10090/10336], Loss: 0.2868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [10092/10336], Loss: 0.0490\n",
      "Epoch [4/5], Step [10094/10336], Loss: 1.4952\n",
      "Epoch [4/5], Step [10096/10336], Loss: 0.1226\n",
      "Epoch [4/5], Step [10098/10336], Loss: 0.1474\n",
      "Epoch [4/5], Step [10100/10336], Loss: 0.1850\n",
      "Epoch [4/5], Step [10102/10336], Loss: 0.3624\n",
      "Epoch [4/5], Step [10104/10336], Loss: 0.3621\n",
      "Epoch [4/5], Step [10106/10336], Loss: 0.4509\n",
      "Epoch [4/5], Step [10108/10336], Loss: 0.5796\n",
      "Epoch [4/5], Step [10110/10336], Loss: 0.0617\n",
      "Epoch [4/5], Step [10112/10336], Loss: 0.8950\n",
      "Epoch [4/5], Step [10114/10336], Loss: 3.4547\n",
      "Epoch [4/5], Step [10116/10336], Loss: 0.0031\n",
      "Epoch [4/5], Step [10118/10336], Loss: 0.1591\n",
      "Epoch [4/5], Step [10120/10336], Loss: 0.6268\n",
      "Epoch [4/5], Step [10122/10336], Loss: 0.4282\n",
      "Epoch [4/5], Step [10124/10336], Loss: 0.3034\n",
      "Epoch [4/5], Step [10126/10336], Loss: 1.1287\n",
      "Epoch [4/5], Step [10128/10336], Loss: 0.3524\n",
      "Epoch [4/5], Step [10130/10336], Loss: 0.2495\n",
      "Epoch [4/5], Step [10132/10336], Loss: 0.3036\n",
      "Epoch [4/5], Step [10134/10336], Loss: 0.4065\n",
      "Epoch [4/5], Step [10136/10336], Loss: 0.0126\n",
      "Epoch [4/5], Step [10138/10336], Loss: 1.1419\n",
      "Epoch [4/5], Step [10140/10336], Loss: 0.0146\n",
      "Epoch [4/5], Step [10142/10336], Loss: 0.0121\n",
      "Epoch [4/5], Step [10144/10336], Loss: 0.0256\n",
      "Epoch [4/5], Step [10146/10336], Loss: 1.5624\n",
      "Epoch [4/5], Step [10148/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [10150/10336], Loss: 0.2454\n",
      "Epoch [4/5], Step [10152/10336], Loss: 0.1634\n",
      "Epoch [4/5], Step [10154/10336], Loss: 0.6619\n",
      "Epoch [4/5], Step [10156/10336], Loss: 0.6751\n",
      "Epoch [4/5], Step [10158/10336], Loss: 0.9753\n",
      "Epoch [4/5], Step [10160/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [10162/10336], Loss: 0.2919\n",
      "Epoch [4/5], Step [10164/10336], Loss: 0.9888\n",
      "Epoch [4/5], Step [10166/10336], Loss: 0.1607\n",
      "Epoch [4/5], Step [10168/10336], Loss: 0.4888\n",
      "Epoch [4/5], Step [10170/10336], Loss: 0.5220\n",
      "Epoch [4/5], Step [10172/10336], Loss: 0.0525\n",
      "Epoch [4/5], Step [10174/10336], Loss: 0.1499\n",
      "Epoch [4/5], Step [10176/10336], Loss: 0.5051\n",
      "Epoch [4/5], Step [10178/10336], Loss: 0.1072\n",
      "Epoch [4/5], Step [10180/10336], Loss: 3.7759\n",
      "Epoch [4/5], Step [10182/10336], Loss: 1.0349\n",
      "Epoch [4/5], Step [10184/10336], Loss: 0.2476\n",
      "Epoch [4/5], Step [10186/10336], Loss: 0.0210\n",
      "Epoch [4/5], Step [10188/10336], Loss: 0.4017\n",
      "Epoch [4/5], Step [10190/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [10192/10336], Loss: 0.1066\n",
      "Epoch [4/5], Step [10194/10336], Loss: 0.4590\n",
      "Epoch [4/5], Step [10196/10336], Loss: 0.1518\n",
      "Epoch [4/5], Step [10198/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [10200/10336], Loss: 0.3329\n",
      "Epoch [4/5], Step [10202/10336], Loss: 0.0056\n",
      "Epoch [4/5], Step [10204/10336], Loss: 2.1976\n",
      "Epoch [4/5], Step [10206/10336], Loss: 0.1330\n",
      "Epoch [4/5], Step [10208/10336], Loss: 0.3004\n",
      "Epoch [4/5], Step [10210/10336], Loss: 0.1519\n",
      "Epoch [4/5], Step [10212/10336], Loss: 0.4658\n",
      "Epoch [4/5], Step [10214/10336], Loss: 0.0526\n",
      "Epoch [4/5], Step [10216/10336], Loss: 0.3215\n",
      "Epoch [4/5], Step [10218/10336], Loss: 0.3138\n",
      "Epoch [4/5], Step [10220/10336], Loss: 0.0112\n",
      "Epoch [4/5], Step [10222/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [10224/10336], Loss: 0.2663\n",
      "Epoch [4/5], Step [10226/10336], Loss: 0.0836\n",
      "Epoch [4/5], Step [10228/10336], Loss: 0.1661\n",
      "Epoch [4/5], Step [10230/10336], Loss: 0.1353\n",
      "Epoch [4/5], Step [10232/10336], Loss: 0.0278\n",
      "Epoch [4/5], Step [10234/10336], Loss: 0.6420\n",
      "Epoch [4/5], Step [10236/10336], Loss: 0.0428\n",
      "Epoch [4/5], Step [10238/10336], Loss: 0.0386\n",
      "Epoch [4/5], Step [10240/10336], Loss: 1.5888\n",
      "Epoch [4/5], Step [10242/10336], Loss: 0.0699\n",
      "Epoch [4/5], Step [10244/10336], Loss: 0.0478\n",
      "Epoch [4/5], Step [10246/10336], Loss: 0.3260\n",
      "Epoch [4/5], Step [10248/10336], Loss: 2.5263\n",
      "Epoch [4/5], Step [10250/10336], Loss: 0.7922\n",
      "Epoch [4/5], Step [10252/10336], Loss: 0.4745\n",
      "Epoch [4/5], Step [10254/10336], Loss: 0.0126\n",
      "Epoch [4/5], Step [10256/10336], Loss: 0.0169\n",
      "Epoch [4/5], Step [10258/10336], Loss: 1.7494\n",
      "Epoch [4/5], Step [10260/10336], Loss: 0.5112\n",
      "Epoch [4/5], Step [10262/10336], Loss: 0.2725\n",
      "Epoch [4/5], Step [10264/10336], Loss: 2.7471\n",
      "Epoch [4/5], Step [10266/10336], Loss: 0.3031\n",
      "Epoch [4/5], Step [10268/10336], Loss: 0.0167\n",
      "Epoch [4/5], Step [10270/10336], Loss: 0.0277\n",
      "Epoch [4/5], Step [10272/10336], Loss: 5.1068\n",
      "Epoch [4/5], Step [10274/10336], Loss: 0.0162\n",
      "Epoch [4/5], Step [10276/10336], Loss: 0.1452\n",
      "Epoch [4/5], Step [10278/10336], Loss: 0.0357\n",
      "Epoch [4/5], Step [10280/10336], Loss: 3.4360\n",
      "Epoch [4/5], Step [10282/10336], Loss: 0.0803\n",
      "Epoch [4/5], Step [10284/10336], Loss: 0.0883\n",
      "Epoch [4/5], Step [10286/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [10288/10336], Loss: 0.3917\n",
      "Epoch [4/5], Step [10290/10336], Loss: 1.9973\n",
      "Epoch [4/5], Step [10292/10336], Loss: 0.0549\n",
      "Epoch [4/5], Step [10294/10336], Loss: 0.0357\n",
      "Epoch [4/5], Step [10296/10336], Loss: 0.1826\n",
      "Epoch [4/5], Step [10298/10336], Loss: 0.2510\n",
      "Epoch [4/5], Step [10300/10336], Loss: 0.0440\n",
      "Epoch [4/5], Step [10302/10336], Loss: 0.7867\n",
      "Epoch [4/5], Step [10304/10336], Loss: 2.8078\n",
      "Epoch [4/5], Step [10306/10336], Loss: 1.3723\n",
      "Epoch [4/5], Step [10308/10336], Loss: 0.0202\n",
      "Epoch [4/5], Step [10310/10336], Loss: 0.0740\n",
      "Epoch [4/5], Step [10312/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [10314/10336], Loss: 0.2690\n",
      "Epoch [4/5], Step [10316/10336], Loss: 0.2221\n",
      "Epoch [4/5], Step [10318/10336], Loss: 1.2451\n",
      "Epoch [4/5], Step [10320/10336], Loss: 0.5864\n",
      "Epoch [4/5], Step [10322/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [10324/10336], Loss: 3.1770\n",
      "Epoch [4/5], Step [10326/10336], Loss: 0.4326\n",
      "Epoch [4/5], Step [10328/10336], Loss: 1.1294\n",
      "Epoch [4/5], Step [10330/10336], Loss: 0.0455\n",
      "Epoch [4/5], Step [10332/10336], Loss: 0.1343\n",
      "Epoch [4/5], Step [10334/10336], Loss: 0.0680\n",
      "Epoch [4/5], Step [10336/10336], Loss: 0.0281\n",
      "Epoch [5/5], Step [2/10336], Loss: 0.3052\n",
      "Epoch [5/5], Step [4/10336], Loss: 0.4109\n",
      "Epoch [5/5], Step [6/10336], Loss: 0.0586\n",
      "Epoch [5/5], Step [8/10336], Loss: 0.0959\n",
      "Epoch [5/5], Step [10/10336], Loss: 0.1074\n",
      "Epoch [5/5], Step [12/10336], Loss: 0.0415\n",
      "Epoch [5/5], Step [14/10336], Loss: 1.5653\n",
      "Epoch [5/5], Step [16/10336], Loss: 0.0885\n",
      "Epoch [5/5], Step [18/10336], Loss: 0.3073\n",
      "Epoch [5/5], Step [20/10336], Loss: 0.1541\n",
      "Epoch [5/5], Step [22/10336], Loss: 0.1288\n",
      "Epoch [5/5], Step [24/10336], Loss: 0.8022\n",
      "Epoch [5/5], Step [26/10336], Loss: 0.2090\n",
      "Epoch [5/5], Step [28/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [30/10336], Loss: 0.0170\n",
      "Epoch [5/5], Step [32/10336], Loss: 1.1770\n",
      "Epoch [5/5], Step [34/10336], Loss: 1.7148\n",
      "Epoch [5/5], Step [36/10336], Loss: 2.7742\n",
      "Epoch [5/5], Step [38/10336], Loss: 0.6008\n",
      "Epoch [5/5], Step [40/10336], Loss: 1.4992\n",
      "Epoch [5/5], Step [42/10336], Loss: 1.1128\n",
      "Epoch [5/5], Step [44/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [46/10336], Loss: 0.5045\n",
      "Epoch [5/5], Step [48/10336], Loss: 0.2328\n",
      "Epoch [5/5], Step [50/10336], Loss: 0.3085\n",
      "Epoch [5/5], Step [52/10336], Loss: 0.0110\n",
      "Epoch [5/5], Step [54/10336], Loss: 0.5644\n",
      "Epoch [5/5], Step [56/10336], Loss: 0.0749\n",
      "Epoch [5/5], Step [58/10336], Loss: 0.0612\n",
      "Epoch [5/5], Step [60/10336], Loss: 0.0238\n",
      "Epoch [5/5], Step [62/10336], Loss: 1.7864\n",
      "Epoch [5/5], Step [64/10336], Loss: 0.4923\n",
      "Epoch [5/5], Step [66/10336], Loss: 0.1245\n",
      "Epoch [5/5], Step [68/10336], Loss: 5.0079\n",
      "Epoch [5/5], Step [70/10336], Loss: 0.5659\n",
      "Epoch [5/5], Step [72/10336], Loss: 1.4424\n",
      "Epoch [5/5], Step [74/10336], Loss: 0.2030\n",
      "Epoch [5/5], Step [76/10336], Loss: 0.2129\n",
      "Epoch [5/5], Step [78/10336], Loss: 2.2831\n",
      "Epoch [5/5], Step [80/10336], Loss: 0.9266\n",
      "Epoch [5/5], Step [82/10336], Loss: 0.4060\n",
      "Epoch [5/5], Step [84/10336], Loss: 0.2502\n",
      "Epoch [5/5], Step [86/10336], Loss: 1.1032\n",
      "Epoch [5/5], Step [88/10336], Loss: 0.1594\n",
      "Epoch [5/5], Step [90/10336], Loss: 0.2073\n",
      "Epoch [5/5], Step [92/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [94/10336], Loss: 0.6526\n",
      "Epoch [5/5], Step [96/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [98/10336], Loss: 0.0122\n",
      "Epoch [5/5], Step [100/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [102/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [104/10336], Loss: 2.5003\n",
      "Epoch [5/5], Step [106/10336], Loss: 0.0417\n",
      "Epoch [5/5], Step [108/10336], Loss: 0.1242\n",
      "Epoch [5/5], Step [110/10336], Loss: 0.0825\n",
      "Epoch [5/5], Step [112/10336], Loss: 0.0139\n",
      "Epoch [5/5], Step [114/10336], Loss: 0.4830\n",
      "Epoch [5/5], Step [116/10336], Loss: 0.9634\n",
      "Epoch [5/5], Step [118/10336], Loss: 3.4786\n",
      "Epoch [5/5], Step [120/10336], Loss: 1.2377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [122/10336], Loss: 0.3838\n",
      "Epoch [5/5], Step [124/10336], Loss: 1.1745\n",
      "Epoch [5/5], Step [126/10336], Loss: 0.1017\n",
      "Epoch [5/5], Step [128/10336], Loss: 0.5304\n",
      "Epoch [5/5], Step [130/10336], Loss: 2.0703\n",
      "Epoch [5/5], Step [132/10336], Loss: 0.3467\n",
      "Epoch [5/5], Step [134/10336], Loss: 0.4048\n",
      "Epoch [5/5], Step [136/10336], Loss: 0.9406\n",
      "Epoch [5/5], Step [138/10336], Loss: 0.0369\n",
      "Epoch [5/5], Step [140/10336], Loss: 0.0612\n",
      "Epoch [5/5], Step [142/10336], Loss: 0.3872\n",
      "Epoch [5/5], Step [144/10336], Loss: 0.9791\n",
      "Epoch [5/5], Step [146/10336], Loss: 0.0744\n",
      "Epoch [5/5], Step [148/10336], Loss: 0.8192\n",
      "Epoch [5/5], Step [150/10336], Loss: 0.3257\n",
      "Epoch [5/5], Step [152/10336], Loss: 0.2874\n",
      "Epoch [5/5], Step [154/10336], Loss: 0.2433\n",
      "Epoch [5/5], Step [156/10336], Loss: 0.4823\n",
      "Epoch [5/5], Step [158/10336], Loss: 0.0194\n",
      "Epoch [5/5], Step [160/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [162/10336], Loss: 0.0719\n",
      "Epoch [5/5], Step [164/10336], Loss: 1.1165\n",
      "Epoch [5/5], Step [166/10336], Loss: 0.0157\n",
      "Epoch [5/5], Step [168/10336], Loss: 0.1217\n",
      "Epoch [5/5], Step [170/10336], Loss: 0.0734\n",
      "Epoch [5/5], Step [172/10336], Loss: 0.3415\n",
      "Epoch [5/5], Step [174/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [176/10336], Loss: 0.0948\n",
      "Epoch [5/5], Step [178/10336], Loss: 1.2359\n",
      "Epoch [5/5], Step [180/10336], Loss: 0.1514\n",
      "Epoch [5/5], Step [182/10336], Loss: 0.2109\n",
      "Epoch [5/5], Step [184/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [186/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [188/10336], Loss: 0.0055\n",
      "Epoch [5/5], Step [190/10336], Loss: 0.0776\n",
      "Epoch [5/5], Step [192/10336], Loss: 0.8858\n",
      "Epoch [5/5], Step [194/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [196/10336], Loss: 0.0969\n",
      "Epoch [5/5], Step [198/10336], Loss: 0.2698\n",
      "Epoch [5/5], Step [200/10336], Loss: 1.6087\n",
      "Epoch [5/5], Step [202/10336], Loss: 1.6646\n",
      "Epoch [5/5], Step [204/10336], Loss: 0.5104\n",
      "Epoch [5/5], Step [206/10336], Loss: 3.9748\n",
      "Epoch [5/5], Step [208/10336], Loss: 0.0697\n",
      "Epoch [5/5], Step [210/10336], Loss: 0.6693\n",
      "Epoch [5/5], Step [212/10336], Loss: 0.0309\n",
      "Epoch [5/5], Step [214/10336], Loss: 0.3510\n",
      "Epoch [5/5], Step [216/10336], Loss: 0.2801\n",
      "Epoch [5/5], Step [218/10336], Loss: 2.7160\n",
      "Epoch [5/5], Step [220/10336], Loss: 0.3313\n",
      "Epoch [5/5], Step [222/10336], Loss: 0.3875\n",
      "Epoch [5/5], Step [224/10336], Loss: 1.1612\n",
      "Epoch [5/5], Step [226/10336], Loss: 2.4676\n",
      "Epoch [5/5], Step [228/10336], Loss: 0.0158\n",
      "Epoch [5/5], Step [230/10336], Loss: 0.1312\n",
      "Epoch [5/5], Step [232/10336], Loss: 0.1216\n",
      "Epoch [5/5], Step [234/10336], Loss: 0.1792\n",
      "Epoch [5/5], Step [236/10336], Loss: 0.0919\n",
      "Epoch [5/5], Step [238/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [240/10336], Loss: 0.2523\n",
      "Epoch [5/5], Step [242/10336], Loss: 0.7342\n",
      "Epoch [5/5], Step [244/10336], Loss: 0.6558\n",
      "Epoch [5/5], Step [246/10336], Loss: 0.2482\n",
      "Epoch [5/5], Step [248/10336], Loss: 0.1574\n",
      "Epoch [5/5], Step [250/10336], Loss: 0.2476\n",
      "Epoch [5/5], Step [252/10336], Loss: 0.7351\n",
      "Epoch [5/5], Step [254/10336], Loss: 0.0237\n",
      "Epoch [5/5], Step [256/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [258/10336], Loss: 2.5204\n",
      "Epoch [5/5], Step [260/10336], Loss: 2.0033\n",
      "Epoch [5/5], Step [262/10336], Loss: 0.0879\n",
      "Epoch [5/5], Step [264/10336], Loss: 0.3352\n",
      "Epoch [5/5], Step [266/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [268/10336], Loss: 0.0264\n",
      "Epoch [5/5], Step [270/10336], Loss: 0.4011\n",
      "Epoch [5/5], Step [272/10336], Loss: 0.2631\n",
      "Epoch [5/5], Step [274/10336], Loss: 1.2175\n",
      "Epoch [5/5], Step [276/10336], Loss: 0.0782\n",
      "Epoch [5/5], Step [278/10336], Loss: 0.0271\n",
      "Epoch [5/5], Step [280/10336], Loss: 0.0442\n",
      "Epoch [5/5], Step [282/10336], Loss: 0.3338\n",
      "Epoch [5/5], Step [284/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [286/10336], Loss: 0.0100\n",
      "Epoch [5/5], Step [288/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [290/10336], Loss: 0.1209\n",
      "Epoch [5/5], Step [292/10336], Loss: 0.4151\n",
      "Epoch [5/5], Step [294/10336], Loss: 1.6535\n",
      "Epoch [5/5], Step [296/10336], Loss: 3.9975\n",
      "Epoch [5/5], Step [298/10336], Loss: 3.3279\n",
      "Epoch [5/5], Step [300/10336], Loss: 0.0632\n",
      "Epoch [5/5], Step [302/10336], Loss: 0.0212\n",
      "Epoch [5/5], Step [304/10336], Loss: 0.6048\n",
      "Epoch [5/5], Step [306/10336], Loss: 0.0390\n",
      "Epoch [5/5], Step [308/10336], Loss: 4.1648\n",
      "Epoch [5/5], Step [310/10336], Loss: 0.1110\n",
      "Epoch [5/5], Step [312/10336], Loss: 0.0345\n",
      "Epoch [5/5], Step [314/10336], Loss: 0.0420\n",
      "Epoch [5/5], Step [316/10336], Loss: 2.2256\n",
      "Epoch [5/5], Step [318/10336], Loss: 0.0891\n",
      "Epoch [5/5], Step [320/10336], Loss: 2.0020\n",
      "Epoch [5/5], Step [322/10336], Loss: 2.5620\n",
      "Epoch [5/5], Step [324/10336], Loss: 0.3688\n",
      "Epoch [5/5], Step [326/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [328/10336], Loss: 0.0562\n",
      "Epoch [5/5], Step [330/10336], Loss: 4.7880\n",
      "Epoch [5/5], Step [332/10336], Loss: 0.1339\n",
      "Epoch [5/5], Step [334/10336], Loss: 0.1050\n",
      "Epoch [5/5], Step [336/10336], Loss: 0.0053\n",
      "Epoch [5/5], Step [338/10336], Loss: 0.0245\n",
      "Epoch [5/5], Step [340/10336], Loss: 0.3712\n",
      "Epoch [5/5], Step [342/10336], Loss: 0.0366\n",
      "Epoch [5/5], Step [344/10336], Loss: 0.1886\n",
      "Epoch [5/5], Step [346/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [348/10336], Loss: 0.2399\n",
      "Epoch [5/5], Step [350/10336], Loss: 0.0241\n",
      "Epoch [5/5], Step [352/10336], Loss: 0.2037\n",
      "Epoch [5/5], Step [354/10336], Loss: 0.2752\n",
      "Epoch [5/5], Step [356/10336], Loss: 0.0073\n",
      "Epoch [5/5], Step [358/10336], Loss: 0.0184\n",
      "Epoch [5/5], Step [360/10336], Loss: 0.0250\n",
      "Epoch [5/5], Step [362/10336], Loss: 0.1336\n",
      "Epoch [5/5], Step [364/10336], Loss: 0.3074\n",
      "Epoch [5/5], Step [366/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [368/10336], Loss: 0.1934\n",
      "Epoch [5/5], Step [370/10336], Loss: 0.2023\n",
      "Epoch [5/5], Step [372/10336], Loss: 0.0896\n",
      "Epoch [5/5], Step [374/10336], Loss: 0.0316\n",
      "Epoch [5/5], Step [376/10336], Loss: 0.2269\n",
      "Epoch [5/5], Step [378/10336], Loss: 0.2200\n",
      "Epoch [5/5], Step [380/10336], Loss: 1.8233\n",
      "Epoch [5/5], Step [382/10336], Loss: 1.3140\n",
      "Epoch [5/5], Step [384/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [386/10336], Loss: 1.5600\n",
      "Epoch [5/5], Step [388/10336], Loss: 0.0580\n",
      "Epoch [5/5], Step [390/10336], Loss: 2.4156\n",
      "Epoch [5/5], Step [392/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [394/10336], Loss: 0.2100\n",
      "Epoch [5/5], Step [396/10336], Loss: 0.1366\n",
      "Epoch [5/5], Step [398/10336], Loss: 0.0996\n",
      "Epoch [5/5], Step [400/10336], Loss: 0.1527\n",
      "Epoch [5/5], Step [402/10336], Loss: 0.5145\n",
      "Epoch [5/5], Step [404/10336], Loss: 0.4598\n",
      "Epoch [5/5], Step [406/10336], Loss: 0.3130\n",
      "Epoch [5/5], Step [408/10336], Loss: 0.0610\n",
      "Epoch [5/5], Step [410/10336], Loss: 1.5703\n",
      "Epoch [5/5], Step [412/10336], Loss: 0.3827\n",
      "Epoch [5/5], Step [414/10336], Loss: 0.0237\n",
      "Epoch [5/5], Step [416/10336], Loss: 3.5668\n",
      "Epoch [5/5], Step [418/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [420/10336], Loss: 1.1727\n",
      "Epoch [5/5], Step [422/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [424/10336], Loss: 3.5850\n",
      "Epoch [5/5], Step [426/10336], Loss: 2.7375\n",
      "Epoch [5/5], Step [428/10336], Loss: 0.0203\n",
      "Epoch [5/5], Step [430/10336], Loss: 0.2062\n",
      "Epoch [5/5], Step [432/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [434/10336], Loss: 0.0078\n",
      "Epoch [5/5], Step [436/10336], Loss: 0.4458\n",
      "Epoch [5/5], Step [438/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [440/10336], Loss: 0.0506\n",
      "Epoch [5/5], Step [442/10336], Loss: 0.3903\n",
      "Epoch [5/5], Step [444/10336], Loss: 0.3056\n",
      "Epoch [5/5], Step [446/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [448/10336], Loss: 0.0521\n",
      "Epoch [5/5], Step [450/10336], Loss: 0.0553\n",
      "Epoch [5/5], Step [452/10336], Loss: 1.7208\n",
      "Epoch [5/5], Step [454/10336], Loss: 0.0155\n",
      "Epoch [5/5], Step [456/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [458/10336], Loss: 1.3711\n",
      "Epoch [5/5], Step [460/10336], Loss: 0.0436\n",
      "Epoch [5/5], Step [462/10336], Loss: 0.1552\n",
      "Epoch [5/5], Step [464/10336], Loss: 0.1345\n",
      "Epoch [5/5], Step [466/10336], Loss: 0.0272\n",
      "Epoch [5/5], Step [468/10336], Loss: 0.2367\n",
      "Epoch [5/5], Step [470/10336], Loss: 0.0469\n",
      "Epoch [5/5], Step [472/10336], Loss: 0.0764\n",
      "Epoch [5/5], Step [474/10336], Loss: 1.1468\n",
      "Epoch [5/5], Step [476/10336], Loss: 1.8437\n",
      "Epoch [5/5], Step [478/10336], Loss: 4.1127\n",
      "Epoch [5/5], Step [480/10336], Loss: 1.2579\n",
      "Epoch [5/5], Step [482/10336], Loss: 0.6242\n",
      "Epoch [5/5], Step [484/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [486/10336], Loss: 0.0748\n",
      "Epoch [5/5], Step [488/10336], Loss: 1.9800\n",
      "Epoch [5/5], Step [490/10336], Loss: 0.1887\n",
      "Epoch [5/5], Step [492/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [494/10336], Loss: 0.0060\n",
      "Epoch [5/5], Step [496/10336], Loss: 0.1656\n",
      "Epoch [5/5], Step [498/10336], Loss: 0.0767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [500/10336], Loss: 0.0086\n",
      "Epoch [5/5], Step [502/10336], Loss: 0.5721\n",
      "Epoch [5/5], Step [504/10336], Loss: 0.4562\n",
      "Epoch [5/5], Step [506/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [508/10336], Loss: 0.2613\n",
      "Epoch [5/5], Step [510/10336], Loss: 0.7723\n",
      "Epoch [5/5], Step [512/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [514/10336], Loss: 0.1541\n",
      "Epoch [5/5], Step [516/10336], Loss: 1.2323\n",
      "Epoch [5/5], Step [518/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [520/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [522/10336], Loss: 0.1371\n",
      "Epoch [5/5], Step [524/10336], Loss: 0.1763\n",
      "Epoch [5/5], Step [526/10336], Loss: 0.0079\n",
      "Epoch [5/5], Step [528/10336], Loss: 4.1312\n",
      "Epoch [5/5], Step [530/10336], Loss: 0.9438\n",
      "Epoch [5/5], Step [532/10336], Loss: 0.0911\n",
      "Epoch [5/5], Step [534/10336], Loss: 0.2400\n",
      "Epoch [5/5], Step [536/10336], Loss: 0.0823\n",
      "Epoch [5/5], Step [538/10336], Loss: 0.4768\n",
      "Epoch [5/5], Step [540/10336], Loss: 1.4827\n",
      "Epoch [5/5], Step [542/10336], Loss: 0.2949\n",
      "Epoch [5/5], Step [544/10336], Loss: 3.7148\n",
      "Epoch [5/5], Step [546/10336], Loss: 0.0577\n",
      "Epoch [5/5], Step [548/10336], Loss: 0.0591\n",
      "Epoch [5/5], Step [550/10336], Loss: 1.0756\n",
      "Epoch [5/5], Step [552/10336], Loss: 0.3020\n",
      "Epoch [5/5], Step [554/10336], Loss: 0.0471\n",
      "Epoch [5/5], Step [556/10336], Loss: 0.0545\n",
      "Epoch [5/5], Step [558/10336], Loss: 1.0073\n",
      "Epoch [5/5], Step [560/10336], Loss: 0.2439\n",
      "Epoch [5/5], Step [562/10336], Loss: 0.0082\n",
      "Epoch [5/5], Step [564/10336], Loss: 0.0174\n",
      "Epoch [5/5], Step [566/10336], Loss: 0.2210\n",
      "Epoch [5/5], Step [568/10336], Loss: 0.0125\n",
      "Epoch [5/5], Step [570/10336], Loss: 0.0100\n",
      "Epoch [5/5], Step [572/10336], Loss: 0.4517\n",
      "Epoch [5/5], Step [574/10336], Loss: 0.7540\n",
      "Epoch [5/5], Step [576/10336], Loss: 1.3480\n",
      "Epoch [5/5], Step [578/10336], Loss: 0.3429\n",
      "Epoch [5/5], Step [580/10336], Loss: 0.3399\n",
      "Epoch [5/5], Step [582/10336], Loss: 0.3693\n",
      "Epoch [5/5], Step [584/10336], Loss: 0.1818\n",
      "Epoch [5/5], Step [586/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [588/10336], Loss: 1.1649\n",
      "Epoch [5/5], Step [590/10336], Loss: 0.0309\n",
      "Epoch [5/5], Step [592/10336], Loss: 0.0904\n",
      "Epoch [5/5], Step [594/10336], Loss: 1.8791\n",
      "Epoch [5/5], Step [596/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [598/10336], Loss: 0.0725\n",
      "Epoch [5/5], Step [600/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [602/10336], Loss: 0.0533\n",
      "Epoch [5/5], Step [604/10336], Loss: 0.0386\n",
      "Epoch [5/5], Step [606/10336], Loss: 0.2504\n",
      "Epoch [5/5], Step [608/10336], Loss: 3.2827\n",
      "Epoch [5/5], Step [610/10336], Loss: 0.0581\n",
      "Epoch [5/5], Step [612/10336], Loss: 0.0113\n",
      "Epoch [5/5], Step [614/10336], Loss: 0.3673\n",
      "Epoch [5/5], Step [616/10336], Loss: 0.3558\n",
      "Epoch [5/5], Step [618/10336], Loss: 0.2989\n",
      "Epoch [5/5], Step [620/10336], Loss: 0.6671\n",
      "Epoch [5/5], Step [622/10336], Loss: 0.2948\n",
      "Epoch [5/5], Step [624/10336], Loss: 0.1741\n",
      "Epoch [5/5], Step [626/10336], Loss: 0.2093\n",
      "Epoch [5/5], Step [628/10336], Loss: 0.3965\n",
      "Epoch [5/5], Step [630/10336], Loss: 0.3890\n",
      "Epoch [5/5], Step [632/10336], Loss: 0.0234\n",
      "Epoch [5/5], Step [634/10336], Loss: 2.5096\n",
      "Epoch [5/5], Step [636/10336], Loss: 0.0293\n",
      "Epoch [5/5], Step [638/10336], Loss: 0.2847\n",
      "Epoch [5/5], Step [640/10336], Loss: 0.5016\n",
      "Epoch [5/5], Step [642/10336], Loss: 1.5028\n",
      "Epoch [5/5], Step [644/10336], Loss: 0.8698\n",
      "Epoch [5/5], Step [646/10336], Loss: 0.2304\n",
      "Epoch [5/5], Step [648/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [650/10336], Loss: 0.0382\n",
      "Epoch [5/5], Step [652/10336], Loss: 0.0831\n",
      "Epoch [5/5], Step [654/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [656/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [658/10336], Loss: 0.0192\n",
      "Epoch [5/5], Step [660/10336], Loss: 0.1876\n",
      "Epoch [5/5], Step [662/10336], Loss: 0.1953\n",
      "Epoch [5/5], Step [664/10336], Loss: 0.0087\n",
      "Epoch [5/5], Step [666/10336], Loss: 0.2806\n",
      "Epoch [5/5], Step [668/10336], Loss: 1.4219\n",
      "Epoch [5/5], Step [670/10336], Loss: 3.5415\n",
      "Epoch [5/5], Step [672/10336], Loss: 0.1139\n",
      "Epoch [5/5], Step [674/10336], Loss: 1.7621\n",
      "Epoch [5/5], Step [676/10336], Loss: 0.1290\n",
      "Epoch [5/5], Step [678/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [680/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [682/10336], Loss: 0.5474\n",
      "Epoch [5/5], Step [684/10336], Loss: 0.2943\n",
      "Epoch [5/5], Step [686/10336], Loss: 0.1596\n",
      "Epoch [5/5], Step [688/10336], Loss: 0.0537\n",
      "Epoch [5/5], Step [690/10336], Loss: 5.0551\n",
      "Epoch [5/5], Step [692/10336], Loss: 3.4683\n",
      "Epoch [5/5], Step [694/10336], Loss: 1.7160\n",
      "Epoch [5/5], Step [696/10336], Loss: 0.8477\n",
      "Epoch [5/5], Step [698/10336], Loss: 0.2629\n",
      "Epoch [5/5], Step [700/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [702/10336], Loss: 4.2927\n",
      "Epoch [5/5], Step [704/10336], Loss: 0.0201\n",
      "Epoch [5/5], Step [706/10336], Loss: 0.8927\n",
      "Epoch [5/5], Step [708/10336], Loss: 0.0430\n",
      "Epoch [5/5], Step [710/10336], Loss: 0.1001\n",
      "Epoch [5/5], Step [712/10336], Loss: 1.0633\n",
      "Epoch [5/5], Step [714/10336], Loss: 2.2013\n",
      "Epoch [5/5], Step [716/10336], Loss: 0.1860\n",
      "Epoch [5/5], Step [718/10336], Loss: 0.3426\n",
      "Epoch [5/5], Step [720/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [722/10336], Loss: 1.5472\n",
      "Epoch [5/5], Step [724/10336], Loss: 1.8022\n",
      "Epoch [5/5], Step [726/10336], Loss: 0.8136\n",
      "Epoch [5/5], Step [728/10336], Loss: 0.0546\n",
      "Epoch [5/5], Step [730/10336], Loss: 0.0149\n",
      "Epoch [5/5], Step [732/10336], Loss: 0.0326\n",
      "Epoch [5/5], Step [734/10336], Loss: 0.0201\n",
      "Epoch [5/5], Step [736/10336], Loss: 0.9295\n",
      "Epoch [5/5], Step [738/10336], Loss: 4.9339\n",
      "Epoch [5/5], Step [740/10336], Loss: 0.1856\n",
      "Epoch [5/5], Step [742/10336], Loss: 0.1397\n",
      "Epoch [5/5], Step [744/10336], Loss: 0.2654\n",
      "Epoch [5/5], Step [746/10336], Loss: 0.5324\n",
      "Epoch [5/5], Step [748/10336], Loss: 0.0371\n",
      "Epoch [5/5], Step [750/10336], Loss: 2.2333\n",
      "Epoch [5/5], Step [752/10336], Loss: 1.4581\n",
      "Epoch [5/5], Step [754/10336], Loss: 0.0301\n",
      "Epoch [5/5], Step [756/10336], Loss: 0.5995\n",
      "Epoch [5/5], Step [758/10336], Loss: 0.0482\n",
      "Epoch [5/5], Step [760/10336], Loss: 0.1583\n",
      "Epoch [5/5], Step [762/10336], Loss: 0.8242\n",
      "Epoch [5/5], Step [764/10336], Loss: 0.0162\n",
      "Epoch [5/5], Step [766/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [768/10336], Loss: 0.2890\n",
      "Epoch [5/5], Step [770/10336], Loss: 0.0075\n",
      "Epoch [5/5], Step [772/10336], Loss: 0.5522\n",
      "Epoch [5/5], Step [774/10336], Loss: 0.1924\n",
      "Epoch [5/5], Step [776/10336], Loss: 0.0071\n",
      "Epoch [5/5], Step [778/10336], Loss: 1.4565\n",
      "Epoch [5/5], Step [780/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [782/10336], Loss: 0.0414\n",
      "Epoch [5/5], Step [784/10336], Loss: 3.0232\n",
      "Epoch [5/5], Step [786/10336], Loss: 0.2249\n",
      "Epoch [5/5], Step [788/10336], Loss: 0.0885\n",
      "Epoch [5/5], Step [790/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [792/10336], Loss: 0.8258\n",
      "Epoch [5/5], Step [794/10336], Loss: 0.0287\n",
      "Epoch [5/5], Step [796/10336], Loss: 0.0199\n",
      "Epoch [5/5], Step [798/10336], Loss: 0.3538\n",
      "Epoch [5/5], Step [800/10336], Loss: 0.2082\n",
      "Epoch [5/5], Step [802/10336], Loss: 1.2099\n",
      "Epoch [5/5], Step [804/10336], Loss: 0.2837\n",
      "Epoch [5/5], Step [806/10336], Loss: 0.0102\n",
      "Epoch [5/5], Step [808/10336], Loss: 0.3543\n",
      "Epoch [5/5], Step [810/10336], Loss: 3.7429\n",
      "Epoch [5/5], Step [812/10336], Loss: 0.0217\n",
      "Epoch [5/5], Step [814/10336], Loss: 0.0327\n",
      "Epoch [5/5], Step [816/10336], Loss: 0.2719\n",
      "Epoch [5/5], Step [818/10336], Loss: 0.2306\n",
      "Epoch [5/5], Step [820/10336], Loss: 0.5943\n",
      "Epoch [5/5], Step [822/10336], Loss: 3.2657\n",
      "Epoch [5/5], Step [824/10336], Loss: 0.0362\n",
      "Epoch [5/5], Step [826/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [828/10336], Loss: 0.0477\n",
      "Epoch [5/5], Step [830/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [832/10336], Loss: 0.0675\n",
      "Epoch [5/5], Step [834/10336], Loss: 0.1821\n",
      "Epoch [5/5], Step [836/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [838/10336], Loss: 2.2283\n",
      "Epoch [5/5], Step [840/10336], Loss: 0.0708\n",
      "Epoch [5/5], Step [842/10336], Loss: 2.4231\n",
      "Epoch [5/5], Step [844/10336], Loss: 0.7192\n",
      "Epoch [5/5], Step [846/10336], Loss: 0.0074\n",
      "Epoch [5/5], Step [848/10336], Loss: 0.9697\n",
      "Epoch [5/5], Step [850/10336], Loss: 0.7250\n",
      "Epoch [5/5], Step [852/10336], Loss: 0.1730\n",
      "Epoch [5/5], Step [854/10336], Loss: 0.0318\n",
      "Epoch [5/5], Step [856/10336], Loss: 0.7962\n",
      "Epoch [5/5], Step [858/10336], Loss: 1.3006\n",
      "Epoch [5/5], Step [860/10336], Loss: 0.2778\n",
      "Epoch [5/5], Step [862/10336], Loss: 0.0922\n",
      "Epoch [5/5], Step [864/10336], Loss: 0.8626\n",
      "Epoch [5/5], Step [866/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [868/10336], Loss: 1.7166\n",
      "Epoch [5/5], Step [870/10336], Loss: 0.0365\n",
      "Epoch [5/5], Step [872/10336], Loss: 0.0517\n",
      "Epoch [5/5], Step [874/10336], Loss: 1.3540\n",
      "Epoch [5/5], Step [876/10336], Loss: 0.3040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [878/10336], Loss: 0.0540\n",
      "Epoch [5/5], Step [880/10336], Loss: 0.3629\n",
      "Epoch [5/5], Step [882/10336], Loss: 0.1891\n",
      "Epoch [5/5], Step [884/10336], Loss: 0.4345\n",
      "Epoch [5/5], Step [886/10336], Loss: 0.3359\n",
      "Epoch [5/5], Step [888/10336], Loss: 5.4884\n",
      "Epoch [5/5], Step [890/10336], Loss: 0.5846\n",
      "Epoch [5/5], Step [892/10336], Loss: 0.0147\n",
      "Epoch [5/5], Step [894/10336], Loss: 1.6669\n",
      "Epoch [5/5], Step [896/10336], Loss: 0.0419\n",
      "Epoch [5/5], Step [898/10336], Loss: 0.0603\n",
      "Epoch [5/5], Step [900/10336], Loss: 3.6195\n",
      "Epoch [5/5], Step [902/10336], Loss: 0.1453\n",
      "Epoch [5/5], Step [904/10336], Loss: 0.1436\n",
      "Epoch [5/5], Step [906/10336], Loss: 0.0255\n",
      "Epoch [5/5], Step [908/10336], Loss: 3.4967\n",
      "Epoch [5/5], Step [910/10336], Loss: 0.1794\n",
      "Epoch [5/5], Step [912/10336], Loss: 1.6705\n",
      "Epoch [5/5], Step [914/10336], Loss: 4.6266\n",
      "Epoch [5/5], Step [916/10336], Loss: 0.9610\n",
      "Epoch [5/5], Step [918/10336], Loss: 0.0968\n",
      "Epoch [5/5], Step [920/10336], Loss: 0.0194\n",
      "Epoch [5/5], Step [922/10336], Loss: 0.1687\n",
      "Epoch [5/5], Step [924/10336], Loss: 0.0667\n",
      "Epoch [5/5], Step [926/10336], Loss: 0.0264\n",
      "Epoch [5/5], Step [928/10336], Loss: 0.2639\n",
      "Epoch [5/5], Step [930/10336], Loss: 0.0678\n",
      "Epoch [5/5], Step [932/10336], Loss: 0.3494\n",
      "Epoch [5/5], Step [934/10336], Loss: 2.0370\n",
      "Epoch [5/5], Step [936/10336], Loss: 1.2685\n",
      "Epoch [5/5], Step [938/10336], Loss: 0.2693\n",
      "Epoch [5/5], Step [940/10336], Loss: 0.8262\n",
      "Epoch [5/5], Step [942/10336], Loss: 0.0317\n",
      "Epoch [5/5], Step [944/10336], Loss: 0.1011\n",
      "Epoch [5/5], Step [946/10336], Loss: 0.0584\n",
      "Epoch [5/5], Step [948/10336], Loss: 0.4596\n",
      "Epoch [5/5], Step [950/10336], Loss: 0.0500\n",
      "Epoch [5/5], Step [952/10336], Loss: 0.7606\n",
      "Epoch [5/5], Step [954/10336], Loss: 1.5458\n",
      "Epoch [5/5], Step [956/10336], Loss: 0.4419\n",
      "Epoch [5/5], Step [958/10336], Loss: 0.9116\n",
      "Epoch [5/5], Step [960/10336], Loss: 0.1962\n",
      "Epoch [5/5], Step [962/10336], Loss: 0.0246\n",
      "Epoch [5/5], Step [964/10336], Loss: 0.0111\n",
      "Epoch [5/5], Step [966/10336], Loss: 0.2780\n",
      "Epoch [5/5], Step [968/10336], Loss: 1.0676\n",
      "Epoch [5/5], Step [970/10336], Loss: 0.6891\n",
      "Epoch [5/5], Step [972/10336], Loss: 0.4127\n",
      "Epoch [5/5], Step [974/10336], Loss: 0.0371\n",
      "Epoch [5/5], Step [976/10336], Loss: 0.0381\n",
      "Epoch [5/5], Step [978/10336], Loss: 0.1866\n",
      "Epoch [5/5], Step [980/10336], Loss: 0.5210\n",
      "Epoch [5/5], Step [982/10336], Loss: 0.1064\n",
      "Epoch [5/5], Step [984/10336], Loss: 0.1023\n",
      "Epoch [5/5], Step [986/10336], Loss: 0.2993\n",
      "Epoch [5/5], Step [988/10336], Loss: 0.1004\n",
      "Epoch [5/5], Step [990/10336], Loss: 0.1656\n",
      "Epoch [5/5], Step [992/10336], Loss: 0.4772\n",
      "Epoch [5/5], Step [994/10336], Loss: 0.5513\n",
      "Epoch [5/5], Step [996/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [998/10336], Loss: 0.0445\n",
      "Epoch [5/5], Step [1000/10336], Loss: 0.5822\n",
      "Epoch [5/5], Step [1002/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [1004/10336], Loss: 2.0789\n",
      "Epoch [5/5], Step [1006/10336], Loss: 0.5708\n",
      "Epoch [5/5], Step [1008/10336], Loss: 0.1652\n",
      "Epoch [5/5], Step [1010/10336], Loss: 3.7580\n",
      "Epoch [5/5], Step [1012/10336], Loss: 1.2266\n",
      "Epoch [5/5], Step [1014/10336], Loss: 0.2180\n",
      "Epoch [5/5], Step [1016/10336], Loss: 1.4805\n",
      "Epoch [5/5], Step [1018/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [1020/10336], Loss: 0.1089\n",
      "Epoch [5/5], Step [1022/10336], Loss: 0.4272\n",
      "Epoch [5/5], Step [1024/10336], Loss: 0.2835\n",
      "Epoch [5/5], Step [1026/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [1028/10336], Loss: 0.0225\n",
      "Epoch [5/5], Step [1030/10336], Loss: 0.2040\n",
      "Epoch [5/5], Step [1032/10336], Loss: 0.1778\n",
      "Epoch [5/5], Step [1034/10336], Loss: 0.1031\n",
      "Epoch [5/5], Step [1036/10336], Loss: 0.0644\n",
      "Epoch [5/5], Step [1038/10336], Loss: 0.0464\n",
      "Epoch [5/5], Step [1040/10336], Loss: 0.0139\n",
      "Epoch [5/5], Step [1042/10336], Loss: 0.1508\n",
      "Epoch [5/5], Step [1044/10336], Loss: 2.7561\n",
      "Epoch [5/5], Step [1046/10336], Loss: 0.1759\n",
      "Epoch [5/5], Step [1048/10336], Loss: 0.6266\n",
      "Epoch [5/5], Step [1050/10336], Loss: 0.0087\n",
      "Epoch [5/5], Step [1052/10336], Loss: 0.8321\n",
      "Epoch [5/5], Step [1054/10336], Loss: 0.0298\n",
      "Epoch [5/5], Step [1056/10336], Loss: 2.9984\n",
      "Epoch [5/5], Step [1058/10336], Loss: 2.5031\n",
      "Epoch [5/5], Step [1060/10336], Loss: 0.2382\n",
      "Epoch [5/5], Step [1062/10336], Loss: 0.0780\n",
      "Epoch [5/5], Step [1064/10336], Loss: 0.2389\n",
      "Epoch [5/5], Step [1066/10336], Loss: 0.1164\n",
      "Epoch [5/5], Step [1068/10336], Loss: 0.4792\n",
      "Epoch [5/5], Step [1070/10336], Loss: 0.0453\n",
      "Epoch [5/5], Step [1072/10336], Loss: 1.3733\n",
      "Epoch [5/5], Step [1074/10336], Loss: 0.9384\n",
      "Epoch [5/5], Step [1076/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [1078/10336], Loss: 0.0232\n",
      "Epoch [5/5], Step [1080/10336], Loss: 0.0239\n",
      "Epoch [5/5], Step [1082/10336], Loss: 0.0911\n",
      "Epoch [5/5], Step [1084/10336], Loss: 0.6167\n",
      "Epoch [5/5], Step [1086/10336], Loss: 0.3444\n",
      "Epoch [5/5], Step [1088/10336], Loss: 0.9246\n",
      "Epoch [5/5], Step [1090/10336], Loss: 1.1721\n",
      "Epoch [5/5], Step [1092/10336], Loss: 1.5109\n",
      "Epoch [5/5], Step [1094/10336], Loss: 0.7636\n",
      "Epoch [5/5], Step [1096/10336], Loss: 0.1309\n",
      "Epoch [5/5], Step [1098/10336], Loss: 0.4950\n",
      "Epoch [5/5], Step [1100/10336], Loss: 0.4233\n",
      "Epoch [5/5], Step [1102/10336], Loss: 0.2076\n",
      "Epoch [5/5], Step [1104/10336], Loss: 0.0139\n",
      "Epoch [5/5], Step [1106/10336], Loss: 0.1172\n",
      "Epoch [5/5], Step [1108/10336], Loss: 0.0715\n",
      "Epoch [5/5], Step [1110/10336], Loss: 0.2485\n",
      "Epoch [5/5], Step [1112/10336], Loss: 0.4465\n",
      "Epoch [5/5], Step [1114/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [1116/10336], Loss: 0.5656\n",
      "Epoch [5/5], Step [1118/10336], Loss: 0.3236\n",
      "Epoch [5/5], Step [1120/10336], Loss: 0.1315\n",
      "Epoch [5/5], Step [1122/10336], Loss: 0.3667\n",
      "Epoch [5/5], Step [1124/10336], Loss: 0.2063\n",
      "Epoch [5/5], Step [1126/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [1128/10336], Loss: 0.9976\n",
      "Epoch [5/5], Step [1130/10336], Loss: 0.0425\n",
      "Epoch [5/5], Step [1132/10336], Loss: 0.1490\n",
      "Epoch [5/5], Step [1134/10336], Loss: 0.1779\n",
      "Epoch [5/5], Step [1136/10336], Loss: 0.0580\n",
      "Epoch [5/5], Step [1138/10336], Loss: 0.0201\n",
      "Epoch [5/5], Step [1140/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [1142/10336], Loss: 1.1725\n",
      "Epoch [5/5], Step [1144/10336], Loss: 0.2539\n",
      "Epoch [5/5], Step [1146/10336], Loss: 0.4468\n",
      "Epoch [5/5], Step [1148/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [1150/10336], Loss: 0.6010\n",
      "Epoch [5/5], Step [1152/10336], Loss: 0.1982\n",
      "Epoch [5/5], Step [1154/10336], Loss: 0.9453\n",
      "Epoch [5/5], Step [1156/10336], Loss: 0.0439\n",
      "Epoch [5/5], Step [1158/10336], Loss: 0.0576\n",
      "Epoch [5/5], Step [1160/10336], Loss: 0.3639\n",
      "Epoch [5/5], Step [1162/10336], Loss: 0.0512\n",
      "Epoch [5/5], Step [1164/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [1166/10336], Loss: 0.1611\n",
      "Epoch [5/5], Step [1168/10336], Loss: 0.0699\n",
      "Epoch [5/5], Step [1170/10336], Loss: 0.5984\n",
      "Epoch [5/5], Step [1172/10336], Loss: 0.3858\n",
      "Epoch [5/5], Step [1174/10336], Loss: 0.0859\n",
      "Epoch [5/5], Step [1176/10336], Loss: 0.3560\n",
      "Epoch [5/5], Step [1178/10336], Loss: 0.5663\n",
      "Epoch [5/5], Step [1180/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [1182/10336], Loss: 0.5596\n",
      "Epoch [5/5], Step [1184/10336], Loss: 0.1028\n",
      "Epoch [5/5], Step [1186/10336], Loss: 0.6185\n",
      "Epoch [5/5], Step [1188/10336], Loss: 0.1107\n",
      "Epoch [5/5], Step [1190/10336], Loss: 2.3347\n",
      "Epoch [5/5], Step [1192/10336], Loss: 0.1847\n",
      "Epoch [5/5], Step [1194/10336], Loss: 1.4557\n",
      "Epoch [5/5], Step [1196/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [1198/10336], Loss: 1.8977\n",
      "Epoch [5/5], Step [1200/10336], Loss: 2.0985\n",
      "Epoch [5/5], Step [1202/10336], Loss: 0.6715\n",
      "Epoch [5/5], Step [1204/10336], Loss: 0.9246\n",
      "Epoch [5/5], Step [1206/10336], Loss: 1.0547\n",
      "Epoch [5/5], Step [1208/10336], Loss: 0.5697\n",
      "Epoch [5/5], Step [1210/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [1212/10336], Loss: 1.5640\n",
      "Epoch [5/5], Step [1214/10336], Loss: 0.3463\n",
      "Epoch [5/5], Step [1216/10336], Loss: 0.2034\n",
      "Epoch [5/5], Step [1218/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [1220/10336], Loss: 0.0140\n",
      "Epoch [5/5], Step [1222/10336], Loss: 0.0456\n",
      "Epoch [5/5], Step [1224/10336], Loss: 0.0142\n",
      "Epoch [5/5], Step [1226/10336], Loss: 0.1256\n",
      "Epoch [5/5], Step [1228/10336], Loss: 5.9634\n",
      "Epoch [5/5], Step [1230/10336], Loss: 1.1166\n",
      "Epoch [5/5], Step [1232/10336], Loss: 0.0927\n",
      "Epoch [5/5], Step [1234/10336], Loss: 0.5360\n",
      "Epoch [5/5], Step [1236/10336], Loss: 0.6635\n",
      "Epoch [5/5], Step [1238/10336], Loss: 0.0205\n",
      "Epoch [5/5], Step [1240/10336], Loss: 1.3175\n",
      "Epoch [5/5], Step [1242/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [1244/10336], Loss: 0.8949\n",
      "Epoch [5/5], Step [1246/10336], Loss: 0.0406\n",
      "Epoch [5/5], Step [1248/10336], Loss: 4.1052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [1250/10336], Loss: 0.2667\n",
      "Epoch [5/5], Step [1252/10336], Loss: 0.1621\n",
      "Epoch [5/5], Step [1254/10336], Loss: 0.2665\n",
      "Epoch [5/5], Step [1256/10336], Loss: 0.0693\n",
      "Epoch [5/5], Step [1258/10336], Loss: 0.0728\n",
      "Epoch [5/5], Step [1260/10336], Loss: 1.1310\n",
      "Epoch [5/5], Step [1262/10336], Loss: 0.2949\n",
      "Epoch [5/5], Step [1264/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [1266/10336], Loss: 0.2152\n",
      "Epoch [5/5], Step [1268/10336], Loss: 3.8688\n",
      "Epoch [5/5], Step [1270/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [1272/10336], Loss: 0.1043\n",
      "Epoch [5/5], Step [1274/10336], Loss: 1.0812\n",
      "Epoch [5/5], Step [1276/10336], Loss: 2.2538\n",
      "Epoch [5/5], Step [1278/10336], Loss: 1.3351\n",
      "Epoch [5/5], Step [1280/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [1282/10336], Loss: 0.1685\n",
      "Epoch [5/5], Step [1284/10336], Loss: 2.3478\n",
      "Epoch [5/5], Step [1286/10336], Loss: 4.2658\n",
      "Epoch [5/5], Step [1288/10336], Loss: 1.2017\n",
      "Epoch [5/5], Step [1290/10336], Loss: 0.6828\n",
      "Epoch [5/5], Step [1292/10336], Loss: 0.3617\n",
      "Epoch [5/5], Step [1294/10336], Loss: 0.1384\n",
      "Epoch [5/5], Step [1296/10336], Loss: 0.0849\n",
      "Epoch [5/5], Step [1298/10336], Loss: 0.0551\n",
      "Epoch [5/5], Step [1300/10336], Loss: 1.0572\n",
      "Epoch [5/5], Step [1302/10336], Loss: 0.0316\n",
      "Epoch [5/5], Step [1304/10336], Loss: 0.6794\n",
      "Epoch [5/5], Step [1306/10336], Loss: 0.2855\n",
      "Epoch [5/5], Step [1308/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [1310/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [1312/10336], Loss: 2.4682\n",
      "Epoch [5/5], Step [1314/10336], Loss: 0.6862\n",
      "Epoch [5/5], Step [1316/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [1318/10336], Loss: 0.2249\n",
      "Epoch [5/5], Step [1320/10336], Loss: 0.1711\n",
      "Epoch [5/5], Step [1322/10336], Loss: 0.0808\n",
      "Epoch [5/5], Step [1324/10336], Loss: 0.9511\n",
      "Epoch [5/5], Step [1326/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [1328/10336], Loss: 0.1459\n",
      "Epoch [5/5], Step [1330/10336], Loss: 0.0398\n",
      "Epoch [5/5], Step [1332/10336], Loss: 0.0823\n",
      "Epoch [5/5], Step [1334/10336], Loss: 0.2304\n",
      "Epoch [5/5], Step [1336/10336], Loss: 0.9410\n",
      "Epoch [5/5], Step [1338/10336], Loss: 0.2095\n",
      "Epoch [5/5], Step [1340/10336], Loss: 0.2761\n",
      "Epoch [5/5], Step [1342/10336], Loss: 0.0066\n",
      "Epoch [5/5], Step [1344/10336], Loss: 0.1663\n",
      "Epoch [5/5], Step [1346/10336], Loss: 0.0269\n",
      "Epoch [5/5], Step [1348/10336], Loss: 0.0504\n",
      "Epoch [5/5], Step [1350/10336], Loss: 0.1162\n",
      "Epoch [5/5], Step [1352/10336], Loss: 0.4960\n",
      "Epoch [5/5], Step [1354/10336], Loss: 0.0625\n",
      "Epoch [5/5], Step [1356/10336], Loss: 0.1859\n",
      "Epoch [5/5], Step [1358/10336], Loss: 0.1762\n",
      "Epoch [5/5], Step [1360/10336], Loss: 4.7321\n",
      "Epoch [5/5], Step [1362/10336], Loss: 0.0604\n",
      "Epoch [5/5], Step [1364/10336], Loss: 0.0826\n",
      "Epoch [5/5], Step [1366/10336], Loss: 0.0306\n",
      "Epoch [5/5], Step [1368/10336], Loss: 0.0844\n",
      "Epoch [5/5], Step [1370/10336], Loss: 0.3096\n",
      "Epoch [5/5], Step [1372/10336], Loss: 0.1024\n",
      "Epoch [5/5], Step [1374/10336], Loss: 0.3073\n",
      "Epoch [5/5], Step [1376/10336], Loss: 0.0411\n",
      "Epoch [5/5], Step [1378/10336], Loss: 0.5917\n",
      "Epoch [5/5], Step [1380/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [1382/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [1384/10336], Loss: 2.6672\n",
      "Epoch [5/5], Step [1386/10336], Loss: 0.4507\n",
      "Epoch [5/5], Step [1388/10336], Loss: 0.0208\n",
      "Epoch [5/5], Step [1390/10336], Loss: 0.5002\n",
      "Epoch [5/5], Step [1392/10336], Loss: 0.0433\n",
      "Epoch [5/5], Step [1394/10336], Loss: 0.1398\n",
      "Epoch [5/5], Step [1396/10336], Loss: 0.0691\n",
      "Epoch [5/5], Step [1398/10336], Loss: 0.0389\n",
      "Epoch [5/5], Step [1400/10336], Loss: 0.7119\n",
      "Epoch [5/5], Step [1402/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [1404/10336], Loss: 0.0158\n",
      "Epoch [5/5], Step [1406/10336], Loss: 0.0333\n",
      "Epoch [5/5], Step [1408/10336], Loss: 0.2167\n",
      "Epoch [5/5], Step [1410/10336], Loss: 0.0304\n",
      "Epoch [5/5], Step [1412/10336], Loss: 0.0899\n",
      "Epoch [5/5], Step [1414/10336], Loss: 0.5581\n",
      "Epoch [5/5], Step [1416/10336], Loss: 0.3563\n",
      "Epoch [5/5], Step [1418/10336], Loss: 0.7673\n",
      "Epoch [5/5], Step [1420/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [1422/10336], Loss: 0.1078\n",
      "Epoch [5/5], Step [1424/10336], Loss: 0.6529\n",
      "Epoch [5/5], Step [1426/10336], Loss: 0.1049\n",
      "Epoch [5/5], Step [1428/10336], Loss: 1.2734\n",
      "Epoch [5/5], Step [1430/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [1432/10336], Loss: 0.0298\n",
      "Epoch [5/5], Step [1434/10336], Loss: 0.1951\n",
      "Epoch [5/5], Step [1436/10336], Loss: 0.0820\n",
      "Epoch [5/5], Step [1438/10336], Loss: 0.7814\n",
      "Epoch [5/5], Step [1440/10336], Loss: 1.8000\n",
      "Epoch [5/5], Step [1442/10336], Loss: 0.1970\n",
      "Epoch [5/5], Step [1444/10336], Loss: 0.3719\n",
      "Epoch [5/5], Step [1446/10336], Loss: 0.0217\n",
      "Epoch [5/5], Step [1448/10336], Loss: 0.3405\n",
      "Epoch [5/5], Step [1450/10336], Loss: 2.8903\n",
      "Epoch [5/5], Step [1452/10336], Loss: 0.0129\n",
      "Epoch [5/5], Step [1454/10336], Loss: 0.0339\n",
      "Epoch [5/5], Step [1456/10336], Loss: 0.1798\n",
      "Epoch [5/5], Step [1458/10336], Loss: 0.8634\n",
      "Epoch [5/5], Step [1460/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [1462/10336], Loss: 1.9582\n",
      "Epoch [5/5], Step [1464/10336], Loss: 0.0211\n",
      "Epoch [5/5], Step [1466/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [1468/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [1470/10336], Loss: 1.2460\n",
      "Epoch [5/5], Step [1472/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [1474/10336], Loss: 1.1286\n",
      "Epoch [5/5], Step [1476/10336], Loss: 2.6065\n",
      "Epoch [5/5], Step [1478/10336], Loss: 0.3244\n",
      "Epoch [5/5], Step [1480/10336], Loss: 0.0104\n",
      "Epoch [5/5], Step [1482/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [1484/10336], Loss: 0.5446\n",
      "Epoch [5/5], Step [1486/10336], Loss: 1.5087\n",
      "Epoch [5/5], Step [1488/10336], Loss: 0.0841\n",
      "Epoch [5/5], Step [1490/10336], Loss: 0.1636\n",
      "Epoch [5/5], Step [1492/10336], Loss: 0.0936\n",
      "Epoch [5/5], Step [1494/10336], Loss: 0.7909\n",
      "Epoch [5/5], Step [1496/10336], Loss: 0.0721\n",
      "Epoch [5/5], Step [1498/10336], Loss: 0.0322\n",
      "Epoch [5/5], Step [1500/10336], Loss: 0.8819\n",
      "Epoch [5/5], Step [1502/10336], Loss: 0.0417\n",
      "Epoch [5/5], Step [1504/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [1506/10336], Loss: 0.0504\n",
      "Epoch [5/5], Step [1508/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [1510/10336], Loss: 0.3678\n",
      "Epoch [5/5], Step [1512/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [1514/10336], Loss: 0.4029\n",
      "Epoch [5/5], Step [1516/10336], Loss: 0.0149\n",
      "Epoch [5/5], Step [1518/10336], Loss: 0.6926\n",
      "Epoch [5/5], Step [1520/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [1522/10336], Loss: 0.0135\n",
      "Epoch [5/5], Step [1524/10336], Loss: 0.1045\n",
      "Epoch [5/5], Step [1526/10336], Loss: 0.8733\n",
      "Epoch [5/5], Step [1528/10336], Loss: 0.3258\n",
      "Epoch [5/5], Step [1530/10336], Loss: 0.3190\n",
      "Epoch [5/5], Step [1532/10336], Loss: 0.8681\n",
      "Epoch [5/5], Step [1534/10336], Loss: 0.0245\n",
      "Epoch [5/5], Step [1536/10336], Loss: 0.0649\n",
      "Epoch [5/5], Step [1538/10336], Loss: 0.0082\n",
      "Epoch [5/5], Step [1540/10336], Loss: 0.2639\n",
      "Epoch [5/5], Step [1542/10336], Loss: 0.1057\n",
      "Epoch [5/5], Step [1544/10336], Loss: 0.4032\n",
      "Epoch [5/5], Step [1546/10336], Loss: 0.0718\n",
      "Epoch [5/5], Step [1548/10336], Loss: 0.0147\n",
      "Epoch [5/5], Step [1550/10336], Loss: 0.6032\n",
      "Epoch [5/5], Step [1552/10336], Loss: 0.6615\n",
      "Epoch [5/5], Step [1554/10336], Loss: 0.0432\n",
      "Epoch [5/5], Step [1556/10336], Loss: 0.5721\n",
      "Epoch [5/5], Step [1558/10336], Loss: 0.0250\n",
      "Epoch [5/5], Step [1560/10336], Loss: 0.1794\n",
      "Epoch [5/5], Step [1562/10336], Loss: 0.1139\n",
      "Epoch [5/5], Step [1564/10336], Loss: 0.1962\n",
      "Epoch [5/5], Step [1566/10336], Loss: 2.2883\n",
      "Epoch [5/5], Step [1568/10336], Loss: 0.9608\n",
      "Epoch [5/5], Step [1570/10336], Loss: 0.3272\n",
      "Epoch [5/5], Step [1572/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [1574/10336], Loss: 0.0373\n",
      "Epoch [5/5], Step [1576/10336], Loss: 0.8923\n",
      "Epoch [5/5], Step [1578/10336], Loss: 0.4099\n",
      "Epoch [5/5], Step [1580/10336], Loss: 0.0339\n",
      "Epoch [5/5], Step [1582/10336], Loss: 0.7605\n",
      "Epoch [5/5], Step [1584/10336], Loss: 0.0128\n",
      "Epoch [5/5], Step [1586/10336], Loss: 0.0126\n",
      "Epoch [5/5], Step [1588/10336], Loss: 0.3252\n",
      "Epoch [5/5], Step [1590/10336], Loss: 0.0442\n",
      "Epoch [5/5], Step [1592/10336], Loss: 0.0264\n",
      "Epoch [5/5], Step [1594/10336], Loss: 0.0479\n",
      "Epoch [5/5], Step [1596/10336], Loss: 2.5831\n",
      "Epoch [5/5], Step [1598/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [1600/10336], Loss: 0.1134\n",
      "Epoch [5/5], Step [1602/10336], Loss: 5.9529\n",
      "Epoch [5/5], Step [1604/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [1606/10336], Loss: 0.2696\n",
      "Epoch [5/5], Step [1608/10336], Loss: 6.1084\n",
      "Epoch [5/5], Step [1610/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [1612/10336], Loss: 0.4057\n",
      "Epoch [5/5], Step [1614/10336], Loss: 0.0573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [1616/10336], Loss: 0.1245\n",
      "Epoch [5/5], Step [1618/10336], Loss: 0.1537\n",
      "Epoch [5/5], Step [1620/10336], Loss: 0.1113\n",
      "Epoch [5/5], Step [1622/10336], Loss: 0.9573\n",
      "Epoch [5/5], Step [1624/10336], Loss: 0.1150\n",
      "Epoch [5/5], Step [1626/10336], Loss: 0.2237\n",
      "Epoch [5/5], Step [1628/10336], Loss: 0.4248\n",
      "Epoch [5/5], Step [1630/10336], Loss: 0.1545\n",
      "Epoch [5/5], Step [1632/10336], Loss: 0.0345\n",
      "Epoch [5/5], Step [1634/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [1636/10336], Loss: 0.0993\n",
      "Epoch [5/5], Step [1638/10336], Loss: 0.0769\n",
      "Epoch [5/5], Step [1640/10336], Loss: 0.0412\n",
      "Epoch [5/5], Step [1642/10336], Loss: 0.0985\n",
      "Epoch [5/5], Step [1644/10336], Loss: 0.0901\n",
      "Epoch [5/5], Step [1646/10336], Loss: 0.3552\n",
      "Epoch [5/5], Step [1648/10336], Loss: 0.0661\n",
      "Epoch [5/5], Step [1650/10336], Loss: 0.1086\n",
      "Epoch [5/5], Step [1652/10336], Loss: 0.3651\n",
      "Epoch [5/5], Step [1654/10336], Loss: 0.0219\n",
      "Epoch [5/5], Step [1656/10336], Loss: 0.1431\n",
      "Epoch [5/5], Step [1658/10336], Loss: 0.1296\n",
      "Epoch [5/5], Step [1660/10336], Loss: 0.0486\n",
      "Epoch [5/5], Step [1662/10336], Loss: 2.9548\n",
      "Epoch [5/5], Step [1664/10336], Loss: 1.2334\n",
      "Epoch [5/5], Step [1666/10336], Loss: 0.3553\n",
      "Epoch [5/5], Step [1668/10336], Loss: 0.7675\n",
      "Epoch [5/5], Step [1670/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [1672/10336], Loss: 0.1331\n",
      "Epoch [5/5], Step [1674/10336], Loss: 0.0587\n",
      "Epoch [5/5], Step [1676/10336], Loss: 0.2096\n",
      "Epoch [5/5], Step [1678/10336], Loss: 0.0929\n",
      "Epoch [5/5], Step [1680/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [1682/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [1684/10336], Loss: 1.5910\n",
      "Epoch [5/5], Step [1686/10336], Loss: 0.1872\n",
      "Epoch [5/5], Step [1688/10336], Loss: 0.3224\n",
      "Epoch [5/5], Step [1690/10336], Loss: 0.2315\n",
      "Epoch [5/5], Step [1692/10336], Loss: 0.0970\n",
      "Epoch [5/5], Step [1694/10336], Loss: 1.3020\n",
      "Epoch [5/5], Step [1696/10336], Loss: 0.3764\n",
      "Epoch [5/5], Step [1698/10336], Loss: 0.1799\n",
      "Epoch [5/5], Step [1700/10336], Loss: 1.2282\n",
      "Epoch [5/5], Step [1702/10336], Loss: 0.5784\n",
      "Epoch [5/5], Step [1704/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [1706/10336], Loss: 1.5914\n",
      "Epoch [5/5], Step [1708/10336], Loss: 0.0938\n",
      "Epoch [5/5], Step [1710/10336], Loss: 0.0217\n",
      "Epoch [5/5], Step [1712/10336], Loss: 0.1331\n",
      "Epoch [5/5], Step [1714/10336], Loss: 1.6952\n",
      "Epoch [5/5], Step [1716/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [1718/10336], Loss: 0.2963\n",
      "Epoch [5/5], Step [1720/10336], Loss: 0.0865\n",
      "Epoch [5/5], Step [1722/10336], Loss: 0.0601\n",
      "Epoch [5/5], Step [1724/10336], Loss: 0.0847\n",
      "Epoch [5/5], Step [1726/10336], Loss: 0.1437\n",
      "Epoch [5/5], Step [1728/10336], Loss: 0.0334\n",
      "Epoch [5/5], Step [1730/10336], Loss: 0.0587\n",
      "Epoch [5/5], Step [1732/10336], Loss: 0.1604\n",
      "Epoch [5/5], Step [1734/10336], Loss: 0.2096\n",
      "Epoch [5/5], Step [1736/10336], Loss: 2.8478\n",
      "Epoch [5/5], Step [1738/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [1740/10336], Loss: 0.1338\n",
      "Epoch [5/5], Step [1742/10336], Loss: 0.1062\n",
      "Epoch [5/5], Step [1744/10336], Loss: 0.2868\n",
      "Epoch [5/5], Step [1746/10336], Loss: 0.2817\n",
      "Epoch [5/5], Step [1748/10336], Loss: 0.0168\n",
      "Epoch [5/5], Step [1750/10336], Loss: 0.0364\n",
      "Epoch [5/5], Step [1752/10336], Loss: 0.4078\n",
      "Epoch [5/5], Step [1754/10336], Loss: 0.0529\n",
      "Epoch [5/5], Step [1756/10336], Loss: 0.6916\n",
      "Epoch [5/5], Step [1758/10336], Loss: 0.6642\n",
      "Epoch [5/5], Step [1760/10336], Loss: 0.0387\n",
      "Epoch [5/5], Step [1762/10336], Loss: 0.1720\n",
      "Epoch [5/5], Step [1764/10336], Loss: 0.5360\n",
      "Epoch [5/5], Step [1766/10336], Loss: 0.2429\n",
      "Epoch [5/5], Step [1768/10336], Loss: 0.5220\n",
      "Epoch [5/5], Step [1770/10336], Loss: 2.1366\n",
      "Epoch [5/5], Step [1772/10336], Loss: 3.9016\n",
      "Epoch [5/5], Step [1774/10336], Loss: 0.0083\n",
      "Epoch [5/5], Step [1776/10336], Loss: 0.8520\n",
      "Epoch [5/5], Step [1778/10336], Loss: 0.3618\n",
      "Epoch [5/5], Step [1780/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [1782/10336], Loss: 0.0650\n",
      "Epoch [5/5], Step [1784/10336], Loss: 0.0233\n",
      "Epoch [5/5], Step [1786/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [1788/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [1790/10336], Loss: 0.0630\n",
      "Epoch [5/5], Step [1792/10336], Loss: 0.0349\n",
      "Epoch [5/5], Step [1794/10336], Loss: 0.0581\n",
      "Epoch [5/5], Step [1796/10336], Loss: 3.9953\n",
      "Epoch [5/5], Step [1798/10336], Loss: 0.3181\n",
      "Epoch [5/5], Step [1800/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [1802/10336], Loss: 0.0824\n",
      "Epoch [5/5], Step [1804/10336], Loss: 0.7181\n",
      "Epoch [5/5], Step [1806/10336], Loss: 0.5710\n",
      "Epoch [5/5], Step [1808/10336], Loss: 0.2537\n",
      "Epoch [5/5], Step [1810/10336], Loss: 0.0208\n",
      "Epoch [5/5], Step [1812/10336], Loss: 0.1164\n",
      "Epoch [5/5], Step [1814/10336], Loss: 0.2571\n",
      "Epoch [5/5], Step [1816/10336], Loss: 0.4108\n",
      "Epoch [5/5], Step [1818/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [1820/10336], Loss: 0.0707\n",
      "Epoch [5/5], Step [1822/10336], Loss: 0.2689\n",
      "Epoch [5/5], Step [1824/10336], Loss: 3.2866\n",
      "Epoch [5/5], Step [1826/10336], Loss: 0.6970\n",
      "Epoch [5/5], Step [1828/10336], Loss: 0.0281\n",
      "Epoch [5/5], Step [1830/10336], Loss: 0.2137\n",
      "Epoch [5/5], Step [1832/10336], Loss: 0.0976\n",
      "Epoch [5/5], Step [1834/10336], Loss: 0.3271\n",
      "Epoch [5/5], Step [1836/10336], Loss: 0.0192\n",
      "Epoch [5/5], Step [1838/10336], Loss: 0.0320\n",
      "Epoch [5/5], Step [1840/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [1842/10336], Loss: 0.8424\n",
      "Epoch [5/5], Step [1844/10336], Loss: 0.1461\n",
      "Epoch [5/5], Step [1846/10336], Loss: 0.0193\n",
      "Epoch [5/5], Step [1848/10336], Loss: 0.0959\n",
      "Epoch [5/5], Step [1850/10336], Loss: 0.0210\n",
      "Epoch [5/5], Step [1852/10336], Loss: 2.0946\n",
      "Epoch [5/5], Step [1854/10336], Loss: 0.2403\n",
      "Epoch [5/5], Step [1856/10336], Loss: 0.0186\n",
      "Epoch [5/5], Step [1858/10336], Loss: 1.4537\n",
      "Epoch [5/5], Step [1860/10336], Loss: 0.2716\n",
      "Epoch [5/5], Step [1862/10336], Loss: 0.0100\n",
      "Epoch [5/5], Step [1864/10336], Loss: 0.5323\n",
      "Epoch [5/5], Step [1866/10336], Loss: 0.0748\n",
      "Epoch [5/5], Step [1868/10336], Loss: 0.0152\n",
      "Epoch [5/5], Step [1870/10336], Loss: 0.8527\n",
      "Epoch [5/5], Step [1872/10336], Loss: 1.1509\n",
      "Epoch [5/5], Step [1874/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [1876/10336], Loss: 0.5202\n",
      "Epoch [5/5], Step [1878/10336], Loss: 0.2481\n",
      "Epoch [5/5], Step [1880/10336], Loss: 0.1417\n",
      "Epoch [5/5], Step [1882/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [1884/10336], Loss: 0.0461\n",
      "Epoch [5/5], Step [1886/10336], Loss: 0.1466\n",
      "Epoch [5/5], Step [1888/10336], Loss: 0.0939\n",
      "Epoch [5/5], Step [1890/10336], Loss: 0.0056\n",
      "Epoch [5/5], Step [1892/10336], Loss: 0.0278\n",
      "Epoch [5/5], Step [1894/10336], Loss: 0.0884\n",
      "Epoch [5/5], Step [1896/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [1898/10336], Loss: 0.1355\n",
      "Epoch [5/5], Step [1900/10336], Loss: 2.3566\n",
      "Epoch [5/5], Step [1902/10336], Loss: 0.0346\n",
      "Epoch [5/5], Step [1904/10336], Loss: 0.0271\n",
      "Epoch [5/5], Step [1906/10336], Loss: 0.1939\n",
      "Epoch [5/5], Step [1908/10336], Loss: 0.2145\n",
      "Epoch [5/5], Step [1910/10336], Loss: 0.2851\n",
      "Epoch [5/5], Step [1912/10336], Loss: 1.7505\n",
      "Epoch [5/5], Step [1914/10336], Loss: 2.9162\n",
      "Epoch [5/5], Step [1916/10336], Loss: 0.7631\n",
      "Epoch [5/5], Step [1918/10336], Loss: 0.6954\n",
      "Epoch [5/5], Step [1920/10336], Loss: 0.4647\n",
      "Epoch [5/5], Step [1922/10336], Loss: 0.0515\n",
      "Epoch [5/5], Step [1924/10336], Loss: 3.5008\n",
      "Epoch [5/5], Step [1926/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [1928/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [1930/10336], Loss: 0.5064\n",
      "Epoch [5/5], Step [1932/10336], Loss: 0.0946\n",
      "Epoch [5/5], Step [1934/10336], Loss: 0.0153\n",
      "Epoch [5/5], Step [1936/10336], Loss: 0.5765\n",
      "Epoch [5/5], Step [1938/10336], Loss: 0.0039\n",
      "Epoch [5/5], Step [1940/10336], Loss: 0.0356\n",
      "Epoch [5/5], Step [1942/10336], Loss: 0.2533\n",
      "Epoch [5/5], Step [1944/10336], Loss: 0.1598\n",
      "Epoch [5/5], Step [1946/10336], Loss: 0.5191\n",
      "Epoch [5/5], Step [1948/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [1950/10336], Loss: 0.2152\n",
      "Epoch [5/5], Step [1952/10336], Loss: 0.0272\n",
      "Epoch [5/5], Step [1954/10336], Loss: 0.0245\n",
      "Epoch [5/5], Step [1956/10336], Loss: 2.5495\n",
      "Epoch [5/5], Step [1958/10336], Loss: 0.1829\n",
      "Epoch [5/5], Step [1960/10336], Loss: 1.0160\n",
      "Epoch [5/5], Step [1962/10336], Loss: 0.2366\n",
      "Epoch [5/5], Step [1964/10336], Loss: 0.0106\n",
      "Epoch [5/5], Step [1966/10336], Loss: 0.0108\n",
      "Epoch [5/5], Step [1968/10336], Loss: 0.0975\n",
      "Epoch [5/5], Step [1970/10336], Loss: 0.1037\n",
      "Epoch [5/5], Step [1972/10336], Loss: 0.2314\n",
      "Epoch [5/5], Step [1974/10336], Loss: 0.9126\n",
      "Epoch [5/5], Step [1976/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [1978/10336], Loss: 0.4943\n",
      "Epoch [5/5], Step [1980/10336], Loss: 0.1988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [1982/10336], Loss: 0.3136\n",
      "Epoch [5/5], Step [1984/10336], Loss: 0.5802\n",
      "Epoch [5/5], Step [1986/10336], Loss: 0.3454\n",
      "Epoch [5/5], Step [1988/10336], Loss: 0.0490\n",
      "Epoch [5/5], Step [1990/10336], Loss: 1.8793\n",
      "Epoch [5/5], Step [1992/10336], Loss: 0.1982\n",
      "Epoch [5/5], Step [1994/10336], Loss: 0.1894\n",
      "Epoch [5/5], Step [1996/10336], Loss: 3.9023\n",
      "Epoch [5/5], Step [1998/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [2000/10336], Loss: 0.1307\n",
      "Epoch [5/5], Step [2002/10336], Loss: 0.0353\n",
      "Epoch [5/5], Step [2004/10336], Loss: 0.4443\n",
      "Epoch [5/5], Step [2006/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [2008/10336], Loss: 4.0094\n",
      "Epoch [5/5], Step [2010/10336], Loss: 0.0098\n",
      "Epoch [5/5], Step [2012/10336], Loss: 0.2389\n",
      "Epoch [5/5], Step [2014/10336], Loss: 2.7665\n",
      "Epoch [5/5], Step [2016/10336], Loss: 0.0310\n",
      "Epoch [5/5], Step [2018/10336], Loss: 0.2527\n",
      "Epoch [5/5], Step [2020/10336], Loss: 0.0594\n",
      "Epoch [5/5], Step [2022/10336], Loss: 2.6700\n",
      "Epoch [5/5], Step [2024/10336], Loss: 0.4305\n",
      "Epoch [5/5], Step [2026/10336], Loss: 0.0249\n",
      "Epoch [5/5], Step [2028/10336], Loss: 0.0749\n",
      "Epoch [5/5], Step [2030/10336], Loss: 0.3630\n",
      "Epoch [5/5], Step [2032/10336], Loss: 0.3395\n",
      "Epoch [5/5], Step [2034/10336], Loss: 0.1135\n",
      "Epoch [5/5], Step [2036/10336], Loss: 0.2244\n",
      "Epoch [5/5], Step [2038/10336], Loss: 0.0349\n",
      "Epoch [5/5], Step [2040/10336], Loss: 1.3814\n",
      "Epoch [5/5], Step [2042/10336], Loss: 0.8292\n",
      "Epoch [5/5], Step [2044/10336], Loss: 0.0527\n",
      "Epoch [5/5], Step [2046/10336], Loss: 0.3829\n",
      "Epoch [5/5], Step [2048/10336], Loss: 0.0302\n",
      "Epoch [5/5], Step [2050/10336], Loss: 5.4184\n",
      "Epoch [5/5], Step [2052/10336], Loss: 1.6944\n",
      "Epoch [5/5], Step [2054/10336], Loss: 1.4315\n",
      "Epoch [5/5], Step [2056/10336], Loss: 0.1026\n",
      "Epoch [5/5], Step [2058/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [2060/10336], Loss: 0.0391\n",
      "Epoch [5/5], Step [2062/10336], Loss: 0.0978\n",
      "Epoch [5/5], Step [2064/10336], Loss: 0.2069\n",
      "Epoch [5/5], Step [2066/10336], Loss: 0.2546\n",
      "Epoch [5/5], Step [2068/10336], Loss: 0.0577\n",
      "Epoch [5/5], Step [2070/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [2072/10336], Loss: 1.7359\n",
      "Epoch [5/5], Step [2074/10336], Loss: 0.8522\n",
      "Epoch [5/5], Step [2076/10336], Loss: 0.0915\n",
      "Epoch [5/5], Step [2078/10336], Loss: 4.0603\n",
      "Epoch [5/5], Step [2080/10336], Loss: 0.1428\n",
      "Epoch [5/5], Step [2082/10336], Loss: 1.3213\n",
      "Epoch [5/5], Step [2084/10336], Loss: 0.0066\n",
      "Epoch [5/5], Step [2086/10336], Loss: 0.3721\n",
      "Epoch [5/5], Step [2088/10336], Loss: 2.5281\n",
      "Epoch [5/5], Step [2090/10336], Loss: 0.3427\n",
      "Epoch [5/5], Step [2092/10336], Loss: 0.0577\n",
      "Epoch [5/5], Step [2094/10336], Loss: 0.1372\n",
      "Epoch [5/5], Step [2096/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [2098/10336], Loss: 0.0450\n",
      "Epoch [5/5], Step [2100/10336], Loss: 0.0911\n",
      "Epoch [5/5], Step [2102/10336], Loss: 0.0832\n",
      "Epoch [5/5], Step [2104/10336], Loss: 0.1658\n",
      "Epoch [5/5], Step [2106/10336], Loss: 0.2692\n",
      "Epoch [5/5], Step [2108/10336], Loss: 0.6147\n",
      "Epoch [5/5], Step [2110/10336], Loss: 0.0714\n",
      "Epoch [5/5], Step [2112/10336], Loss: 0.5867\n",
      "Epoch [5/5], Step [2114/10336], Loss: 0.2009\n",
      "Epoch [5/5], Step [2116/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [2118/10336], Loss: 1.0800\n",
      "Epoch [5/5], Step [2120/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [2122/10336], Loss: 0.0066\n",
      "Epoch [5/5], Step [2124/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [2126/10336], Loss: 1.1644\n",
      "Epoch [5/5], Step [2128/10336], Loss: 0.0319\n",
      "Epoch [5/5], Step [2130/10336], Loss: 0.0485\n",
      "Epoch [5/5], Step [2132/10336], Loss: 0.4129\n",
      "Epoch [5/5], Step [2134/10336], Loss: 1.4895\n",
      "Epoch [5/5], Step [2136/10336], Loss: 0.2645\n",
      "Epoch [5/5], Step [2138/10336], Loss: 0.0327\n",
      "Epoch [5/5], Step [2140/10336], Loss: 0.0132\n",
      "Epoch [5/5], Step [2142/10336], Loss: 0.6208\n",
      "Epoch [5/5], Step [2144/10336], Loss: 0.0405\n",
      "Epoch [5/5], Step [2146/10336], Loss: 0.0119\n",
      "Epoch [5/5], Step [2148/10336], Loss: 0.5390\n",
      "Epoch [5/5], Step [2150/10336], Loss: 0.0954\n",
      "Epoch [5/5], Step [2152/10336], Loss: 0.3143\n",
      "Epoch [5/5], Step [2154/10336], Loss: 0.0472\n",
      "Epoch [5/5], Step [2156/10336], Loss: 1.1464\n",
      "Epoch [5/5], Step [2158/10336], Loss: 0.0159\n",
      "Epoch [5/5], Step [2160/10336], Loss: 1.0801\n",
      "Epoch [5/5], Step [2162/10336], Loss: 1.6486\n",
      "Epoch [5/5], Step [2164/10336], Loss: 0.2506\n",
      "Epoch [5/5], Step [2166/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [2168/10336], Loss: 0.0079\n",
      "Epoch [5/5], Step [2170/10336], Loss: 0.0902\n",
      "Epoch [5/5], Step [2172/10336], Loss: 0.6156\n",
      "Epoch [5/5], Step [2174/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [2176/10336], Loss: 0.0075\n",
      "Epoch [5/5], Step [2178/10336], Loss: 0.0858\n",
      "Epoch [5/5], Step [2180/10336], Loss: 0.0266\n",
      "Epoch [5/5], Step [2182/10336], Loss: 0.7387\n",
      "Epoch [5/5], Step [2184/10336], Loss: 0.9876\n",
      "Epoch [5/5], Step [2186/10336], Loss: 0.0940\n",
      "Epoch [5/5], Step [2188/10336], Loss: 0.3742\n",
      "Epoch [5/5], Step [2190/10336], Loss: 0.0445\n",
      "Epoch [5/5], Step [2192/10336], Loss: 0.2231\n",
      "Epoch [5/5], Step [2194/10336], Loss: 0.0229\n",
      "Epoch [5/5], Step [2196/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [2198/10336], Loss: 1.8607\n",
      "Epoch [5/5], Step [2200/10336], Loss: 0.5113\n",
      "Epoch [5/5], Step [2202/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [2204/10336], Loss: 0.1062\n",
      "Epoch [5/5], Step [2206/10336], Loss: 0.1083\n",
      "Epoch [5/5], Step [2208/10336], Loss: 0.2366\n",
      "Epoch [5/5], Step [2210/10336], Loss: 0.2188\n",
      "Epoch [5/5], Step [2212/10336], Loss: 0.1209\n",
      "Epoch [5/5], Step [2214/10336], Loss: 0.0165\n",
      "Epoch [5/5], Step [2216/10336], Loss: 1.2787\n",
      "Epoch [5/5], Step [2218/10336], Loss: 0.0418\n",
      "Epoch [5/5], Step [2220/10336], Loss: 2.8661\n",
      "Epoch [5/5], Step [2222/10336], Loss: 0.0324\n",
      "Epoch [5/5], Step [2224/10336], Loss: 0.6167\n",
      "Epoch [5/5], Step [2226/10336], Loss: 0.1523\n",
      "Epoch [5/5], Step [2228/10336], Loss: 0.6195\n",
      "Epoch [5/5], Step [2230/10336], Loss: 0.1528\n",
      "Epoch [5/5], Step [2232/10336], Loss: 0.0877\n",
      "Epoch [5/5], Step [2234/10336], Loss: 1.5119\n",
      "Epoch [5/5], Step [2236/10336], Loss: 0.1564\n",
      "Epoch [5/5], Step [2238/10336], Loss: 0.6522\n",
      "Epoch [5/5], Step [2240/10336], Loss: 0.6536\n",
      "Epoch [5/5], Step [2242/10336], Loss: 0.3788\n",
      "Epoch [5/5], Step [2244/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [2246/10336], Loss: 0.1889\n",
      "Epoch [5/5], Step [2248/10336], Loss: 0.2015\n",
      "Epoch [5/5], Step [2250/10336], Loss: 0.0158\n",
      "Epoch [5/5], Step [2252/10336], Loss: 0.0048\n",
      "Epoch [5/5], Step [2254/10336], Loss: 0.1458\n",
      "Epoch [5/5], Step [2256/10336], Loss: 0.1758\n",
      "Epoch [5/5], Step [2258/10336], Loss: 0.1316\n",
      "Epoch [5/5], Step [2260/10336], Loss: 0.3875\n",
      "Epoch [5/5], Step [2262/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [2264/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [2266/10336], Loss: 0.1196\n",
      "Epoch [5/5], Step [2268/10336], Loss: 0.0925\n",
      "Epoch [5/5], Step [2270/10336], Loss: 0.3721\n",
      "Epoch [5/5], Step [2272/10336], Loss: 0.7025\n",
      "Epoch [5/5], Step [2274/10336], Loss: 0.2318\n",
      "Epoch [5/5], Step [2276/10336], Loss: 0.0569\n",
      "Epoch [5/5], Step [2278/10336], Loss: 0.0267\n",
      "Epoch [5/5], Step [2280/10336], Loss: 0.1609\n",
      "Epoch [5/5], Step [2282/10336], Loss: 0.0596\n",
      "Epoch [5/5], Step [2284/10336], Loss: 0.1857\n",
      "Epoch [5/5], Step [2286/10336], Loss: 0.0972\n",
      "Epoch [5/5], Step [2288/10336], Loss: 0.3934\n",
      "Epoch [5/5], Step [2290/10336], Loss: 0.1066\n",
      "Epoch [5/5], Step [2292/10336], Loss: 0.0265\n",
      "Epoch [5/5], Step [2294/10336], Loss: 0.6890\n",
      "Epoch [5/5], Step [2296/10336], Loss: 0.1712\n",
      "Epoch [5/5], Step [2298/10336], Loss: 0.0174\n",
      "Epoch [5/5], Step [2300/10336], Loss: 0.0414\n",
      "Epoch [5/5], Step [2302/10336], Loss: 0.0176\n",
      "Epoch [5/5], Step [2304/10336], Loss: 0.1410\n",
      "Epoch [5/5], Step [2306/10336], Loss: 3.4872\n",
      "Epoch [5/5], Step [2308/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [2310/10336], Loss: 0.0104\n",
      "Epoch [5/5], Step [2312/10336], Loss: 0.3966\n",
      "Epoch [5/5], Step [2314/10336], Loss: 0.0814\n",
      "Epoch [5/5], Step [2316/10336], Loss: 0.7164\n",
      "Epoch [5/5], Step [2318/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [2320/10336], Loss: 0.0087\n",
      "Epoch [5/5], Step [2322/10336], Loss: 0.0292\n",
      "Epoch [5/5], Step [2324/10336], Loss: 1.8936\n",
      "Epoch [5/5], Step [2326/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [2328/10336], Loss: 0.2201\n",
      "Epoch [5/5], Step [2330/10336], Loss: 0.0554\n",
      "Epoch [5/5], Step [2332/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [2334/10336], Loss: 0.3378\n",
      "Epoch [5/5], Step [2336/10336], Loss: 2.7767\n",
      "Epoch [5/5], Step [2338/10336], Loss: 0.0508\n",
      "Epoch [5/5], Step [2340/10336], Loss: 1.5192\n",
      "Epoch [5/5], Step [2342/10336], Loss: 0.1498\n",
      "Epoch [5/5], Step [2344/10336], Loss: 0.1130\n",
      "Epoch [5/5], Step [2346/10336], Loss: 0.0885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [2348/10336], Loss: 0.1758\n",
      "Epoch [5/5], Step [2350/10336], Loss: 3.3512\n",
      "Epoch [5/5], Step [2352/10336], Loss: 0.9296\n",
      "Epoch [5/5], Step [2354/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [2356/10336], Loss: 3.2310\n",
      "Epoch [5/5], Step [2358/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [2360/10336], Loss: 0.0766\n",
      "Epoch [5/5], Step [2362/10336], Loss: 0.5157\n",
      "Epoch [5/5], Step [2364/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [2366/10336], Loss: 0.8498\n",
      "Epoch [5/5], Step [2368/10336], Loss: 1.2177\n",
      "Epoch [5/5], Step [2370/10336], Loss: 0.0400\n",
      "Epoch [5/5], Step [2372/10336], Loss: 0.2433\n",
      "Epoch [5/5], Step [2374/10336], Loss: 0.0346\n",
      "Epoch [5/5], Step [2376/10336], Loss: 0.9834\n",
      "Epoch [5/5], Step [2378/10336], Loss: 0.5942\n",
      "Epoch [5/5], Step [2380/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [2382/10336], Loss: 0.6314\n",
      "Epoch [5/5], Step [2384/10336], Loss: 0.7248\n",
      "Epoch [5/5], Step [2386/10336], Loss: 0.0223\n",
      "Epoch [5/5], Step [2388/10336], Loss: 1.8022\n",
      "Epoch [5/5], Step [2390/10336], Loss: 0.1426\n",
      "Epoch [5/5], Step [2392/10336], Loss: 0.2078\n",
      "Epoch [5/5], Step [2394/10336], Loss: 0.0566\n",
      "Epoch [5/5], Step [2396/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [2398/10336], Loss: 0.0881\n",
      "Epoch [5/5], Step [2400/10336], Loss: 0.7169\n",
      "Epoch [5/5], Step [2402/10336], Loss: 0.0200\n",
      "Epoch [5/5], Step [2404/10336], Loss: 0.4560\n",
      "Epoch [5/5], Step [2406/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [2408/10336], Loss: 0.0775\n",
      "Epoch [5/5], Step [2410/10336], Loss: 1.1819\n",
      "Epoch [5/5], Step [2412/10336], Loss: 0.1905\n",
      "Epoch [5/5], Step [2414/10336], Loss: 0.5824\n",
      "Epoch [5/5], Step [2416/10336], Loss: 0.2865\n",
      "Epoch [5/5], Step [2418/10336], Loss: 1.9880\n",
      "Epoch [5/5], Step [2420/10336], Loss: 0.3175\n",
      "Epoch [5/5], Step [2422/10336], Loss: 0.2922\n",
      "Epoch [5/5], Step [2424/10336], Loss: 0.3763\n",
      "Epoch [5/5], Step [2426/10336], Loss: 0.5941\n",
      "Epoch [5/5], Step [2428/10336], Loss: 0.0089\n",
      "Epoch [5/5], Step [2430/10336], Loss: 0.0237\n",
      "Epoch [5/5], Step [2432/10336], Loss: 0.0940\n",
      "Epoch [5/5], Step [2434/10336], Loss: 0.4965\n",
      "Epoch [5/5], Step [2436/10336], Loss: 0.1167\n",
      "Epoch [5/5], Step [2438/10336], Loss: 0.0271\n",
      "Epoch [5/5], Step [2440/10336], Loss: 0.0826\n",
      "Epoch [5/5], Step [2442/10336], Loss: 0.3638\n",
      "Epoch [5/5], Step [2444/10336], Loss: 0.0677\n",
      "Epoch [5/5], Step [2446/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [2448/10336], Loss: 0.1759\n",
      "Epoch [5/5], Step [2450/10336], Loss: 1.5361\n",
      "Epoch [5/5], Step [2452/10336], Loss: 0.0879\n",
      "Epoch [5/5], Step [2454/10336], Loss: 0.0344\n",
      "Epoch [5/5], Step [2456/10336], Loss: 0.0407\n",
      "Epoch [5/5], Step [2458/10336], Loss: 0.2169\n",
      "Epoch [5/5], Step [2460/10336], Loss: 0.0190\n",
      "Epoch [5/5], Step [2462/10336], Loss: 0.4327\n",
      "Epoch [5/5], Step [2464/10336], Loss: 0.2939\n",
      "Epoch [5/5], Step [2466/10336], Loss: 0.0121\n",
      "Epoch [5/5], Step [2468/10336], Loss: 2.2073\n",
      "Epoch [5/5], Step [2470/10336], Loss: 0.0468\n",
      "Epoch [5/5], Step [2472/10336], Loss: 0.2412\n",
      "Epoch [5/5], Step [2474/10336], Loss: 0.0655\n",
      "Epoch [5/5], Step [2476/10336], Loss: 0.0719\n",
      "Epoch [5/5], Step [2478/10336], Loss: 1.4264\n",
      "Epoch [5/5], Step [2480/10336], Loss: 1.5934\n",
      "Epoch [5/5], Step [2482/10336], Loss: 1.0531\n",
      "Epoch [5/5], Step [2484/10336], Loss: 0.9478\n",
      "Epoch [5/5], Step [2486/10336], Loss: 0.7943\n",
      "Epoch [5/5], Step [2488/10336], Loss: 0.0049\n",
      "Epoch [5/5], Step [2490/10336], Loss: 0.0143\n",
      "Epoch [5/5], Step [2492/10336], Loss: 0.0329\n",
      "Epoch [5/5], Step [2494/10336], Loss: 0.1297\n",
      "Epoch [5/5], Step [2496/10336], Loss: 0.1379\n",
      "Epoch [5/5], Step [2498/10336], Loss: 0.9510\n",
      "Epoch [5/5], Step [2500/10336], Loss: 0.0966\n",
      "Epoch [5/5], Step [2502/10336], Loss: 0.0209\n",
      "Epoch [5/5], Step [2504/10336], Loss: 0.2550\n",
      "Epoch [5/5], Step [2506/10336], Loss: 0.2876\n",
      "Epoch [5/5], Step [2508/10336], Loss: 0.0309\n",
      "Epoch [5/5], Step [2510/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [2512/10336], Loss: 0.2912\n",
      "Epoch [5/5], Step [2514/10336], Loss: 0.1931\n",
      "Epoch [5/5], Step [2516/10336], Loss: 0.4624\n",
      "Epoch [5/5], Step [2518/10336], Loss: 0.0373\n",
      "Epoch [5/5], Step [2520/10336], Loss: 1.5169\n",
      "Epoch [5/5], Step [2522/10336], Loss: 3.2737\n",
      "Epoch [5/5], Step [2524/10336], Loss: 1.9813\n",
      "Epoch [5/5], Step [2526/10336], Loss: 0.0418\n",
      "Epoch [5/5], Step [2528/10336], Loss: 0.1328\n",
      "Epoch [5/5], Step [2530/10336], Loss: 0.0240\n",
      "Epoch [5/5], Step [2532/10336], Loss: 1.2525\n",
      "Epoch [5/5], Step [2534/10336], Loss: 0.1626\n",
      "Epoch [5/5], Step [2536/10336], Loss: 0.0324\n",
      "Epoch [5/5], Step [2538/10336], Loss: 0.1827\n",
      "Epoch [5/5], Step [2540/10336], Loss: 1.6174\n",
      "Epoch [5/5], Step [2542/10336], Loss: 2.5526\n",
      "Epoch [5/5], Step [2544/10336], Loss: 0.2284\n",
      "Epoch [5/5], Step [2546/10336], Loss: 0.3370\n",
      "Epoch [5/5], Step [2548/10336], Loss: 0.2897\n",
      "Epoch [5/5], Step [2550/10336], Loss: 0.2066\n",
      "Epoch [5/5], Step [2552/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [2554/10336], Loss: 2.3901\n",
      "Epoch [5/5], Step [2556/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [2558/10336], Loss: 0.0230\n",
      "Epoch [5/5], Step [2560/10336], Loss: 0.1108\n",
      "Epoch [5/5], Step [2562/10336], Loss: 0.1009\n",
      "Epoch [5/5], Step [2564/10336], Loss: 0.0476\n",
      "Epoch [5/5], Step [2566/10336], Loss: 1.1023\n",
      "Epoch [5/5], Step [2568/10336], Loss: 0.1795\n",
      "Epoch [5/5], Step [2570/10336], Loss: 2.7493\n",
      "Epoch [5/5], Step [2572/10336], Loss: 0.0535\n",
      "Epoch [5/5], Step [2574/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [2576/10336], Loss: 0.0460\n",
      "Epoch [5/5], Step [2578/10336], Loss: 0.1721\n",
      "Epoch [5/5], Step [2580/10336], Loss: 1.0682\n",
      "Epoch [5/5], Step [2582/10336], Loss: 0.6233\n",
      "Epoch [5/5], Step [2584/10336], Loss: 2.2368\n",
      "Epoch [5/5], Step [2586/10336], Loss: 1.7389\n",
      "Epoch [5/5], Step [2588/10336], Loss: 0.2922\n",
      "Epoch [5/5], Step [2590/10336], Loss: 0.4575\n",
      "Epoch [5/5], Step [2592/10336], Loss: 0.5346\n",
      "Epoch [5/5], Step [2594/10336], Loss: 0.2493\n",
      "Epoch [5/5], Step [2596/10336], Loss: 0.9022\n",
      "Epoch [5/5], Step [2598/10336], Loss: 1.2253\n",
      "Epoch [5/5], Step [2600/10336], Loss: 0.3106\n",
      "Epoch [5/5], Step [2602/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [2604/10336], Loss: 0.4043\n",
      "Epoch [5/5], Step [2606/10336], Loss: 0.6018\n",
      "Epoch [5/5], Step [2608/10336], Loss: 0.0476\n",
      "Epoch [5/5], Step [2610/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [2612/10336], Loss: 0.3626\n",
      "Epoch [5/5], Step [2614/10336], Loss: 0.5876\n",
      "Epoch [5/5], Step [2616/10336], Loss: 0.5474\n",
      "Epoch [5/5], Step [2618/10336], Loss: 0.0282\n",
      "Epoch [5/5], Step [2620/10336], Loss: 2.8964\n",
      "Epoch [5/5], Step [2622/10336], Loss: 0.2966\n",
      "Epoch [5/5], Step [2624/10336], Loss: 1.1329\n",
      "Epoch [5/5], Step [2626/10336], Loss: 1.0378\n",
      "Epoch [5/5], Step [2628/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [2630/10336], Loss: 0.0897\n",
      "Epoch [5/5], Step [2632/10336], Loss: 0.0218\n",
      "Epoch [5/5], Step [2634/10336], Loss: 0.3060\n",
      "Epoch [5/5], Step [2636/10336], Loss: 0.4581\n",
      "Epoch [5/5], Step [2638/10336], Loss: 2.5027\n",
      "Epoch [5/5], Step [2640/10336], Loss: 2.1791\n",
      "Epoch [5/5], Step [2642/10336], Loss: 0.0451\n",
      "Epoch [5/5], Step [2644/10336], Loss: 0.2742\n",
      "Epoch [5/5], Step [2646/10336], Loss: 0.2180\n",
      "Epoch [5/5], Step [2648/10336], Loss: 0.2863\n",
      "Epoch [5/5], Step [2650/10336], Loss: 0.2584\n",
      "Epoch [5/5], Step [2652/10336], Loss: 2.2027\n",
      "Epoch [5/5], Step [2654/10336], Loss: 0.4252\n",
      "Epoch [5/5], Step [2656/10336], Loss: 0.0327\n",
      "Epoch [5/5], Step [2658/10336], Loss: 0.0327\n",
      "Epoch [5/5], Step [2660/10336], Loss: 0.2679\n",
      "Epoch [5/5], Step [2662/10336], Loss: 5.9907\n",
      "Epoch [5/5], Step [2664/10336], Loss: 0.0295\n",
      "Epoch [5/5], Step [2666/10336], Loss: 0.0315\n",
      "Epoch [5/5], Step [2668/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [2670/10336], Loss: 0.1564\n",
      "Epoch [5/5], Step [2672/10336], Loss: 0.5009\n",
      "Epoch [5/5], Step [2674/10336], Loss: 1.9868\n",
      "Epoch [5/5], Step [2676/10336], Loss: 0.0371\n",
      "Epoch [5/5], Step [2678/10336], Loss: 0.0102\n",
      "Epoch [5/5], Step [2680/10336], Loss: 0.1353\n",
      "Epoch [5/5], Step [2682/10336], Loss: 0.0561\n",
      "Epoch [5/5], Step [2684/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [2686/10336], Loss: 0.2338\n",
      "Epoch [5/5], Step [2688/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [2690/10336], Loss: 1.4042\n",
      "Epoch [5/5], Step [2692/10336], Loss: 0.6325\n",
      "Epoch [5/5], Step [2694/10336], Loss: 0.0773\n",
      "Epoch [5/5], Step [2696/10336], Loss: 0.0190\n",
      "Epoch [5/5], Step [2698/10336], Loss: 2.7846\n",
      "Epoch [5/5], Step [2700/10336], Loss: 1.8968\n",
      "Epoch [5/5], Step [2702/10336], Loss: 0.1478\n",
      "Epoch [5/5], Step [2704/10336], Loss: 0.0580\n",
      "Epoch [5/5], Step [2706/10336], Loss: 1.5918\n",
      "Epoch [5/5], Step [2708/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [2710/10336], Loss: 0.1312\n",
      "Epoch [5/5], Step [2712/10336], Loss: 0.0066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [2714/10336], Loss: 0.0350\n",
      "Epoch [5/5], Step [2716/10336], Loss: 0.0450\n",
      "Epoch [5/5], Step [2718/10336], Loss: 0.0469\n",
      "Epoch [5/5], Step [2720/10336], Loss: 0.0490\n",
      "Epoch [5/5], Step [2722/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [2724/10336], Loss: 0.1421\n",
      "Epoch [5/5], Step [2726/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [2728/10336], Loss: 0.0783\n",
      "Epoch [5/5], Step [2730/10336], Loss: 2.1510\n",
      "Epoch [5/5], Step [2732/10336], Loss: 0.5170\n",
      "Epoch [5/5], Step [2734/10336], Loss: 0.2047\n",
      "Epoch [5/5], Step [2736/10336], Loss: 0.0289\n",
      "Epoch [5/5], Step [2738/10336], Loss: 0.4762\n",
      "Epoch [5/5], Step [2740/10336], Loss: 0.1078\n",
      "Epoch [5/5], Step [2742/10336], Loss: 0.0219\n",
      "Epoch [5/5], Step [2744/10336], Loss: 0.3042\n",
      "Epoch [5/5], Step [2746/10336], Loss: 0.3010\n",
      "Epoch [5/5], Step [2748/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [2750/10336], Loss: 0.1454\n",
      "Epoch [5/5], Step [2752/10336], Loss: 0.5217\n",
      "Epoch [5/5], Step [2754/10336], Loss: 0.5618\n",
      "Epoch [5/5], Step [2756/10336], Loss: 0.1261\n",
      "Epoch [5/5], Step [2758/10336], Loss: 2.9859\n",
      "Epoch [5/5], Step [2760/10336], Loss: 1.2281\n",
      "Epoch [5/5], Step [2762/10336], Loss: 0.0521\n",
      "Epoch [5/5], Step [2764/10336], Loss: 0.0076\n",
      "Epoch [5/5], Step [2766/10336], Loss: 0.5759\n",
      "Epoch [5/5], Step [2768/10336], Loss: 2.9778\n",
      "Epoch [5/5], Step [2770/10336], Loss: 0.3805\n",
      "Epoch [5/5], Step [2772/10336], Loss: 0.0177\n",
      "Epoch [5/5], Step [2774/10336], Loss: 0.2296\n",
      "Epoch [5/5], Step [2776/10336], Loss: 0.3689\n",
      "Epoch [5/5], Step [2778/10336], Loss: 0.0785\n",
      "Epoch [5/5], Step [2780/10336], Loss: 0.3290\n",
      "Epoch [5/5], Step [2782/10336], Loss: 1.4163\n",
      "Epoch [5/5], Step [2784/10336], Loss: 0.1487\n",
      "Epoch [5/5], Step [2786/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [2788/10336], Loss: 0.0172\n",
      "Epoch [5/5], Step [2790/10336], Loss: 0.5842\n",
      "Epoch [5/5], Step [2792/10336], Loss: 0.3178\n",
      "Epoch [5/5], Step [2794/10336], Loss: 0.0554\n",
      "Epoch [5/5], Step [2796/10336], Loss: 0.0092\n",
      "Epoch [5/5], Step [2798/10336], Loss: 3.9971\n",
      "Epoch [5/5], Step [2800/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [2802/10336], Loss: 2.0890\n",
      "Epoch [5/5], Step [2804/10336], Loss: 0.7945\n",
      "Epoch [5/5], Step [2806/10336], Loss: 0.0053\n",
      "Epoch [5/5], Step [2808/10336], Loss: 0.1981\n",
      "Epoch [5/5], Step [2810/10336], Loss: 0.0405\n",
      "Epoch [5/5], Step [2812/10336], Loss: 0.3779\n",
      "Epoch [5/5], Step [2814/10336], Loss: 0.0879\n",
      "Epoch [5/5], Step [2816/10336], Loss: 0.1673\n",
      "Epoch [5/5], Step [2818/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [2820/10336], Loss: 0.1992\n",
      "Epoch [5/5], Step [2822/10336], Loss: 0.2054\n",
      "Epoch [5/5], Step [2824/10336], Loss: 0.0348\n",
      "Epoch [5/5], Step [2826/10336], Loss: 0.1112\n",
      "Epoch [5/5], Step [2828/10336], Loss: 3.2454\n",
      "Epoch [5/5], Step [2830/10336], Loss: 0.0833\n",
      "Epoch [5/5], Step [2832/10336], Loss: 0.2348\n",
      "Epoch [5/5], Step [2834/10336], Loss: 1.3973\n",
      "Epoch [5/5], Step [2836/10336], Loss: 0.4911\n",
      "Epoch [5/5], Step [2838/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [2840/10336], Loss: 0.5816\n",
      "Epoch [5/5], Step [2842/10336], Loss: 1.8077\n",
      "Epoch [5/5], Step [2844/10336], Loss: 4.6351\n",
      "Epoch [5/5], Step [2846/10336], Loss: 1.5549\n",
      "Epoch [5/5], Step [2848/10336], Loss: 4.8496\n",
      "Epoch [5/5], Step [2850/10336], Loss: 0.4613\n",
      "Epoch [5/5], Step [2852/10336], Loss: 0.0685\n",
      "Epoch [5/5], Step [2854/10336], Loss: 1.2507\n",
      "Epoch [5/5], Step [2856/10336], Loss: 0.4207\n",
      "Epoch [5/5], Step [2858/10336], Loss: 0.1652\n",
      "Epoch [5/5], Step [2860/10336], Loss: 0.6933\n",
      "Epoch [5/5], Step [2862/10336], Loss: 0.1954\n",
      "Epoch [5/5], Step [2864/10336], Loss: 0.2298\n",
      "Epoch [5/5], Step [2866/10336], Loss: 0.2789\n",
      "Epoch [5/5], Step [2868/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [2870/10336], Loss: 0.0099\n",
      "Epoch [5/5], Step [2872/10336], Loss: 0.5041\n",
      "Epoch [5/5], Step [2874/10336], Loss: 0.0404\n",
      "Epoch [5/5], Step [2876/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [2878/10336], Loss: 0.0868\n",
      "Epoch [5/5], Step [2880/10336], Loss: 0.2167\n",
      "Epoch [5/5], Step [2882/10336], Loss: 0.0498\n",
      "Epoch [5/5], Step [2884/10336], Loss: 0.7226\n",
      "Epoch [5/5], Step [2886/10336], Loss: 0.5801\n",
      "Epoch [5/5], Step [2888/10336], Loss: 0.0603\n",
      "Epoch [5/5], Step [2890/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [2892/10336], Loss: 1.5556\n",
      "Epoch [5/5], Step [2894/10336], Loss: 0.7458\n",
      "Epoch [5/5], Step [2896/10336], Loss: 0.2130\n",
      "Epoch [5/5], Step [2898/10336], Loss: 0.1719\n",
      "Epoch [5/5], Step [2900/10336], Loss: 0.4838\n",
      "Epoch [5/5], Step [2902/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [2904/10336], Loss: 0.0074\n",
      "Epoch [5/5], Step [2906/10336], Loss: 0.1732\n",
      "Epoch [5/5], Step [2908/10336], Loss: 0.2152\n",
      "Epoch [5/5], Step [2910/10336], Loss: 0.2610\n",
      "Epoch [5/5], Step [2912/10336], Loss: 0.2425\n",
      "Epoch [5/5], Step [2914/10336], Loss: 0.2460\n",
      "Epoch [5/5], Step [2916/10336], Loss: 0.1957\n",
      "Epoch [5/5], Step [2918/10336], Loss: 0.1972\n",
      "Epoch [5/5], Step [2920/10336], Loss: 0.8668\n",
      "Epoch [5/5], Step [2922/10336], Loss: 1.9581\n",
      "Epoch [5/5], Step [2924/10336], Loss: 0.2388\n",
      "Epoch [5/5], Step [2926/10336], Loss: 0.6364\n",
      "Epoch [5/5], Step [2928/10336], Loss: 0.0133\n",
      "Epoch [5/5], Step [2930/10336], Loss: 0.2600\n",
      "Epoch [5/5], Step [2932/10336], Loss: 0.4325\n",
      "Epoch [5/5], Step [2934/10336], Loss: 0.0969\n",
      "Epoch [5/5], Step [2936/10336], Loss: 0.3083\n",
      "Epoch [5/5], Step [2938/10336], Loss: 0.0131\n",
      "Epoch [5/5], Step [2940/10336], Loss: 0.2853\n",
      "Epoch [5/5], Step [2942/10336], Loss: 0.0387\n",
      "Epoch [5/5], Step [2944/10336], Loss: 0.0693\n",
      "Epoch [5/5], Step [2946/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [2948/10336], Loss: 0.1090\n",
      "Epoch [5/5], Step [2950/10336], Loss: 0.0403\n",
      "Epoch [5/5], Step [2952/10336], Loss: 0.0794\n",
      "Epoch [5/5], Step [2954/10336], Loss: 1.5342\n",
      "Epoch [5/5], Step [2956/10336], Loss: 0.8387\n",
      "Epoch [5/5], Step [2958/10336], Loss: 4.6406\n",
      "Epoch [5/5], Step [2960/10336], Loss: 0.3996\n",
      "Epoch [5/5], Step [2962/10336], Loss: 0.7406\n",
      "Epoch [5/5], Step [2964/10336], Loss: 0.9112\n",
      "Epoch [5/5], Step [2966/10336], Loss: 0.1871\n",
      "Epoch [5/5], Step [2968/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [2970/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [2972/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [2974/10336], Loss: 1.3274\n",
      "Epoch [5/5], Step [2976/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [2978/10336], Loss: 0.3590\n",
      "Epoch [5/5], Step [2980/10336], Loss: 0.2583\n",
      "Epoch [5/5], Step [2982/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [2984/10336], Loss: 0.2261\n",
      "Epoch [5/5], Step [2986/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [2988/10336], Loss: 0.0769\n",
      "Epoch [5/5], Step [2990/10336], Loss: 0.0495\n",
      "Epoch [5/5], Step [2992/10336], Loss: 1.3275\n",
      "Epoch [5/5], Step [2994/10336], Loss: 0.2155\n",
      "Epoch [5/5], Step [2996/10336], Loss: 0.8987\n",
      "Epoch [5/5], Step [2998/10336], Loss: 0.0122\n",
      "Epoch [5/5], Step [3000/10336], Loss: 0.1090\n",
      "Epoch [5/5], Step [3002/10336], Loss: 0.0111\n",
      "Epoch [5/5], Step [3004/10336], Loss: 2.0975\n",
      "Epoch [5/5], Step [3006/10336], Loss: 0.1019\n",
      "Epoch [5/5], Step [3008/10336], Loss: 0.3149\n",
      "Epoch [5/5], Step [3010/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [3012/10336], Loss: 0.2976\n",
      "Epoch [5/5], Step [3014/10336], Loss: 0.1288\n",
      "Epoch [5/5], Step [3016/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [3018/10336], Loss: 0.1203\n",
      "Epoch [5/5], Step [3020/10336], Loss: 0.6112\n",
      "Epoch [5/5], Step [3022/10336], Loss: 0.0220\n",
      "Epoch [5/5], Step [3024/10336], Loss: 0.7646\n",
      "Epoch [5/5], Step [3026/10336], Loss: 0.0120\n",
      "Epoch [5/5], Step [3028/10336], Loss: 0.2515\n",
      "Epoch [5/5], Step [3030/10336], Loss: 0.1485\n",
      "Epoch [5/5], Step [3032/10336], Loss: 0.0074\n",
      "Epoch [5/5], Step [3034/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [3036/10336], Loss: 0.6171\n",
      "Epoch [5/5], Step [3038/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [3040/10336], Loss: 4.7204\n",
      "Epoch [5/5], Step [3042/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [3044/10336], Loss: 0.0066\n",
      "Epoch [5/5], Step [3046/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [3048/10336], Loss: 0.0523\n",
      "Epoch [5/5], Step [3050/10336], Loss: 0.3769\n",
      "Epoch [5/5], Step [3052/10336], Loss: 3.4447\n",
      "Epoch [5/5], Step [3054/10336], Loss: 0.0366\n",
      "Epoch [5/5], Step [3056/10336], Loss: 0.7248\n",
      "Epoch [5/5], Step [3058/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [3060/10336], Loss: 0.5102\n",
      "Epoch [5/5], Step [3062/10336], Loss: 0.3841\n",
      "Epoch [5/5], Step [3064/10336], Loss: 0.5540\n",
      "Epoch [5/5], Step [3066/10336], Loss: 0.1497\n",
      "Epoch [5/5], Step [3068/10336], Loss: 0.4003\n",
      "Epoch [5/5], Step [3070/10336], Loss: 0.0398\n",
      "Epoch [5/5], Step [3072/10336], Loss: 0.3799\n",
      "Epoch [5/5], Step [3074/10336], Loss: 0.2308\n",
      "Epoch [5/5], Step [3076/10336], Loss: 0.1781\n",
      "Epoch [5/5], Step [3078/10336], Loss: 0.1814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [3080/10336], Loss: 0.0128\n",
      "Epoch [5/5], Step [3082/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [3084/10336], Loss: 2.4388\n",
      "Epoch [5/5], Step [3086/10336], Loss: 0.0426\n",
      "Epoch [5/5], Step [3088/10336], Loss: 0.4399\n",
      "Epoch [5/5], Step [3090/10336], Loss: 0.1139\n",
      "Epoch [5/5], Step [3092/10336], Loss: 0.0712\n",
      "Epoch [5/5], Step [3094/10336], Loss: 0.1901\n",
      "Epoch [5/5], Step [3096/10336], Loss: 0.1185\n",
      "Epoch [5/5], Step [3098/10336], Loss: 0.2597\n",
      "Epoch [5/5], Step [3100/10336], Loss: 1.9627\n",
      "Epoch [5/5], Step [3102/10336], Loss: 0.0572\n",
      "Epoch [5/5], Step [3104/10336], Loss: 0.0732\n",
      "Epoch [5/5], Step [3106/10336], Loss: 0.0199\n",
      "Epoch [5/5], Step [3108/10336], Loss: 0.6421\n",
      "Epoch [5/5], Step [3110/10336], Loss: 1.2234\n",
      "Epoch [5/5], Step [3112/10336], Loss: 0.3535\n",
      "Epoch [5/5], Step [3114/10336], Loss: 0.0161\n",
      "Epoch [5/5], Step [3116/10336], Loss: 0.9390\n",
      "Epoch [5/5], Step [3118/10336], Loss: 0.0570\n",
      "Epoch [5/5], Step [3120/10336], Loss: 0.6385\n",
      "Epoch [5/5], Step [3122/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [3124/10336], Loss: 0.1536\n",
      "Epoch [5/5], Step [3126/10336], Loss: 0.1607\n",
      "Epoch [5/5], Step [3128/10336], Loss: 0.0552\n",
      "Epoch [5/5], Step [3130/10336], Loss: 0.1633\n",
      "Epoch [5/5], Step [3132/10336], Loss: 0.3584\n",
      "Epoch [5/5], Step [3134/10336], Loss: 4.4030\n",
      "Epoch [5/5], Step [3136/10336], Loss: 0.0139\n",
      "Epoch [5/5], Step [3138/10336], Loss: 0.3731\n",
      "Epoch [5/5], Step [3140/10336], Loss: 2.3123\n",
      "Epoch [5/5], Step [3142/10336], Loss: 0.0146\n",
      "Epoch [5/5], Step [3144/10336], Loss: 0.0573\n",
      "Epoch [5/5], Step [3146/10336], Loss: 0.6382\n",
      "Epoch [5/5], Step [3148/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [3150/10336], Loss: 0.1997\n",
      "Epoch [5/5], Step [3152/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [3154/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [3156/10336], Loss: 0.1249\n",
      "Epoch [5/5], Step [3158/10336], Loss: 0.0167\n",
      "Epoch [5/5], Step [3160/10336], Loss: 2.0947\n",
      "Epoch [5/5], Step [3162/10336], Loss: 0.4038\n",
      "Epoch [5/5], Step [3164/10336], Loss: 0.6265\n",
      "Epoch [5/5], Step [3166/10336], Loss: 0.0205\n",
      "Epoch [5/5], Step [3168/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [3170/10336], Loss: 0.3766\n",
      "Epoch [5/5], Step [3172/10336], Loss: 3.6923\n",
      "Epoch [5/5], Step [3174/10336], Loss: 0.2229\n",
      "Epoch [5/5], Step [3176/10336], Loss: 0.0131\n",
      "Epoch [5/5], Step [3178/10336], Loss: 0.0826\n",
      "Epoch [5/5], Step [3180/10336], Loss: 0.8862\n",
      "Epoch [5/5], Step [3182/10336], Loss: 0.1193\n",
      "Epoch [5/5], Step [3184/10336], Loss: 0.7536\n",
      "Epoch [5/5], Step [3186/10336], Loss: 0.2158\n",
      "Epoch [5/5], Step [3188/10336], Loss: 0.0192\n",
      "Epoch [5/5], Step [3190/10336], Loss: 0.0888\n",
      "Epoch [5/5], Step [3192/10336], Loss: 1.9792\n",
      "Epoch [5/5], Step [3194/10336], Loss: 0.1249\n",
      "Epoch [5/5], Step [3196/10336], Loss: 0.3007\n",
      "Epoch [5/5], Step [3198/10336], Loss: 0.2878\n",
      "Epoch [5/5], Step [3200/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [3202/10336], Loss: 0.3029\n",
      "Epoch [5/5], Step [3204/10336], Loss: 0.1884\n",
      "Epoch [5/5], Step [3206/10336], Loss: 0.0087\n",
      "Epoch [5/5], Step [3208/10336], Loss: 0.1480\n",
      "Epoch [5/5], Step [3210/10336], Loss: 2.1665\n",
      "Epoch [5/5], Step [3212/10336], Loss: 0.0775\n",
      "Epoch [5/5], Step [3214/10336], Loss: 0.0118\n",
      "Epoch [5/5], Step [3216/10336], Loss: 0.0078\n",
      "Epoch [5/5], Step [3218/10336], Loss: 0.0110\n",
      "Epoch [5/5], Step [3220/10336], Loss: 0.4562\n",
      "Epoch [5/5], Step [3222/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [3224/10336], Loss: 3.3032\n",
      "Epoch [5/5], Step [3226/10336], Loss: 0.0424\n",
      "Epoch [5/5], Step [3228/10336], Loss: 0.0071\n",
      "Epoch [5/5], Step [3230/10336], Loss: 0.3335\n",
      "Epoch [5/5], Step [3232/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [3234/10336], Loss: 0.5926\n",
      "Epoch [5/5], Step [3236/10336], Loss: 0.8024\n",
      "Epoch [5/5], Step [3238/10336], Loss: 0.0879\n",
      "Epoch [5/5], Step [3240/10336], Loss: 0.0229\n",
      "Epoch [5/5], Step [3242/10336], Loss: 0.5527\n",
      "Epoch [5/5], Step [3244/10336], Loss: 0.7937\n",
      "Epoch [5/5], Step [3246/10336], Loss: 0.2193\n",
      "Epoch [5/5], Step [3248/10336], Loss: 2.1044\n",
      "Epoch [5/5], Step [3250/10336], Loss: 2.7855\n",
      "Epoch [5/5], Step [3252/10336], Loss: 0.0100\n",
      "Epoch [5/5], Step [3254/10336], Loss: 0.1547\n",
      "Epoch [5/5], Step [3256/10336], Loss: 0.8014\n",
      "Epoch [5/5], Step [3258/10336], Loss: 0.0034\n",
      "Epoch [5/5], Step [3260/10336], Loss: 0.1874\n",
      "Epoch [5/5], Step [3262/10336], Loss: 0.2374\n",
      "Epoch [5/5], Step [3264/10336], Loss: 0.5063\n",
      "Epoch [5/5], Step [3266/10336], Loss: 0.3590\n",
      "Epoch [5/5], Step [3268/10336], Loss: 0.4365\n",
      "Epoch [5/5], Step [3270/10336], Loss: 0.0104\n",
      "Epoch [5/5], Step [3272/10336], Loss: 0.7000\n",
      "Epoch [5/5], Step [3274/10336], Loss: 2.7959\n",
      "Epoch [5/5], Step [3276/10336], Loss: 1.7343\n",
      "Epoch [5/5], Step [3278/10336], Loss: 0.0347\n",
      "Epoch [5/5], Step [3280/10336], Loss: 0.1173\n",
      "Epoch [5/5], Step [3282/10336], Loss: 0.0194\n",
      "Epoch [5/5], Step [3284/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [3286/10336], Loss: 0.0621\n",
      "Epoch [5/5], Step [3288/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [3290/10336], Loss: 1.1425\n",
      "Epoch [5/5], Step [3292/10336], Loss: 0.4344\n",
      "Epoch [5/5], Step [3294/10336], Loss: 0.3462\n",
      "Epoch [5/5], Step [3296/10336], Loss: 1.0033\n",
      "Epoch [5/5], Step [3298/10336], Loss: 0.2111\n",
      "Epoch [5/5], Step [3300/10336], Loss: 0.0869\n",
      "Epoch [5/5], Step [3302/10336], Loss: 0.4219\n",
      "Epoch [5/5], Step [3304/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [3306/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [3308/10336], Loss: 0.0164\n",
      "Epoch [5/5], Step [3310/10336], Loss: 0.2678\n",
      "Epoch [5/5], Step [3312/10336], Loss: 1.7125\n",
      "Epoch [5/5], Step [3314/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [3316/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [3318/10336], Loss: 1.0706\n",
      "Epoch [5/5], Step [3320/10336], Loss: 0.1791\n",
      "Epoch [5/5], Step [3322/10336], Loss: 0.0194\n",
      "Epoch [5/5], Step [3324/10336], Loss: 0.6069\n",
      "Epoch [5/5], Step [3326/10336], Loss: 0.0781\n",
      "Epoch [5/5], Step [3328/10336], Loss: 0.0079\n",
      "Epoch [5/5], Step [3330/10336], Loss: 0.2476\n",
      "Epoch [5/5], Step [3332/10336], Loss: 0.9362\n",
      "Epoch [5/5], Step [3334/10336], Loss: 0.6180\n",
      "Epoch [5/5], Step [3336/10336], Loss: 0.0524\n",
      "Epoch [5/5], Step [3338/10336], Loss: 0.1004\n",
      "Epoch [5/5], Step [3340/10336], Loss: 0.8442\n",
      "Epoch [5/5], Step [3342/10336], Loss: 0.0076\n",
      "Epoch [5/5], Step [3344/10336], Loss: 0.1254\n",
      "Epoch [5/5], Step [3346/10336], Loss: 0.0579\n",
      "Epoch [5/5], Step [3348/10336], Loss: 0.0448\n",
      "Epoch [5/5], Step [3350/10336], Loss: 0.0180\n",
      "Epoch [5/5], Step [3352/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [3354/10336], Loss: 0.0379\n",
      "Epoch [5/5], Step [3356/10336], Loss: 0.9268\n",
      "Epoch [5/5], Step [3358/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [3360/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [3362/10336], Loss: 0.6112\n",
      "Epoch [5/5], Step [3364/10336], Loss: 0.9462\n",
      "Epoch [5/5], Step [3366/10336], Loss: 0.6686\n",
      "Epoch [5/5], Step [3368/10336], Loss: 3.4081\n",
      "Epoch [5/5], Step [3370/10336], Loss: 0.1801\n",
      "Epoch [5/5], Step [3372/10336], Loss: 0.7120\n",
      "Epoch [5/5], Step [3374/10336], Loss: 0.4594\n",
      "Epoch [5/5], Step [3376/10336], Loss: 2.3053\n",
      "Epoch [5/5], Step [3378/10336], Loss: 0.6741\n",
      "Epoch [5/5], Step [3380/10336], Loss: 1.9625\n",
      "Epoch [5/5], Step [3382/10336], Loss: 0.3262\n",
      "Epoch [5/5], Step [3384/10336], Loss: 0.8007\n",
      "Epoch [5/5], Step [3386/10336], Loss: 0.7846\n",
      "Epoch [5/5], Step [3388/10336], Loss: 0.2100\n",
      "Epoch [5/5], Step [3390/10336], Loss: 0.2038\n",
      "Epoch [5/5], Step [3392/10336], Loss: 2.3661\n",
      "Epoch [5/5], Step [3394/10336], Loss: 0.2560\n",
      "Epoch [5/5], Step [3396/10336], Loss: 0.5625\n",
      "Epoch [5/5], Step [3398/10336], Loss: 0.0341\n",
      "Epoch [5/5], Step [3400/10336], Loss: 0.0842\n",
      "Epoch [5/5], Step [3402/10336], Loss: 0.5964\n",
      "Epoch [5/5], Step [3404/10336], Loss: 0.0293\n",
      "Epoch [5/5], Step [3406/10336], Loss: 0.0583\n",
      "Epoch [5/5], Step [3408/10336], Loss: 0.5099\n",
      "Epoch [5/5], Step [3410/10336], Loss: 0.2507\n",
      "Epoch [5/5], Step [3412/10336], Loss: 0.0727\n",
      "Epoch [5/5], Step [3414/10336], Loss: 0.1103\n",
      "Epoch [5/5], Step [3416/10336], Loss: 0.2095\n",
      "Epoch [5/5], Step [3418/10336], Loss: 1.7852\n",
      "Epoch [5/5], Step [3420/10336], Loss: 0.7924\n",
      "Epoch [5/5], Step [3422/10336], Loss: 2.0107\n",
      "Epoch [5/5], Step [3424/10336], Loss: 0.2127\n",
      "Epoch [5/5], Step [3426/10336], Loss: 0.7747\n",
      "Epoch [5/5], Step [3428/10336], Loss: 0.0355\n",
      "Epoch [5/5], Step [3430/10336], Loss: 0.2816\n",
      "Epoch [5/5], Step [3432/10336], Loss: 0.4416\n",
      "Epoch [5/5], Step [3434/10336], Loss: 0.1078\n",
      "Epoch [5/5], Step [3436/10336], Loss: 0.1275\n",
      "Epoch [5/5], Step [3438/10336], Loss: 0.0609\n",
      "Epoch [5/5], Step [3440/10336], Loss: 0.6608\n",
      "Epoch [5/5], Step [3442/10336], Loss: 3.0588\n",
      "Epoch [5/5], Step [3444/10336], Loss: 0.0588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [3446/10336], Loss: 0.1977\n",
      "Epoch [5/5], Step [3448/10336], Loss: 1.0569\n",
      "Epoch [5/5], Step [3450/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [3452/10336], Loss: 0.0163\n",
      "Epoch [5/5], Step [3454/10336], Loss: 0.9757\n",
      "Epoch [5/5], Step [3456/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [3458/10336], Loss: 0.1925\n",
      "Epoch [5/5], Step [3460/10336], Loss: 0.0143\n",
      "Epoch [5/5], Step [3462/10336], Loss: 0.1160\n",
      "Epoch [5/5], Step [3464/10336], Loss: 0.6866\n",
      "Epoch [5/5], Step [3466/10336], Loss: 0.1294\n",
      "Epoch [5/5], Step [3468/10336], Loss: 0.4655\n",
      "Epoch [5/5], Step [3470/10336], Loss: 1.8837\n",
      "Epoch [5/5], Step [3472/10336], Loss: 0.1236\n",
      "Epoch [5/5], Step [3474/10336], Loss: 0.6565\n",
      "Epoch [5/5], Step [3476/10336], Loss: 0.1360\n",
      "Epoch [5/5], Step [3478/10336], Loss: 0.0625\n",
      "Epoch [5/5], Step [3480/10336], Loss: 0.9627\n",
      "Epoch [5/5], Step [3482/10336], Loss: 1.0839\n",
      "Epoch [5/5], Step [3484/10336], Loss: 0.5318\n",
      "Epoch [5/5], Step [3486/10336], Loss: 0.0841\n",
      "Epoch [5/5], Step [3488/10336], Loss: 0.6427\n",
      "Epoch [5/5], Step [3490/10336], Loss: 0.3115\n",
      "Epoch [5/5], Step [3492/10336], Loss: 0.0277\n",
      "Epoch [5/5], Step [3494/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [3496/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [3498/10336], Loss: 1.5618\n",
      "Epoch [5/5], Step [3500/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [3502/10336], Loss: 0.0166\n",
      "Epoch [5/5], Step [3504/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [3506/10336], Loss: 0.0362\n",
      "Epoch [5/5], Step [3508/10336], Loss: 0.6803\n",
      "Epoch [5/5], Step [3510/10336], Loss: 0.0803\n",
      "Epoch [5/5], Step [3512/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [3514/10336], Loss: 3.7925\n",
      "Epoch [5/5], Step [3516/10336], Loss: 0.0083\n",
      "Epoch [5/5], Step [3518/10336], Loss: 1.7241\n",
      "Epoch [5/5], Step [3520/10336], Loss: 0.6427\n",
      "Epoch [5/5], Step [3522/10336], Loss: 0.0693\n",
      "Epoch [5/5], Step [3524/10336], Loss: 0.0203\n",
      "Epoch [5/5], Step [3526/10336], Loss: 0.2836\n",
      "Epoch [5/5], Step [3528/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [3530/10336], Loss: 0.0205\n",
      "Epoch [5/5], Step [3532/10336], Loss: 0.0712\n",
      "Epoch [5/5], Step [3534/10336], Loss: 5.6034\n",
      "Epoch [5/5], Step [3536/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [3538/10336], Loss: 0.0985\n",
      "Epoch [5/5], Step [3540/10336], Loss: 0.0209\n",
      "Epoch [5/5], Step [3542/10336], Loss: 0.0502\n",
      "Epoch [5/5], Step [3544/10336], Loss: 0.0212\n",
      "Epoch [5/5], Step [3546/10336], Loss: 0.3851\n",
      "Epoch [5/5], Step [3548/10336], Loss: 0.3076\n",
      "Epoch [5/5], Step [3550/10336], Loss: 0.0243\n",
      "Epoch [5/5], Step [3552/10336], Loss: 3.4191\n",
      "Epoch [5/5], Step [3554/10336], Loss: 0.9813\n",
      "Epoch [5/5], Step [3556/10336], Loss: 0.0071\n",
      "Epoch [5/5], Step [3558/10336], Loss: 0.3179\n",
      "Epoch [5/5], Step [3560/10336], Loss: 0.8139\n",
      "Epoch [5/5], Step [3562/10336], Loss: 2.8095\n",
      "Epoch [5/5], Step [3564/10336], Loss: 0.0294\n",
      "Epoch [5/5], Step [3566/10336], Loss: 0.1620\n",
      "Epoch [5/5], Step [3568/10336], Loss: 0.0173\n",
      "Epoch [5/5], Step [3570/10336], Loss: 0.9629\n",
      "Epoch [5/5], Step [3572/10336], Loss: 0.0325\n",
      "Epoch [5/5], Step [3574/10336], Loss: 0.1169\n",
      "Epoch [5/5], Step [3576/10336], Loss: 1.7751\n",
      "Epoch [5/5], Step [3578/10336], Loss: 0.7643\n",
      "Epoch [5/5], Step [3580/10336], Loss: 0.0273\n",
      "Epoch [5/5], Step [3582/10336], Loss: 0.1708\n",
      "Epoch [5/5], Step [3584/10336], Loss: 1.8839\n",
      "Epoch [5/5], Step [3586/10336], Loss: 2.2725\n",
      "Epoch [5/5], Step [3588/10336], Loss: 0.1236\n",
      "Epoch [5/5], Step [3590/10336], Loss: 0.2503\n",
      "Epoch [5/5], Step [3592/10336], Loss: 1.8372\n",
      "Epoch [5/5], Step [3594/10336], Loss: 0.0595\n",
      "Epoch [5/5], Step [3596/10336], Loss: 0.5425\n",
      "Epoch [5/5], Step [3598/10336], Loss: 2.7552\n",
      "Epoch [5/5], Step [3600/10336], Loss: 0.4830\n",
      "Epoch [5/5], Step [3602/10336], Loss: 0.6253\n",
      "Epoch [5/5], Step [3604/10336], Loss: 0.8380\n",
      "Epoch [5/5], Step [3606/10336], Loss: 0.0076\n",
      "Epoch [5/5], Step [3608/10336], Loss: 1.6293\n",
      "Epoch [5/5], Step [3610/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [3612/10336], Loss: 0.0428\n",
      "Epoch [5/5], Step [3614/10336], Loss: 1.6640\n",
      "Epoch [5/5], Step [3616/10336], Loss: 1.4752\n",
      "Epoch [5/5], Step [3618/10336], Loss: 0.6988\n",
      "Epoch [5/5], Step [3620/10336], Loss: 0.6298\n",
      "Epoch [5/5], Step [3622/10336], Loss: 0.1580\n",
      "Epoch [5/5], Step [3624/10336], Loss: 0.1258\n",
      "Epoch [5/5], Step [3626/10336], Loss: 0.1185\n",
      "Epoch [5/5], Step [3628/10336], Loss: 0.6259\n",
      "Epoch [5/5], Step [3630/10336], Loss: 0.1116\n",
      "Epoch [5/5], Step [3632/10336], Loss: 0.0540\n",
      "Epoch [5/5], Step [3634/10336], Loss: 0.1272\n",
      "Epoch [5/5], Step [3636/10336], Loss: 0.3330\n",
      "Epoch [5/5], Step [3638/10336], Loss: 0.3685\n",
      "Epoch [5/5], Step [3640/10336], Loss: 0.0071\n",
      "Epoch [5/5], Step [3642/10336], Loss: 0.2285\n",
      "Epoch [5/5], Step [3644/10336], Loss: 0.6567\n",
      "Epoch [5/5], Step [3646/10336], Loss: 0.0708\n",
      "Epoch [5/5], Step [3648/10336], Loss: 1.3224\n",
      "Epoch [5/5], Step [3650/10336], Loss: 0.1626\n",
      "Epoch [5/5], Step [3652/10336], Loss: 0.0770\n",
      "Epoch [5/5], Step [3654/10336], Loss: 1.6333\n",
      "Epoch [5/5], Step [3656/10336], Loss: 0.2119\n",
      "Epoch [5/5], Step [3658/10336], Loss: 0.0801\n",
      "Epoch [5/5], Step [3660/10336], Loss: 0.0557\n",
      "Epoch [5/5], Step [3662/10336], Loss: 0.0894\n",
      "Epoch [5/5], Step [3664/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [3666/10336], Loss: 0.4062\n",
      "Epoch [5/5], Step [3668/10336], Loss: 0.9249\n",
      "Epoch [5/5], Step [3670/10336], Loss: 0.1312\n",
      "Epoch [5/5], Step [3672/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [3674/10336], Loss: 0.7841\n",
      "Epoch [5/5], Step [3676/10336], Loss: 0.4715\n",
      "Epoch [5/5], Step [3678/10336], Loss: 0.5796\n",
      "Epoch [5/5], Step [3680/10336], Loss: 1.2660\n",
      "Epoch [5/5], Step [3682/10336], Loss: 1.3692\n",
      "Epoch [5/5], Step [3684/10336], Loss: 0.1627\n",
      "Epoch [5/5], Step [3686/10336], Loss: 1.6010\n",
      "Epoch [5/5], Step [3688/10336], Loss: 0.1576\n",
      "Epoch [5/5], Step [3690/10336], Loss: 0.1405\n",
      "Epoch [5/5], Step [3692/10336], Loss: 1.9387\n",
      "Epoch [5/5], Step [3694/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [3696/10336], Loss: 0.1948\n",
      "Epoch [5/5], Step [3698/10336], Loss: 0.1595\n",
      "Epoch [5/5], Step [3700/10336], Loss: 0.0440\n",
      "Epoch [5/5], Step [3702/10336], Loss: 0.2830\n",
      "Epoch [5/5], Step [3704/10336], Loss: 3.6991\n",
      "Epoch [5/5], Step [3706/10336], Loss: 0.1775\n",
      "Epoch [5/5], Step [3708/10336], Loss: 0.0979\n",
      "Epoch [5/5], Step [3710/10336], Loss: 1.1266\n",
      "Epoch [5/5], Step [3712/10336], Loss: 0.9433\n",
      "Epoch [5/5], Step [3714/10336], Loss: 0.3392\n",
      "Epoch [5/5], Step [3716/10336], Loss: 0.0126\n",
      "Epoch [5/5], Step [3718/10336], Loss: 0.6324\n",
      "Epoch [5/5], Step [3720/10336], Loss: 0.8948\n",
      "Epoch [5/5], Step [3722/10336], Loss: 0.3671\n",
      "Epoch [5/5], Step [3724/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [3726/10336], Loss: 0.3108\n",
      "Epoch [5/5], Step [3728/10336], Loss: 0.1877\n",
      "Epoch [5/5], Step [3730/10336], Loss: 0.2431\n",
      "Epoch [5/5], Step [3732/10336], Loss: 0.4569\n",
      "Epoch [5/5], Step [3734/10336], Loss: 1.0131\n",
      "Epoch [5/5], Step [3736/10336], Loss: 0.0301\n",
      "Epoch [5/5], Step [3738/10336], Loss: 4.0982\n",
      "Epoch [5/5], Step [3740/10336], Loss: 0.4852\n",
      "Epoch [5/5], Step [3742/10336], Loss: 0.2208\n",
      "Epoch [5/5], Step [3744/10336], Loss: 0.2305\n",
      "Epoch [5/5], Step [3746/10336], Loss: 2.5346\n",
      "Epoch [5/5], Step [3748/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [3750/10336], Loss: 0.2646\n",
      "Epoch [5/5], Step [3752/10336], Loss: 0.3329\n",
      "Epoch [5/5], Step [3754/10336], Loss: 0.2075\n",
      "Epoch [5/5], Step [3756/10336], Loss: 0.2775\n",
      "Epoch [5/5], Step [3758/10336], Loss: 0.0338\n",
      "Epoch [5/5], Step [3760/10336], Loss: 0.3336\n",
      "Epoch [5/5], Step [3762/10336], Loss: 0.0444\n",
      "Epoch [5/5], Step [3764/10336], Loss: 0.0780\n",
      "Epoch [5/5], Step [3766/10336], Loss: 0.0723\n",
      "Epoch [5/5], Step [3768/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [3770/10336], Loss: 2.4675\n",
      "Epoch [5/5], Step [3772/10336], Loss: 0.3784\n",
      "Epoch [5/5], Step [3774/10336], Loss: 0.0102\n",
      "Epoch [5/5], Step [3776/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [3778/10336], Loss: 3.3052\n",
      "Epoch [5/5], Step [3780/10336], Loss: 0.2278\n",
      "Epoch [5/5], Step [3782/10336], Loss: 0.0065\n",
      "Epoch [5/5], Step [3784/10336], Loss: 0.0224\n",
      "Epoch [5/5], Step [3786/10336], Loss: 0.2738\n",
      "Epoch [5/5], Step [3788/10336], Loss: 0.0995\n",
      "Epoch [5/5], Step [3790/10336], Loss: 0.0708\n",
      "Epoch [5/5], Step [3792/10336], Loss: 0.0100\n",
      "Epoch [5/5], Step [3794/10336], Loss: 0.3734\n",
      "Epoch [5/5], Step [3796/10336], Loss: 0.0224\n",
      "Epoch [5/5], Step [3798/10336], Loss: 2.1396\n",
      "Epoch [5/5], Step [3800/10336], Loss: 0.8933\n",
      "Epoch [5/5], Step [3802/10336], Loss: 0.6964\n",
      "Epoch [5/5], Step [3804/10336], Loss: 0.3648\n",
      "Epoch [5/5], Step [3806/10336], Loss: 1.0199\n",
      "Epoch [5/5], Step [3808/10336], Loss: 2.0296\n",
      "Epoch [5/5], Step [3810/10336], Loss: 1.1103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [3812/10336], Loss: 0.0882\n",
      "Epoch [5/5], Step [3814/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [3816/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [3818/10336], Loss: 1.5731\n",
      "Epoch [5/5], Step [3820/10336], Loss: 0.3616\n",
      "Epoch [5/5], Step [3822/10336], Loss: 1.4974\n",
      "Epoch [5/5], Step [3824/10336], Loss: 0.1958\n",
      "Epoch [5/5], Step [3826/10336], Loss: 1.8728\n",
      "Epoch [5/5], Step [3828/10336], Loss: 1.5835\n",
      "Epoch [5/5], Step [3830/10336], Loss: 0.1345\n",
      "Epoch [5/5], Step [3832/10336], Loss: 0.8811\n",
      "Epoch [5/5], Step [3834/10336], Loss: 0.0867\n",
      "Epoch [5/5], Step [3836/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [3838/10336], Loss: 3.2040\n",
      "Epoch [5/5], Step [3840/10336], Loss: 0.3226\n",
      "Epoch [5/5], Step [3842/10336], Loss: 0.0629\n",
      "Epoch [5/5], Step [3844/10336], Loss: 2.4399\n",
      "Epoch [5/5], Step [3846/10336], Loss: 0.2705\n",
      "Epoch [5/5], Step [3848/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [3850/10336], Loss: 0.5855\n",
      "Epoch [5/5], Step [3852/10336], Loss: 1.1731\n",
      "Epoch [5/5], Step [3854/10336], Loss: 1.8042\n",
      "Epoch [5/5], Step [3856/10336], Loss: 0.4067\n",
      "Epoch [5/5], Step [3858/10336], Loss: 1.6031\n",
      "Epoch [5/5], Step [3860/10336], Loss: 0.0065\n",
      "Epoch [5/5], Step [3862/10336], Loss: 0.1520\n",
      "Epoch [5/5], Step [3864/10336], Loss: 0.3927\n",
      "Epoch [5/5], Step [3866/10336], Loss: 0.7134\n",
      "Epoch [5/5], Step [3868/10336], Loss: 0.0273\n",
      "Epoch [5/5], Step [3870/10336], Loss: 2.0125\n",
      "Epoch [5/5], Step [3872/10336], Loss: 0.3132\n",
      "Epoch [5/5], Step [3874/10336], Loss: 0.0211\n",
      "Epoch [5/5], Step [3876/10336], Loss: 0.3408\n",
      "Epoch [5/5], Step [3878/10336], Loss: 0.5016\n",
      "Epoch [5/5], Step [3880/10336], Loss: 0.0053\n",
      "Epoch [5/5], Step [3882/10336], Loss: 0.0153\n",
      "Epoch [5/5], Step [3884/10336], Loss: 1.9391\n",
      "Epoch [5/5], Step [3886/10336], Loss: 0.3867\n",
      "Epoch [5/5], Step [3888/10336], Loss: 0.6816\n",
      "Epoch [5/5], Step [3890/10336], Loss: 0.1234\n",
      "Epoch [5/5], Step [3892/10336], Loss: 0.0863\n",
      "Epoch [5/5], Step [3894/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [3896/10336], Loss: 0.2022\n",
      "Epoch [5/5], Step [3898/10336], Loss: 0.0129\n",
      "Epoch [5/5], Step [3900/10336], Loss: 0.0699\n",
      "Epoch [5/5], Step [3902/10336], Loss: 1.4997\n",
      "Epoch [5/5], Step [3904/10336], Loss: 0.6457\n",
      "Epoch [5/5], Step [3906/10336], Loss: 0.9591\n",
      "Epoch [5/5], Step [3908/10336], Loss: 0.0102\n",
      "Epoch [5/5], Step [3910/10336], Loss: 1.3381\n",
      "Epoch [5/5], Step [3912/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [3914/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [3916/10336], Loss: 0.0429\n",
      "Epoch [5/5], Step [3918/10336], Loss: 0.1738\n",
      "Epoch [5/5], Step [3920/10336], Loss: 2.4267\n",
      "Epoch [5/5], Step [3922/10336], Loss: 0.4403\n",
      "Epoch [5/5], Step [3924/10336], Loss: 2.3839\n",
      "Epoch [5/5], Step [3926/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [3928/10336], Loss: 3.4668\n",
      "Epoch [5/5], Step [3930/10336], Loss: 2.8988\n",
      "Epoch [5/5], Step [3932/10336], Loss: 0.1995\n",
      "Epoch [5/5], Step [3934/10336], Loss: 0.0200\n",
      "Epoch [5/5], Step [3936/10336], Loss: 4.7141\n",
      "Epoch [5/5], Step [3938/10336], Loss: 0.0063\n",
      "Epoch [5/5], Step [3940/10336], Loss: 0.6732\n",
      "Epoch [5/5], Step [3942/10336], Loss: 0.5819\n",
      "Epoch [5/5], Step [3944/10336], Loss: 0.0080\n",
      "Epoch [5/5], Step [3946/10336], Loss: 0.7777\n",
      "Epoch [5/5], Step [3948/10336], Loss: 0.2297\n",
      "Epoch [5/5], Step [3950/10336], Loss: 2.6066\n",
      "Epoch [5/5], Step [3952/10336], Loss: 0.8807\n",
      "Epoch [5/5], Step [3954/10336], Loss: 1.4793\n",
      "Epoch [5/5], Step [3956/10336], Loss: 0.2276\n",
      "Epoch [5/5], Step [3958/10336], Loss: 0.5882\n",
      "Epoch [5/5], Step [3960/10336], Loss: 0.1267\n",
      "Epoch [5/5], Step [3962/10336], Loss: 0.2384\n",
      "Epoch [5/5], Step [3964/10336], Loss: 0.9219\n",
      "Epoch [5/5], Step [3966/10336], Loss: 0.1140\n",
      "Epoch [5/5], Step [3968/10336], Loss: 0.6338\n",
      "Epoch [5/5], Step [3970/10336], Loss: 0.4110\n",
      "Epoch [5/5], Step [3972/10336], Loss: 1.3928\n",
      "Epoch [5/5], Step [3974/10336], Loss: 0.1244\n",
      "Epoch [5/5], Step [3976/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [3978/10336], Loss: 0.2678\n",
      "Epoch [5/5], Step [3980/10336], Loss: 0.0416\n",
      "Epoch [5/5], Step [3982/10336], Loss: 0.6665\n",
      "Epoch [5/5], Step [3984/10336], Loss: 0.0598\n",
      "Epoch [5/5], Step [3986/10336], Loss: 0.0090\n",
      "Epoch [5/5], Step [3988/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [3990/10336], Loss: 1.4662\n",
      "Epoch [5/5], Step [3992/10336], Loss: 0.5980\n",
      "Epoch [5/5], Step [3994/10336], Loss: 1.9666\n",
      "Epoch [5/5], Step [3996/10336], Loss: 0.8177\n",
      "Epoch [5/5], Step [3998/10336], Loss: 6.3403\n",
      "Epoch [5/5], Step [4000/10336], Loss: 1.0040\n",
      "Epoch [5/5], Step [4002/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [4004/10336], Loss: 0.2884\n",
      "Epoch [5/5], Step [4006/10336], Loss: 0.1209\n",
      "Epoch [5/5], Step [4008/10336], Loss: 0.7786\n",
      "Epoch [5/5], Step [4010/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [4012/10336], Loss: 0.5637\n",
      "Epoch [5/5], Step [4014/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [4016/10336], Loss: 0.4267\n",
      "Epoch [5/5], Step [4018/10336], Loss: 0.4224\n",
      "Epoch [5/5], Step [4020/10336], Loss: 0.0389\n",
      "Epoch [5/5], Step [4022/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [4024/10336], Loss: 0.0978\n",
      "Epoch [5/5], Step [4026/10336], Loss: 1.4819\n",
      "Epoch [5/5], Step [4028/10336], Loss: 0.0619\n",
      "Epoch [5/5], Step [4030/10336], Loss: 0.2536\n",
      "Epoch [5/5], Step [4032/10336], Loss: 1.8798\n",
      "Epoch [5/5], Step [4034/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [4036/10336], Loss: 0.1781\n",
      "Epoch [5/5], Step [4038/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [4040/10336], Loss: 0.0078\n",
      "Epoch [5/5], Step [4042/10336], Loss: 0.7080\n",
      "Epoch [5/5], Step [4044/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [4046/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [4048/10336], Loss: 0.0769\n",
      "Epoch [5/5], Step [4050/10336], Loss: 0.0303\n",
      "Epoch [5/5], Step [4052/10336], Loss: 0.0183\n",
      "Epoch [5/5], Step [4054/10336], Loss: 0.3431\n",
      "Epoch [5/5], Step [4056/10336], Loss: 0.8871\n",
      "Epoch [5/5], Step [4058/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [4060/10336], Loss: 3.4906\n",
      "Epoch [5/5], Step [4062/10336], Loss: 0.0180\n",
      "Epoch [5/5], Step [4064/10336], Loss: 0.0075\n",
      "Epoch [5/5], Step [4066/10336], Loss: 0.4478\n",
      "Epoch [5/5], Step [4068/10336], Loss: 0.1359\n",
      "Epoch [5/5], Step [4070/10336], Loss: 1.7998\n",
      "Epoch [5/5], Step [4072/10336], Loss: 0.0140\n",
      "Epoch [5/5], Step [4074/10336], Loss: 0.7133\n",
      "Epoch [5/5], Step [4076/10336], Loss: 0.0782\n",
      "Epoch [5/5], Step [4078/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [4080/10336], Loss: 0.4409\n",
      "Epoch [5/5], Step [4082/10336], Loss: 0.2182\n",
      "Epoch [5/5], Step [4084/10336], Loss: 0.3551\n",
      "Epoch [5/5], Step [4086/10336], Loss: 0.2600\n",
      "Epoch [5/5], Step [4088/10336], Loss: 0.0094\n",
      "Epoch [5/5], Step [4090/10336], Loss: 0.0099\n",
      "Epoch [5/5], Step [4092/10336], Loss: 0.3019\n",
      "Epoch [5/5], Step [4094/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [4096/10336], Loss: 0.1569\n",
      "Epoch [5/5], Step [4098/10336], Loss: 0.2475\n",
      "Epoch [5/5], Step [4100/10336], Loss: 0.1998\n",
      "Epoch [5/5], Step [4102/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [4104/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [4106/10336], Loss: 0.0060\n",
      "Epoch [5/5], Step [4108/10336], Loss: 0.0425\n",
      "Epoch [5/5], Step [4110/10336], Loss: 0.1738\n",
      "Epoch [5/5], Step [4112/10336], Loss: 0.0502\n",
      "Epoch [5/5], Step [4114/10336], Loss: 0.1549\n",
      "Epoch [5/5], Step [4116/10336], Loss: 1.1633\n",
      "Epoch [5/5], Step [4118/10336], Loss: 0.6520\n",
      "Epoch [5/5], Step [4120/10336], Loss: 0.0581\n",
      "Epoch [5/5], Step [4122/10336], Loss: 1.9528\n",
      "Epoch [5/5], Step [4124/10336], Loss: 0.1326\n",
      "Epoch [5/5], Step [4126/10336], Loss: 0.0185\n",
      "Epoch [5/5], Step [4128/10336], Loss: 0.0233\n",
      "Epoch [5/5], Step [4130/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [4132/10336], Loss: 1.3431\n",
      "Epoch [5/5], Step [4134/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [4136/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [4138/10336], Loss: 1.4461\n",
      "Epoch [5/5], Step [4140/10336], Loss: 0.0107\n",
      "Epoch [5/5], Step [4142/10336], Loss: 0.2121\n",
      "Epoch [5/5], Step [4144/10336], Loss: 0.2779\n",
      "Epoch [5/5], Step [4146/10336], Loss: 4.2485\n",
      "Epoch [5/5], Step [4148/10336], Loss: 0.0241\n",
      "Epoch [5/5], Step [4150/10336], Loss: 0.2909\n",
      "Epoch [5/5], Step [4152/10336], Loss: 0.0791\n",
      "Epoch [5/5], Step [4154/10336], Loss: 1.0392\n",
      "Epoch [5/5], Step [4156/10336], Loss: 0.7612\n",
      "Epoch [5/5], Step [4158/10336], Loss: 1.9728\n",
      "Epoch [5/5], Step [4160/10336], Loss: 1.0818\n",
      "Epoch [5/5], Step [4162/10336], Loss: 0.0073\n",
      "Epoch [5/5], Step [4164/10336], Loss: 0.0924\n",
      "Epoch [5/5], Step [4166/10336], Loss: 0.1623\n",
      "Epoch [5/5], Step [4168/10336], Loss: 0.0613\n",
      "Epoch [5/5], Step [4170/10336], Loss: 0.0583\n",
      "Epoch [5/5], Step [4172/10336], Loss: 0.3874\n",
      "Epoch [5/5], Step [4174/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [4176/10336], Loss: 0.0782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [4178/10336], Loss: 0.0766\n",
      "Epoch [5/5], Step [4180/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [4182/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [4184/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [4186/10336], Loss: 1.5151\n",
      "Epoch [5/5], Step [4188/10336], Loss: 0.0393\n",
      "Epoch [5/5], Step [4190/10336], Loss: 0.0707\n",
      "Epoch [5/5], Step [4192/10336], Loss: 0.0812\n",
      "Epoch [5/5], Step [4194/10336], Loss: 0.0661\n",
      "Epoch [5/5], Step [4196/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [4198/10336], Loss: 0.0359\n",
      "Epoch [5/5], Step [4200/10336], Loss: 0.5813\n",
      "Epoch [5/5], Step [4202/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [4204/10336], Loss: 0.8290\n",
      "Epoch [5/5], Step [4206/10336], Loss: 0.0541\n",
      "Epoch [5/5], Step [4208/10336], Loss: 0.5274\n",
      "Epoch [5/5], Step [4210/10336], Loss: 0.0888\n",
      "Epoch [5/5], Step [4212/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [4214/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [4216/10336], Loss: 0.1350\n",
      "Epoch [5/5], Step [4218/10336], Loss: 0.1108\n",
      "Epoch [5/5], Step [4220/10336], Loss: 0.8771\n",
      "Epoch [5/5], Step [4222/10336], Loss: 0.2097\n",
      "Epoch [5/5], Step [4224/10336], Loss: 0.6662\n",
      "Epoch [5/5], Step [4226/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [4228/10336], Loss: 0.0826\n",
      "Epoch [5/5], Step [4230/10336], Loss: 1.4947\n",
      "Epoch [5/5], Step [4232/10336], Loss: 2.2231\n",
      "Epoch [5/5], Step [4234/10336], Loss: 0.9068\n",
      "Epoch [5/5], Step [4236/10336], Loss: 1.2413\n",
      "Epoch [5/5], Step [4238/10336], Loss: 0.1666\n",
      "Epoch [5/5], Step [4240/10336], Loss: 1.7662\n",
      "Epoch [5/5], Step [4242/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [4244/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [4246/10336], Loss: 0.8515\n",
      "Epoch [5/5], Step [4248/10336], Loss: 0.0504\n",
      "Epoch [5/5], Step [4250/10336], Loss: 4.6410\n",
      "Epoch [5/5], Step [4252/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [4254/10336], Loss: 0.7208\n",
      "Epoch [5/5], Step [4256/10336], Loss: 2.7561\n",
      "Epoch [5/5], Step [4258/10336], Loss: 0.0425\n",
      "Epoch [5/5], Step [4260/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [4262/10336], Loss: 0.0129\n",
      "Epoch [5/5], Step [4264/10336], Loss: 0.1151\n",
      "Epoch [5/5], Step [4266/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [4268/10336], Loss: 2.4539\n",
      "Epoch [5/5], Step [4270/10336], Loss: 0.1040\n",
      "Epoch [5/5], Step [4272/10336], Loss: 1.2521\n",
      "Epoch [5/5], Step [4274/10336], Loss: 2.2244\n",
      "Epoch [5/5], Step [4276/10336], Loss: 0.2363\n",
      "Epoch [5/5], Step [4278/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [4280/10336], Loss: 0.3919\n",
      "Epoch [5/5], Step [4282/10336], Loss: 0.0679\n",
      "Epoch [5/5], Step [4284/10336], Loss: 0.0351\n",
      "Epoch [5/5], Step [4286/10336], Loss: 0.0233\n",
      "Epoch [5/5], Step [4288/10336], Loss: 1.6193\n",
      "Epoch [5/5], Step [4290/10336], Loss: 0.0245\n",
      "Epoch [5/5], Step [4292/10336], Loss: 0.6335\n",
      "Epoch [5/5], Step [4294/10336], Loss: 0.0186\n",
      "Epoch [5/5], Step [4296/10336], Loss: 0.4861\n",
      "Epoch [5/5], Step [4298/10336], Loss: 0.1265\n",
      "Epoch [5/5], Step [4300/10336], Loss: 0.1258\n",
      "Epoch [5/5], Step [4302/10336], Loss: 2.4547\n",
      "Epoch [5/5], Step [4304/10336], Loss: 0.1516\n",
      "Epoch [5/5], Step [4306/10336], Loss: 2.8536\n",
      "Epoch [5/5], Step [4308/10336], Loss: 2.5436\n",
      "Epoch [5/5], Step [4310/10336], Loss: 0.5469\n",
      "Epoch [5/5], Step [4312/10336], Loss: 2.6364\n",
      "Epoch [5/5], Step [4314/10336], Loss: 0.3646\n",
      "Epoch [5/5], Step [4316/10336], Loss: 0.0056\n",
      "Epoch [5/5], Step [4318/10336], Loss: 0.0429\n",
      "Epoch [5/5], Step [4320/10336], Loss: 0.7593\n",
      "Epoch [5/5], Step [4322/10336], Loss: 0.8169\n",
      "Epoch [5/5], Step [4324/10336], Loss: 0.4168\n",
      "Epoch [5/5], Step [4326/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [4328/10336], Loss: 0.2134\n",
      "Epoch [5/5], Step [4330/10336], Loss: 0.1243\n",
      "Epoch [5/5], Step [4332/10336], Loss: 0.9885\n",
      "Epoch [5/5], Step [4334/10336], Loss: 0.1754\n",
      "Epoch [5/5], Step [4336/10336], Loss: 1.5753\n",
      "Epoch [5/5], Step [4338/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [4340/10336], Loss: 0.0508\n",
      "Epoch [5/5], Step [4342/10336], Loss: 1.8869\n",
      "Epoch [5/5], Step [4344/10336], Loss: 0.2684\n",
      "Epoch [5/5], Step [4346/10336], Loss: 3.3904\n",
      "Epoch [5/5], Step [4348/10336], Loss: 0.3223\n",
      "Epoch [5/5], Step [4350/10336], Loss: 0.0101\n",
      "Epoch [5/5], Step [4352/10336], Loss: 0.2003\n",
      "Epoch [5/5], Step [4354/10336], Loss: 0.3973\n",
      "Epoch [5/5], Step [4356/10336], Loss: 0.2501\n",
      "Epoch [5/5], Step [4358/10336], Loss: 0.6602\n",
      "Epoch [5/5], Step [4360/10336], Loss: 0.1360\n",
      "Epoch [5/5], Step [4362/10336], Loss: 0.9897\n",
      "Epoch [5/5], Step [4364/10336], Loss: 2.4939\n",
      "Epoch [5/5], Step [4366/10336], Loss: 2.1000\n",
      "Epoch [5/5], Step [4368/10336], Loss: 0.2573\n",
      "Epoch [5/5], Step [4370/10336], Loss: 2.2983\n",
      "Epoch [5/5], Step [4372/10336], Loss: 0.0327\n",
      "Epoch [5/5], Step [4374/10336], Loss: 2.5360\n",
      "Epoch [5/5], Step [4376/10336], Loss: 0.4629\n",
      "Epoch [5/5], Step [4378/10336], Loss: 0.0172\n",
      "Epoch [5/5], Step [4380/10336], Loss: 0.2418\n",
      "Epoch [5/5], Step [4382/10336], Loss: 0.1332\n",
      "Epoch [5/5], Step [4384/10336], Loss: 1.8758\n",
      "Epoch [5/5], Step [4386/10336], Loss: 0.3988\n",
      "Epoch [5/5], Step [4388/10336], Loss: 0.0249\n",
      "Epoch [5/5], Step [4390/10336], Loss: 0.8861\n",
      "Epoch [5/5], Step [4392/10336], Loss: 0.1001\n",
      "Epoch [5/5], Step [4394/10336], Loss: 0.1949\n",
      "Epoch [5/5], Step [4396/10336], Loss: 0.2819\n",
      "Epoch [5/5], Step [4398/10336], Loss: 0.1466\n",
      "Epoch [5/5], Step [4400/10336], Loss: 0.7913\n",
      "Epoch [5/5], Step [4402/10336], Loss: 0.6335\n",
      "Epoch [5/5], Step [4404/10336], Loss: 1.5545\n",
      "Epoch [5/5], Step [4406/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [4408/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [4410/10336], Loss: 0.0173\n",
      "Epoch [5/5], Step [4412/10336], Loss: 0.2830\n",
      "Epoch [5/5], Step [4414/10336], Loss: 0.3075\n",
      "Epoch [5/5], Step [4416/10336], Loss: 0.0415\n",
      "Epoch [5/5], Step [4418/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [4420/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [4422/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [4424/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [4426/10336], Loss: 0.0845\n",
      "Epoch [5/5], Step [4428/10336], Loss: 0.2558\n",
      "Epoch [5/5], Step [4430/10336], Loss: 0.0624\n",
      "Epoch [5/5], Step [4432/10336], Loss: 1.9562\n",
      "Epoch [5/5], Step [4434/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [4436/10336], Loss: 0.8931\n",
      "Epoch [5/5], Step [4438/10336], Loss: 0.0242\n",
      "Epoch [5/5], Step [4440/10336], Loss: 0.5358\n",
      "Epoch [5/5], Step [4442/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [4444/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [4446/10336], Loss: 0.4613\n",
      "Epoch [5/5], Step [4448/10336], Loss: 0.0079\n",
      "Epoch [5/5], Step [4450/10336], Loss: 1.6101\n",
      "Epoch [5/5], Step [4452/10336], Loss: 1.5312\n",
      "Epoch [5/5], Step [4454/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [4456/10336], Loss: 0.1382\n",
      "Epoch [5/5], Step [4458/10336], Loss: 0.3155\n",
      "Epoch [5/5], Step [4460/10336], Loss: 0.2077\n",
      "Epoch [5/5], Step [4462/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [4464/10336], Loss: 0.2093\n",
      "Epoch [5/5], Step [4466/10336], Loss: 0.2169\n",
      "Epoch [5/5], Step [4468/10336], Loss: 0.0903\n",
      "Epoch [5/5], Step [4470/10336], Loss: 0.1737\n",
      "Epoch [5/5], Step [4472/10336], Loss: 0.0602\n",
      "Epoch [5/5], Step [4474/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [4476/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [4478/10336], Loss: 2.1931\n",
      "Epoch [5/5], Step [4480/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [4482/10336], Loss: 0.0830\n",
      "Epoch [5/5], Step [4484/10336], Loss: 0.1367\n",
      "Epoch [5/5], Step [4486/10336], Loss: 0.2492\n",
      "Epoch [5/5], Step [4488/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [4490/10336], Loss: 0.1423\n",
      "Epoch [5/5], Step [4492/10336], Loss: 0.0731\n",
      "Epoch [5/5], Step [4494/10336], Loss: 0.0482\n",
      "Epoch [5/5], Step [4496/10336], Loss: 0.1808\n",
      "Epoch [5/5], Step [4498/10336], Loss: 0.0274\n",
      "Epoch [5/5], Step [4500/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [4502/10336], Loss: 0.2891\n",
      "Epoch [5/5], Step [4504/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [4506/10336], Loss: 0.0361\n",
      "Epoch [5/5], Step [4508/10336], Loss: 0.0341\n",
      "Epoch [5/5], Step [4510/10336], Loss: 0.2131\n",
      "Epoch [5/5], Step [4512/10336], Loss: 2.4399\n",
      "Epoch [5/5], Step [4514/10336], Loss: 1.7604\n",
      "Epoch [5/5], Step [4516/10336], Loss: 0.0241\n",
      "Epoch [5/5], Step [4518/10336], Loss: 0.0121\n",
      "Epoch [5/5], Step [4520/10336], Loss: 0.3810\n",
      "Epoch [5/5], Step [4522/10336], Loss: 0.5566\n",
      "Epoch [5/5], Step [4524/10336], Loss: 0.1306\n",
      "Epoch [5/5], Step [4526/10336], Loss: 0.6073\n",
      "Epoch [5/5], Step [4528/10336], Loss: 0.1696\n",
      "Epoch [5/5], Step [4530/10336], Loss: 0.0472\n",
      "Epoch [5/5], Step [4532/10336], Loss: 0.0477\n",
      "Epoch [5/5], Step [4534/10336], Loss: 0.3530\n",
      "Epoch [5/5], Step [4536/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [4538/10336], Loss: 0.0241\n",
      "Epoch [5/5], Step [4540/10336], Loss: 0.2117\n",
      "Epoch [5/5], Step [4542/10336], Loss: 0.3888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [4544/10336], Loss: 0.5917\n",
      "Epoch [5/5], Step [4546/10336], Loss: 0.0685\n",
      "Epoch [5/5], Step [4548/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [4550/10336], Loss: 1.8969\n",
      "Epoch [5/5], Step [4552/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [4554/10336], Loss: 0.1621\n",
      "Epoch [5/5], Step [4556/10336], Loss: 0.1328\n",
      "Epoch [5/5], Step [4558/10336], Loss: 0.1851\n",
      "Epoch [5/5], Step [4560/10336], Loss: 0.6759\n",
      "Epoch [5/5], Step [4562/10336], Loss: 3.6473\n",
      "Epoch [5/5], Step [4564/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [4566/10336], Loss: 0.0193\n",
      "Epoch [5/5], Step [4568/10336], Loss: 0.2141\n",
      "Epoch [5/5], Step [4570/10336], Loss: 1.3459\n",
      "Epoch [5/5], Step [4572/10336], Loss: 0.0119\n",
      "Epoch [5/5], Step [4574/10336], Loss: 3.0557\n",
      "Epoch [5/5], Step [4576/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [4578/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [4580/10336], Loss: 0.3294\n",
      "Epoch [5/5], Step [4582/10336], Loss: 0.0333\n",
      "Epoch [5/5], Step [4584/10336], Loss: 0.1895\n",
      "Epoch [5/5], Step [4586/10336], Loss: 1.1687\n",
      "Epoch [5/5], Step [4588/10336], Loss: 0.3375\n",
      "Epoch [5/5], Step [4590/10336], Loss: 0.0837\n",
      "Epoch [5/5], Step [4592/10336], Loss: 0.1954\n",
      "Epoch [5/5], Step [4594/10336], Loss: 0.3966\n",
      "Epoch [5/5], Step [4596/10336], Loss: 0.0349\n",
      "Epoch [5/5], Step [4598/10336], Loss: 0.4202\n",
      "Epoch [5/5], Step [4600/10336], Loss: 0.7160\n",
      "Epoch [5/5], Step [4602/10336], Loss: 0.2306\n",
      "Epoch [5/5], Step [4604/10336], Loss: 0.0566\n",
      "Epoch [5/5], Step [4606/10336], Loss: 0.1417\n",
      "Epoch [5/5], Step [4608/10336], Loss: 0.1596\n",
      "Epoch [5/5], Step [4610/10336], Loss: 0.2870\n",
      "Epoch [5/5], Step [4612/10336], Loss: 0.2687\n",
      "Epoch [5/5], Step [4614/10336], Loss: 0.6933\n",
      "Epoch [5/5], Step [4616/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [4618/10336], Loss: 1.6240\n",
      "Epoch [5/5], Step [4620/10336], Loss: 0.1819\n",
      "Epoch [5/5], Step [4622/10336], Loss: 0.0107\n",
      "Epoch [5/5], Step [4624/10336], Loss: 2.3095\n",
      "Epoch [5/5], Step [4626/10336], Loss: 1.3439\n",
      "Epoch [5/5], Step [4628/10336], Loss: 0.0414\n",
      "Epoch [5/5], Step [4630/10336], Loss: 0.9998\n",
      "Epoch [5/5], Step [4632/10336], Loss: 0.4418\n",
      "Epoch [5/5], Step [4634/10336], Loss: 0.2803\n",
      "Epoch [5/5], Step [4636/10336], Loss: 4.5140\n",
      "Epoch [5/5], Step [4638/10336], Loss: 0.3705\n",
      "Epoch [5/5], Step [4640/10336], Loss: 0.0962\n",
      "Epoch [5/5], Step [4642/10336], Loss: 0.2117\n",
      "Epoch [5/5], Step [4644/10336], Loss: 0.0746\n",
      "Epoch [5/5], Step [4646/10336], Loss: 0.1383\n",
      "Epoch [5/5], Step [4648/10336], Loss: 0.9598\n",
      "Epoch [5/5], Step [4650/10336], Loss: 0.3461\n",
      "Epoch [5/5], Step [4652/10336], Loss: 2.0914\n",
      "Epoch [5/5], Step [4654/10336], Loss: 1.2432\n",
      "Epoch [5/5], Step [4656/10336], Loss: 0.2391\n",
      "Epoch [5/5], Step [4658/10336], Loss: 0.4821\n",
      "Epoch [5/5], Step [4660/10336], Loss: 0.2295\n",
      "Epoch [5/5], Step [4662/10336], Loss: 0.1331\n",
      "Epoch [5/5], Step [4664/10336], Loss: 0.1149\n",
      "Epoch [5/5], Step [4666/10336], Loss: 0.8888\n",
      "Epoch [5/5], Step [4668/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [4670/10336], Loss: 0.0280\n",
      "Epoch [5/5], Step [4672/10336], Loss: 0.0912\n",
      "Epoch [5/5], Step [4674/10336], Loss: 0.1493\n",
      "Epoch [5/5], Step [4676/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [4678/10336], Loss: 0.0763\n",
      "Epoch [5/5], Step [4680/10336], Loss: 0.4981\n",
      "Epoch [5/5], Step [4682/10336], Loss: 0.4814\n",
      "Epoch [5/5], Step [4684/10336], Loss: 1.7308\n",
      "Epoch [5/5], Step [4686/10336], Loss: 0.0221\n",
      "Epoch [5/5], Step [4688/10336], Loss: 0.0857\n",
      "Epoch [5/5], Step [4690/10336], Loss: 1.1727\n",
      "Epoch [5/5], Step [4692/10336], Loss: 0.0336\n",
      "Epoch [5/5], Step [4694/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [4696/10336], Loss: 0.0419\n",
      "Epoch [5/5], Step [4698/10336], Loss: 1.6329\n",
      "Epoch [5/5], Step [4700/10336], Loss: 0.9345\n",
      "Epoch [5/5], Step [4702/10336], Loss: 0.0497\n",
      "Epoch [5/5], Step [4704/10336], Loss: 0.1249\n",
      "Epoch [5/5], Step [4706/10336], Loss: 0.0264\n",
      "Epoch [5/5], Step [4708/10336], Loss: 1.2882\n",
      "Epoch [5/5], Step [4710/10336], Loss: 0.7209\n",
      "Epoch [5/5], Step [4712/10336], Loss: 2.2651\n",
      "Epoch [5/5], Step [4714/10336], Loss: 0.1858\n",
      "Epoch [5/5], Step [4716/10336], Loss: 0.2052\n",
      "Epoch [5/5], Step [4718/10336], Loss: 0.2534\n",
      "Epoch [5/5], Step [4720/10336], Loss: 0.0208\n",
      "Epoch [5/5], Step [4722/10336], Loss: 0.0901\n",
      "Epoch [5/5], Step [4724/10336], Loss: 0.1549\n",
      "Epoch [5/5], Step [4726/10336], Loss: 0.0373\n",
      "Epoch [5/5], Step [4728/10336], Loss: 0.7937\n",
      "Epoch [5/5], Step [4730/10336], Loss: 0.7416\n",
      "Epoch [5/5], Step [4732/10336], Loss: 0.1031\n",
      "Epoch [5/5], Step [4734/10336], Loss: 0.0355\n",
      "Epoch [5/5], Step [4736/10336], Loss: 0.0347\n",
      "Epoch [5/5], Step [4738/10336], Loss: 1.4358\n",
      "Epoch [5/5], Step [4740/10336], Loss: 0.7426\n",
      "Epoch [5/5], Step [4742/10336], Loss: 0.0329\n",
      "Epoch [5/5], Step [4744/10336], Loss: 7.6021\n",
      "Epoch [5/5], Step [4746/10336], Loss: 0.1127\n",
      "Epoch [5/5], Step [4748/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [4750/10336], Loss: 0.7863\n",
      "Epoch [5/5], Step [4752/10336], Loss: 1.0426\n",
      "Epoch [5/5], Step [4754/10336], Loss: 0.2839\n",
      "Epoch [5/5], Step [4756/10336], Loss: 0.8446\n",
      "Epoch [5/5], Step [4758/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [4760/10336], Loss: 0.0684\n",
      "Epoch [5/5], Step [4762/10336], Loss: 0.2274\n",
      "Epoch [5/5], Step [4764/10336], Loss: 0.0386\n",
      "Epoch [5/5], Step [4766/10336], Loss: 0.9904\n",
      "Epoch [5/5], Step [4768/10336], Loss: 0.0049\n",
      "Epoch [5/5], Step [4770/10336], Loss: 0.3111\n",
      "Epoch [5/5], Step [4772/10336], Loss: 1.8410\n",
      "Epoch [5/5], Step [4774/10336], Loss: 0.9066\n",
      "Epoch [5/5], Step [4776/10336], Loss: 0.2658\n",
      "Epoch [5/5], Step [4778/10336], Loss: 0.2339\n",
      "Epoch [5/5], Step [4780/10336], Loss: 0.0824\n",
      "Epoch [5/5], Step [4782/10336], Loss: 0.2448\n",
      "Epoch [5/5], Step [4784/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [4786/10336], Loss: 0.1872\n",
      "Epoch [5/5], Step [4788/10336], Loss: 0.0629\n",
      "Epoch [5/5], Step [4790/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [4792/10336], Loss: 0.0116\n",
      "Epoch [5/5], Step [4794/10336], Loss: 0.3824\n",
      "Epoch [5/5], Step [4796/10336], Loss: 0.0231\n",
      "Epoch [5/5], Step [4798/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [4800/10336], Loss: 0.3395\n",
      "Epoch [5/5], Step [4802/10336], Loss: 0.3805\n",
      "Epoch [5/5], Step [4804/10336], Loss: 2.2758\n",
      "Epoch [5/5], Step [4806/10336], Loss: 0.0039\n",
      "Epoch [5/5], Step [4808/10336], Loss: 0.0310\n",
      "Epoch [5/5], Step [4810/10336], Loss: 0.6004\n",
      "Epoch [5/5], Step [4812/10336], Loss: 0.4085\n",
      "Epoch [5/5], Step [4814/10336], Loss: 0.0796\n",
      "Epoch [5/5], Step [4816/10336], Loss: 0.0572\n",
      "Epoch [5/5], Step [4818/10336], Loss: 0.0668\n",
      "Epoch [5/5], Step [4820/10336], Loss: 0.1579\n",
      "Epoch [5/5], Step [4822/10336], Loss: 1.1098\n",
      "Epoch [5/5], Step [4824/10336], Loss: 0.2288\n",
      "Epoch [5/5], Step [4826/10336], Loss: 0.0786\n",
      "Epoch [5/5], Step [4828/10336], Loss: 0.2031\n",
      "Epoch [5/5], Step [4830/10336], Loss: 0.0463\n",
      "Epoch [5/5], Step [4832/10336], Loss: 0.5314\n",
      "Epoch [5/5], Step [4834/10336], Loss: 0.2447\n",
      "Epoch [5/5], Step [4836/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [4838/10336], Loss: 0.2730\n",
      "Epoch [5/5], Step [4840/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [4842/10336], Loss: 0.2395\n",
      "Epoch [5/5], Step [4844/10336], Loss: 0.9975\n",
      "Epoch [5/5], Step [4846/10336], Loss: 0.0119\n",
      "Epoch [5/5], Step [4848/10336], Loss: 1.2018\n",
      "Epoch [5/5], Step [4850/10336], Loss: 0.2609\n",
      "Epoch [5/5], Step [4852/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [4854/10336], Loss: 0.0184\n",
      "Epoch [5/5], Step [4856/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [4858/10336], Loss: 3.2570\n",
      "Epoch [5/5], Step [4860/10336], Loss: 1.3002\n",
      "Epoch [5/5], Step [4862/10336], Loss: 0.0409\n",
      "Epoch [5/5], Step [4864/10336], Loss: 1.2610\n",
      "Epoch [5/5], Step [4866/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [4868/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [4870/10336], Loss: 0.1698\n",
      "Epoch [5/5], Step [4872/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [4874/10336], Loss: 0.1943\n",
      "Epoch [5/5], Step [4876/10336], Loss: 0.9001\n",
      "Epoch [5/5], Step [4878/10336], Loss: 0.0501\n",
      "Epoch [5/5], Step [4880/10336], Loss: 0.0752\n",
      "Epoch [5/5], Step [4882/10336], Loss: 0.9406\n",
      "Epoch [5/5], Step [4884/10336], Loss: 0.4093\n",
      "Epoch [5/5], Step [4886/10336], Loss: 0.5665\n",
      "Epoch [5/5], Step [4888/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [4890/10336], Loss: 0.2971\n",
      "Epoch [5/5], Step [4892/10336], Loss: 2.3198\n",
      "Epoch [5/5], Step [4894/10336], Loss: 0.2206\n",
      "Epoch [5/5], Step [4896/10336], Loss: 0.0440\n",
      "Epoch [5/5], Step [4898/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [4900/10336], Loss: 0.0248\n",
      "Epoch [5/5], Step [4902/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [4904/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [4906/10336], Loss: 3.5898\n",
      "Epoch [5/5], Step [4908/10336], Loss: 1.6245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [4910/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [4912/10336], Loss: 3.7922\n",
      "Epoch [5/5], Step [4914/10336], Loss: 0.1198\n",
      "Epoch [5/5], Step [4916/10336], Loss: 1.3040\n",
      "Epoch [5/5], Step [4918/10336], Loss: 0.5951\n",
      "Epoch [5/5], Step [4920/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [4922/10336], Loss: 0.0147\n",
      "Epoch [5/5], Step [4924/10336], Loss: 2.2749\n",
      "Epoch [5/5], Step [4926/10336], Loss: 1.2007\n",
      "Epoch [5/5], Step [4928/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [4930/10336], Loss: 0.3028\n",
      "Epoch [5/5], Step [4932/10336], Loss: 0.7428\n",
      "Epoch [5/5], Step [4934/10336], Loss: 0.4029\n",
      "Epoch [5/5], Step [4936/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [4938/10336], Loss: 1.3213\n",
      "Epoch [5/5], Step [4940/10336], Loss: 0.1877\n",
      "Epoch [5/5], Step [4942/10336], Loss: 0.6352\n",
      "Epoch [5/5], Step [4944/10336], Loss: 0.2815\n",
      "Epoch [5/5], Step [4946/10336], Loss: 1.3288\n",
      "Epoch [5/5], Step [4948/10336], Loss: 0.4063\n",
      "Epoch [5/5], Step [4950/10336], Loss: 0.1059\n",
      "Epoch [5/5], Step [4952/10336], Loss: 0.2411\n",
      "Epoch [5/5], Step [4954/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [4956/10336], Loss: 0.2205\n",
      "Epoch [5/5], Step [4958/10336], Loss: 0.0466\n",
      "Epoch [5/5], Step [4960/10336], Loss: 1.0481\n",
      "Epoch [5/5], Step [4962/10336], Loss: 0.0832\n",
      "Epoch [5/5], Step [4964/10336], Loss: 0.0114\n",
      "Epoch [5/5], Step [4966/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [4968/10336], Loss: 0.0282\n",
      "Epoch [5/5], Step [4970/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [4972/10336], Loss: 1.3117\n",
      "Epoch [5/5], Step [4974/10336], Loss: 0.0125\n",
      "Epoch [5/5], Step [4976/10336], Loss: 1.6217\n",
      "Epoch [5/5], Step [4978/10336], Loss: 1.7180\n",
      "Epoch [5/5], Step [4980/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [4982/10336], Loss: 0.0100\n",
      "Epoch [5/5], Step [4984/10336], Loss: 0.4583\n",
      "Epoch [5/5], Step [4986/10336], Loss: 1.1858\n",
      "Epoch [5/5], Step [4988/10336], Loss: 2.7088\n",
      "Epoch [5/5], Step [4990/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [4992/10336], Loss: 0.3149\n",
      "Epoch [5/5], Step [4994/10336], Loss: 0.1046\n",
      "Epoch [5/5], Step [4996/10336], Loss: 1.1997\n",
      "Epoch [5/5], Step [4998/10336], Loss: 0.0065\n",
      "Epoch [5/5], Step [5000/10336], Loss: 0.9734\n",
      "Epoch [5/5], Step [5002/10336], Loss: 1.6696\n",
      "Epoch [5/5], Step [5004/10336], Loss: 0.2151\n",
      "Epoch [5/5], Step [5006/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [5008/10336], Loss: 0.3091\n",
      "Epoch [5/5], Step [5010/10336], Loss: 0.1756\n",
      "Epoch [5/5], Step [5012/10336], Loss: 0.2824\n",
      "Epoch [5/5], Step [5014/10336], Loss: 1.8338\n",
      "Epoch [5/5], Step [5016/10336], Loss: 0.3173\n",
      "Epoch [5/5], Step [5018/10336], Loss: 2.7099\n",
      "Epoch [5/5], Step [5020/10336], Loss: 0.1191\n",
      "Epoch [5/5], Step [5022/10336], Loss: 0.3037\n",
      "Epoch [5/5], Step [5024/10336], Loss: 0.0291\n",
      "Epoch [5/5], Step [5026/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [5028/10336], Loss: 0.0724\n",
      "Epoch [5/5], Step [5030/10336], Loss: 0.1917\n",
      "Epoch [5/5], Step [5032/10336], Loss: 0.2099\n",
      "Epoch [5/5], Step [5034/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [5036/10336], Loss: 0.5185\n",
      "Epoch [5/5], Step [5038/10336], Loss: 0.0158\n",
      "Epoch [5/5], Step [5040/10336], Loss: 0.0959\n",
      "Epoch [5/5], Step [5042/10336], Loss: 1.1343\n",
      "Epoch [5/5], Step [5044/10336], Loss: 0.4700\n",
      "Epoch [5/5], Step [5046/10336], Loss: 0.2523\n",
      "Epoch [5/5], Step [5048/10336], Loss: 0.1780\n",
      "Epoch [5/5], Step [5050/10336], Loss: 1.1002\n",
      "Epoch [5/5], Step [5052/10336], Loss: 0.1628\n",
      "Epoch [5/5], Step [5054/10336], Loss: 0.1322\n",
      "Epoch [5/5], Step [5056/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [5058/10336], Loss: 0.3499\n",
      "Epoch [5/5], Step [5060/10336], Loss: 0.7524\n",
      "Epoch [5/5], Step [5062/10336], Loss: 0.0146\n",
      "Epoch [5/5], Step [5064/10336], Loss: 0.1964\n",
      "Epoch [5/5], Step [5066/10336], Loss: 0.7052\n",
      "Epoch [5/5], Step [5068/10336], Loss: 2.9211\n",
      "Epoch [5/5], Step [5070/10336], Loss: 0.0341\n",
      "Epoch [5/5], Step [5072/10336], Loss: 0.0631\n",
      "Epoch [5/5], Step [5074/10336], Loss: 0.6038\n",
      "Epoch [5/5], Step [5076/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [5078/10336], Loss: 0.6052\n",
      "Epoch [5/5], Step [5080/10336], Loss: 1.3955\n",
      "Epoch [5/5], Step [5082/10336], Loss: 0.2150\n",
      "Epoch [5/5], Step [5084/10336], Loss: 0.1703\n",
      "Epoch [5/5], Step [5086/10336], Loss: 0.3908\n",
      "Epoch [5/5], Step [5088/10336], Loss: 0.4424\n",
      "Epoch [5/5], Step [5090/10336], Loss: 0.0510\n",
      "Epoch [5/5], Step [5092/10336], Loss: 0.7975\n",
      "Epoch [5/5], Step [5094/10336], Loss: 1.1575\n",
      "Epoch [5/5], Step [5096/10336], Loss: 0.0096\n",
      "Epoch [5/5], Step [5098/10336], Loss: 0.3350\n",
      "Epoch [5/5], Step [5100/10336], Loss: 0.0489\n",
      "Epoch [5/5], Step [5102/10336], Loss: 0.5825\n",
      "Epoch [5/5], Step [5104/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [5106/10336], Loss: 0.1117\n",
      "Epoch [5/5], Step [5108/10336], Loss: 0.0708\n",
      "Epoch [5/5], Step [5110/10336], Loss: 2.7955\n",
      "Epoch [5/5], Step [5112/10336], Loss: 0.0606\n",
      "Epoch [5/5], Step [5114/10336], Loss: 0.5234\n",
      "Epoch [5/5], Step [5116/10336], Loss: 0.0348\n",
      "Epoch [5/5], Step [5118/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [5120/10336], Loss: 0.5115\n",
      "Epoch [5/5], Step [5122/10336], Loss: 1.0161\n",
      "Epoch [5/5], Step [5124/10336], Loss: 0.1440\n",
      "Epoch [5/5], Step [5126/10336], Loss: 0.4455\n",
      "Epoch [5/5], Step [5128/10336], Loss: 0.0120\n",
      "Epoch [5/5], Step [5130/10336], Loss: 0.1042\n",
      "Epoch [5/5], Step [5132/10336], Loss: 0.0337\n",
      "Epoch [5/5], Step [5134/10336], Loss: 0.1279\n",
      "Epoch [5/5], Step [5136/10336], Loss: 0.0717\n",
      "Epoch [5/5], Step [5138/10336], Loss: 1.7761\n",
      "Epoch [5/5], Step [5140/10336], Loss: 0.2348\n",
      "Epoch [5/5], Step [5142/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [5144/10336], Loss: 0.4827\n",
      "Epoch [5/5], Step [5146/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [5148/10336], Loss: 0.0693\n",
      "Epoch [5/5], Step [5150/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [5152/10336], Loss: 0.2602\n",
      "Epoch [5/5], Step [5154/10336], Loss: 0.0129\n",
      "Epoch [5/5], Step [5156/10336], Loss: 1.7311\n",
      "Epoch [5/5], Step [5158/10336], Loss: 5.2944\n",
      "Epoch [5/5], Step [5160/10336], Loss: 0.2993\n",
      "Epoch [5/5], Step [5162/10336], Loss: 0.1250\n",
      "Epoch [5/5], Step [5164/10336], Loss: 0.0155\n",
      "Epoch [5/5], Step [5166/10336], Loss: 0.4719\n",
      "Epoch [5/5], Step [5168/10336], Loss: 0.2752\n",
      "Epoch [5/5], Step [5170/10336], Loss: 0.8653\n",
      "Epoch [5/5], Step [5172/10336], Loss: 0.6167\n",
      "Epoch [5/5], Step [5174/10336], Loss: 0.3581\n",
      "Epoch [5/5], Step [5176/10336], Loss: 2.9219\n",
      "Epoch [5/5], Step [5178/10336], Loss: 0.2271\n",
      "Epoch [5/5], Step [5180/10336], Loss: 0.0712\n",
      "Epoch [5/5], Step [5182/10336], Loss: 0.0905\n",
      "Epoch [5/5], Step [5184/10336], Loss: 0.0118\n",
      "Epoch [5/5], Step [5186/10336], Loss: 0.1486\n",
      "Epoch [5/5], Step [5188/10336], Loss: 0.0371\n",
      "Epoch [5/5], Step [5190/10336], Loss: 0.0618\n",
      "Epoch [5/5], Step [5192/10336], Loss: 0.1777\n",
      "Epoch [5/5], Step [5194/10336], Loss: 0.0313\n",
      "Epoch [5/5], Step [5196/10336], Loss: 0.0325\n",
      "Epoch [5/5], Step [5198/10336], Loss: 0.0450\n",
      "Epoch [5/5], Step [5200/10336], Loss: 5.9474\n",
      "Epoch [5/5], Step [5202/10336], Loss: 0.3737\n",
      "Epoch [5/5], Step [5204/10336], Loss: 0.0169\n",
      "Epoch [5/5], Step [5206/10336], Loss: 0.1943\n",
      "Epoch [5/5], Step [5208/10336], Loss: 0.7853\n",
      "Epoch [5/5], Step [5210/10336], Loss: 0.0205\n",
      "Epoch [5/5], Step [5212/10336], Loss: 0.2636\n",
      "Epoch [5/5], Step [5214/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [5216/10336], Loss: 0.2208\n",
      "Epoch [5/5], Step [5218/10336], Loss: 1.0479\n",
      "Epoch [5/5], Step [5220/10336], Loss: 3.8908\n",
      "Epoch [5/5], Step [5222/10336], Loss: 0.0086\n",
      "Epoch [5/5], Step [5224/10336], Loss: 0.0075\n",
      "Epoch [5/5], Step [5226/10336], Loss: 0.3010\n",
      "Epoch [5/5], Step [5228/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [5230/10336], Loss: 0.0850\n",
      "Epoch [5/5], Step [5232/10336], Loss: 0.4777\n",
      "Epoch [5/5], Step [5234/10336], Loss: 0.0074\n",
      "Epoch [5/5], Step [5236/10336], Loss: 0.4703\n",
      "Epoch [5/5], Step [5238/10336], Loss: 1.2428\n",
      "Epoch [5/5], Step [5240/10336], Loss: 0.0116\n",
      "Epoch [5/5], Step [5242/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [5244/10336], Loss: 0.1236\n",
      "Epoch [5/5], Step [5246/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [5248/10336], Loss: 0.2424\n",
      "Epoch [5/5], Step [5250/10336], Loss: 0.4217\n",
      "Epoch [5/5], Step [5252/10336], Loss: 1.3202\n",
      "Epoch [5/5], Step [5254/10336], Loss: 0.3773\n",
      "Epoch [5/5], Step [5256/10336], Loss: 0.0267\n",
      "Epoch [5/5], Step [5258/10336], Loss: 0.0660\n",
      "Epoch [5/5], Step [5260/10336], Loss: 1.5397\n",
      "Epoch [5/5], Step [5262/10336], Loss: 0.5883\n",
      "Epoch [5/5], Step [5264/10336], Loss: 0.0049\n",
      "Epoch [5/5], Step [5266/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [5268/10336], Loss: 3.2230\n",
      "Epoch [5/5], Step [5270/10336], Loss: 0.2224\n",
      "Epoch [5/5], Step [5272/10336], Loss: 0.2548\n",
      "Epoch [5/5], Step [5274/10336], Loss: 2.6805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [5276/10336], Loss: 0.0458\n",
      "Epoch [5/5], Step [5278/10336], Loss: 0.0429\n",
      "Epoch [5/5], Step [5280/10336], Loss: 0.9998\n",
      "Epoch [5/5], Step [5282/10336], Loss: 0.1796\n",
      "Epoch [5/5], Step [5284/10336], Loss: 0.4255\n",
      "Epoch [5/5], Step [5286/10336], Loss: 0.1109\n",
      "Epoch [5/5], Step [5288/10336], Loss: 0.2168\n",
      "Epoch [5/5], Step [5290/10336], Loss: 0.3213\n",
      "Epoch [5/5], Step [5292/10336], Loss: 0.1712\n",
      "Epoch [5/5], Step [5294/10336], Loss: 2.2063\n",
      "Epoch [5/5], Step [5296/10336], Loss: 0.2738\n",
      "Epoch [5/5], Step [5298/10336], Loss: 0.5084\n",
      "Epoch [5/5], Step [5300/10336], Loss: 0.1558\n",
      "Epoch [5/5], Step [5302/10336], Loss: 0.0686\n",
      "Epoch [5/5], Step [5304/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [5306/10336], Loss: 0.1731\n",
      "Epoch [5/5], Step [5308/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [5310/10336], Loss: 0.0311\n",
      "Epoch [5/5], Step [5312/10336], Loss: 3.5189\n",
      "Epoch [5/5], Step [5314/10336], Loss: 0.4257\n",
      "Epoch [5/5], Step [5316/10336], Loss: 0.1373\n",
      "Epoch [5/5], Step [5318/10336], Loss: 0.1179\n",
      "Epoch [5/5], Step [5320/10336], Loss: 0.0094\n",
      "Epoch [5/5], Step [5322/10336], Loss: 0.2431\n",
      "Epoch [5/5], Step [5324/10336], Loss: 0.0183\n",
      "Epoch [5/5], Step [5326/10336], Loss: 1.0422\n",
      "Epoch [5/5], Step [5328/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [5330/10336], Loss: 0.2370\n",
      "Epoch [5/5], Step [5332/10336], Loss: 2.0927\n",
      "Epoch [5/5], Step [5334/10336], Loss: 0.0749\n",
      "Epoch [5/5], Step [5336/10336], Loss: 0.2462\n",
      "Epoch [5/5], Step [5338/10336], Loss: 3.2563\n",
      "Epoch [5/5], Step [5340/10336], Loss: 0.0101\n",
      "Epoch [5/5], Step [5342/10336], Loss: 0.0242\n",
      "Epoch [5/5], Step [5344/10336], Loss: 2.7677\n",
      "Epoch [5/5], Step [5346/10336], Loss: 1.2506\n",
      "Epoch [5/5], Step [5348/10336], Loss: 0.0477\n",
      "Epoch [5/5], Step [5350/10336], Loss: 0.3370\n",
      "Epoch [5/5], Step [5352/10336], Loss: 0.9721\n",
      "Epoch [5/5], Step [5354/10336], Loss: 0.2712\n",
      "Epoch [5/5], Step [5356/10336], Loss: 1.0277\n",
      "Epoch [5/5], Step [5358/10336], Loss: 0.6311\n",
      "Epoch [5/5], Step [5360/10336], Loss: 0.0065\n",
      "Epoch [5/5], Step [5362/10336], Loss: 0.2167\n",
      "Epoch [5/5], Step [5364/10336], Loss: 0.0951\n",
      "Epoch [5/5], Step [5366/10336], Loss: 0.1303\n",
      "Epoch [5/5], Step [5368/10336], Loss: 2.0765\n",
      "Epoch [5/5], Step [5370/10336], Loss: 0.0266\n",
      "Epoch [5/5], Step [5372/10336], Loss: 1.0998\n",
      "Epoch [5/5], Step [5374/10336], Loss: 0.3392\n",
      "Epoch [5/5], Step [5376/10336], Loss: 0.4919\n",
      "Epoch [5/5], Step [5378/10336], Loss: 0.0474\n",
      "Epoch [5/5], Step [5380/10336], Loss: 0.1262\n",
      "Epoch [5/5], Step [5382/10336], Loss: 0.2922\n",
      "Epoch [5/5], Step [5384/10336], Loss: 1.2681\n",
      "Epoch [5/5], Step [5386/10336], Loss: 0.0487\n",
      "Epoch [5/5], Step [5388/10336], Loss: 0.1414\n",
      "Epoch [5/5], Step [5390/10336], Loss: 0.0386\n",
      "Epoch [5/5], Step [5392/10336], Loss: 0.0542\n",
      "Epoch [5/5], Step [5394/10336], Loss: 0.0183\n",
      "Epoch [5/5], Step [5396/10336], Loss: 0.2590\n",
      "Epoch [5/5], Step [5398/10336], Loss: 0.7489\n",
      "Epoch [5/5], Step [5400/10336], Loss: 0.2020\n",
      "Epoch [5/5], Step [5402/10336], Loss: 0.0460\n",
      "Epoch [5/5], Step [5404/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [5406/10336], Loss: 0.0093\n",
      "Epoch [5/5], Step [5408/10336], Loss: 4.0405\n",
      "Epoch [5/5], Step [5410/10336], Loss: 0.4525\n",
      "Epoch [5/5], Step [5412/10336], Loss: 0.0053\n",
      "Epoch [5/5], Step [5414/10336], Loss: 0.0307\n",
      "Epoch [5/5], Step [5416/10336], Loss: 2.3968\n",
      "Epoch [5/5], Step [5418/10336], Loss: 0.2412\n",
      "Epoch [5/5], Step [5420/10336], Loss: 0.8870\n",
      "Epoch [5/5], Step [5422/10336], Loss: 2.9714\n",
      "Epoch [5/5], Step [5424/10336], Loss: 0.0448\n",
      "Epoch [5/5], Step [5426/10336], Loss: 0.1327\n",
      "Epoch [5/5], Step [5428/10336], Loss: 0.4267\n",
      "Epoch [5/5], Step [5430/10336], Loss: 0.4167\n",
      "Epoch [5/5], Step [5432/10336], Loss: 0.1475\n",
      "Epoch [5/5], Step [5434/10336], Loss: 0.6701\n",
      "Epoch [5/5], Step [5436/10336], Loss: 0.0908\n",
      "Epoch [5/5], Step [5438/10336], Loss: 0.2984\n",
      "Epoch [5/5], Step [5440/10336], Loss: 0.8692\n",
      "Epoch [5/5], Step [5442/10336], Loss: 1.5149\n",
      "Epoch [5/5], Step [5444/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [5446/10336], Loss: 0.4653\n",
      "Epoch [5/5], Step [5448/10336], Loss: 0.3042\n",
      "Epoch [5/5], Step [5450/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [5452/10336], Loss: 0.3216\n",
      "Epoch [5/5], Step [5454/10336], Loss: 1.2524\n",
      "Epoch [5/5], Step [5456/10336], Loss: 0.0309\n",
      "Epoch [5/5], Step [5458/10336], Loss: 2.3473\n",
      "Epoch [5/5], Step [5460/10336], Loss: 0.0271\n",
      "Epoch [5/5], Step [5462/10336], Loss: 0.0560\n",
      "Epoch [5/5], Step [5464/10336], Loss: 0.3053\n",
      "Epoch [5/5], Step [5466/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [5468/10336], Loss: 0.3460\n",
      "Epoch [5/5], Step [5470/10336], Loss: 2.9585\n",
      "Epoch [5/5], Step [5472/10336], Loss: 0.5415\n",
      "Epoch [5/5], Step [5474/10336], Loss: 0.3988\n",
      "Epoch [5/5], Step [5476/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [5478/10336], Loss: 3.2736\n",
      "Epoch [5/5], Step [5480/10336], Loss: 0.0352\n",
      "Epoch [5/5], Step [5482/10336], Loss: 0.2110\n",
      "Epoch [5/5], Step [5484/10336], Loss: 0.2959\n",
      "Epoch [5/5], Step [5486/10336], Loss: 0.0795\n",
      "Epoch [5/5], Step [5488/10336], Loss: 0.0429\n",
      "Epoch [5/5], Step [5490/10336], Loss: 0.0380\n",
      "Epoch [5/5], Step [5492/10336], Loss: 0.0473\n",
      "Epoch [5/5], Step [5494/10336], Loss: 0.0529\n",
      "Epoch [5/5], Step [5496/10336], Loss: 0.8756\n",
      "Epoch [5/5], Step [5498/10336], Loss: 0.0552\n",
      "Epoch [5/5], Step [5500/10336], Loss: 1.7103\n",
      "Epoch [5/5], Step [5502/10336], Loss: 1.0171\n",
      "Epoch [5/5], Step [5504/10336], Loss: 0.3460\n",
      "Epoch [5/5], Step [5506/10336], Loss: 1.4914\n",
      "Epoch [5/5], Step [5508/10336], Loss: 0.1766\n",
      "Epoch [5/5], Step [5510/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [5512/10336], Loss: 0.2834\n",
      "Epoch [5/5], Step [5514/10336], Loss: 0.0441\n",
      "Epoch [5/5], Step [5516/10336], Loss: 0.0742\n",
      "Epoch [5/5], Step [5518/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [5520/10336], Loss: 0.1157\n",
      "Epoch [5/5], Step [5522/10336], Loss: 0.8133\n",
      "Epoch [5/5], Step [5524/10336], Loss: 0.0399\n",
      "Epoch [5/5], Step [5526/10336], Loss: 0.5354\n",
      "Epoch [5/5], Step [5528/10336], Loss: 1.5027\n",
      "Epoch [5/5], Step [5530/10336], Loss: 0.3273\n",
      "Epoch [5/5], Step [5532/10336], Loss: 0.0789\n",
      "Epoch [5/5], Step [5534/10336], Loss: 0.2553\n",
      "Epoch [5/5], Step [5536/10336], Loss: 0.0082\n",
      "Epoch [5/5], Step [5538/10336], Loss: 0.1840\n",
      "Epoch [5/5], Step [5540/10336], Loss: 0.1690\n",
      "Epoch [5/5], Step [5542/10336], Loss: 3.8315\n",
      "Epoch [5/5], Step [5544/10336], Loss: 0.2630\n",
      "Epoch [5/5], Step [5546/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [5548/10336], Loss: 1.0999\n",
      "Epoch [5/5], Step [5550/10336], Loss: 0.0289\n",
      "Epoch [5/5], Step [5552/10336], Loss: 0.0201\n",
      "Epoch [5/5], Step [5554/10336], Loss: 1.0667\n",
      "Epoch [5/5], Step [5556/10336], Loss: 0.0215\n",
      "Epoch [5/5], Step [5558/10336], Loss: 0.1999\n",
      "Epoch [5/5], Step [5560/10336], Loss: 0.1527\n",
      "Epoch [5/5], Step [5562/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [5564/10336], Loss: 0.2312\n",
      "Epoch [5/5], Step [5566/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [5568/10336], Loss: 0.0418\n",
      "Epoch [5/5], Step [5570/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [5572/10336], Loss: 3.3232\n",
      "Epoch [5/5], Step [5574/10336], Loss: 4.0558\n",
      "Epoch [5/5], Step [5576/10336], Loss: 1.5286\n",
      "Epoch [5/5], Step [5578/10336], Loss: 0.5992\n",
      "Epoch [5/5], Step [5580/10336], Loss: 0.1401\n",
      "Epoch [5/5], Step [5582/10336], Loss: 0.0214\n",
      "Epoch [5/5], Step [5584/10336], Loss: 0.1784\n",
      "Epoch [5/5], Step [5586/10336], Loss: 0.1920\n",
      "Epoch [5/5], Step [5588/10336], Loss: 0.0510\n",
      "Epoch [5/5], Step [5590/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [5592/10336], Loss: 0.9143\n",
      "Epoch [5/5], Step [5594/10336], Loss: 2.4482\n",
      "Epoch [5/5], Step [5596/10336], Loss: 0.0318\n",
      "Epoch [5/5], Step [5598/10336], Loss: 0.0507\n",
      "Epoch [5/5], Step [5600/10336], Loss: 0.0977\n",
      "Epoch [5/5], Step [5602/10336], Loss: 0.0771\n",
      "Epoch [5/5], Step [5604/10336], Loss: 0.4754\n",
      "Epoch [5/5], Step [5606/10336], Loss: 0.0262\n",
      "Epoch [5/5], Step [5608/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [5610/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [5612/10336], Loss: 3.1130\n",
      "Epoch [5/5], Step [5614/10336], Loss: 0.4222\n",
      "Epoch [5/5], Step [5616/10336], Loss: 0.0572\n",
      "Epoch [5/5], Step [5618/10336], Loss: 0.0930\n",
      "Epoch [5/5], Step [5620/10336], Loss: 0.6287\n",
      "Epoch [5/5], Step [5622/10336], Loss: 0.0493\n",
      "Epoch [5/5], Step [5624/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [5626/10336], Loss: 3.0319\n",
      "Epoch [5/5], Step [5628/10336], Loss: 3.6611\n",
      "Epoch [5/5], Step [5630/10336], Loss: 0.2279\n",
      "Epoch [5/5], Step [5632/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [5634/10336], Loss: 0.1056\n",
      "Epoch [5/5], Step [5636/10336], Loss: 1.3443\n",
      "Epoch [5/5], Step [5638/10336], Loss: 0.0168\n",
      "Epoch [5/5], Step [5640/10336], Loss: 0.2551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [5642/10336], Loss: 0.2225\n",
      "Epoch [5/5], Step [5644/10336], Loss: 0.1211\n",
      "Epoch [5/5], Step [5646/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [5648/10336], Loss: 0.3196\n",
      "Epoch [5/5], Step [5650/10336], Loss: 2.9577\n",
      "Epoch [5/5], Step [5652/10336], Loss: 0.0502\n",
      "Epoch [5/5], Step [5654/10336], Loss: 0.0751\n",
      "Epoch [5/5], Step [5656/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [5658/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [5660/10336], Loss: 0.0128\n",
      "Epoch [5/5], Step [5662/10336], Loss: 0.0258\n",
      "Epoch [5/5], Step [5664/10336], Loss: 0.4554\n",
      "Epoch [5/5], Step [5666/10336], Loss: 0.0119\n",
      "Epoch [5/5], Step [5668/10336], Loss: 0.0581\n",
      "Epoch [5/5], Step [5670/10336], Loss: 2.4595\n",
      "Epoch [5/5], Step [5672/10336], Loss: 0.0933\n",
      "Epoch [5/5], Step [5674/10336], Loss: 0.1499\n",
      "Epoch [5/5], Step [5676/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [5678/10336], Loss: 0.0279\n",
      "Epoch [5/5], Step [5680/10336], Loss: 0.2034\n",
      "Epoch [5/5], Step [5682/10336], Loss: 0.0254\n",
      "Epoch [5/5], Step [5684/10336], Loss: 0.4251\n",
      "Epoch [5/5], Step [5686/10336], Loss: 0.7450\n",
      "Epoch [5/5], Step [5688/10336], Loss: 0.2248\n",
      "Epoch [5/5], Step [5690/10336], Loss: 0.0253\n",
      "Epoch [5/5], Step [5692/10336], Loss: 0.8376\n",
      "Epoch [5/5], Step [5694/10336], Loss: 0.1162\n",
      "Epoch [5/5], Step [5696/10336], Loss: 0.0108\n",
      "Epoch [5/5], Step [5698/10336], Loss: 0.0417\n",
      "Epoch [5/5], Step [5700/10336], Loss: 0.1185\n",
      "Epoch [5/5], Step [5702/10336], Loss: 0.0570\n",
      "Epoch [5/5], Step [5704/10336], Loss: 0.1765\n",
      "Epoch [5/5], Step [5706/10336], Loss: 0.0486\n",
      "Epoch [5/5], Step [5708/10336], Loss: 0.4519\n",
      "Epoch [5/5], Step [5710/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [5712/10336], Loss: 0.0720\n",
      "Epoch [5/5], Step [5714/10336], Loss: 0.0671\n",
      "Epoch [5/5], Step [5716/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [5718/10336], Loss: 0.1829\n",
      "Epoch [5/5], Step [5720/10336], Loss: 1.1624\n",
      "Epoch [5/5], Step [5722/10336], Loss: 0.0228\n",
      "Epoch [5/5], Step [5724/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [5726/10336], Loss: 0.0099\n",
      "Epoch [5/5], Step [5728/10336], Loss: 0.1189\n",
      "Epoch [5/5], Step [5730/10336], Loss: 0.1581\n",
      "Epoch [5/5], Step [5732/10336], Loss: 5.0229\n",
      "Epoch [5/5], Step [5734/10336], Loss: 0.2644\n",
      "Epoch [5/5], Step [5736/10336], Loss: 0.1466\n",
      "Epoch [5/5], Step [5738/10336], Loss: 0.1226\n",
      "Epoch [5/5], Step [5740/10336], Loss: 0.2911\n",
      "Epoch [5/5], Step [5742/10336], Loss: 0.0216\n",
      "Epoch [5/5], Step [5744/10336], Loss: 0.9855\n",
      "Epoch [5/5], Step [5746/10336], Loss: 3.4774\n",
      "Epoch [5/5], Step [5748/10336], Loss: 0.0830\n",
      "Epoch [5/5], Step [5750/10336], Loss: 0.1296\n",
      "Epoch [5/5], Step [5752/10336], Loss: 1.1340\n",
      "Epoch [5/5], Step [5754/10336], Loss: 1.6414\n",
      "Epoch [5/5], Step [5756/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [5758/10336], Loss: 1.4808\n",
      "Epoch [5/5], Step [5760/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [5762/10336], Loss: 1.3231\n",
      "Epoch [5/5], Step [5764/10336], Loss: 0.0290\n",
      "Epoch [5/5], Step [5766/10336], Loss: 2.0534\n",
      "Epoch [5/5], Step [5768/10336], Loss: 0.2807\n",
      "Epoch [5/5], Step [5770/10336], Loss: 0.2663\n",
      "Epoch [5/5], Step [5772/10336], Loss: 0.2646\n",
      "Epoch [5/5], Step [5774/10336], Loss: 0.0502\n",
      "Epoch [5/5], Step [5776/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [5778/10336], Loss: 0.0631\n",
      "Epoch [5/5], Step [5780/10336], Loss: 0.8682\n",
      "Epoch [5/5], Step [5782/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [5784/10336], Loss: 0.0745\n",
      "Epoch [5/5], Step [5786/10336], Loss: 0.0579\n",
      "Epoch [5/5], Step [5788/10336], Loss: 0.2878\n",
      "Epoch [5/5], Step [5790/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [5792/10336], Loss: 2.9072\n",
      "Epoch [5/5], Step [5794/10336], Loss: 0.8579\n",
      "Epoch [5/5], Step [5796/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [5798/10336], Loss: 0.2631\n",
      "Epoch [5/5], Step [5800/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [5802/10336], Loss: 1.1832\n",
      "Epoch [5/5], Step [5804/10336], Loss: 0.2894\n",
      "Epoch [5/5], Step [5806/10336], Loss: 0.0104\n",
      "Epoch [5/5], Step [5808/10336], Loss: 0.4867\n",
      "Epoch [5/5], Step [5810/10336], Loss: 0.5905\n",
      "Epoch [5/5], Step [5812/10336], Loss: 2.5972\n",
      "Epoch [5/5], Step [5814/10336], Loss: 0.1854\n",
      "Epoch [5/5], Step [5816/10336], Loss: 0.0440\n",
      "Epoch [5/5], Step [5818/10336], Loss: 1.2929\n",
      "Epoch [5/5], Step [5820/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [5822/10336], Loss: 0.2441\n",
      "Epoch [5/5], Step [5824/10336], Loss: 0.4862\n",
      "Epoch [5/5], Step [5826/10336], Loss: 0.2893\n",
      "Epoch [5/5], Step [5828/10336], Loss: 1.5764\n",
      "Epoch [5/5], Step [5830/10336], Loss: 0.1919\n",
      "Epoch [5/5], Step [5832/10336], Loss: 0.7894\n",
      "Epoch [5/5], Step [5834/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [5836/10336], Loss: 0.0161\n",
      "Epoch [5/5], Step [5838/10336], Loss: 0.8701\n",
      "Epoch [5/5], Step [5840/10336], Loss: 0.1141\n",
      "Epoch [5/5], Step [5842/10336], Loss: 0.5757\n",
      "Epoch [5/5], Step [5844/10336], Loss: 0.0098\n",
      "Epoch [5/5], Step [5846/10336], Loss: 0.0302\n",
      "Epoch [5/5], Step [5848/10336], Loss: 0.4832\n",
      "Epoch [5/5], Step [5850/10336], Loss: 0.1756\n",
      "Epoch [5/5], Step [5852/10336], Loss: 0.3793\n",
      "Epoch [5/5], Step [5854/10336], Loss: 1.5423\n",
      "Epoch [5/5], Step [5856/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [5858/10336], Loss: 2.2134\n",
      "Epoch [5/5], Step [5860/10336], Loss: 2.1796\n",
      "Epoch [5/5], Step [5862/10336], Loss: 0.0197\n",
      "Epoch [5/5], Step [5864/10336], Loss: 0.0173\n",
      "Epoch [5/5], Step [5866/10336], Loss: 0.6831\n",
      "Epoch [5/5], Step [5868/10336], Loss: 0.0279\n",
      "Epoch [5/5], Step [5870/10336], Loss: 0.1408\n",
      "Epoch [5/5], Step [5872/10336], Loss: 0.2918\n",
      "Epoch [5/5], Step [5874/10336], Loss: 2.6891\n",
      "Epoch [5/5], Step [5876/10336], Loss: 0.2751\n",
      "Epoch [5/5], Step [5878/10336], Loss: 0.7993\n",
      "Epoch [5/5], Step [5880/10336], Loss: 0.5394\n",
      "Epoch [5/5], Step [5882/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [5884/10336], Loss: 0.3567\n",
      "Epoch [5/5], Step [5886/10336], Loss: 0.0362\n",
      "Epoch [5/5], Step [5888/10336], Loss: 1.8932\n",
      "Epoch [5/5], Step [5890/10336], Loss: 0.6846\n",
      "Epoch [5/5], Step [5892/10336], Loss: 0.0073\n",
      "Epoch [5/5], Step [5894/10336], Loss: 0.7252\n",
      "Epoch [5/5], Step [5896/10336], Loss: 0.2363\n",
      "Epoch [5/5], Step [5898/10336], Loss: 0.0775\n",
      "Epoch [5/5], Step [5900/10336], Loss: 0.0298\n",
      "Epoch [5/5], Step [5902/10336], Loss: 0.0226\n",
      "Epoch [5/5], Step [5904/10336], Loss: 0.3235\n",
      "Epoch [5/5], Step [5906/10336], Loss: 0.1217\n",
      "Epoch [5/5], Step [5908/10336], Loss: 3.0591\n",
      "Epoch [5/5], Step [5910/10336], Loss: 0.2334\n",
      "Epoch [5/5], Step [5912/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [5914/10336], Loss: 3.7134\n",
      "Epoch [5/5], Step [5916/10336], Loss: 0.0879\n",
      "Epoch [5/5], Step [5918/10336], Loss: 0.3199\n",
      "Epoch [5/5], Step [5920/10336], Loss: 0.5004\n",
      "Epoch [5/5], Step [5922/10336], Loss: 0.0327\n",
      "Epoch [5/5], Step [5924/10336], Loss: 0.3450\n",
      "Epoch [5/5], Step [5926/10336], Loss: 0.0250\n",
      "Epoch [5/5], Step [5928/10336], Loss: 0.0617\n",
      "Epoch [5/5], Step [5930/10336], Loss: 0.9005\n",
      "Epoch [5/5], Step [5932/10336], Loss: 0.0311\n",
      "Epoch [5/5], Step [5934/10336], Loss: 0.0487\n",
      "Epoch [5/5], Step [5936/10336], Loss: 0.3017\n",
      "Epoch [5/5], Step [5938/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [5940/10336], Loss: 0.0444\n",
      "Epoch [5/5], Step [5942/10336], Loss: 1.2291\n",
      "Epoch [5/5], Step [5944/10336], Loss: 0.0812\n",
      "Epoch [5/5], Step [5946/10336], Loss: 0.0910\n",
      "Epoch [5/5], Step [5948/10336], Loss: 0.4248\n",
      "Epoch [5/5], Step [5950/10336], Loss: 0.5984\n",
      "Epoch [5/5], Step [5952/10336], Loss: 0.0063\n",
      "Epoch [5/5], Step [5954/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [5956/10336], Loss: 0.2569\n",
      "Epoch [5/5], Step [5958/10336], Loss: 0.1287\n",
      "Epoch [5/5], Step [5960/10336], Loss: 0.2066\n",
      "Epoch [5/5], Step [5962/10336], Loss: 0.2214\n",
      "Epoch [5/5], Step [5964/10336], Loss: 0.0417\n",
      "Epoch [5/5], Step [5966/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [5968/10336], Loss: 2.4924\n",
      "Epoch [5/5], Step [5970/10336], Loss: 0.3152\n",
      "Epoch [5/5], Step [5972/10336], Loss: 0.2569\n",
      "Epoch [5/5], Step [5974/10336], Loss: 0.2003\n",
      "Epoch [5/5], Step [5976/10336], Loss: 0.0572\n",
      "Epoch [5/5], Step [5978/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [5980/10336], Loss: 0.1001\n",
      "Epoch [5/5], Step [5982/10336], Loss: 0.0385\n",
      "Epoch [5/5], Step [5984/10336], Loss: 2.2582\n",
      "Epoch [5/5], Step [5986/10336], Loss: 1.3619\n",
      "Epoch [5/5], Step [5988/10336], Loss: 0.2361\n",
      "Epoch [5/5], Step [5990/10336], Loss: 0.0179\n",
      "Epoch [5/5], Step [5992/10336], Loss: 0.1520\n",
      "Epoch [5/5], Step [5994/10336], Loss: 0.3270\n",
      "Epoch [5/5], Step [5996/10336], Loss: 0.0298\n",
      "Epoch [5/5], Step [5998/10336], Loss: 0.0307\n",
      "Epoch [5/5], Step [6000/10336], Loss: 0.3848\n",
      "Epoch [5/5], Step [6002/10336], Loss: 0.0251\n",
      "Epoch [5/5], Step [6004/10336], Loss: 0.7307\n",
      "Epoch [5/5], Step [6006/10336], Loss: 0.1153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [6008/10336], Loss: 0.0088\n",
      "Epoch [5/5], Step [6010/10336], Loss: 0.0422\n",
      "Epoch [5/5], Step [6012/10336], Loss: 0.2268\n",
      "Epoch [5/5], Step [6014/10336], Loss: 0.2861\n",
      "Epoch [5/5], Step [6016/10336], Loss: 3.4217\n",
      "Epoch [5/5], Step [6018/10336], Loss: 3.8566\n",
      "Epoch [5/5], Step [6020/10336], Loss: 1.7747\n",
      "Epoch [5/5], Step [6022/10336], Loss: 1.4317\n",
      "Epoch [5/5], Step [6024/10336], Loss: 0.1492\n",
      "Epoch [5/5], Step [6026/10336], Loss: 0.3775\n",
      "Epoch [5/5], Step [6028/10336], Loss: 0.1375\n",
      "Epoch [5/5], Step [6030/10336], Loss: 0.1444\n",
      "Epoch [5/5], Step [6032/10336], Loss: 1.7223\n",
      "Epoch [5/5], Step [6034/10336], Loss: 1.0760\n",
      "Epoch [5/5], Step [6036/10336], Loss: 0.5709\n",
      "Epoch [5/5], Step [6038/10336], Loss: 0.3894\n",
      "Epoch [5/5], Step [6040/10336], Loss: 1.0159\n",
      "Epoch [5/5], Step [6042/10336], Loss: 0.5090\n",
      "Epoch [5/5], Step [6044/10336], Loss: 3.6349\n",
      "Epoch [5/5], Step [6046/10336], Loss: 0.2209\n",
      "Epoch [5/5], Step [6048/10336], Loss: 0.0147\n",
      "Epoch [5/5], Step [6050/10336], Loss: 0.0130\n",
      "Epoch [5/5], Step [6052/10336], Loss: 0.2153\n",
      "Epoch [5/5], Step [6054/10336], Loss: 0.1631\n",
      "Epoch [5/5], Step [6056/10336], Loss: 0.1817\n",
      "Epoch [5/5], Step [6058/10336], Loss: 2.0477\n",
      "Epoch [5/5], Step [6060/10336], Loss: 0.1623\n",
      "Epoch [5/5], Step [6062/10336], Loss: 0.4527\n",
      "Epoch [5/5], Step [6064/10336], Loss: 0.8546\n",
      "Epoch [5/5], Step [6066/10336], Loss: 1.0903\n",
      "Epoch [5/5], Step [6068/10336], Loss: 0.0120\n",
      "Epoch [5/5], Step [6070/10336], Loss: 0.0943\n",
      "Epoch [5/5], Step [6072/10336], Loss: 0.1648\n",
      "Epoch [5/5], Step [6074/10336], Loss: 0.0171\n",
      "Epoch [5/5], Step [6076/10336], Loss: 0.8063\n",
      "Epoch [5/5], Step [6078/10336], Loss: 0.1632\n",
      "Epoch [5/5], Step [6080/10336], Loss: 0.0297\n",
      "Epoch [5/5], Step [6082/10336], Loss: 0.1980\n",
      "Epoch [5/5], Step [6084/10336], Loss: 0.1298\n",
      "Epoch [5/5], Step [6086/10336], Loss: 0.3695\n",
      "Epoch [5/5], Step [6088/10336], Loss: 0.1224\n",
      "Epoch [5/5], Step [6090/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [6092/10336], Loss: 1.1528\n",
      "Epoch [5/5], Step [6094/10336], Loss: 0.6486\n",
      "Epoch [5/5], Step [6096/10336], Loss: 1.8454\n",
      "Epoch [5/5], Step [6098/10336], Loss: 0.0386\n",
      "Epoch [5/5], Step [6100/10336], Loss: 2.6592\n",
      "Epoch [5/5], Step [6102/10336], Loss: 1.6010\n",
      "Epoch [5/5], Step [6104/10336], Loss: 0.3180\n",
      "Epoch [5/5], Step [6106/10336], Loss: 0.0679\n",
      "Epoch [5/5], Step [6108/10336], Loss: 0.2962\n",
      "Epoch [5/5], Step [6110/10336], Loss: 3.7957\n",
      "Epoch [5/5], Step [6112/10336], Loss: 4.5928\n",
      "Epoch [5/5], Step [6114/10336], Loss: 0.1244\n",
      "Epoch [5/5], Step [6116/10336], Loss: 1.5949\n",
      "Epoch [5/5], Step [6118/10336], Loss: 3.0787\n",
      "Epoch [5/5], Step [6120/10336], Loss: 0.0292\n",
      "Epoch [5/5], Step [6122/10336], Loss: 1.4805\n",
      "Epoch [5/5], Step [6124/10336], Loss: 0.1388\n",
      "Epoch [5/5], Step [6126/10336], Loss: 1.0320\n",
      "Epoch [5/5], Step [6128/10336], Loss: 0.0167\n",
      "Epoch [5/5], Step [6130/10336], Loss: 0.2540\n",
      "Epoch [5/5], Step [6132/10336], Loss: 0.0945\n",
      "Epoch [5/5], Step [6134/10336], Loss: 0.0609\n",
      "Epoch [5/5], Step [6136/10336], Loss: 0.7637\n",
      "Epoch [5/5], Step [6138/10336], Loss: 0.1030\n",
      "Epoch [5/5], Step [6140/10336], Loss: 0.7262\n",
      "Epoch [5/5], Step [6142/10336], Loss: 1.5627\n",
      "Epoch [5/5], Step [6144/10336], Loss: 0.0787\n",
      "Epoch [5/5], Step [6146/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [6148/10336], Loss: 2.5137\n",
      "Epoch [5/5], Step [6150/10336], Loss: 0.3367\n",
      "Epoch [5/5], Step [6152/10336], Loss: 3.7298\n",
      "Epoch [5/5], Step [6154/10336], Loss: 1.9733\n",
      "Epoch [5/5], Step [6156/10336], Loss: 0.0063\n",
      "Epoch [5/5], Step [6158/10336], Loss: 0.1109\n",
      "Epoch [5/5], Step [6160/10336], Loss: 0.1234\n",
      "Epoch [5/5], Step [6162/10336], Loss: 0.0786\n",
      "Epoch [5/5], Step [6164/10336], Loss: 2.1322\n",
      "Epoch [5/5], Step [6166/10336], Loss: 0.5281\n",
      "Epoch [5/5], Step [6168/10336], Loss: 0.0074\n",
      "Epoch [5/5], Step [6170/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [6172/10336], Loss: 0.3973\n",
      "Epoch [5/5], Step [6174/10336], Loss: 0.0344\n",
      "Epoch [5/5], Step [6176/10336], Loss: 0.2107\n",
      "Epoch [5/5], Step [6178/10336], Loss: 0.2076\n",
      "Epoch [5/5], Step [6180/10336], Loss: 0.0571\n",
      "Epoch [5/5], Step [6182/10336], Loss: 0.0894\n",
      "Epoch [5/5], Step [6184/10336], Loss: 0.2941\n",
      "Epoch [5/5], Step [6186/10336], Loss: 3.1363\n",
      "Epoch [5/5], Step [6188/10336], Loss: 0.1497\n",
      "Epoch [5/5], Step [6190/10336], Loss: 0.1377\n",
      "Epoch [5/5], Step [6192/10336], Loss: 0.1147\n",
      "Epoch [5/5], Step [6194/10336], Loss: 1.2885\n",
      "Epoch [5/5], Step [6196/10336], Loss: 2.0984\n",
      "Epoch [5/5], Step [6198/10336], Loss: 0.2813\n",
      "Epoch [5/5], Step [6200/10336], Loss: 0.0180\n",
      "Epoch [5/5], Step [6202/10336], Loss: 0.5682\n",
      "Epoch [5/5], Step [6204/10336], Loss: 0.2835\n",
      "Epoch [5/5], Step [6206/10336], Loss: 0.3037\n",
      "Epoch [5/5], Step [6208/10336], Loss: 0.0482\n",
      "Epoch [5/5], Step [6210/10336], Loss: 0.0279\n",
      "Epoch [5/5], Step [6212/10336], Loss: 0.0205\n",
      "Epoch [5/5], Step [6214/10336], Loss: 0.0116\n",
      "Epoch [5/5], Step [6216/10336], Loss: 0.5872\n",
      "Epoch [5/5], Step [6218/10336], Loss: 0.2258\n",
      "Epoch [5/5], Step [6220/10336], Loss: 0.0996\n",
      "Epoch [5/5], Step [6222/10336], Loss: 0.1224\n",
      "Epoch [5/5], Step [6224/10336], Loss: 0.0995\n",
      "Epoch [5/5], Step [6226/10336], Loss: 0.3131\n",
      "Epoch [5/5], Step [6228/10336], Loss: 0.0589\n",
      "Epoch [5/5], Step [6230/10336], Loss: 2.8207\n",
      "Epoch [5/5], Step [6232/10336], Loss: 0.7552\n",
      "Epoch [5/5], Step [6234/10336], Loss: 0.1369\n",
      "Epoch [5/5], Step [6236/10336], Loss: 0.7889\n",
      "Epoch [5/5], Step [6238/10336], Loss: 0.1800\n",
      "Epoch [5/5], Step [6240/10336], Loss: 1.7262\n",
      "Epoch [5/5], Step [6242/10336], Loss: 0.0420\n",
      "Epoch [5/5], Step [6244/10336], Loss: 0.0522\n",
      "Epoch [5/5], Step [6246/10336], Loss: 2.5941\n",
      "Epoch [5/5], Step [6248/10336], Loss: 0.0154\n",
      "Epoch [5/5], Step [6250/10336], Loss: 0.0684\n",
      "Epoch [5/5], Step [6252/10336], Loss: 0.2580\n",
      "Epoch [5/5], Step [6254/10336], Loss: 0.3195\n",
      "Epoch [5/5], Step [6256/10336], Loss: 1.2494\n",
      "Epoch [5/5], Step [6258/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [6260/10336], Loss: 0.4698\n",
      "Epoch [5/5], Step [6262/10336], Loss: 0.2764\n",
      "Epoch [5/5], Step [6264/10336], Loss: 2.1170\n",
      "Epoch [5/5], Step [6266/10336], Loss: 0.0399\n",
      "Epoch [5/5], Step [6268/10336], Loss: 0.1738\n",
      "Epoch [5/5], Step [6270/10336], Loss: 0.0296\n",
      "Epoch [5/5], Step [6272/10336], Loss: 0.9779\n",
      "Epoch [5/5], Step [6274/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [6276/10336], Loss: 0.0092\n",
      "Epoch [5/5], Step [6278/10336], Loss: 0.0385\n",
      "Epoch [5/5], Step [6280/10336], Loss: 0.1253\n",
      "Epoch [5/5], Step [6282/10336], Loss: 0.3089\n",
      "Epoch [5/5], Step [6284/10336], Loss: 0.0839\n",
      "Epoch [5/5], Step [6286/10336], Loss: 0.1988\n",
      "Epoch [5/5], Step [6288/10336], Loss: 0.0307\n",
      "Epoch [5/5], Step [6290/10336], Loss: 0.4101\n",
      "Epoch [5/5], Step [6292/10336], Loss: 0.9482\n",
      "Epoch [5/5], Step [6294/10336], Loss: 0.2120\n",
      "Epoch [5/5], Step [6296/10336], Loss: 0.2037\n",
      "Epoch [5/5], Step [6298/10336], Loss: 0.0155\n",
      "Epoch [5/5], Step [6300/10336], Loss: 0.0190\n",
      "Epoch [5/5], Step [6302/10336], Loss: 3.5115\n",
      "Epoch [5/5], Step [6304/10336], Loss: 0.0665\n",
      "Epoch [5/5], Step [6306/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [6308/10336], Loss: 0.1523\n",
      "Epoch [5/5], Step [6310/10336], Loss: 1.2085\n",
      "Epoch [5/5], Step [6312/10336], Loss: 0.0477\n",
      "Epoch [5/5], Step [6314/10336], Loss: 0.2632\n",
      "Epoch [5/5], Step [6316/10336], Loss: 0.6318\n",
      "Epoch [5/5], Step [6318/10336], Loss: 0.3963\n",
      "Epoch [5/5], Step [6320/10336], Loss: 0.9605\n",
      "Epoch [5/5], Step [6322/10336], Loss: 0.3086\n",
      "Epoch [5/5], Step [6324/10336], Loss: 0.5272\n",
      "Epoch [5/5], Step [6326/10336], Loss: 0.4487\n",
      "Epoch [5/5], Step [6328/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [6330/10336], Loss: 0.7368\n",
      "Epoch [5/5], Step [6332/10336], Loss: 0.1855\n",
      "Epoch [5/5], Step [6334/10336], Loss: 0.2049\n",
      "Epoch [5/5], Step [6336/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [6338/10336], Loss: 0.0842\n",
      "Epoch [5/5], Step [6340/10336], Loss: 0.1158\n",
      "Epoch [5/5], Step [6342/10336], Loss: 3.9051\n",
      "Epoch [5/5], Step [6344/10336], Loss: 0.1242\n",
      "Epoch [5/5], Step [6346/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [6348/10336], Loss: 0.0080\n",
      "Epoch [5/5], Step [6350/10336], Loss: 0.3621\n",
      "Epoch [5/5], Step [6352/10336], Loss: 0.1130\n",
      "Epoch [5/5], Step [6354/10336], Loss: 0.1405\n",
      "Epoch [5/5], Step [6356/10336], Loss: 0.0491\n",
      "Epoch [5/5], Step [6358/10336], Loss: 2.9155\n",
      "Epoch [5/5], Step [6360/10336], Loss: 0.1591\n",
      "Epoch [5/5], Step [6362/10336], Loss: 0.0598\n",
      "Epoch [5/5], Step [6364/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [6366/10336], Loss: 0.9291\n",
      "Epoch [5/5], Step [6368/10336], Loss: 0.1858\n",
      "Epoch [5/5], Step [6370/10336], Loss: 0.0868\n",
      "Epoch [5/5], Step [6372/10336], Loss: 0.2749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [6374/10336], Loss: 5.0707\n",
      "Epoch [5/5], Step [6376/10336], Loss: 0.7685\n",
      "Epoch [5/5], Step [6378/10336], Loss: 0.1151\n",
      "Epoch [5/5], Step [6380/10336], Loss: 0.0600\n",
      "Epoch [5/5], Step [6382/10336], Loss: 0.6895\n",
      "Epoch [5/5], Step [6384/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [6386/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [6388/10336], Loss: 0.7005\n",
      "Epoch [5/5], Step [6390/10336], Loss: 0.0270\n",
      "Epoch [5/5], Step [6392/10336], Loss: 0.9175\n",
      "Epoch [5/5], Step [6394/10336], Loss: 0.2098\n",
      "Epoch [5/5], Step [6396/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [6398/10336], Loss: 0.0121\n",
      "Epoch [5/5], Step [6400/10336], Loss: 0.2037\n",
      "Epoch [5/5], Step [6402/10336], Loss: 1.1045\n",
      "Epoch [5/5], Step [6404/10336], Loss: 0.0182\n",
      "Epoch [5/5], Step [6406/10336], Loss: 1.2667\n",
      "Epoch [5/5], Step [6408/10336], Loss: 1.3052\n",
      "Epoch [5/5], Step [6410/10336], Loss: 0.0461\n",
      "Epoch [5/5], Step [6412/10336], Loss: 3.5775\n",
      "Epoch [5/5], Step [6414/10336], Loss: 0.2139\n",
      "Epoch [5/5], Step [6416/10336], Loss: 0.1093\n",
      "Epoch [5/5], Step [6418/10336], Loss: 0.3570\n",
      "Epoch [5/5], Step [6420/10336], Loss: 1.3730\n",
      "Epoch [5/5], Step [6422/10336], Loss: 0.1584\n",
      "Epoch [5/5], Step [6424/10336], Loss: 0.2589\n",
      "Epoch [5/5], Step [6426/10336], Loss: 0.0122\n",
      "Epoch [5/5], Step [6428/10336], Loss: 0.8361\n",
      "Epoch [5/5], Step [6430/10336], Loss: 0.0601\n",
      "Epoch [5/5], Step [6432/10336], Loss: 2.6723\n",
      "Epoch [5/5], Step [6434/10336], Loss: 0.3007\n",
      "Epoch [5/5], Step [6436/10336], Loss: 0.1558\n",
      "Epoch [5/5], Step [6438/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [6440/10336], Loss: 0.2525\n",
      "Epoch [5/5], Step [6442/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [6444/10336], Loss: 0.0118\n",
      "Epoch [5/5], Step [6446/10336], Loss: 0.0338\n",
      "Epoch [5/5], Step [6448/10336], Loss: 0.3675\n",
      "Epoch [5/5], Step [6450/10336], Loss: 0.0089\n",
      "Epoch [5/5], Step [6452/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [6454/10336], Loss: 1.3000\n",
      "Epoch [5/5], Step [6456/10336], Loss: 0.2954\n",
      "Epoch [5/5], Step [6458/10336], Loss: 0.3774\n",
      "Epoch [5/5], Step [6460/10336], Loss: 0.6214\n",
      "Epoch [5/5], Step [6462/10336], Loss: 0.2014\n",
      "Epoch [5/5], Step [6464/10336], Loss: 0.0325\n",
      "Epoch [5/5], Step [6466/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [6468/10336], Loss: 2.8322\n",
      "Epoch [5/5], Step [6470/10336], Loss: 0.0633\n",
      "Epoch [5/5], Step [6472/10336], Loss: 0.1896\n",
      "Epoch [5/5], Step [6474/10336], Loss: 0.4192\n",
      "Epoch [5/5], Step [6476/10336], Loss: 0.0332\n",
      "Epoch [5/5], Step [6478/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [6480/10336], Loss: 1.3334\n",
      "Epoch [5/5], Step [6482/10336], Loss: 0.0115\n",
      "Epoch [5/5], Step [6484/10336], Loss: 0.5555\n",
      "Epoch [5/5], Step [6486/10336], Loss: 0.6325\n",
      "Epoch [5/5], Step [6488/10336], Loss: 0.2097\n",
      "Epoch [5/5], Step [6490/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [6492/10336], Loss: 0.0181\n",
      "Epoch [5/5], Step [6494/10336], Loss: 0.8855\n",
      "Epoch [5/5], Step [6496/10336], Loss: 1.5983\n",
      "Epoch [5/5], Step [6498/10336], Loss: 0.4070\n",
      "Epoch [5/5], Step [6500/10336], Loss: 0.4136\n",
      "Epoch [5/5], Step [6502/10336], Loss: 0.0545\n",
      "Epoch [5/5], Step [6504/10336], Loss: 0.2188\n",
      "Epoch [5/5], Step [6506/10336], Loss: 0.1228\n",
      "Epoch [5/5], Step [6508/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [6510/10336], Loss: 0.2072\n",
      "Epoch [5/5], Step [6512/10336], Loss: 1.8958\n",
      "Epoch [5/5], Step [6514/10336], Loss: 0.2862\n",
      "Epoch [5/5], Step [6516/10336], Loss: 0.6348\n",
      "Epoch [5/5], Step [6518/10336], Loss: 0.0662\n",
      "Epoch [5/5], Step [6520/10336], Loss: 0.0538\n",
      "Epoch [5/5], Step [6522/10336], Loss: 0.0348\n",
      "Epoch [5/5], Step [6524/10336], Loss: 0.1865\n",
      "Epoch [5/5], Step [6526/10336], Loss: 0.7555\n",
      "Epoch [5/5], Step [6528/10336], Loss: 0.1362\n",
      "Epoch [5/5], Step [6530/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [6532/10336], Loss: 2.5454\n",
      "Epoch [5/5], Step [6534/10336], Loss: 0.0351\n",
      "Epoch [5/5], Step [6536/10336], Loss: 0.3958\n",
      "Epoch [5/5], Step [6538/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [6540/10336], Loss: 1.0588\n",
      "Epoch [5/5], Step [6542/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [6544/10336], Loss: 1.4071\n",
      "Epoch [5/5], Step [6546/10336], Loss: 0.1903\n",
      "Epoch [5/5], Step [6548/10336], Loss: 0.0213\n",
      "Epoch [5/5], Step [6550/10336], Loss: 1.0902\n",
      "Epoch [5/5], Step [6552/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [6554/10336], Loss: 0.1527\n",
      "Epoch [5/5], Step [6556/10336], Loss: 0.1215\n",
      "Epoch [5/5], Step [6558/10336], Loss: 0.9319\n",
      "Epoch [5/5], Step [6560/10336], Loss: 0.5887\n",
      "Epoch [5/5], Step [6562/10336], Loss: 0.3322\n",
      "Epoch [5/5], Step [6564/10336], Loss: 0.3617\n",
      "Epoch [5/5], Step [6566/10336], Loss: 0.1538\n",
      "Epoch [5/5], Step [6568/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [6570/10336], Loss: 0.2316\n",
      "Epoch [5/5], Step [6572/10336], Loss: 0.0512\n",
      "Epoch [5/5], Step [6574/10336], Loss: 0.0650\n",
      "Epoch [5/5], Step [6576/10336], Loss: 0.2913\n",
      "Epoch [5/5], Step [6578/10336], Loss: 0.0071\n",
      "Epoch [5/5], Step [6580/10336], Loss: 2.9357\n",
      "Epoch [5/5], Step [6582/10336], Loss: 0.0130\n",
      "Epoch [5/5], Step [6584/10336], Loss: 1.0968\n",
      "Epoch [5/5], Step [6586/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [6588/10336], Loss: 0.5008\n",
      "Epoch [5/5], Step [6590/10336], Loss: 0.0246\n",
      "Epoch [5/5], Step [6592/10336], Loss: 2.8023\n",
      "Epoch [5/5], Step [6594/10336], Loss: 0.0578\n",
      "Epoch [5/5], Step [6596/10336], Loss: 2.5610\n",
      "Epoch [5/5], Step [6598/10336], Loss: 4.8276\n",
      "Epoch [5/5], Step [6600/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [6602/10336], Loss: 0.0512\n",
      "Epoch [5/5], Step [6604/10336], Loss: 1.2823\n",
      "Epoch [5/5], Step [6606/10336], Loss: 0.3763\n",
      "Epoch [5/5], Step [6608/10336], Loss: 0.4388\n",
      "Epoch [5/5], Step [6610/10336], Loss: 4.2840\n",
      "Epoch [5/5], Step [6612/10336], Loss: 2.0503\n",
      "Epoch [5/5], Step [6614/10336], Loss: 1.1642\n",
      "Epoch [5/5], Step [6616/10336], Loss: 2.6109\n",
      "Epoch [5/5], Step [6618/10336], Loss: 0.1239\n",
      "Epoch [5/5], Step [6620/10336], Loss: 0.0447\n",
      "Epoch [5/5], Step [6622/10336], Loss: 0.1383\n",
      "Epoch [5/5], Step [6624/10336], Loss: 0.2324\n",
      "Epoch [5/5], Step [6626/10336], Loss: 0.6558\n",
      "Epoch [5/5], Step [6628/10336], Loss: 0.1108\n",
      "Epoch [5/5], Step [6630/10336], Loss: 0.1939\n",
      "Epoch [5/5], Step [6632/10336], Loss: 0.3547\n",
      "Epoch [5/5], Step [6634/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [6636/10336], Loss: 0.0821\n",
      "Epoch [5/5], Step [6638/10336], Loss: 0.1802\n",
      "Epoch [5/5], Step [6640/10336], Loss: 0.7691\n",
      "Epoch [5/5], Step [6642/10336], Loss: 0.0885\n",
      "Epoch [5/5], Step [6644/10336], Loss: 0.7555\n",
      "Epoch [5/5], Step [6646/10336], Loss: 0.5270\n",
      "Epoch [5/5], Step [6648/10336], Loss: 0.1399\n",
      "Epoch [5/5], Step [6650/10336], Loss: 0.1538\n",
      "Epoch [5/5], Step [6652/10336], Loss: 0.2700\n",
      "Epoch [5/5], Step [6654/10336], Loss: 0.1304\n",
      "Epoch [5/5], Step [6656/10336], Loss: 0.0305\n",
      "Epoch [5/5], Step [6658/10336], Loss: 1.3923\n",
      "Epoch [5/5], Step [6660/10336], Loss: 1.4622\n",
      "Epoch [5/5], Step [6662/10336], Loss: 0.8534\n",
      "Epoch [5/5], Step [6664/10336], Loss: 0.1812\n",
      "Epoch [5/5], Step [6666/10336], Loss: 0.4259\n",
      "Epoch [5/5], Step [6668/10336], Loss: 0.2627\n",
      "Epoch [5/5], Step [6670/10336], Loss: 0.0748\n",
      "Epoch [5/5], Step [6672/10336], Loss: 2.5009\n",
      "Epoch [5/5], Step [6674/10336], Loss: 0.8010\n",
      "Epoch [5/5], Step [6676/10336], Loss: 0.1156\n",
      "Epoch [5/5], Step [6678/10336], Loss: 0.0278\n",
      "Epoch [5/5], Step [6680/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [6682/10336], Loss: 0.1175\n",
      "Epoch [5/5], Step [6684/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [6686/10336], Loss: 0.8736\n",
      "Epoch [5/5], Step [6688/10336], Loss: 0.0220\n",
      "Epoch [5/5], Step [6690/10336], Loss: 0.1819\n",
      "Epoch [5/5], Step [6692/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [6694/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [6696/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [6698/10336], Loss: 0.1084\n",
      "Epoch [5/5], Step [6700/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [6702/10336], Loss: 0.1670\n",
      "Epoch [5/5], Step [6704/10336], Loss: 0.0706\n",
      "Epoch [5/5], Step [6706/10336], Loss: 3.1794\n",
      "Epoch [5/5], Step [6708/10336], Loss: 3.1961\n",
      "Epoch [5/5], Step [6710/10336], Loss: 0.0523\n",
      "Epoch [5/5], Step [6712/10336], Loss: 1.9913\n",
      "Epoch [5/5], Step [6714/10336], Loss: 1.1859\n",
      "Epoch [5/5], Step [6716/10336], Loss: 0.3348\n",
      "Epoch [5/5], Step [6718/10336], Loss: 0.0543\n",
      "Epoch [5/5], Step [6720/10336], Loss: 0.1362\n",
      "Epoch [5/5], Step [6722/10336], Loss: 0.0174\n",
      "Epoch [5/5], Step [6724/10336], Loss: 0.1409\n",
      "Epoch [5/5], Step [6726/10336], Loss: 0.0180\n",
      "Epoch [5/5], Step [6728/10336], Loss: 0.3138\n",
      "Epoch [5/5], Step [6730/10336], Loss: 0.0167\n",
      "Epoch [5/5], Step [6732/10336], Loss: 1.6138\n",
      "Epoch [5/5], Step [6734/10336], Loss: 1.0491\n",
      "Epoch [5/5], Step [6736/10336], Loss: 0.2869\n",
      "Epoch [5/5], Step [6738/10336], Loss: 0.4910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [6740/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [6742/10336], Loss: 0.0312\n",
      "Epoch [5/5], Step [6744/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [6746/10336], Loss: 1.7348\n",
      "Epoch [5/5], Step [6748/10336], Loss: 0.0356\n",
      "Epoch [5/5], Step [6750/10336], Loss: 0.0373\n",
      "Epoch [5/5], Step [6752/10336], Loss: 0.0511\n",
      "Epoch [5/5], Step [6754/10336], Loss: 0.1463\n",
      "Epoch [5/5], Step [6756/10336], Loss: 0.1332\n",
      "Epoch [5/5], Step [6758/10336], Loss: 0.3749\n",
      "Epoch [5/5], Step [6760/10336], Loss: 1.0674\n",
      "Epoch [5/5], Step [6762/10336], Loss: 0.1522\n",
      "Epoch [5/5], Step [6764/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [6766/10336], Loss: 0.0229\n",
      "Epoch [5/5], Step [6768/10336], Loss: 5.3720\n",
      "Epoch [5/5], Step [6770/10336], Loss: 0.1245\n",
      "Epoch [5/5], Step [6772/10336], Loss: 0.2429\n",
      "Epoch [5/5], Step [6774/10336], Loss: 0.0592\n",
      "Epoch [5/5], Step [6776/10336], Loss: 0.2828\n",
      "Epoch [5/5], Step [6778/10336], Loss: 0.3469\n",
      "Epoch [5/5], Step [6780/10336], Loss: 0.0253\n",
      "Epoch [5/5], Step [6782/10336], Loss: 0.4857\n",
      "Epoch [5/5], Step [6784/10336], Loss: 0.0755\n",
      "Epoch [5/5], Step [6786/10336], Loss: 0.1962\n",
      "Epoch [5/5], Step [6788/10336], Loss: 0.0297\n",
      "Epoch [5/5], Step [6790/10336], Loss: 0.0635\n",
      "Epoch [5/5], Step [6792/10336], Loss: 0.0226\n",
      "Epoch [5/5], Step [6794/10336], Loss: 0.1939\n",
      "Epoch [5/5], Step [6796/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [6798/10336], Loss: 2.1994\n",
      "Epoch [5/5], Step [6800/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [6802/10336], Loss: 1.2387\n",
      "Epoch [5/5], Step [6804/10336], Loss: 0.4729\n",
      "Epoch [5/5], Step [6806/10336], Loss: 0.7572\n",
      "Epoch [5/5], Step [6808/10336], Loss: 0.2395\n",
      "Epoch [5/5], Step [6810/10336], Loss: 0.3786\n",
      "Epoch [5/5], Step [6812/10336], Loss: 3.6046\n",
      "Epoch [5/5], Step [6814/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [6816/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [6818/10336], Loss: 0.1901\n",
      "Epoch [5/5], Step [6820/10336], Loss: 0.3545\n",
      "Epoch [5/5], Step [6822/10336], Loss: 0.0194\n",
      "Epoch [5/5], Step [6824/10336], Loss: 0.0169\n",
      "Epoch [5/5], Step [6826/10336], Loss: 0.4555\n",
      "Epoch [5/5], Step [6828/10336], Loss: 0.0797\n",
      "Epoch [5/5], Step [6830/10336], Loss: 0.4979\n",
      "Epoch [5/5], Step [6832/10336], Loss: 0.0328\n",
      "Epoch [5/5], Step [6834/10336], Loss: 0.0322\n",
      "Epoch [5/5], Step [6836/10336], Loss: 0.0337\n",
      "Epoch [5/5], Step [6838/10336], Loss: 0.2530\n",
      "Epoch [5/5], Step [6840/10336], Loss: 1.5964\n",
      "Epoch [5/5], Step [6842/10336], Loss: 0.4743\n",
      "Epoch [5/5], Step [6844/10336], Loss: 0.1088\n",
      "Epoch [5/5], Step [6846/10336], Loss: 0.0802\n",
      "Epoch [5/5], Step [6848/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [6850/10336], Loss: 0.6463\n",
      "Epoch [5/5], Step [6852/10336], Loss: 0.0437\n",
      "Epoch [5/5], Step [6854/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [6856/10336], Loss: 0.0124\n",
      "Epoch [5/5], Step [6858/10336], Loss: 1.0749\n",
      "Epoch [5/5], Step [6860/10336], Loss: 0.4141\n",
      "Epoch [5/5], Step [6862/10336], Loss: 0.7450\n",
      "Epoch [5/5], Step [6864/10336], Loss: 2.4835\n",
      "Epoch [5/5], Step [6866/10336], Loss: 0.0516\n",
      "Epoch [5/5], Step [6868/10336], Loss: 0.0327\n",
      "Epoch [5/5], Step [6870/10336], Loss: 0.0264\n",
      "Epoch [5/5], Step [6872/10336], Loss: 0.0203\n",
      "Epoch [5/5], Step [6874/10336], Loss: 0.0384\n",
      "Epoch [5/5], Step [6876/10336], Loss: 0.4896\n",
      "Epoch [5/5], Step [6878/10336], Loss: 0.3105\n",
      "Epoch [5/5], Step [6880/10336], Loss: 0.0145\n",
      "Epoch [5/5], Step [6882/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [6884/10336], Loss: 2.1383\n",
      "Epoch [5/5], Step [6886/10336], Loss: 2.3338\n",
      "Epoch [5/5], Step [6888/10336], Loss: 2.0992\n",
      "Epoch [5/5], Step [6890/10336], Loss: 0.0429\n",
      "Epoch [5/5], Step [6892/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [6894/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [6896/10336], Loss: 4.4102\n",
      "Epoch [5/5], Step [6898/10336], Loss: 1.1795\n",
      "Epoch [5/5], Step [6900/10336], Loss: 0.1177\n",
      "Epoch [5/5], Step [6902/10336], Loss: 0.5281\n",
      "Epoch [5/5], Step [6904/10336], Loss: 0.1909\n",
      "Epoch [5/5], Step [6906/10336], Loss: 0.4188\n",
      "Epoch [5/5], Step [6908/10336], Loss: 0.8023\n",
      "Epoch [5/5], Step [6910/10336], Loss: 0.0666\n",
      "Epoch [5/5], Step [6912/10336], Loss: 0.0291\n",
      "Epoch [5/5], Step [6914/10336], Loss: 0.0135\n",
      "Epoch [5/5], Step [6916/10336], Loss: 0.0282\n",
      "Epoch [5/5], Step [6918/10336], Loss: 0.0382\n",
      "Epoch [5/5], Step [6920/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [6922/10336], Loss: 0.2719\n",
      "Epoch [5/5], Step [6924/10336], Loss: 3.2882\n",
      "Epoch [5/5], Step [6926/10336], Loss: 0.4088\n",
      "Epoch [5/5], Step [6928/10336], Loss: 0.0578\n",
      "Epoch [5/5], Step [6930/10336], Loss: 0.3086\n",
      "Epoch [5/5], Step [6932/10336], Loss: 1.0295\n",
      "Epoch [5/5], Step [6934/10336], Loss: 0.0563\n",
      "Epoch [5/5], Step [6936/10336], Loss: 0.2073\n",
      "Epoch [5/5], Step [6938/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [6940/10336], Loss: 0.6534\n",
      "Epoch [5/5], Step [6942/10336], Loss: 0.3970\n",
      "Epoch [5/5], Step [6944/10336], Loss: 2.9098\n",
      "Epoch [5/5], Step [6946/10336], Loss: 0.1295\n",
      "Epoch [5/5], Step [6948/10336], Loss: 0.0266\n",
      "Epoch [5/5], Step [6950/10336], Loss: 0.0529\n",
      "Epoch [5/5], Step [6952/10336], Loss: 0.3491\n",
      "Epoch [5/5], Step [6954/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [6956/10336], Loss: 0.1364\n",
      "Epoch [5/5], Step [6958/10336], Loss: 0.2962\n",
      "Epoch [5/5], Step [6960/10336], Loss: 2.6028\n",
      "Epoch [5/5], Step [6962/10336], Loss: 0.0702\n",
      "Epoch [5/5], Step [6964/10336], Loss: 0.4381\n",
      "Epoch [5/5], Step [6966/10336], Loss: 0.0092\n",
      "Epoch [5/5], Step [6968/10336], Loss: 0.1351\n",
      "Epoch [5/5], Step [6970/10336], Loss: 0.1906\n",
      "Epoch [5/5], Step [6972/10336], Loss: 0.2033\n",
      "Epoch [5/5], Step [6974/10336], Loss: 1.0415\n",
      "Epoch [5/5], Step [6976/10336], Loss: 0.1697\n",
      "Epoch [5/5], Step [6978/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [6980/10336], Loss: 0.5578\n",
      "Epoch [5/5], Step [6982/10336], Loss: 0.0640\n",
      "Epoch [5/5], Step [6984/10336], Loss: 0.5698\n",
      "Epoch [5/5], Step [6986/10336], Loss: 0.0751\n",
      "Epoch [5/5], Step [6988/10336], Loss: 1.2257\n",
      "Epoch [5/5], Step [6990/10336], Loss: 0.1118\n",
      "Epoch [5/5], Step [6992/10336], Loss: 1.9135\n",
      "Epoch [5/5], Step [6994/10336], Loss: 0.0186\n",
      "Epoch [5/5], Step [6996/10336], Loss: 0.6196\n",
      "Epoch [5/5], Step [6998/10336], Loss: 1.0584\n",
      "Epoch [5/5], Step [7000/10336], Loss: 0.0147\n",
      "Epoch [5/5], Step [7002/10336], Loss: 0.1142\n",
      "Epoch [5/5], Step [7004/10336], Loss: 0.0851\n",
      "Epoch [5/5], Step [7006/10336], Loss: 0.0914\n",
      "Epoch [5/5], Step [7008/10336], Loss: 0.2188\n",
      "Epoch [5/5], Step [7010/10336], Loss: 1.1625\n",
      "Epoch [5/5], Step [7012/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [7014/10336], Loss: 0.0226\n",
      "Epoch [5/5], Step [7016/10336], Loss: 0.1988\n",
      "Epoch [5/5], Step [7018/10336], Loss: 0.1140\n",
      "Epoch [5/5], Step [7020/10336], Loss: 0.8198\n",
      "Epoch [5/5], Step [7022/10336], Loss: 0.5875\n",
      "Epoch [5/5], Step [7024/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [7026/10336], Loss: 0.3909\n",
      "Epoch [5/5], Step [7028/10336], Loss: 0.0305\n",
      "Epoch [5/5], Step [7030/10336], Loss: 0.3069\n",
      "Epoch [5/5], Step [7032/10336], Loss: 2.3346\n",
      "Epoch [5/5], Step [7034/10336], Loss: 0.9910\n",
      "Epoch [5/5], Step [7036/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [7038/10336], Loss: 0.2794\n",
      "Epoch [5/5], Step [7040/10336], Loss: 0.2869\n",
      "Epoch [5/5], Step [7042/10336], Loss: 3.6868\n",
      "Epoch [5/5], Step [7044/10336], Loss: 0.2762\n",
      "Epoch [5/5], Step [7046/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [7048/10336], Loss: 0.1641\n",
      "Epoch [5/5], Step [7050/10336], Loss: 0.0049\n",
      "Epoch [5/5], Step [7052/10336], Loss: 0.2701\n",
      "Epoch [5/5], Step [7054/10336], Loss: 1.9594\n",
      "Epoch [5/5], Step [7056/10336], Loss: 0.0181\n",
      "Epoch [5/5], Step [7058/10336], Loss: 0.2790\n",
      "Epoch [5/5], Step [7060/10336], Loss: 0.1813\n",
      "Epoch [5/5], Step [7062/10336], Loss: 3.1651\n",
      "Epoch [5/5], Step [7064/10336], Loss: 1.4711\n",
      "Epoch [5/5], Step [7066/10336], Loss: 0.5324\n",
      "Epoch [5/5], Step [7068/10336], Loss: 1.4267\n",
      "Epoch [5/5], Step [7070/10336], Loss: 0.5348\n",
      "Epoch [5/5], Step [7072/10336], Loss: 0.0087\n",
      "Epoch [5/5], Step [7074/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [7076/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [7078/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [7080/10336], Loss: 0.2146\n",
      "Epoch [5/5], Step [7082/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [7084/10336], Loss: 2.5309\n",
      "Epoch [5/5], Step [7086/10336], Loss: 0.0249\n",
      "Epoch [5/5], Step [7088/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [7090/10336], Loss: 0.3505\n",
      "Epoch [5/5], Step [7092/10336], Loss: 0.4038\n",
      "Epoch [5/5], Step [7094/10336], Loss: 0.0153\n",
      "Epoch [5/5], Step [7096/10336], Loss: 0.4442\n",
      "Epoch [5/5], Step [7098/10336], Loss: 0.2718\n",
      "Epoch [5/5], Step [7100/10336], Loss: 0.0541\n",
      "Epoch [5/5], Step [7102/10336], Loss: 0.0536\n",
      "Epoch [5/5], Step [7104/10336], Loss: 0.2666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [7106/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [7108/10336], Loss: 0.0418\n",
      "Epoch [5/5], Step [7110/10336], Loss: 0.1581\n",
      "Epoch [5/5], Step [7112/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [7114/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [7116/10336], Loss: 0.0854\n",
      "Epoch [5/5], Step [7118/10336], Loss: 1.6742\n",
      "Epoch [5/5], Step [7120/10336], Loss: 0.2713\n",
      "Epoch [5/5], Step [7122/10336], Loss: 1.8993\n",
      "Epoch [5/5], Step [7124/10336], Loss: 0.1674\n",
      "Epoch [5/5], Step [7126/10336], Loss: 1.0751\n",
      "Epoch [5/5], Step [7128/10336], Loss: 0.0512\n",
      "Epoch [5/5], Step [7130/10336], Loss: 0.7882\n",
      "Epoch [5/5], Step [7132/10336], Loss: 0.1858\n",
      "Epoch [5/5], Step [7134/10336], Loss: 0.0383\n",
      "Epoch [5/5], Step [7136/10336], Loss: 0.1961\n",
      "Epoch [5/5], Step [7138/10336], Loss: 3.4631\n",
      "Epoch [5/5], Step [7140/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [7142/10336], Loss: 0.3445\n",
      "Epoch [5/5], Step [7144/10336], Loss: 1.6837\n",
      "Epoch [5/5], Step [7146/10336], Loss: 0.5036\n",
      "Epoch [5/5], Step [7148/10336], Loss: 0.0266\n",
      "Epoch [5/5], Step [7150/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [7152/10336], Loss: 1.3933\n",
      "Epoch [5/5], Step [7154/10336], Loss: 0.4716\n",
      "Epoch [5/5], Step [7156/10336], Loss: 0.0363\n",
      "Epoch [5/5], Step [7158/10336], Loss: 0.1727\n",
      "Epoch [5/5], Step [7160/10336], Loss: 0.1047\n",
      "Epoch [5/5], Step [7162/10336], Loss: 2.2909\n",
      "Epoch [5/5], Step [7164/10336], Loss: 0.0738\n",
      "Epoch [5/5], Step [7166/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [7168/10336], Loss: 1.3270\n",
      "Epoch [5/5], Step [7170/10336], Loss: 0.0502\n",
      "Epoch [5/5], Step [7172/10336], Loss: 0.0336\n",
      "Epoch [5/5], Step [7174/10336], Loss: 0.2961\n",
      "Epoch [5/5], Step [7176/10336], Loss: 0.4417\n",
      "Epoch [5/5], Step [7178/10336], Loss: 0.1193\n",
      "Epoch [5/5], Step [7180/10336], Loss: 0.0464\n",
      "Epoch [5/5], Step [7182/10336], Loss: 0.9603\n",
      "Epoch [5/5], Step [7184/10336], Loss: 0.1211\n",
      "Epoch [5/5], Step [7186/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [7188/10336], Loss: 0.0649\n",
      "Epoch [5/5], Step [7190/10336], Loss: 0.2085\n",
      "Epoch [5/5], Step [7192/10336], Loss: 0.3139\n",
      "Epoch [5/5], Step [7194/10336], Loss: 0.7968\n",
      "Epoch [5/5], Step [7196/10336], Loss: 0.0312\n",
      "Epoch [5/5], Step [7198/10336], Loss: 0.0769\n",
      "Epoch [5/5], Step [7200/10336], Loss: 0.1150\n",
      "Epoch [5/5], Step [7202/10336], Loss: 0.0373\n",
      "Epoch [5/5], Step [7204/10336], Loss: 2.3644\n",
      "Epoch [5/5], Step [7206/10336], Loss: 0.1185\n",
      "Epoch [5/5], Step [7208/10336], Loss: 0.2665\n",
      "Epoch [5/5], Step [7210/10336], Loss: 0.3079\n",
      "Epoch [5/5], Step [7212/10336], Loss: 0.0340\n",
      "Epoch [5/5], Step [7214/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [7216/10336], Loss: 0.0522\n",
      "Epoch [5/5], Step [7218/10336], Loss: 2.3554\n",
      "Epoch [5/5], Step [7220/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [7222/10336], Loss: 0.2848\n",
      "Epoch [5/5], Step [7224/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [7226/10336], Loss: 0.1002\n",
      "Epoch [5/5], Step [7228/10336], Loss: 1.5981\n",
      "Epoch [5/5], Step [7230/10336], Loss: 0.6792\n",
      "Epoch [5/5], Step [7232/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [7234/10336], Loss: 0.0273\n",
      "Epoch [5/5], Step [7236/10336], Loss: 0.2100\n",
      "Epoch [5/5], Step [7238/10336], Loss: 0.2249\n",
      "Epoch [5/5], Step [7240/10336], Loss: 0.6276\n",
      "Epoch [5/5], Step [7242/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [7244/10336], Loss: 0.0961\n",
      "Epoch [5/5], Step [7246/10336], Loss: 0.1415\n",
      "Epoch [5/5], Step [7248/10336], Loss: 0.0695\n",
      "Epoch [5/5], Step [7250/10336], Loss: 0.2799\n",
      "Epoch [5/5], Step [7252/10336], Loss: 0.0208\n",
      "Epoch [5/5], Step [7254/10336], Loss: 0.5142\n",
      "Epoch [5/5], Step [7256/10336], Loss: 0.9578\n",
      "Epoch [5/5], Step [7258/10336], Loss: 2.9526\n",
      "Epoch [5/5], Step [7260/10336], Loss: 0.0318\n",
      "Epoch [5/5], Step [7262/10336], Loss: 0.2469\n",
      "Epoch [5/5], Step [7264/10336], Loss: 0.5389\n",
      "Epoch [5/5], Step [7266/10336], Loss: 0.0337\n",
      "Epoch [5/5], Step [7268/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [7270/10336], Loss: 1.0385\n",
      "Epoch [5/5], Step [7272/10336], Loss: 0.0783\n",
      "Epoch [5/5], Step [7274/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [7276/10336], Loss: 0.5009\n",
      "Epoch [5/5], Step [7278/10336], Loss: 0.0971\n",
      "Epoch [5/5], Step [7280/10336], Loss: 0.0119\n",
      "Epoch [5/5], Step [7282/10336], Loss: 3.0307\n",
      "Epoch [5/5], Step [7284/10336], Loss: 0.0082\n",
      "Epoch [5/5], Step [7286/10336], Loss: 0.0140\n",
      "Epoch [5/5], Step [7288/10336], Loss: 0.2769\n",
      "Epoch [5/5], Step [7290/10336], Loss: 0.1459\n",
      "Epoch [5/5], Step [7292/10336], Loss: 0.0730\n",
      "Epoch [5/5], Step [7294/10336], Loss: 0.1269\n",
      "Epoch [5/5], Step [7296/10336], Loss: 0.0151\n",
      "Epoch [5/5], Step [7298/10336], Loss: 0.0606\n",
      "Epoch [5/5], Step [7300/10336], Loss: 0.2230\n",
      "Epoch [5/5], Step [7302/10336], Loss: 0.0823\n",
      "Epoch [5/5], Step [7304/10336], Loss: 0.7774\n",
      "Epoch [5/5], Step [7306/10336], Loss: 0.1834\n",
      "Epoch [5/5], Step [7308/10336], Loss: 0.8739\n",
      "Epoch [5/5], Step [7310/10336], Loss: 0.3306\n",
      "Epoch [5/5], Step [7312/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [7314/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [7316/10336], Loss: 0.5826\n",
      "Epoch [5/5], Step [7318/10336], Loss: 0.1614\n",
      "Epoch [5/5], Step [7320/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [7322/10336], Loss: 0.1477\n",
      "Epoch [5/5], Step [7324/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [7326/10336], Loss: 0.7299\n",
      "Epoch [5/5], Step [7328/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [7330/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [7332/10336], Loss: 0.4702\n",
      "Epoch [5/5], Step [7334/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [7336/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [7338/10336], Loss: 0.0873\n",
      "Epoch [5/5], Step [7340/10336], Loss: 1.4119\n",
      "Epoch [5/5], Step [7342/10336], Loss: 0.4006\n",
      "Epoch [5/5], Step [7344/10336], Loss: 0.9282\n",
      "Epoch [5/5], Step [7346/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [7348/10336], Loss: 0.1579\n",
      "Epoch [5/5], Step [7350/10336], Loss: 0.0723\n",
      "Epoch [5/5], Step [7352/10336], Loss: 0.9093\n",
      "Epoch [5/5], Step [7354/10336], Loss: 0.1858\n",
      "Epoch [5/5], Step [7356/10336], Loss: 0.2641\n",
      "Epoch [5/5], Step [7358/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [7360/10336], Loss: 0.7168\n",
      "Epoch [5/5], Step [7362/10336], Loss: 0.6826\n",
      "Epoch [5/5], Step [7364/10336], Loss: 2.6722\n",
      "Epoch [5/5], Step [7366/10336], Loss: 2.5801\n",
      "Epoch [5/5], Step [7368/10336], Loss: 0.0639\n",
      "Epoch [5/5], Step [7370/10336], Loss: 0.2811\n",
      "Epoch [5/5], Step [7372/10336], Loss: 0.1252\n",
      "Epoch [5/5], Step [7374/10336], Loss: 1.1201\n",
      "Epoch [5/5], Step [7376/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [7378/10336], Loss: 1.1449\n",
      "Epoch [5/5], Step [7380/10336], Loss: 1.5160\n",
      "Epoch [5/5], Step [7382/10336], Loss: 1.7783\n",
      "Epoch [5/5], Step [7384/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [7386/10336], Loss: 1.9270\n",
      "Epoch [5/5], Step [7388/10336], Loss: 0.2361\n",
      "Epoch [5/5], Step [7390/10336], Loss: 0.5929\n",
      "Epoch [5/5], Step [7392/10336], Loss: 0.1377\n",
      "Epoch [5/5], Step [7394/10336], Loss: 0.4001\n",
      "Epoch [5/5], Step [7396/10336], Loss: 0.0856\n",
      "Epoch [5/5], Step [7398/10336], Loss: 0.0082\n",
      "Epoch [5/5], Step [7400/10336], Loss: 0.6279\n",
      "Epoch [5/5], Step [7402/10336], Loss: 0.0484\n",
      "Epoch [5/5], Step [7404/10336], Loss: 0.1051\n",
      "Epoch [5/5], Step [7406/10336], Loss: 0.2353\n",
      "Epoch [5/5], Step [7408/10336], Loss: 0.6267\n",
      "Epoch [5/5], Step [7410/10336], Loss: 0.1354\n",
      "Epoch [5/5], Step [7412/10336], Loss: 0.3844\n",
      "Epoch [5/5], Step [7414/10336], Loss: 0.1727\n",
      "Epoch [5/5], Step [7416/10336], Loss: 0.2499\n",
      "Epoch [5/5], Step [7418/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [7420/10336], Loss: 0.7281\n",
      "Epoch [5/5], Step [7422/10336], Loss: 0.0173\n",
      "Epoch [5/5], Step [7424/10336], Loss: 0.0048\n",
      "Epoch [5/5], Step [7426/10336], Loss: 0.4562\n",
      "Epoch [5/5], Step [7428/10336], Loss: 2.6108\n",
      "Epoch [5/5], Step [7430/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [7432/10336], Loss: 0.4885\n",
      "Epoch [5/5], Step [7434/10336], Loss: 0.0839\n",
      "Epoch [5/5], Step [7436/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [7438/10336], Loss: 0.0573\n",
      "Epoch [5/5], Step [7440/10336], Loss: 0.2334\n",
      "Epoch [5/5], Step [7442/10336], Loss: 0.0231\n",
      "Epoch [5/5], Step [7444/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [7446/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [7448/10336], Loss: 0.2428\n",
      "Epoch [5/5], Step [7450/10336], Loss: 0.1602\n",
      "Epoch [5/5], Step [7452/10336], Loss: 0.7491\n",
      "Epoch [5/5], Step [7454/10336], Loss: 0.1079\n",
      "Epoch [5/5], Step [7456/10336], Loss: 0.1763\n",
      "Epoch [5/5], Step [7458/10336], Loss: 0.9595\n",
      "Epoch [5/5], Step [7460/10336], Loss: 2.9783\n",
      "Epoch [5/5], Step [7462/10336], Loss: 0.1631\n",
      "Epoch [5/5], Step [7464/10336], Loss: 1.8787\n",
      "Epoch [5/5], Step [7466/10336], Loss: 0.0613\n",
      "Epoch [5/5], Step [7468/10336], Loss: 0.2893\n",
      "Epoch [5/5], Step [7470/10336], Loss: 0.4289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [7472/10336], Loss: 0.4151\n",
      "Epoch [5/5], Step [7474/10336], Loss: 0.5779\n",
      "Epoch [5/5], Step [7476/10336], Loss: 0.0133\n",
      "Epoch [5/5], Step [7478/10336], Loss: 2.7790\n",
      "Epoch [5/5], Step [7480/10336], Loss: 0.1855\n",
      "Epoch [5/5], Step [7482/10336], Loss: 3.8689\n",
      "Epoch [5/5], Step [7484/10336], Loss: 0.0347\n",
      "Epoch [5/5], Step [7486/10336], Loss: 0.4554\n",
      "Epoch [5/5], Step [7488/10336], Loss: 0.0435\n",
      "Epoch [5/5], Step [7490/10336], Loss: 0.2121\n",
      "Epoch [5/5], Step [7492/10336], Loss: 0.0227\n",
      "Epoch [5/5], Step [7494/10336], Loss: 0.2662\n",
      "Epoch [5/5], Step [7496/10336], Loss: 0.0506\n",
      "Epoch [5/5], Step [7498/10336], Loss: 0.8409\n",
      "Epoch [5/5], Step [7500/10336], Loss: 0.2916\n",
      "Epoch [5/5], Step [7502/10336], Loss: 0.4619\n",
      "Epoch [5/5], Step [7504/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [7506/10336], Loss: 1.8557\n",
      "Epoch [5/5], Step [7508/10336], Loss: 1.8603\n",
      "Epoch [5/5], Step [7510/10336], Loss: 1.6136\n",
      "Epoch [5/5], Step [7512/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [7514/10336], Loss: 1.4098\n",
      "Epoch [5/5], Step [7516/10336], Loss: 0.1591\n",
      "Epoch [5/5], Step [7518/10336], Loss: 0.0949\n",
      "Epoch [5/5], Step [7520/10336], Loss: 0.0244\n",
      "Epoch [5/5], Step [7522/10336], Loss: 0.1909\n",
      "Epoch [5/5], Step [7524/10336], Loss: 0.2239\n",
      "Epoch [5/5], Step [7526/10336], Loss: 0.0573\n",
      "Epoch [5/5], Step [7528/10336], Loss: 0.0292\n",
      "Epoch [5/5], Step [7530/10336], Loss: 0.3587\n",
      "Epoch [5/5], Step [7532/10336], Loss: 0.0076\n",
      "Epoch [5/5], Step [7534/10336], Loss: 0.9512\n",
      "Epoch [5/5], Step [7536/10336], Loss: 0.3425\n",
      "Epoch [5/5], Step [7538/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [7540/10336], Loss: 0.6564\n",
      "Epoch [5/5], Step [7542/10336], Loss: 0.3506\n",
      "Epoch [5/5], Step [7544/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [7546/10336], Loss: 0.4693\n",
      "Epoch [5/5], Step [7548/10336], Loss: 0.4458\n",
      "Epoch [5/5], Step [7550/10336], Loss: 0.2668\n",
      "Epoch [5/5], Step [7552/10336], Loss: 0.0925\n",
      "Epoch [5/5], Step [7554/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [7556/10336], Loss: 0.2384\n",
      "Epoch [5/5], Step [7558/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [7560/10336], Loss: 0.2097\n",
      "Epoch [5/5], Step [7562/10336], Loss: 1.8047\n",
      "Epoch [5/5], Step [7564/10336], Loss: 0.1368\n",
      "Epoch [5/5], Step [7566/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [7568/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [7570/10336], Loss: 0.0842\n",
      "Epoch [5/5], Step [7572/10336], Loss: 0.0148\n",
      "Epoch [5/5], Step [7574/10336], Loss: 0.1976\n",
      "Epoch [5/5], Step [7576/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [7578/10336], Loss: 0.4786\n",
      "Epoch [5/5], Step [7580/10336], Loss: 0.1058\n",
      "Epoch [5/5], Step [7582/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [7584/10336], Loss: 0.2536\n",
      "Epoch [5/5], Step [7586/10336], Loss: 1.6604\n",
      "Epoch [5/5], Step [7588/10336], Loss: 0.0475\n",
      "Epoch [5/5], Step [7590/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [7592/10336], Loss: 0.6922\n",
      "Epoch [5/5], Step [7594/10336], Loss: 0.0711\n",
      "Epoch [5/5], Step [7596/10336], Loss: 0.0541\n",
      "Epoch [5/5], Step [7598/10336], Loss: 0.0391\n",
      "Epoch [5/5], Step [7600/10336], Loss: 0.2937\n",
      "Epoch [5/5], Step [7602/10336], Loss: 0.0873\n",
      "Epoch [5/5], Step [7604/10336], Loss: 0.0224\n",
      "Epoch [5/5], Step [7606/10336], Loss: 0.1599\n",
      "Epoch [5/5], Step [7608/10336], Loss: 0.2678\n",
      "Epoch [5/5], Step [7610/10336], Loss: 0.0232\n",
      "Epoch [5/5], Step [7612/10336], Loss: 0.0053\n",
      "Epoch [5/5], Step [7614/10336], Loss: 3.1795\n",
      "Epoch [5/5], Step [7616/10336], Loss: 0.0178\n",
      "Epoch [5/5], Step [7618/10336], Loss: 0.0763\n",
      "Epoch [5/5], Step [7620/10336], Loss: 0.3214\n",
      "Epoch [5/5], Step [7622/10336], Loss: 2.7533\n",
      "Epoch [5/5], Step [7624/10336], Loss: 0.2267\n",
      "Epoch [5/5], Step [7626/10336], Loss: 3.4170\n",
      "Epoch [5/5], Step [7628/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [7630/10336], Loss: 0.0383\n",
      "Epoch [5/5], Step [7632/10336], Loss: 0.1311\n",
      "Epoch [5/5], Step [7634/10336], Loss: 0.2605\n",
      "Epoch [5/5], Step [7636/10336], Loss: 0.1898\n",
      "Epoch [5/5], Step [7638/10336], Loss: 0.1476\n",
      "Epoch [5/5], Step [7640/10336], Loss: 0.1042\n",
      "Epoch [5/5], Step [7642/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [7644/10336], Loss: 0.1248\n",
      "Epoch [5/5], Step [7646/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [7648/10336], Loss: 1.5298\n",
      "Epoch [5/5], Step [7650/10336], Loss: 0.1332\n",
      "Epoch [5/5], Step [7652/10336], Loss: 0.0130\n",
      "Epoch [5/5], Step [7654/10336], Loss: 1.9109\n",
      "Epoch [5/5], Step [7656/10336], Loss: 0.0874\n",
      "Epoch [5/5], Step [7658/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [7660/10336], Loss: 0.3755\n",
      "Epoch [5/5], Step [7662/10336], Loss: 4.8708\n",
      "Epoch [5/5], Step [7664/10336], Loss: 0.1740\n",
      "Epoch [5/5], Step [7666/10336], Loss: 0.0557\n",
      "Epoch [5/5], Step [7668/10336], Loss: 0.1258\n",
      "Epoch [5/5], Step [7670/10336], Loss: 0.1685\n",
      "Epoch [5/5], Step [7672/10336], Loss: 0.3092\n",
      "Epoch [5/5], Step [7674/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [7676/10336], Loss: 0.5941\n",
      "Epoch [5/5], Step [7678/10336], Loss: 0.2357\n",
      "Epoch [5/5], Step [7680/10336], Loss: 0.0116\n",
      "Epoch [5/5], Step [7682/10336], Loss: 0.2037\n",
      "Epoch [5/5], Step [7684/10336], Loss: 0.3006\n",
      "Epoch [5/5], Step [7686/10336], Loss: 0.6051\n",
      "Epoch [5/5], Step [7688/10336], Loss: 0.0627\n",
      "Epoch [5/5], Step [7690/10336], Loss: 0.0119\n",
      "Epoch [5/5], Step [7692/10336], Loss: 0.3518\n",
      "Epoch [5/5], Step [7694/10336], Loss: 0.0609\n",
      "Epoch [5/5], Step [7696/10336], Loss: 0.2278\n",
      "Epoch [5/5], Step [7698/10336], Loss: 0.3978\n",
      "Epoch [5/5], Step [7700/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [7702/10336], Loss: 0.4594\n",
      "Epoch [5/5], Step [7704/10336], Loss: 0.0969\n",
      "Epoch [5/5], Step [7706/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [7708/10336], Loss: 0.1230\n",
      "Epoch [5/5], Step [7710/10336], Loss: 1.1681\n",
      "Epoch [5/5], Step [7712/10336], Loss: 0.0451\n",
      "Epoch [5/5], Step [7714/10336], Loss: 0.0106\n",
      "Epoch [5/5], Step [7716/10336], Loss: 0.2844\n",
      "Epoch [5/5], Step [7718/10336], Loss: 1.6945\n",
      "Epoch [5/5], Step [7720/10336], Loss: 0.0615\n",
      "Epoch [5/5], Step [7722/10336], Loss: 0.0797\n",
      "Epoch [5/5], Step [7724/10336], Loss: 0.0175\n",
      "Epoch [5/5], Step [7726/10336], Loss: 1.1556\n",
      "Epoch [5/5], Step [7728/10336], Loss: 1.9071\n",
      "Epoch [5/5], Step [7730/10336], Loss: 3.7252\n",
      "Epoch [5/5], Step [7732/10336], Loss: 0.5114\n",
      "Epoch [5/5], Step [7734/10336], Loss: 0.1880\n",
      "Epoch [5/5], Step [7736/10336], Loss: 2.8224\n",
      "Epoch [5/5], Step [7738/10336], Loss: 0.2533\n",
      "Epoch [5/5], Step [7740/10336], Loss: 0.0621\n",
      "Epoch [5/5], Step [7742/10336], Loss: 0.0220\n",
      "Epoch [5/5], Step [7744/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [7746/10336], Loss: 0.2197\n",
      "Epoch [5/5], Step [7748/10336], Loss: 0.0096\n",
      "Epoch [5/5], Step [7750/10336], Loss: 0.0400\n",
      "Epoch [5/5], Step [7752/10336], Loss: 2.8290\n",
      "Epoch [5/5], Step [7754/10336], Loss: 2.6442\n",
      "Epoch [5/5], Step [7756/10336], Loss: 0.0793\n",
      "Epoch [5/5], Step [7758/10336], Loss: 0.0590\n",
      "Epoch [5/5], Step [7760/10336], Loss: 2.6330\n",
      "Epoch [5/5], Step [7762/10336], Loss: 0.5293\n",
      "Epoch [5/5], Step [7764/10336], Loss: 0.0385\n",
      "Epoch [5/5], Step [7766/10336], Loss: 0.6555\n",
      "Epoch [5/5], Step [7768/10336], Loss: 1.6687\n",
      "Epoch [5/5], Step [7770/10336], Loss: 0.3928\n",
      "Epoch [5/5], Step [7772/10336], Loss: 0.0301\n",
      "Epoch [5/5], Step [7774/10336], Loss: 3.0517\n",
      "Epoch [5/5], Step [7776/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [7778/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [7780/10336], Loss: 0.2412\n",
      "Epoch [5/5], Step [7782/10336], Loss: 0.4064\n",
      "Epoch [5/5], Step [7784/10336], Loss: 0.2006\n",
      "Epoch [5/5], Step [7786/10336], Loss: 0.1972\n",
      "Epoch [5/5], Step [7788/10336], Loss: 0.4702\n",
      "Epoch [5/5], Step [7790/10336], Loss: 0.0244\n",
      "Epoch [5/5], Step [7792/10336], Loss: 3.3297\n",
      "Epoch [5/5], Step [7794/10336], Loss: 0.5546\n",
      "Epoch [5/5], Step [7796/10336], Loss: 0.5122\n",
      "Epoch [5/5], Step [7798/10336], Loss: 0.8068\n",
      "Epoch [5/5], Step [7800/10336], Loss: 0.2107\n",
      "Epoch [5/5], Step [7802/10336], Loss: 0.7462\n",
      "Epoch [5/5], Step [7804/10336], Loss: 0.0442\n",
      "Epoch [5/5], Step [7806/10336], Loss: 4.3975\n",
      "Epoch [5/5], Step [7808/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [7810/10336], Loss: 0.0991\n",
      "Epoch [5/5], Step [7812/10336], Loss: 0.0952\n",
      "Epoch [5/5], Step [7814/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [7816/10336], Loss: 0.4021\n",
      "Epoch [5/5], Step [7818/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [7820/10336], Loss: 0.1324\n",
      "Epoch [5/5], Step [7822/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [7824/10336], Loss: 0.7219\n",
      "Epoch [5/5], Step [7826/10336], Loss: 0.2661\n",
      "Epoch [5/5], Step [7828/10336], Loss: 0.0822\n",
      "Epoch [5/5], Step [7830/10336], Loss: 0.7797\n",
      "Epoch [5/5], Step [7832/10336], Loss: 1.4205\n",
      "Epoch [5/5], Step [7834/10336], Loss: 0.1742\n",
      "Epoch [5/5], Step [7836/10336], Loss: 0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [7838/10336], Loss: 0.1811\n",
      "Epoch [5/5], Step [7840/10336], Loss: 0.0168\n",
      "Epoch [5/5], Step [7842/10336], Loss: 0.6994\n",
      "Epoch [5/5], Step [7844/10336], Loss: 0.4058\n",
      "Epoch [5/5], Step [7846/10336], Loss: 0.0956\n",
      "Epoch [5/5], Step [7848/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [7850/10336], Loss: 1.3745\n",
      "Epoch [5/5], Step [7852/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [7854/10336], Loss: 0.4846\n",
      "Epoch [5/5], Step [7856/10336], Loss: 0.1718\n",
      "Epoch [5/5], Step [7858/10336], Loss: 0.2407\n",
      "Epoch [5/5], Step [7860/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [7862/10336], Loss: 0.4559\n",
      "Epoch [5/5], Step [7864/10336], Loss: 0.0347\n",
      "Epoch [5/5], Step [7866/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [7868/10336], Loss: 1.3423\n",
      "Epoch [5/5], Step [7870/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [7872/10336], Loss: 1.4066\n",
      "Epoch [5/5], Step [7874/10336], Loss: 0.2208\n",
      "Epoch [5/5], Step [7876/10336], Loss: 0.2156\n",
      "Epoch [5/5], Step [7878/10336], Loss: 0.8244\n",
      "Epoch [5/5], Step [7880/10336], Loss: 0.0390\n",
      "Epoch [5/5], Step [7882/10336], Loss: 1.2862\n",
      "Epoch [5/5], Step [7884/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [7886/10336], Loss: 0.0634\n",
      "Epoch [5/5], Step [7888/10336], Loss: 0.0574\n",
      "Epoch [5/5], Step [7890/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [7892/10336], Loss: 0.8166\n",
      "Epoch [5/5], Step [7894/10336], Loss: 0.0531\n",
      "Epoch [5/5], Step [7896/10336], Loss: 0.2861\n",
      "Epoch [5/5], Step [7898/10336], Loss: 0.0945\n",
      "Epoch [5/5], Step [7900/10336], Loss: 1.2676\n",
      "Epoch [5/5], Step [7902/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [7904/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [7906/10336], Loss: 3.1996\n",
      "Epoch [5/5], Step [7908/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [7910/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [7912/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [7914/10336], Loss: 0.4091\n",
      "Epoch [5/5], Step [7916/10336], Loss: 0.1904\n",
      "Epoch [5/5], Step [7918/10336], Loss: 0.3169\n",
      "Epoch [5/5], Step [7920/10336], Loss: 0.5553\n",
      "Epoch [5/5], Step [7922/10336], Loss: 0.0743\n",
      "Epoch [5/5], Step [7924/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [7926/10336], Loss: 0.0557\n",
      "Epoch [5/5], Step [7928/10336], Loss: 0.0213\n",
      "Epoch [5/5], Step [7930/10336], Loss: 0.0754\n",
      "Epoch [5/5], Step [7932/10336], Loss: 1.5540\n",
      "Epoch [5/5], Step [7934/10336], Loss: 0.0153\n",
      "Epoch [5/5], Step [7936/10336], Loss: 1.3806\n",
      "Epoch [5/5], Step [7938/10336], Loss: 0.4357\n",
      "Epoch [5/5], Step [7940/10336], Loss: 0.5061\n",
      "Epoch [5/5], Step [7942/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [7944/10336], Loss: 0.9751\n",
      "Epoch [5/5], Step [7946/10336], Loss: 0.0079\n",
      "Epoch [5/5], Step [7948/10336], Loss: 0.2660\n",
      "Epoch [5/5], Step [7950/10336], Loss: 0.0936\n",
      "Epoch [5/5], Step [7952/10336], Loss: 0.0245\n",
      "Epoch [5/5], Step [7954/10336], Loss: 1.4264\n",
      "Epoch [5/5], Step [7956/10336], Loss: 1.1822\n",
      "Epoch [5/5], Step [7958/10336], Loss: 0.6746\n",
      "Epoch [5/5], Step [7960/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [7962/10336], Loss: 0.2719\n",
      "Epoch [5/5], Step [7964/10336], Loss: 3.5615\n",
      "Epoch [5/5], Step [7966/10336], Loss: 0.0884\n",
      "Epoch [5/5], Step [7968/10336], Loss: 2.7821\n",
      "Epoch [5/5], Step [7970/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [7972/10336], Loss: 0.5272\n",
      "Epoch [5/5], Step [7974/10336], Loss: 0.2249\n",
      "Epoch [5/5], Step [7976/10336], Loss: 0.0098\n",
      "Epoch [5/5], Step [7978/10336], Loss: 0.1412\n",
      "Epoch [5/5], Step [7980/10336], Loss: 0.1916\n",
      "Epoch [5/5], Step [7982/10336], Loss: 0.1353\n",
      "Epoch [5/5], Step [7984/10336], Loss: 0.2929\n",
      "Epoch [5/5], Step [7986/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [7988/10336], Loss: 0.1610\n",
      "Epoch [5/5], Step [7990/10336], Loss: 0.0551\n",
      "Epoch [5/5], Step [7992/10336], Loss: 0.3178\n",
      "Epoch [5/5], Step [7994/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [7996/10336], Loss: 0.5686\n",
      "Epoch [5/5], Step [7998/10336], Loss: 1.6352\n",
      "Epoch [5/5], Step [8000/10336], Loss: 0.5358\n",
      "Epoch [5/5], Step [8002/10336], Loss: 4.0545\n",
      "Epoch [5/5], Step [8004/10336], Loss: 0.5164\n",
      "Epoch [5/5], Step [8006/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [8008/10336], Loss: 0.9267\n",
      "Epoch [5/5], Step [8010/10336], Loss: 0.1280\n",
      "Epoch [5/5], Step [8012/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [8014/10336], Loss: 1.0466\n",
      "Epoch [5/5], Step [8016/10336], Loss: 0.0159\n",
      "Epoch [5/5], Step [8018/10336], Loss: 0.2990\n",
      "Epoch [5/5], Step [8020/10336], Loss: 0.1283\n",
      "Epoch [5/5], Step [8022/10336], Loss: 0.3951\n",
      "Epoch [5/5], Step [8024/10336], Loss: 2.5900\n",
      "Epoch [5/5], Step [8026/10336], Loss: 1.8719\n",
      "Epoch [5/5], Step [8028/10336], Loss: 0.0182\n",
      "Epoch [5/5], Step [8030/10336], Loss: 0.0116\n",
      "Epoch [5/5], Step [8032/10336], Loss: 0.0168\n",
      "Epoch [5/5], Step [8034/10336], Loss: 0.0521\n",
      "Epoch [5/5], Step [8036/10336], Loss: 0.2292\n",
      "Epoch [5/5], Step [8038/10336], Loss: 0.1377\n",
      "Epoch [5/5], Step [8040/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [8042/10336], Loss: 0.2073\n",
      "Epoch [5/5], Step [8044/10336], Loss: 0.0684\n",
      "Epoch [5/5], Step [8046/10336], Loss: 0.0229\n",
      "Epoch [5/5], Step [8048/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [8050/10336], Loss: 0.4564\n",
      "Epoch [5/5], Step [8052/10336], Loss: 3.3853\n",
      "Epoch [5/5], Step [8054/10336], Loss: 0.1086\n",
      "Epoch [5/5], Step [8056/10336], Loss: 0.1940\n",
      "Epoch [5/5], Step [8058/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [8060/10336], Loss: 0.1479\n",
      "Epoch [5/5], Step [8062/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [8064/10336], Loss: 0.7798\n",
      "Epoch [5/5], Step [8066/10336], Loss: 0.1715\n",
      "Epoch [5/5], Step [8068/10336], Loss: 0.1266\n",
      "Epoch [5/5], Step [8070/10336], Loss: 0.1402\n",
      "Epoch [5/5], Step [8072/10336], Loss: 2.0272\n",
      "Epoch [5/5], Step [8074/10336], Loss: 1.5679\n",
      "Epoch [5/5], Step [8076/10336], Loss: 0.5058\n",
      "Epoch [5/5], Step [8078/10336], Loss: 0.0174\n",
      "Epoch [5/5], Step [8080/10336], Loss: 0.1665\n",
      "Epoch [5/5], Step [8082/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [8084/10336], Loss: 0.1749\n",
      "Epoch [5/5], Step [8086/10336], Loss: 1.0336\n",
      "Epoch [5/5], Step [8088/10336], Loss: 0.6399\n",
      "Epoch [5/5], Step [8090/10336], Loss: 0.6329\n",
      "Epoch [5/5], Step [8092/10336], Loss: 0.0481\n",
      "Epoch [5/5], Step [8094/10336], Loss: 0.5052\n",
      "Epoch [5/5], Step [8096/10336], Loss: 0.1003\n",
      "Epoch [5/5], Step [8098/10336], Loss: 0.3272\n",
      "Epoch [5/5], Step [8100/10336], Loss: 0.0470\n",
      "Epoch [5/5], Step [8102/10336], Loss: 0.1217\n",
      "Epoch [5/5], Step [8104/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [8106/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [8108/10336], Loss: 3.5832\n",
      "Epoch [5/5], Step [8110/10336], Loss: 0.2752\n",
      "Epoch [5/5], Step [8112/10336], Loss: 0.1344\n",
      "Epoch [5/5], Step [8114/10336], Loss: 0.3066\n",
      "Epoch [5/5], Step [8116/10336], Loss: 2.8178\n",
      "Epoch [5/5], Step [8118/10336], Loss: 0.0510\n",
      "Epoch [5/5], Step [8120/10336], Loss: 0.1050\n",
      "Epoch [5/5], Step [8122/10336], Loss: 0.4712\n",
      "Epoch [5/5], Step [8124/10336], Loss: 0.0101\n",
      "Epoch [5/5], Step [8126/10336], Loss: 0.2433\n",
      "Epoch [5/5], Step [8128/10336], Loss: 0.2542\n",
      "Epoch [5/5], Step [8130/10336], Loss: 0.0448\n",
      "Epoch [5/5], Step [8132/10336], Loss: 2.7142\n",
      "Epoch [5/5], Step [8134/10336], Loss: 0.6701\n",
      "Epoch [5/5], Step [8136/10336], Loss: 0.1258\n",
      "Epoch [5/5], Step [8138/10336], Loss: 0.0427\n",
      "Epoch [5/5], Step [8140/10336], Loss: 0.4482\n",
      "Epoch [5/5], Step [8142/10336], Loss: 0.0308\n",
      "Epoch [5/5], Step [8144/10336], Loss: 2.7870\n",
      "Epoch [5/5], Step [8146/10336], Loss: 1.8816\n",
      "Epoch [5/5], Step [8148/10336], Loss: 0.0039\n",
      "Epoch [5/5], Step [8150/10336], Loss: 0.0604\n",
      "Epoch [5/5], Step [8152/10336], Loss: 2.3053\n",
      "Epoch [5/5], Step [8154/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [8156/10336], Loss: 2.5620\n",
      "Epoch [5/5], Step [8158/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [8160/10336], Loss: 1.5679\n",
      "Epoch [5/5], Step [8162/10336], Loss: 0.0272\n",
      "Epoch [5/5], Step [8164/10336], Loss: 0.3277\n",
      "Epoch [5/5], Step [8166/10336], Loss: 1.5207\n",
      "Epoch [5/5], Step [8168/10336], Loss: 0.3565\n",
      "Epoch [5/5], Step [8170/10336], Loss: 0.4860\n",
      "Epoch [5/5], Step [8172/10336], Loss: 0.0661\n",
      "Epoch [5/5], Step [8174/10336], Loss: 0.7172\n",
      "Epoch [5/5], Step [8176/10336], Loss: 0.3332\n",
      "Epoch [5/5], Step [8178/10336], Loss: 1.1094\n",
      "Epoch [5/5], Step [8180/10336], Loss: 0.1574\n",
      "Epoch [5/5], Step [8182/10336], Loss: 0.5328\n",
      "Epoch [5/5], Step [8184/10336], Loss: 0.0093\n",
      "Epoch [5/5], Step [8186/10336], Loss: 0.2018\n",
      "Epoch [5/5], Step [8188/10336], Loss: 0.0494\n",
      "Epoch [5/5], Step [8190/10336], Loss: 0.1911\n",
      "Epoch [5/5], Step [8192/10336], Loss: 0.6309\n",
      "Epoch [5/5], Step [8194/10336], Loss: 0.2407\n",
      "Epoch [5/5], Step [8196/10336], Loss: 0.7806\n",
      "Epoch [5/5], Step [8198/10336], Loss: 0.0850\n",
      "Epoch [5/5], Step [8200/10336], Loss: 0.1247\n",
      "Epoch [5/5], Step [8202/10336], Loss: 0.0162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [8204/10336], Loss: 0.4927\n",
      "Epoch [5/5], Step [8206/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [8208/10336], Loss: 0.0099\n",
      "Epoch [5/5], Step [8210/10336], Loss: 0.1306\n",
      "Epoch [5/5], Step [8212/10336], Loss: 0.0167\n",
      "Epoch [5/5], Step [8214/10336], Loss: 0.7180\n",
      "Epoch [5/5], Step [8216/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [8218/10336], Loss: 1.6393\n",
      "Epoch [5/5], Step [8220/10336], Loss: 0.4011\n",
      "Epoch [5/5], Step [8222/10336], Loss: 1.0924\n",
      "Epoch [5/5], Step [8224/10336], Loss: 0.1060\n",
      "Epoch [5/5], Step [8226/10336], Loss: 0.2001\n",
      "Epoch [5/5], Step [8228/10336], Loss: 0.0357\n",
      "Epoch [5/5], Step [8230/10336], Loss: 0.2469\n",
      "Epoch [5/5], Step [8232/10336], Loss: 2.7791\n",
      "Epoch [5/5], Step [8234/10336], Loss: 1.3710\n",
      "Epoch [5/5], Step [8236/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [8238/10336], Loss: 0.3759\n",
      "Epoch [5/5], Step [8240/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [8242/10336], Loss: 2.6917\n",
      "Epoch [5/5], Step [8244/10336], Loss: 0.6526\n",
      "Epoch [5/5], Step [8246/10336], Loss: 0.6379\n",
      "Epoch [5/5], Step [8248/10336], Loss: 0.0532\n",
      "Epoch [5/5], Step [8250/10336], Loss: 0.0536\n",
      "Epoch [5/5], Step [8252/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [8254/10336], Loss: 0.8020\n",
      "Epoch [5/5], Step [8256/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [8258/10336], Loss: 2.1680\n",
      "Epoch [5/5], Step [8260/10336], Loss: 0.0694\n",
      "Epoch [5/5], Step [8262/10336], Loss: 0.0193\n",
      "Epoch [5/5], Step [8264/10336], Loss: 1.2411\n",
      "Epoch [5/5], Step [8266/10336], Loss: 2.0055\n",
      "Epoch [5/5], Step [8268/10336], Loss: 0.1127\n",
      "Epoch [5/5], Step [8270/10336], Loss: 0.0884\n",
      "Epoch [5/5], Step [8272/10336], Loss: 1.3397\n",
      "Epoch [5/5], Step [8274/10336], Loss: 0.2151\n",
      "Epoch [5/5], Step [8276/10336], Loss: 3.2204\n",
      "Epoch [5/5], Step [8278/10336], Loss: 1.3175\n",
      "Epoch [5/5], Step [8280/10336], Loss: 0.0543\n",
      "Epoch [5/5], Step [8282/10336], Loss: 0.0436\n",
      "Epoch [5/5], Step [8284/10336], Loss: 0.7589\n",
      "Epoch [5/5], Step [8286/10336], Loss: 1.1082\n",
      "Epoch [5/5], Step [8288/10336], Loss: 0.5669\n",
      "Epoch [5/5], Step [8290/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [8292/10336], Loss: 0.3446\n",
      "Epoch [5/5], Step [8294/10336], Loss: 0.2674\n",
      "Epoch [5/5], Step [8296/10336], Loss: 2.3159\n",
      "Epoch [5/5], Step [8298/10336], Loss: 0.9161\n",
      "Epoch [5/5], Step [8300/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [8302/10336], Loss: 0.7918\n",
      "Epoch [5/5], Step [8304/10336], Loss: 1.9103\n",
      "Epoch [5/5], Step [8306/10336], Loss: 0.3196\n",
      "Epoch [5/5], Step [8308/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [8310/10336], Loss: 0.0599\n",
      "Epoch [5/5], Step [8312/10336], Loss: 0.4105\n",
      "Epoch [5/5], Step [8314/10336], Loss: 0.0243\n",
      "Epoch [5/5], Step [8316/10336], Loss: 2.0746\n",
      "Epoch [5/5], Step [8318/10336], Loss: 0.0700\n",
      "Epoch [5/5], Step [8320/10336], Loss: 0.5405\n",
      "Epoch [5/5], Step [8322/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [8324/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [8326/10336], Loss: 0.8432\n",
      "Epoch [5/5], Step [8328/10336], Loss: 0.3309\n",
      "Epoch [5/5], Step [8330/10336], Loss: 2.1056\n",
      "Epoch [5/5], Step [8332/10336], Loss: 0.1370\n",
      "Epoch [5/5], Step [8334/10336], Loss: 0.0075\n",
      "Epoch [5/5], Step [8336/10336], Loss: 1.6300\n",
      "Epoch [5/5], Step [8338/10336], Loss: 0.2877\n",
      "Epoch [5/5], Step [8340/10336], Loss: 0.1861\n",
      "Epoch [5/5], Step [8342/10336], Loss: 1.4214\n",
      "Epoch [5/5], Step [8344/10336], Loss: 0.1945\n",
      "Epoch [5/5], Step [8346/10336], Loss: 0.2977\n",
      "Epoch [5/5], Step [8348/10336], Loss: 0.1552\n",
      "Epoch [5/5], Step [8350/10336], Loss: 0.3575\n",
      "Epoch [5/5], Step [8352/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [8354/10336], Loss: 0.4395\n",
      "Epoch [5/5], Step [8356/10336], Loss: 2.8737\n",
      "Epoch [5/5], Step [8358/10336], Loss: 0.0996\n",
      "Epoch [5/5], Step [8360/10336], Loss: 0.1987\n",
      "Epoch [5/5], Step [8362/10336], Loss: 0.0151\n",
      "Epoch [5/5], Step [8364/10336], Loss: 0.7544\n",
      "Epoch [5/5], Step [8366/10336], Loss: 0.1043\n",
      "Epoch [5/5], Step [8368/10336], Loss: 3.0632\n",
      "Epoch [5/5], Step [8370/10336], Loss: 0.3003\n",
      "Epoch [5/5], Step [8372/10336], Loss: 0.0092\n",
      "Epoch [5/5], Step [8374/10336], Loss: 0.1179\n",
      "Epoch [5/5], Step [8376/10336], Loss: 0.1412\n",
      "Epoch [5/5], Step [8378/10336], Loss: 0.0121\n",
      "Epoch [5/5], Step [8380/10336], Loss: 0.8016\n",
      "Epoch [5/5], Step [8382/10336], Loss: 0.0116\n",
      "Epoch [5/5], Step [8384/10336], Loss: 0.8370\n",
      "Epoch [5/5], Step [8386/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [8388/10336], Loss: 0.2592\n",
      "Epoch [5/5], Step [8390/10336], Loss: 0.0772\n",
      "Epoch [5/5], Step [8392/10336], Loss: 0.3998\n",
      "Epoch [5/5], Step [8394/10336], Loss: 0.0782\n",
      "Epoch [5/5], Step [8396/10336], Loss: 0.2953\n",
      "Epoch [5/5], Step [8398/10336], Loss: 0.0055\n",
      "Epoch [5/5], Step [8400/10336], Loss: 0.1062\n",
      "Epoch [5/5], Step [8402/10336], Loss: 0.9378\n",
      "Epoch [5/5], Step [8404/10336], Loss: 0.1273\n",
      "Epoch [5/5], Step [8406/10336], Loss: 0.1683\n",
      "Epoch [5/5], Step [8408/10336], Loss: 0.2475\n",
      "Epoch [5/5], Step [8410/10336], Loss: 0.2264\n",
      "Epoch [5/5], Step [8412/10336], Loss: 0.3329\n",
      "Epoch [5/5], Step [8414/10336], Loss: 0.1585\n",
      "Epoch [5/5], Step [8416/10336], Loss: 0.0152\n",
      "Epoch [5/5], Step [8418/10336], Loss: 0.2784\n",
      "Epoch [5/5], Step [8420/10336], Loss: 0.3828\n",
      "Epoch [5/5], Step [8422/10336], Loss: 0.0990\n",
      "Epoch [5/5], Step [8424/10336], Loss: 0.2658\n",
      "Epoch [5/5], Step [8426/10336], Loss: 0.0607\n",
      "Epoch [5/5], Step [8428/10336], Loss: 0.8004\n",
      "Epoch [5/5], Step [8430/10336], Loss: 0.0134\n",
      "Epoch [5/5], Step [8432/10336], Loss: 0.2398\n",
      "Epoch [5/5], Step [8434/10336], Loss: 0.6436\n",
      "Epoch [5/5], Step [8436/10336], Loss: 0.2738\n",
      "Epoch [5/5], Step [8438/10336], Loss: 0.1966\n",
      "Epoch [5/5], Step [8440/10336], Loss: 0.2664\n",
      "Epoch [5/5], Step [8442/10336], Loss: 0.3429\n",
      "Epoch [5/5], Step [8444/10336], Loss: 0.3032\n",
      "Epoch [5/5], Step [8446/10336], Loss: 0.6943\n",
      "Epoch [5/5], Step [8448/10336], Loss: 0.0784\n",
      "Epoch [5/5], Step [8450/10336], Loss: 0.1020\n",
      "Epoch [5/5], Step [8452/10336], Loss: 0.5788\n",
      "Epoch [5/5], Step [8454/10336], Loss: 0.0138\n",
      "Epoch [5/5], Step [8456/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [8458/10336], Loss: 0.2676\n",
      "Epoch [5/5], Step [8460/10336], Loss: 3.6694\n",
      "Epoch [5/5], Step [8462/10336], Loss: 0.1149\n",
      "Epoch [5/5], Step [8464/10336], Loss: 0.0107\n",
      "Epoch [5/5], Step [8466/10336], Loss: 0.1074\n",
      "Epoch [5/5], Step [8468/10336], Loss: 2.3325\n",
      "Epoch [5/5], Step [8470/10336], Loss: 0.3485\n",
      "Epoch [5/5], Step [8472/10336], Loss: 0.6967\n",
      "Epoch [5/5], Step [8474/10336], Loss: 0.2361\n",
      "Epoch [5/5], Step [8476/10336], Loss: 0.0049\n",
      "Epoch [5/5], Step [8478/10336], Loss: 0.0915\n",
      "Epoch [5/5], Step [8480/10336], Loss: 0.5098\n",
      "Epoch [5/5], Step [8482/10336], Loss: 1.2148\n",
      "Epoch [5/5], Step [8484/10336], Loss: 0.0127\n",
      "Epoch [5/5], Step [8486/10336], Loss: 0.2779\n",
      "Epoch [5/5], Step [8488/10336], Loss: 0.0648\n",
      "Epoch [5/5], Step [8490/10336], Loss: 0.8326\n",
      "Epoch [5/5], Step [8492/10336], Loss: 0.1525\n",
      "Epoch [5/5], Step [8494/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [8496/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [8498/10336], Loss: 0.6535\n",
      "Epoch [5/5], Step [8500/10336], Loss: 3.4227\n",
      "Epoch [5/5], Step [8502/10336], Loss: 0.6293\n",
      "Epoch [5/5], Step [8504/10336], Loss: 0.0164\n",
      "Epoch [5/5], Step [8506/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [8508/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [8510/10336], Loss: 1.0410\n",
      "Epoch [5/5], Step [8512/10336], Loss: 0.3747\n",
      "Epoch [5/5], Step [8514/10336], Loss: 0.1842\n",
      "Epoch [5/5], Step [8516/10336], Loss: 0.1270\n",
      "Epoch [5/5], Step [8518/10336], Loss: 0.0048\n",
      "Epoch [5/5], Step [8520/10336], Loss: 0.0398\n",
      "Epoch [5/5], Step [8522/10336], Loss: 0.1097\n",
      "Epoch [5/5], Step [8524/10336], Loss: 0.0649\n",
      "Epoch [5/5], Step [8526/10336], Loss: 1.5876\n",
      "Epoch [5/5], Step [8528/10336], Loss: 0.1597\n",
      "Epoch [5/5], Step [8530/10336], Loss: 0.3156\n",
      "Epoch [5/5], Step [8532/10336], Loss: 0.0851\n",
      "Epoch [5/5], Step [8534/10336], Loss: 0.2889\n",
      "Epoch [5/5], Step [8536/10336], Loss: 0.0056\n",
      "Epoch [5/5], Step [8538/10336], Loss: 0.0616\n",
      "Epoch [5/5], Step [8540/10336], Loss: 0.0347\n",
      "Epoch [5/5], Step [8542/10336], Loss: 1.2932\n",
      "Epoch [5/5], Step [8544/10336], Loss: 0.0324\n",
      "Epoch [5/5], Step [8546/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [8548/10336], Loss: 0.3593\n",
      "Epoch [5/5], Step [8550/10336], Loss: 0.2078\n",
      "Epoch [5/5], Step [8552/10336], Loss: 0.0094\n",
      "Epoch [5/5], Step [8554/10336], Loss: 0.3917\n",
      "Epoch [5/5], Step [8556/10336], Loss: 0.0858\n",
      "Epoch [5/5], Step [8558/10336], Loss: 0.0312\n",
      "Epoch [5/5], Step [8560/10336], Loss: 0.0776\n",
      "Epoch [5/5], Step [8562/10336], Loss: 1.7766\n",
      "Epoch [5/5], Step [8564/10336], Loss: 1.5142\n",
      "Epoch [5/5], Step [8566/10336], Loss: 0.2224\n",
      "Epoch [5/5], Step [8568/10336], Loss: 0.1293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [8570/10336], Loss: 0.0111\n",
      "Epoch [5/5], Step [8572/10336], Loss: 0.0088\n",
      "Epoch [5/5], Step [8574/10336], Loss: 0.0664\n",
      "Epoch [5/5], Step [8576/10336], Loss: 0.1472\n",
      "Epoch [5/5], Step [8578/10336], Loss: 0.6243\n",
      "Epoch [5/5], Step [8580/10336], Loss: 0.4810\n",
      "Epoch [5/5], Step [8582/10336], Loss: 0.0960\n",
      "Epoch [5/5], Step [8584/10336], Loss: 0.1280\n",
      "Epoch [5/5], Step [8586/10336], Loss: 0.7214\n",
      "Epoch [5/5], Step [8588/10336], Loss: 0.9720\n",
      "Epoch [5/5], Step [8590/10336], Loss: 0.3323\n",
      "Epoch [5/5], Step [8592/10336], Loss: 0.0090\n",
      "Epoch [5/5], Step [8594/10336], Loss: 0.1562\n",
      "Epoch [5/5], Step [8596/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [8598/10336], Loss: 0.0215\n",
      "Epoch [5/5], Step [8600/10336], Loss: 0.5614\n",
      "Epoch [5/5], Step [8602/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [8604/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [8606/10336], Loss: 0.0060\n",
      "Epoch [5/5], Step [8608/10336], Loss: 0.3742\n",
      "Epoch [5/5], Step [8610/10336], Loss: 0.2152\n",
      "Epoch [5/5], Step [8612/10336], Loss: 6.0729\n",
      "Epoch [5/5], Step [8614/10336], Loss: 0.0082\n",
      "Epoch [5/5], Step [8616/10336], Loss: 0.1973\n",
      "Epoch [5/5], Step [8618/10336], Loss: 0.0128\n",
      "Epoch [5/5], Step [8620/10336], Loss: 0.0087\n",
      "Epoch [5/5], Step [8622/10336], Loss: 0.0687\n",
      "Epoch [5/5], Step [8624/10336], Loss: 0.7383\n",
      "Epoch [5/5], Step [8626/10336], Loss: 0.3811\n",
      "Epoch [5/5], Step [8628/10336], Loss: 0.0198\n",
      "Epoch [5/5], Step [8630/10336], Loss: 1.6862\n",
      "Epoch [5/5], Step [8632/10336], Loss: 1.6277\n",
      "Epoch [5/5], Step [8634/10336], Loss: 0.2188\n",
      "Epoch [5/5], Step [8636/10336], Loss: 0.0184\n",
      "Epoch [5/5], Step [8638/10336], Loss: 0.1437\n",
      "Epoch [5/5], Step [8640/10336], Loss: 0.0214\n",
      "Epoch [5/5], Step [8642/10336], Loss: 0.0589\n",
      "Epoch [5/5], Step [8644/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [8646/10336], Loss: 0.0774\n",
      "Epoch [5/5], Step [8648/10336], Loss: 1.0489\n",
      "Epoch [5/5], Step [8650/10336], Loss: 0.0414\n",
      "Epoch [5/5], Step [8652/10336], Loss: 2.1741\n",
      "Epoch [5/5], Step [8654/10336], Loss: 0.3344\n",
      "Epoch [5/5], Step [8656/10336], Loss: 3.0120\n",
      "Epoch [5/5], Step [8658/10336], Loss: 1.7750\n",
      "Epoch [5/5], Step [8660/10336], Loss: 0.0907\n",
      "Epoch [5/5], Step [8662/10336], Loss: 0.2758\n",
      "Epoch [5/5], Step [8664/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [8666/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [8668/10336], Loss: 0.3637\n",
      "Epoch [5/5], Step [8670/10336], Loss: 4.4110\n",
      "Epoch [5/5], Step [8672/10336], Loss: 1.1764\n",
      "Epoch [5/5], Step [8674/10336], Loss: 2.5856\n",
      "Epoch [5/5], Step [8676/10336], Loss: 0.0086\n",
      "Epoch [5/5], Step [8678/10336], Loss: 0.0773\n",
      "Epoch [5/5], Step [8680/10336], Loss: 0.2307\n",
      "Epoch [5/5], Step [8682/10336], Loss: 0.3822\n",
      "Epoch [5/5], Step [8684/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [8686/10336], Loss: 0.6979\n",
      "Epoch [5/5], Step [8688/10336], Loss: 0.9777\n",
      "Epoch [5/5], Step [8690/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [8692/10336], Loss: 0.1350\n",
      "Epoch [5/5], Step [8694/10336], Loss: 0.7565\n",
      "Epoch [5/5], Step [8696/10336], Loss: 0.8082\n",
      "Epoch [5/5], Step [8698/10336], Loss: 0.0684\n",
      "Epoch [5/5], Step [8700/10336], Loss: 0.2362\n",
      "Epoch [5/5], Step [8702/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [8704/10336], Loss: 0.4768\n",
      "Epoch [5/5], Step [8706/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [8708/10336], Loss: 0.1791\n",
      "Epoch [5/5], Step [8710/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [8712/10336], Loss: 0.1741\n",
      "Epoch [5/5], Step [8714/10336], Loss: 0.0140\n",
      "Epoch [5/5], Step [8716/10336], Loss: 0.0114\n",
      "Epoch [5/5], Step [8718/10336], Loss: 0.1889\n",
      "Epoch [5/5], Step [8720/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [8722/10336], Loss: 0.5789\n",
      "Epoch [5/5], Step [8724/10336], Loss: 0.6349\n",
      "Epoch [5/5], Step [8726/10336], Loss: 0.0325\n",
      "Epoch [5/5], Step [8728/10336], Loss: 0.0498\n",
      "Epoch [5/5], Step [8730/10336], Loss: 1.0098\n",
      "Epoch [5/5], Step [8732/10336], Loss: 0.0409\n",
      "Epoch [5/5], Step [8734/10336], Loss: 0.0552\n",
      "Epoch [5/5], Step [8736/10336], Loss: 0.8709\n",
      "Epoch [5/5], Step [8738/10336], Loss: 1.8633\n",
      "Epoch [5/5], Step [8740/10336], Loss: 1.0889\n",
      "Epoch [5/5], Step [8742/10336], Loss: 0.0218\n",
      "Epoch [5/5], Step [8744/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [8746/10336], Loss: 0.2015\n",
      "Epoch [5/5], Step [8748/10336], Loss: 0.1326\n",
      "Epoch [5/5], Step [8750/10336], Loss: 0.7285\n",
      "Epoch [5/5], Step [8752/10336], Loss: 0.0367\n",
      "Epoch [5/5], Step [8754/10336], Loss: 0.0773\n",
      "Epoch [5/5], Step [8756/10336], Loss: 0.1508\n",
      "Epoch [5/5], Step [8758/10336], Loss: 0.2930\n",
      "Epoch [5/5], Step [8760/10336], Loss: 0.6503\n",
      "Epoch [5/5], Step [8762/10336], Loss: 0.9614\n",
      "Epoch [5/5], Step [8764/10336], Loss: 0.1507\n",
      "Epoch [5/5], Step [8766/10336], Loss: 0.0082\n",
      "Epoch [5/5], Step [8768/10336], Loss: 0.8804\n",
      "Epoch [5/5], Step [8770/10336], Loss: 0.2460\n",
      "Epoch [5/5], Step [8772/10336], Loss: 4.0021\n",
      "Epoch [5/5], Step [8774/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [8776/10336], Loss: 1.4216\n",
      "Epoch [5/5], Step [8778/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [8780/10336], Loss: 0.2189\n",
      "Epoch [5/5], Step [8782/10336], Loss: 0.9145\n",
      "Epoch [5/5], Step [8784/10336], Loss: 0.9360\n",
      "Epoch [5/5], Step [8786/10336], Loss: 0.0127\n",
      "Epoch [5/5], Step [8788/10336], Loss: 0.0075\n",
      "Epoch [5/5], Step [8790/10336], Loss: 0.0729\n",
      "Epoch [5/5], Step [8792/10336], Loss: 0.4299\n",
      "Epoch [5/5], Step [8794/10336], Loss: 1.1703\n",
      "Epoch [5/5], Step [8796/10336], Loss: 0.0094\n",
      "Epoch [5/5], Step [8798/10336], Loss: 0.0146\n",
      "Epoch [5/5], Step [8800/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [8802/10336], Loss: 0.4673\n",
      "Epoch [5/5], Step [8804/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [8806/10336], Loss: 0.0209\n",
      "Epoch [5/5], Step [8808/10336], Loss: 0.2070\n",
      "Epoch [5/5], Step [8810/10336], Loss: 3.2749\n",
      "Epoch [5/5], Step [8812/10336], Loss: 0.3519\n",
      "Epoch [5/5], Step [8814/10336], Loss: 0.0570\n",
      "Epoch [5/5], Step [8816/10336], Loss: 0.1260\n",
      "Epoch [5/5], Step [8818/10336], Loss: 0.4568\n",
      "Epoch [5/5], Step [8820/10336], Loss: 1.8223\n",
      "Epoch [5/5], Step [8822/10336], Loss: 0.0094\n",
      "Epoch [5/5], Step [8824/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [8826/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [8828/10336], Loss: 1.6995\n",
      "Epoch [5/5], Step [8830/10336], Loss: 0.0130\n",
      "Epoch [5/5], Step [8832/10336], Loss: 1.8166\n",
      "Epoch [5/5], Step [8834/10336], Loss: 0.4504\n",
      "Epoch [5/5], Step [8836/10336], Loss: 2.2133\n",
      "Epoch [5/5], Step [8838/10336], Loss: 4.6201\n",
      "Epoch [5/5], Step [8840/10336], Loss: 0.9201\n",
      "Epoch [5/5], Step [8842/10336], Loss: 0.0094\n",
      "Epoch [5/5], Step [8844/10336], Loss: 0.0385\n",
      "Epoch [5/5], Step [8846/10336], Loss: 0.1316\n",
      "Epoch [5/5], Step [8848/10336], Loss: 0.0488\n",
      "Epoch [5/5], Step [8850/10336], Loss: 0.0289\n",
      "Epoch [5/5], Step [8852/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [8854/10336], Loss: 1.9727\n",
      "Epoch [5/5], Step [8856/10336], Loss: 1.1515\n",
      "Epoch [5/5], Step [8858/10336], Loss: 1.1707\n",
      "Epoch [5/5], Step [8860/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [8862/10336], Loss: 1.1203\n",
      "Epoch [5/5], Step [8864/10336], Loss: 0.3565\n",
      "Epoch [5/5], Step [8866/10336], Loss: 5.0721\n",
      "Epoch [5/5], Step [8868/10336], Loss: 0.0826\n",
      "Epoch [5/5], Step [8870/10336], Loss: 0.2990\n",
      "Epoch [5/5], Step [8872/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [8874/10336], Loss: 0.2253\n",
      "Epoch [5/5], Step [8876/10336], Loss: 0.2161\n",
      "Epoch [5/5], Step [8878/10336], Loss: 1.9885\n",
      "Epoch [5/5], Step [8880/10336], Loss: 0.3064\n",
      "Epoch [5/5], Step [8882/10336], Loss: 0.2696\n",
      "Epoch [5/5], Step [8884/10336], Loss: 0.4889\n",
      "Epoch [5/5], Step [8886/10336], Loss: 0.0581\n",
      "Epoch [5/5], Step [8888/10336], Loss: 0.0457\n",
      "Epoch [5/5], Step [8890/10336], Loss: 0.2078\n",
      "Epoch [5/5], Step [8892/10336], Loss: 3.2088\n",
      "Epoch [5/5], Step [8894/10336], Loss: 0.0347\n",
      "Epoch [5/5], Step [8896/10336], Loss: 0.0704\n",
      "Epoch [5/5], Step [8898/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [8900/10336], Loss: 0.6235\n",
      "Epoch [5/5], Step [8902/10336], Loss: 0.0820\n",
      "Epoch [5/5], Step [8904/10336], Loss: 0.0148\n",
      "Epoch [5/5], Step [8906/10336], Loss: 0.5766\n",
      "Epoch [5/5], Step [8908/10336], Loss: 0.1592\n",
      "Epoch [5/5], Step [8910/10336], Loss: 0.5241\n",
      "Epoch [5/5], Step [8912/10336], Loss: 0.0731\n",
      "Epoch [5/5], Step [8914/10336], Loss: 0.0391\n",
      "Epoch [5/5], Step [8916/10336], Loss: 0.3164\n",
      "Epoch [5/5], Step [8918/10336], Loss: 0.2170\n",
      "Epoch [5/5], Step [8920/10336], Loss: 0.0102\n",
      "Epoch [5/5], Step [8922/10336], Loss: 0.0260\n",
      "Epoch [5/5], Step [8924/10336], Loss: 0.4109\n",
      "Epoch [5/5], Step [8926/10336], Loss: 0.3795\n",
      "Epoch [5/5], Step [8928/10336], Loss: 0.2143\n",
      "Epoch [5/5], Step [8930/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [8932/10336], Loss: 1.8442\n",
      "Epoch [5/5], Step [8934/10336], Loss: 0.1985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [8936/10336], Loss: 0.0256\n",
      "Epoch [5/5], Step [8938/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [8940/10336], Loss: 2.0777\n",
      "Epoch [5/5], Step [8942/10336], Loss: 0.1295\n",
      "Epoch [5/5], Step [8944/10336], Loss: 0.2642\n",
      "Epoch [5/5], Step [8946/10336], Loss: 0.5375\n",
      "Epoch [5/5], Step [8948/10336], Loss: 0.2086\n",
      "Epoch [5/5], Step [8950/10336], Loss: 0.1259\n",
      "Epoch [5/5], Step [8952/10336], Loss: 0.6505\n",
      "Epoch [5/5], Step [8954/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [8956/10336], Loss: 0.6786\n",
      "Epoch [5/5], Step [8958/10336], Loss: 0.8263\n",
      "Epoch [5/5], Step [8960/10336], Loss: 0.6160\n",
      "Epoch [5/5], Step [8962/10336], Loss: 0.7218\n",
      "Epoch [5/5], Step [8964/10336], Loss: 0.2434\n",
      "Epoch [5/5], Step [8966/10336], Loss: 0.0256\n",
      "Epoch [5/5], Step [8968/10336], Loss: 0.8705\n",
      "Epoch [5/5], Step [8970/10336], Loss: 0.0666\n",
      "Epoch [5/5], Step [8972/10336], Loss: 0.4441\n",
      "Epoch [5/5], Step [8974/10336], Loss: 0.1973\n",
      "Epoch [5/5], Step [8976/10336], Loss: 0.8906\n",
      "Epoch [5/5], Step [8978/10336], Loss: 0.0086\n",
      "Epoch [5/5], Step [8980/10336], Loss: 1.4879\n",
      "Epoch [5/5], Step [8982/10336], Loss: 0.0364\n",
      "Epoch [5/5], Step [8984/10336], Loss: 0.2536\n",
      "Epoch [5/5], Step [8986/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [8988/10336], Loss: 3.8555\n",
      "Epoch [5/5], Step [8990/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [8992/10336], Loss: 0.0713\n",
      "Epoch [5/5], Step [8994/10336], Loss: 0.1370\n",
      "Epoch [5/5], Step [8996/10336], Loss: 0.4732\n",
      "Epoch [5/5], Step [8998/10336], Loss: 0.2145\n",
      "Epoch [5/5], Step [9000/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [9002/10336], Loss: 0.0607\n",
      "Epoch [5/5], Step [9004/10336], Loss: 0.1983\n",
      "Epoch [5/5], Step [9006/10336], Loss: 0.0469\n",
      "Epoch [5/5], Step [9008/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [9010/10336], Loss: 6.2268\n",
      "Epoch [5/5], Step [9012/10336], Loss: 0.7804\n",
      "Epoch [5/5], Step [9014/10336], Loss: 0.0278\n",
      "Epoch [5/5], Step [9016/10336], Loss: 0.5944\n",
      "Epoch [5/5], Step [9018/10336], Loss: 0.6249\n",
      "Epoch [5/5], Step [9020/10336], Loss: 0.1320\n",
      "Epoch [5/5], Step [9022/10336], Loss: 0.7382\n",
      "Epoch [5/5], Step [9024/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [9026/10336], Loss: 0.3492\n",
      "Epoch [5/5], Step [9028/10336], Loss: 0.1993\n",
      "Epoch [5/5], Step [9030/10336], Loss: 0.6131\n",
      "Epoch [5/5], Step [9032/10336], Loss: 0.3668\n",
      "Epoch [5/5], Step [9034/10336], Loss: 0.1254\n",
      "Epoch [5/5], Step [9036/10336], Loss: 0.1260\n",
      "Epoch [5/5], Step [9038/10336], Loss: 0.2693\n",
      "Epoch [5/5], Step [9040/10336], Loss: 0.0579\n",
      "Epoch [5/5], Step [9042/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [9044/10336], Loss: 0.3203\n",
      "Epoch [5/5], Step [9046/10336], Loss: 2.7840\n",
      "Epoch [5/5], Step [9048/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [9050/10336], Loss: 0.0441\n",
      "Epoch [5/5], Step [9052/10336], Loss: 0.2636\n",
      "Epoch [5/5], Step [9054/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [9056/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [9058/10336], Loss: 0.0687\n",
      "Epoch [5/5], Step [9060/10336], Loss: 0.0139\n",
      "Epoch [5/5], Step [9062/10336], Loss: 0.0048\n",
      "Epoch [5/5], Step [9064/10336], Loss: 0.2195\n",
      "Epoch [5/5], Step [9066/10336], Loss: 0.0056\n",
      "Epoch [5/5], Step [9068/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [9070/10336], Loss: 0.2250\n",
      "Epoch [5/5], Step [9072/10336], Loss: 0.1197\n",
      "Epoch [5/5], Step [9074/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [9076/10336], Loss: 3.2952\n",
      "Epoch [5/5], Step [9078/10336], Loss: 0.9469\n",
      "Epoch [5/5], Step [9080/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [9082/10336], Loss: 0.0065\n",
      "Epoch [5/5], Step [9084/10336], Loss: 0.4231\n",
      "Epoch [5/5], Step [9086/10336], Loss: 0.1661\n",
      "Epoch [5/5], Step [9088/10336], Loss: 0.0082\n",
      "Epoch [5/5], Step [9090/10336], Loss: 0.0231\n",
      "Epoch [5/5], Step [9092/10336], Loss: 0.0218\n",
      "Epoch [5/5], Step [9094/10336], Loss: 0.0620\n",
      "Epoch [5/5], Step [9096/10336], Loss: 0.0211\n",
      "Epoch [5/5], Step [9098/10336], Loss: 5.5390\n",
      "Epoch [5/5], Step [9100/10336], Loss: 0.0929\n",
      "Epoch [5/5], Step [9102/10336], Loss: 0.0856\n",
      "Epoch [5/5], Step [9104/10336], Loss: 0.0122\n",
      "Epoch [5/5], Step [9106/10336], Loss: 3.8690\n",
      "Epoch [5/5], Step [9108/10336], Loss: 0.1830\n",
      "Epoch [5/5], Step [9110/10336], Loss: 0.1690\n",
      "Epoch [5/5], Step [9112/10336], Loss: 0.2881\n",
      "Epoch [5/5], Step [9114/10336], Loss: 0.0743\n",
      "Epoch [5/5], Step [9116/10336], Loss: 0.0355\n",
      "Epoch [5/5], Step [9118/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [9120/10336], Loss: 0.0285\n",
      "Epoch [5/5], Step [9122/10336], Loss: 0.1019\n",
      "Epoch [5/5], Step [9124/10336], Loss: 0.2923\n",
      "Epoch [5/5], Step [9126/10336], Loss: 0.0392\n",
      "Epoch [5/5], Step [9128/10336], Loss: 0.7940\n",
      "Epoch [5/5], Step [9130/10336], Loss: 1.3016\n",
      "Epoch [5/5], Step [9132/10336], Loss: 0.2173\n",
      "Epoch [5/5], Step [9134/10336], Loss: 0.0281\n",
      "Epoch [5/5], Step [9136/10336], Loss: 1.4453\n",
      "Epoch [5/5], Step [9138/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [9140/10336], Loss: 1.4496\n",
      "Epoch [5/5], Step [9142/10336], Loss: 0.2580\n",
      "Epoch [5/5], Step [9144/10336], Loss: 0.7422\n",
      "Epoch [5/5], Step [9146/10336], Loss: 3.6443\n",
      "Epoch [5/5], Step [9148/10336], Loss: 0.0427\n",
      "Epoch [5/5], Step [9150/10336], Loss: 0.2984\n",
      "Epoch [5/5], Step [9152/10336], Loss: 0.2238\n",
      "Epoch [5/5], Step [9154/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [9156/10336], Loss: 0.4368\n",
      "Epoch [5/5], Step [9158/10336], Loss: 3.0537\n",
      "Epoch [5/5], Step [9160/10336], Loss: 3.3218\n",
      "Epoch [5/5], Step [9162/10336], Loss: 3.0518\n",
      "Epoch [5/5], Step [9164/10336], Loss: 0.1107\n",
      "Epoch [5/5], Step [9166/10336], Loss: 0.2597\n",
      "Epoch [5/5], Step [9168/10336], Loss: 1.2206\n",
      "Epoch [5/5], Step [9170/10336], Loss: 0.0749\n",
      "Epoch [5/5], Step [9172/10336], Loss: 0.7243\n",
      "Epoch [5/5], Step [9174/10336], Loss: 0.2056\n",
      "Epoch [5/5], Step [9176/10336], Loss: 0.2646\n",
      "Epoch [5/5], Step [9178/10336], Loss: 0.1611\n",
      "Epoch [5/5], Step [9180/10336], Loss: 2.1134\n",
      "Epoch [5/5], Step [9182/10336], Loss: 0.0553\n",
      "Epoch [5/5], Step [9184/10336], Loss: 0.1660\n",
      "Epoch [5/5], Step [9186/10336], Loss: 0.5395\n",
      "Epoch [5/5], Step [9188/10336], Loss: 0.0461\n",
      "Epoch [5/5], Step [9190/10336], Loss: 0.8333\n",
      "Epoch [5/5], Step [9192/10336], Loss: 0.4670\n",
      "Epoch [5/5], Step [9194/10336], Loss: 0.0353\n",
      "Epoch [5/5], Step [9196/10336], Loss: 0.1985\n",
      "Epoch [5/5], Step [9198/10336], Loss: 0.1131\n",
      "Epoch [5/5], Step [9200/10336], Loss: 0.0101\n",
      "Epoch [5/5], Step [9202/10336], Loss: 0.5232\n",
      "Epoch [5/5], Step [9204/10336], Loss: 0.0551\n",
      "Epoch [5/5], Step [9206/10336], Loss: 0.6239\n",
      "Epoch [5/5], Step [9208/10336], Loss: 1.6359\n",
      "Epoch [5/5], Step [9210/10336], Loss: 0.5844\n",
      "Epoch [5/5], Step [9212/10336], Loss: 0.7394\n",
      "Epoch [5/5], Step [9214/10336], Loss: 0.5769\n",
      "Epoch [5/5], Step [9216/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [9218/10336], Loss: 0.8943\n",
      "Epoch [5/5], Step [9220/10336], Loss: 0.0820\n",
      "Epoch [5/5], Step [9222/10336], Loss: 0.8169\n",
      "Epoch [5/5], Step [9224/10336], Loss: 0.0609\n",
      "Epoch [5/5], Step [9226/10336], Loss: 0.7010\n",
      "Epoch [5/5], Step [9228/10336], Loss: 0.1892\n",
      "Epoch [5/5], Step [9230/10336], Loss: 0.2061\n",
      "Epoch [5/5], Step [9232/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [9234/10336], Loss: 0.1094\n",
      "Epoch [5/5], Step [9236/10336], Loss: 0.0049\n",
      "Epoch [5/5], Step [9238/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [9240/10336], Loss: 0.0066\n",
      "Epoch [5/5], Step [9242/10336], Loss: 2.9841\n",
      "Epoch [5/5], Step [9244/10336], Loss: 0.3947\n",
      "Epoch [5/5], Step [9246/10336], Loss: 0.0547\n",
      "Epoch [5/5], Step [9248/10336], Loss: 0.4279\n",
      "Epoch [5/5], Step [9250/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [9252/10336], Loss: 0.5531\n",
      "Epoch [5/5], Step [9254/10336], Loss: 0.1299\n",
      "Epoch [5/5], Step [9256/10336], Loss: 0.0259\n",
      "Epoch [5/5], Step [9258/10336], Loss: 0.1346\n",
      "Epoch [5/5], Step [9260/10336], Loss: 0.3345\n",
      "Epoch [5/5], Step [9262/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [9264/10336], Loss: 0.8544\n",
      "Epoch [5/5], Step [9266/10336], Loss: 0.1492\n",
      "Epoch [5/5], Step [9268/10336], Loss: 0.2058\n",
      "Epoch [5/5], Step [9270/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [9272/10336], Loss: 0.2213\n",
      "Epoch [5/5], Step [9274/10336], Loss: 0.0633\n",
      "Epoch [5/5], Step [9276/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [9278/10336], Loss: 0.7629\n",
      "Epoch [5/5], Step [9280/10336], Loss: 0.0545\n",
      "Epoch [5/5], Step [9282/10336], Loss: 0.1503\n",
      "Epoch [5/5], Step [9284/10336], Loss: 0.6596\n",
      "Epoch [5/5], Step [9286/10336], Loss: 0.0753\n",
      "Epoch [5/5], Step [9288/10336], Loss: 0.2045\n",
      "Epoch [5/5], Step [9290/10336], Loss: 1.2371\n",
      "Epoch [5/5], Step [9292/10336], Loss: 0.2095\n",
      "Epoch [5/5], Step [9294/10336], Loss: 1.6309\n",
      "Epoch [5/5], Step [9296/10336], Loss: 0.1546\n",
      "Epoch [5/5], Step [9298/10336], Loss: 0.1919\n",
      "Epoch [5/5], Step [9300/10336], Loss: 0.0157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [9302/10336], Loss: 1.9119\n",
      "Epoch [5/5], Step [9304/10336], Loss: 0.4244\n",
      "Epoch [5/5], Step [9306/10336], Loss: 0.3301\n",
      "Epoch [5/5], Step [9308/10336], Loss: 0.1581\n",
      "Epoch [5/5], Step [9310/10336], Loss: 1.3010\n",
      "Epoch [5/5], Step [9312/10336], Loss: 0.2469\n",
      "Epoch [5/5], Step [9314/10336], Loss: 0.0431\n",
      "Epoch [5/5], Step [9316/10336], Loss: 2.1841\n",
      "Epoch [5/5], Step [9318/10336], Loss: 0.6977\n",
      "Epoch [5/5], Step [9320/10336], Loss: 0.7704\n",
      "Epoch [5/5], Step [9322/10336], Loss: 0.2144\n",
      "Epoch [5/5], Step [9324/10336], Loss: 0.2576\n",
      "Epoch [5/5], Step [9326/10336], Loss: 0.2297\n",
      "Epoch [5/5], Step [9328/10336], Loss: 1.1620\n",
      "Epoch [5/5], Step [9330/10336], Loss: 0.2310\n",
      "Epoch [5/5], Step [9332/10336], Loss: 0.2055\n",
      "Epoch [5/5], Step [9334/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [9336/10336], Loss: 0.0207\n",
      "Epoch [5/5], Step [9338/10336], Loss: 5.5740\n",
      "Epoch [5/5], Step [9340/10336], Loss: 0.7589\n",
      "Epoch [5/5], Step [9342/10336], Loss: 4.1977\n",
      "Epoch [5/5], Step [9344/10336], Loss: 0.8024\n",
      "Epoch [5/5], Step [9346/10336], Loss: 0.0227\n",
      "Epoch [5/5], Step [9348/10336], Loss: 0.0126\n",
      "Epoch [5/5], Step [9350/10336], Loss: 0.5397\n",
      "Epoch [5/5], Step [9352/10336], Loss: 0.2649\n",
      "Epoch [5/5], Step [9354/10336], Loss: 0.5682\n",
      "Epoch [5/5], Step [9356/10336], Loss: 0.0802\n",
      "Epoch [5/5], Step [9358/10336], Loss: 0.2336\n",
      "Epoch [5/5], Step [9360/10336], Loss: 0.0660\n",
      "Epoch [5/5], Step [9362/10336], Loss: 1.2978\n",
      "Epoch [5/5], Step [9364/10336], Loss: 0.0513\n",
      "Epoch [5/5], Step [9366/10336], Loss: 0.7614\n",
      "Epoch [5/5], Step [9368/10336], Loss: 0.0775\n",
      "Epoch [5/5], Step [9370/10336], Loss: 0.0480\n",
      "Epoch [5/5], Step [9372/10336], Loss: 0.3660\n",
      "Epoch [5/5], Step [9374/10336], Loss: 0.0096\n",
      "Epoch [5/5], Step [9376/10336], Loss: 0.9121\n",
      "Epoch [5/5], Step [9378/10336], Loss: 0.1642\n",
      "Epoch [5/5], Step [9380/10336], Loss: 0.9434\n",
      "Epoch [5/5], Step [9382/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [9384/10336], Loss: 0.0286\n",
      "Epoch [5/5], Step [9386/10336], Loss: 0.7589\n",
      "Epoch [5/5], Step [9388/10336], Loss: 0.4014\n",
      "Epoch [5/5], Step [9390/10336], Loss: 2.2176\n",
      "Epoch [5/5], Step [9392/10336], Loss: 7.9901\n",
      "Epoch [5/5], Step [9394/10336], Loss: 0.3082\n",
      "Epoch [5/5], Step [9396/10336], Loss: 0.1083\n",
      "Epoch [5/5], Step [9398/10336], Loss: 0.9245\n",
      "Epoch [5/5], Step [9400/10336], Loss: 1.1722\n",
      "Epoch [5/5], Step [9402/10336], Loss: 0.4916\n",
      "Epoch [5/5], Step [9404/10336], Loss: 1.8951\n",
      "Epoch [5/5], Step [9406/10336], Loss: 1.5673\n",
      "Epoch [5/5], Step [9408/10336], Loss: 0.0088\n",
      "Epoch [5/5], Step [9410/10336], Loss: 0.0141\n",
      "Epoch [5/5], Step [9412/10336], Loss: 3.6806\n",
      "Epoch [5/5], Step [9414/10336], Loss: 0.1448\n",
      "Epoch [5/5], Step [9416/10336], Loss: 0.0141\n",
      "Epoch [5/5], Step [9418/10336], Loss: 0.1113\n",
      "Epoch [5/5], Step [9420/10336], Loss: 0.0138\n",
      "Epoch [5/5], Step [9422/10336], Loss: 0.3102\n",
      "Epoch [5/5], Step [9424/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [9426/10336], Loss: 0.0640\n",
      "Epoch [5/5], Step [9428/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [9430/10336], Loss: 0.5826\n",
      "Epoch [5/5], Step [9432/10336], Loss: 4.7032\n",
      "Epoch [5/5], Step [9434/10336], Loss: 0.3363\n",
      "Epoch [5/5], Step [9436/10336], Loss: 0.0128\n",
      "Epoch [5/5], Step [9438/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [9440/10336], Loss: 0.0148\n",
      "Epoch [5/5], Step [9442/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [9444/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [9446/10336], Loss: 0.4846\n",
      "Epoch [5/5], Step [9448/10336], Loss: 3.4425\n",
      "Epoch [5/5], Step [9450/10336], Loss: 1.0611\n",
      "Epoch [5/5], Step [9452/10336], Loss: 2.5277\n",
      "Epoch [5/5], Step [9454/10336], Loss: 0.0447\n",
      "Epoch [5/5], Step [9456/10336], Loss: 0.0992\n",
      "Epoch [5/5], Step [9458/10336], Loss: 0.1765\n",
      "Epoch [5/5], Step [9460/10336], Loss: 0.1220\n",
      "Epoch [5/5], Step [9462/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [9464/10336], Loss: 4.2274\n",
      "Epoch [5/5], Step [9466/10336], Loss: 0.1117\n",
      "Epoch [5/5], Step [9468/10336], Loss: 0.0134\n",
      "Epoch [5/5], Step [9470/10336], Loss: 0.5556\n",
      "Epoch [5/5], Step [9472/10336], Loss: 0.1502\n",
      "Epoch [5/5], Step [9474/10336], Loss: 0.2233\n",
      "Epoch [5/5], Step [9476/10336], Loss: 0.1817\n",
      "Epoch [5/5], Step [9478/10336], Loss: 0.0170\n",
      "Epoch [5/5], Step [9480/10336], Loss: 0.0760\n",
      "Epoch [5/5], Step [9482/10336], Loss: 0.2654\n",
      "Epoch [5/5], Step [9484/10336], Loss: 0.0086\n",
      "Epoch [5/5], Step [9486/10336], Loss: 0.0190\n",
      "Epoch [5/5], Step [9488/10336], Loss: 2.9609\n",
      "Epoch [5/5], Step [9490/10336], Loss: 0.1649\n",
      "Epoch [5/5], Step [9492/10336], Loss: 0.0055\n",
      "Epoch [5/5], Step [9494/10336], Loss: 0.1108\n",
      "Epoch [5/5], Step [9496/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [9498/10336], Loss: 10.7561\n",
      "Epoch [5/5], Step [9500/10336], Loss: 2.8201\n",
      "Epoch [5/5], Step [9502/10336], Loss: 0.3476\n",
      "Epoch [5/5], Step [9504/10336], Loss: 0.1771\n",
      "Epoch [5/5], Step [9506/10336], Loss: 2.2955\n",
      "Epoch [5/5], Step [9508/10336], Loss: 0.1480\n",
      "Epoch [5/5], Step [9510/10336], Loss: 0.1148\n",
      "Epoch [5/5], Step [9512/10336], Loss: 0.1121\n",
      "Epoch [5/5], Step [9514/10336], Loss: 0.0066\n",
      "Epoch [5/5], Step [9516/10336], Loss: 3.5403\n",
      "Epoch [5/5], Step [9518/10336], Loss: 0.0066\n",
      "Epoch [5/5], Step [9520/10336], Loss: 0.3077\n",
      "Epoch [5/5], Step [9522/10336], Loss: 0.0625\n",
      "Epoch [5/5], Step [9524/10336], Loss: 1.6318\n",
      "Epoch [5/5], Step [9526/10336], Loss: 0.0742\n",
      "Epoch [5/5], Step [9528/10336], Loss: 0.5785\n",
      "Epoch [5/5], Step [9530/10336], Loss: 0.0337\n",
      "Epoch [5/5], Step [9532/10336], Loss: 0.0194\n",
      "Epoch [5/5], Step [9534/10336], Loss: 0.5103\n",
      "Epoch [5/5], Step [9536/10336], Loss: 0.2927\n",
      "Epoch [5/5], Step [9538/10336], Loss: 0.4285\n",
      "Epoch [5/5], Step [9540/10336], Loss: 0.3358\n",
      "Epoch [5/5], Step [9542/10336], Loss: 0.8356\n",
      "Epoch [5/5], Step [9544/10336], Loss: 0.0914\n",
      "Epoch [5/5], Step [9546/10336], Loss: 0.2246\n",
      "Epoch [5/5], Step [9548/10336], Loss: 1.1793\n",
      "Epoch [5/5], Step [9550/10336], Loss: 2.3010\n",
      "Epoch [5/5], Step [9552/10336], Loss: 0.1400\n",
      "Epoch [5/5], Step [9554/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [9556/10336], Loss: 0.0497\n",
      "Epoch [5/5], Step [9558/10336], Loss: 0.1263\n",
      "Epoch [5/5], Step [9560/10336], Loss: 0.2301\n",
      "Epoch [5/5], Step [9562/10336], Loss: 0.3045\n",
      "Epoch [5/5], Step [9564/10336], Loss: 0.2422\n",
      "Epoch [5/5], Step [9566/10336], Loss: 0.0555\n",
      "Epoch [5/5], Step [9568/10336], Loss: 0.2708\n",
      "Epoch [5/5], Step [9570/10336], Loss: 0.3858\n",
      "Epoch [5/5], Step [9572/10336], Loss: 0.4281\n",
      "Epoch [5/5], Step [9574/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [9576/10336], Loss: 0.0368\n",
      "Epoch [5/5], Step [9578/10336], Loss: 0.0950\n",
      "Epoch [5/5], Step [9580/10336], Loss: 3.9727\n",
      "Epoch [5/5], Step [9582/10336], Loss: 0.0564\n",
      "Epoch [5/5], Step [9584/10336], Loss: 0.0866\n",
      "Epoch [5/5], Step [9586/10336], Loss: 0.3411\n",
      "Epoch [5/5], Step [9588/10336], Loss: 0.1975\n",
      "Epoch [5/5], Step [9590/10336], Loss: 0.0666\n",
      "Epoch [5/5], Step [9592/10336], Loss: 0.1664\n",
      "Epoch [5/5], Step [9594/10336], Loss: 0.0892\n",
      "Epoch [5/5], Step [9596/10336], Loss: 0.0089\n",
      "Epoch [5/5], Step [9598/10336], Loss: 1.2681\n",
      "Epoch [5/5], Step [9600/10336], Loss: 0.1394\n",
      "Epoch [5/5], Step [9602/10336], Loss: 0.0515\n",
      "Epoch [5/5], Step [9604/10336], Loss: 0.1909\n",
      "Epoch [5/5], Step [9606/10336], Loss: 0.0192\n",
      "Epoch [5/5], Step [9608/10336], Loss: 0.0095\n",
      "Epoch [5/5], Step [9610/10336], Loss: 0.0943\n",
      "Epoch [5/5], Step [9612/10336], Loss: 0.9412\n",
      "Epoch [5/5], Step [9614/10336], Loss: 0.8337\n",
      "Epoch [5/5], Step [9616/10336], Loss: 6.4052\n",
      "Epoch [5/5], Step [9618/10336], Loss: 0.7052\n",
      "Epoch [5/5], Step [9620/10336], Loss: 2.0995\n",
      "Epoch [5/5], Step [9622/10336], Loss: 0.3360\n",
      "Epoch [5/5], Step [9624/10336], Loss: 0.3733\n",
      "Epoch [5/5], Step [9626/10336], Loss: 0.0375\n",
      "Epoch [5/5], Step [9628/10336], Loss: 0.0208\n",
      "Epoch [5/5], Step [9630/10336], Loss: 0.0303\n",
      "Epoch [5/5], Step [9632/10336], Loss: 0.5868\n",
      "Epoch [5/5], Step [9634/10336], Loss: 0.2321\n",
      "Epoch [5/5], Step [9636/10336], Loss: 0.1192\n",
      "Epoch [5/5], Step [9638/10336], Loss: 0.2669\n",
      "Epoch [5/5], Step [9640/10336], Loss: 0.5504\n",
      "Epoch [5/5], Step [9642/10336], Loss: 0.1638\n",
      "Epoch [5/5], Step [9644/10336], Loss: 1.0290\n",
      "Epoch [5/5], Step [9646/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [9648/10336], Loss: 3.4051\n",
      "Epoch [5/5], Step [9650/10336], Loss: 0.0352\n",
      "Epoch [5/5], Step [9652/10336], Loss: 0.2454\n",
      "Epoch [5/5], Step [9654/10336], Loss: 0.0719\n",
      "Epoch [5/5], Step [9656/10336], Loss: 0.8936\n",
      "Epoch [5/5], Step [9658/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [9660/10336], Loss: 0.0774\n",
      "Epoch [5/5], Step [9662/10336], Loss: 0.1670\n",
      "Epoch [5/5], Step [9664/10336], Loss: 0.0748\n",
      "Epoch [5/5], Step [9666/10336], Loss: 0.0191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [9668/10336], Loss: 0.4996\n",
      "Epoch [5/5], Step [9670/10336], Loss: 0.1867\n",
      "Epoch [5/5], Step [9672/10336], Loss: 0.0076\n",
      "Epoch [5/5], Step [9674/10336], Loss: 0.1235\n",
      "Epoch [5/5], Step [9676/10336], Loss: 0.0286\n",
      "Epoch [5/5], Step [9678/10336], Loss: 0.0157\n",
      "Epoch [5/5], Step [9680/10336], Loss: 0.2443\n",
      "Epoch [5/5], Step [9682/10336], Loss: 0.1025\n",
      "Epoch [5/5], Step [9684/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [9686/10336], Loss: 3.8591\n",
      "Epoch [5/5], Step [9688/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [9690/10336], Loss: 0.4858\n",
      "Epoch [5/5], Step [9692/10336], Loss: 1.7840\n",
      "Epoch [5/5], Step [9694/10336], Loss: 0.1844\n",
      "Epoch [5/5], Step [9696/10336], Loss: 1.1153\n",
      "Epoch [5/5], Step [9698/10336], Loss: 0.0537\n",
      "Epoch [5/5], Step [9700/10336], Loss: 0.6728\n",
      "Epoch [5/5], Step [9702/10336], Loss: 2.5963\n",
      "Epoch [5/5], Step [9704/10336], Loss: 0.1893\n",
      "Epoch [5/5], Step [9706/10336], Loss: 0.4922\n",
      "Epoch [5/5], Step [9708/10336], Loss: 1.0441\n",
      "Epoch [5/5], Step [9710/10336], Loss: 0.1411\n",
      "Epoch [5/5], Step [9712/10336], Loss: 0.2730\n",
      "Epoch [5/5], Step [9714/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [9716/10336], Loss: 0.8329\n",
      "Epoch [5/5], Step [9718/10336], Loss: 0.1516\n",
      "Epoch [5/5], Step [9720/10336], Loss: 0.3560\n",
      "Epoch [5/5], Step [9722/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [9724/10336], Loss: 0.1691\n",
      "Epoch [5/5], Step [9726/10336], Loss: 0.7840\n",
      "Epoch [5/5], Step [9728/10336], Loss: 0.1393\n",
      "Epoch [5/5], Step [9730/10336], Loss: 0.3101\n",
      "Epoch [5/5], Step [9732/10336], Loss: 3.7306\n",
      "Epoch [5/5], Step [9734/10336], Loss: 0.0293\n",
      "Epoch [5/5], Step [9736/10336], Loss: 0.7574\n",
      "Epoch [5/5], Step [9738/10336], Loss: 0.2412\n",
      "Epoch [5/5], Step [9740/10336], Loss: 2.7406\n",
      "Epoch [5/5], Step [9742/10336], Loss: 0.0642\n",
      "Epoch [5/5], Step [9744/10336], Loss: 0.7530\n",
      "Epoch [5/5], Step [9746/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [9748/10336], Loss: 0.1510\n",
      "Epoch [5/5], Step [9750/10336], Loss: 0.3847\n",
      "Epoch [5/5], Step [9752/10336], Loss: 0.0750\n",
      "Epoch [5/5], Step [9754/10336], Loss: 0.1649\n",
      "Epoch [5/5], Step [9756/10336], Loss: 0.0254\n",
      "Epoch [5/5], Step [9758/10336], Loss: 1.0214\n",
      "Epoch [5/5], Step [9760/10336], Loss: 0.5713\n",
      "Epoch [5/5], Step [9762/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [9764/10336], Loss: 0.7885\n",
      "Epoch [5/5], Step [9766/10336], Loss: 1.0724\n",
      "Epoch [5/5], Step [9768/10336], Loss: 0.0088\n",
      "Epoch [5/5], Step [9770/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [9772/10336], Loss: 3.4633\n",
      "Epoch [5/5], Step [9774/10336], Loss: 0.1235\n",
      "Epoch [5/5], Step [9776/10336], Loss: 0.0959\n",
      "Epoch [5/5], Step [9778/10336], Loss: 0.6718\n",
      "Epoch [5/5], Step [9780/10336], Loss: 0.0939\n",
      "Epoch [5/5], Step [9782/10336], Loss: 1.8160\n",
      "Epoch [5/5], Step [9784/10336], Loss: 0.4340\n",
      "Epoch [5/5], Step [9786/10336], Loss: 4.9933\n",
      "Epoch [5/5], Step [9788/10336], Loss: 3.5245\n",
      "Epoch [5/5], Step [9790/10336], Loss: 0.1730\n",
      "Epoch [5/5], Step [9792/10336], Loss: 0.0203\n",
      "Epoch [5/5], Step [9794/10336], Loss: 0.0487\n",
      "Epoch [5/5], Step [9796/10336], Loss: 0.0613\n",
      "Epoch [5/5], Step [9798/10336], Loss: 0.0875\n",
      "Epoch [5/5], Step [9800/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [9802/10336], Loss: 0.0358\n",
      "Epoch [5/5], Step [9804/10336], Loss: 0.2934\n",
      "Epoch [5/5], Step [9806/10336], Loss: 1.8989\n",
      "Epoch [5/5], Step [9808/10336], Loss: 0.3940\n",
      "Epoch [5/5], Step [9810/10336], Loss: 0.7027\n",
      "Epoch [5/5], Step [9812/10336], Loss: 0.5472\n",
      "Epoch [5/5], Step [9814/10336], Loss: 1.4709\n",
      "Epoch [5/5], Step [9816/10336], Loss: 0.1102\n",
      "Epoch [5/5], Step [9818/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [9820/10336], Loss: 1.5539\n",
      "Epoch [5/5], Step [9822/10336], Loss: 0.2848\n",
      "Epoch [5/5], Step [9824/10336], Loss: 0.0268\n",
      "Epoch [5/5], Step [9826/10336], Loss: 0.0342\n",
      "Epoch [5/5], Step [9828/10336], Loss: 0.1362\n",
      "Epoch [5/5], Step [9830/10336], Loss: 1.1611\n",
      "Epoch [5/5], Step [9832/10336], Loss: 0.0873\n",
      "Epoch [5/5], Step [9834/10336], Loss: 0.2255\n",
      "Epoch [5/5], Step [9836/10336], Loss: 1.8153\n",
      "Epoch [5/5], Step [9838/10336], Loss: 1.0823\n",
      "Epoch [5/5], Step [9840/10336], Loss: 0.3342\n",
      "Epoch [5/5], Step [9842/10336], Loss: 1.9129\n",
      "Epoch [5/5], Step [9844/10336], Loss: 0.2254\n",
      "Epoch [5/5], Step [9846/10336], Loss: 0.1623\n",
      "Epoch [5/5], Step [9848/10336], Loss: 2.0492\n",
      "Epoch [5/5], Step [9850/10336], Loss: 0.0279\n",
      "Epoch [5/5], Step [9852/10336], Loss: 0.0622\n",
      "Epoch [5/5], Step [9854/10336], Loss: 0.3169\n",
      "Epoch [5/5], Step [9856/10336], Loss: 0.2821\n",
      "Epoch [5/5], Step [9858/10336], Loss: 0.0263\n",
      "Epoch [5/5], Step [9860/10336], Loss: 0.0217\n",
      "Epoch [5/5], Step [9862/10336], Loss: 1.7221\n",
      "Epoch [5/5], Step [9864/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [9866/10336], Loss: 0.0510\n",
      "Epoch [5/5], Step [9868/10336], Loss: 0.2839\n",
      "Epoch [5/5], Step [9870/10336], Loss: 0.5653\n",
      "Epoch [5/5], Step [9872/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [9874/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [9876/10336], Loss: 0.5884\n",
      "Epoch [5/5], Step [9878/10336], Loss: 0.3271\n",
      "Epoch [5/5], Step [9880/10336], Loss: 0.0667\n",
      "Epoch [5/5], Step [9882/10336], Loss: 0.4339\n",
      "Epoch [5/5], Step [9884/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [9886/10336], Loss: 0.1119\n",
      "Epoch [5/5], Step [9888/10336], Loss: 0.8015\n",
      "Epoch [5/5], Step [9890/10336], Loss: 1.0553\n",
      "Epoch [5/5], Step [9892/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [9894/10336], Loss: 0.8737\n",
      "Epoch [5/5], Step [9896/10336], Loss: 0.2747\n",
      "Epoch [5/5], Step [9898/10336], Loss: 0.5178\n",
      "Epoch [5/5], Step [9900/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [9902/10336], Loss: 2.7027\n",
      "Epoch [5/5], Step [9904/10336], Loss: 0.0728\n",
      "Epoch [5/5], Step [9906/10336], Loss: 0.0094\n",
      "Epoch [5/5], Step [9908/10336], Loss: 0.7310\n",
      "Epoch [5/5], Step [9910/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [9912/10336], Loss: 0.2969\n",
      "Epoch [5/5], Step [9914/10336], Loss: 0.0060\n",
      "Epoch [5/5], Step [9916/10336], Loss: 0.1816\n",
      "Epoch [5/5], Step [9918/10336], Loss: 0.7942\n",
      "Epoch [5/5], Step [9920/10336], Loss: 3.4554\n",
      "Epoch [5/5], Step [9922/10336], Loss: 0.0530\n",
      "Epoch [5/5], Step [9924/10336], Loss: 0.0231\n",
      "Epoch [5/5], Step [9926/10336], Loss: 0.9028\n",
      "Epoch [5/5], Step [9928/10336], Loss: 1.0137\n",
      "Epoch [5/5], Step [9930/10336], Loss: 0.2933\n",
      "Epoch [5/5], Step [9932/10336], Loss: 2.5295\n",
      "Epoch [5/5], Step [9934/10336], Loss: 3.2506\n",
      "Epoch [5/5], Step [9936/10336], Loss: 1.2936\n",
      "Epoch [5/5], Step [9938/10336], Loss: 0.2104\n",
      "Epoch [5/5], Step [9940/10336], Loss: 0.4489\n",
      "Epoch [5/5], Step [9942/10336], Loss: 0.0370\n",
      "Epoch [5/5], Step [9944/10336], Loss: 1.2664\n",
      "Epoch [5/5], Step [9946/10336], Loss: 0.2187\n",
      "Epoch [5/5], Step [9948/10336], Loss: 0.0794\n",
      "Epoch [5/5], Step [9950/10336], Loss: 0.0593\n",
      "Epoch [5/5], Step [9952/10336], Loss: 0.1127\n",
      "Epoch [5/5], Step [9954/10336], Loss: 0.1789\n",
      "Epoch [5/5], Step [9956/10336], Loss: 0.2992\n",
      "Epoch [5/5], Step [9958/10336], Loss: 0.1810\n",
      "Epoch [5/5], Step [9960/10336], Loss: 1.0140\n",
      "Epoch [5/5], Step [9962/10336], Loss: 0.1505\n",
      "Epoch [5/5], Step [9964/10336], Loss: 0.1245\n",
      "Epoch [5/5], Step [9966/10336], Loss: 0.0185\n",
      "Epoch [5/5], Step [9968/10336], Loss: 0.3646\n",
      "Epoch [5/5], Step [9970/10336], Loss: 0.1599\n",
      "Epoch [5/5], Step [9972/10336], Loss: 0.0633\n",
      "Epoch [5/5], Step [9974/10336], Loss: 0.0073\n",
      "Epoch [5/5], Step [9976/10336], Loss: 0.0723\n",
      "Epoch [5/5], Step [9978/10336], Loss: 0.0172\n",
      "Epoch [5/5], Step [9980/10336], Loss: 1.9734\n",
      "Epoch [5/5], Step [9982/10336], Loss: 0.0687\n",
      "Epoch [5/5], Step [9984/10336], Loss: 0.1212\n",
      "Epoch [5/5], Step [9986/10336], Loss: 0.0166\n",
      "Epoch [5/5], Step [9988/10336], Loss: 0.0106\n",
      "Epoch [5/5], Step [9990/10336], Loss: 0.2157\n",
      "Epoch [5/5], Step [9992/10336], Loss: 0.0886\n",
      "Epoch [5/5], Step [9994/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [9996/10336], Loss: 0.4278\n",
      "Epoch [5/5], Step [9998/10336], Loss: 1.6675\n",
      "Epoch [5/5], Step [10000/10336], Loss: 3.5309\n",
      "Epoch [5/5], Step [10002/10336], Loss: 0.4997\n",
      "Epoch [5/5], Step [10004/10336], Loss: 0.0527\n",
      "Epoch [5/5], Step [10006/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [10008/10336], Loss: 0.1913\n",
      "Epoch [5/5], Step [10010/10336], Loss: 0.1846\n",
      "Epoch [5/5], Step [10012/10336], Loss: 0.9465\n",
      "Epoch [5/5], Step [10014/10336], Loss: 0.8251\n",
      "Epoch [5/5], Step [10016/10336], Loss: 1.2108\n",
      "Epoch [5/5], Step [10018/10336], Loss: 0.1189\n",
      "Epoch [5/5], Step [10020/10336], Loss: 0.1292\n",
      "Epoch [5/5], Step [10022/10336], Loss: 0.8567\n",
      "Epoch [5/5], Step [10024/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [10026/10336], Loss: 2.1554\n",
      "Epoch [5/5], Step [10028/10336], Loss: 0.0659\n",
      "Epoch [5/5], Step [10030/10336], Loss: 0.3921\n",
      "Epoch [5/5], Step [10032/10336], Loss: 0.3084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [10034/10336], Loss: 0.1903\n",
      "Epoch [5/5], Step [10036/10336], Loss: 0.0106\n",
      "Epoch [5/5], Step [10038/10336], Loss: 0.6195\n",
      "Epoch [5/5], Step [10040/10336], Loss: 0.6135\n",
      "Epoch [5/5], Step [10042/10336], Loss: 0.2263\n",
      "Epoch [5/5], Step [10044/10336], Loss: 3.0226\n",
      "Epoch [5/5], Step [10046/10336], Loss: 0.1446\n",
      "Epoch [5/5], Step [10048/10336], Loss: 2.6918\n",
      "Epoch [5/5], Step [10050/10336], Loss: 0.6511\n",
      "Epoch [5/5], Step [10052/10336], Loss: 0.2655\n",
      "Epoch [5/5], Step [10054/10336], Loss: 0.3412\n",
      "Epoch [5/5], Step [10056/10336], Loss: 0.2536\n",
      "Epoch [5/5], Step [10058/10336], Loss: 0.3488\n",
      "Epoch [5/5], Step [10060/10336], Loss: 0.3150\n",
      "Epoch [5/5], Step [10062/10336], Loss: 0.4860\n",
      "Epoch [5/5], Step [10064/10336], Loss: 0.0216\n",
      "Epoch [5/5], Step [10066/10336], Loss: 0.5683\n",
      "Epoch [5/5], Step [10068/10336], Loss: 0.0495\n",
      "Epoch [5/5], Step [10070/10336], Loss: 0.0517\n",
      "Epoch [5/5], Step [10072/10336], Loss: 0.7521\n",
      "Epoch [5/5], Step [10074/10336], Loss: 0.8624\n",
      "Epoch [5/5], Step [10076/10336], Loss: 0.0992\n",
      "Epoch [5/5], Step [10078/10336], Loss: 0.9037\n",
      "Epoch [5/5], Step [10080/10336], Loss: 0.0119\n",
      "Epoch [5/5], Step [10082/10336], Loss: 0.1497\n",
      "Epoch [5/5], Step [10084/10336], Loss: 1.0648\n",
      "Epoch [5/5], Step [10086/10336], Loss: 0.7177\n",
      "Epoch [5/5], Step [10088/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [10090/10336], Loss: 0.0508\n",
      "Epoch [5/5], Step [10092/10336], Loss: 3.6907\n",
      "Epoch [5/5], Step [10094/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [10096/10336], Loss: 0.0473\n",
      "Epoch [5/5], Step [10098/10336], Loss: 0.5152\n",
      "Epoch [5/5], Step [10100/10336], Loss: 0.1007\n",
      "Epoch [5/5], Step [10102/10336], Loss: 0.0654\n",
      "Epoch [5/5], Step [10104/10336], Loss: 0.0499\n",
      "Epoch [5/5], Step [10106/10336], Loss: 0.6517\n",
      "Epoch [5/5], Step [10108/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [10110/10336], Loss: 0.3570\n",
      "Epoch [5/5], Step [10112/10336], Loss: 0.0159\n",
      "Epoch [5/5], Step [10114/10336], Loss: 0.1451\n",
      "Epoch [5/5], Step [10116/10336], Loss: 2.3283\n",
      "Epoch [5/5], Step [10118/10336], Loss: 0.1269\n",
      "Epoch [5/5], Step [10120/10336], Loss: 0.0757\n",
      "Epoch [5/5], Step [10122/10336], Loss: 1.1076\n",
      "Epoch [5/5], Step [10124/10336], Loss: 1.1641\n",
      "Epoch [5/5], Step [10126/10336], Loss: 0.2568\n",
      "Epoch [5/5], Step [10128/10336], Loss: 0.2894\n",
      "Epoch [5/5], Step [10130/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [10132/10336], Loss: 0.0431\n",
      "Epoch [5/5], Step [10134/10336], Loss: 1.2794\n",
      "Epoch [5/5], Step [10136/10336], Loss: 0.1949\n",
      "Epoch [5/5], Step [10138/10336], Loss: 0.3721\n",
      "Epoch [5/5], Step [10140/10336], Loss: 0.0693\n",
      "Epoch [5/5], Step [10142/10336], Loss: 0.3253\n",
      "Epoch [5/5], Step [10144/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [10146/10336], Loss: 0.6477\n",
      "Epoch [5/5], Step [10148/10336], Loss: 0.2936\n",
      "Epoch [5/5], Step [10150/10336], Loss: 1.0468\n",
      "Epoch [5/5], Step [10152/10336], Loss: 0.0343\n",
      "Epoch [5/5], Step [10154/10336], Loss: 0.0325\n",
      "Epoch [5/5], Step [10156/10336], Loss: 3.7563\n",
      "Epoch [5/5], Step [10158/10336], Loss: 4.0983\n",
      "Epoch [5/5], Step [10160/10336], Loss: 0.0786\n",
      "Epoch [5/5], Step [10162/10336], Loss: 0.1448\n",
      "Epoch [5/5], Step [10164/10336], Loss: 0.2306\n",
      "Epoch [5/5], Step [10166/10336], Loss: 0.2408\n",
      "Epoch [5/5], Step [10168/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [10170/10336], Loss: 1.5593\n",
      "Epoch [5/5], Step [10172/10336], Loss: 0.0458\n",
      "Epoch [5/5], Step [10174/10336], Loss: 0.0743\n",
      "Epoch [5/5], Step [10176/10336], Loss: 1.2034\n",
      "Epoch [5/5], Step [10178/10336], Loss: 0.0925\n",
      "Epoch [5/5], Step [10180/10336], Loss: 0.0987\n",
      "Epoch [5/5], Step [10182/10336], Loss: 1.5416\n",
      "Epoch [5/5], Step [10184/10336], Loss: 0.0179\n",
      "Epoch [5/5], Step [10186/10336], Loss: 0.0463\n",
      "Epoch [5/5], Step [10188/10336], Loss: 0.1894\n",
      "Epoch [5/5], Step [10190/10336], Loss: 0.2307\n",
      "Epoch [5/5], Step [10192/10336], Loss: 0.0315\n",
      "Epoch [5/5], Step [10194/10336], Loss: 0.0312\n",
      "Epoch [5/5], Step [10196/10336], Loss: 3.2051\n",
      "Epoch [5/5], Step [10198/10336], Loss: 0.0254\n",
      "Epoch [5/5], Step [10200/10336], Loss: 0.0206\n",
      "Epoch [5/5], Step [10202/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [10204/10336], Loss: 0.0111\n",
      "Epoch [5/5], Step [10206/10336], Loss: 0.1228\n",
      "Epoch [5/5], Step [10208/10336], Loss: 0.0327\n",
      "Epoch [5/5], Step [10210/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [10212/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [10214/10336], Loss: 0.6435\n",
      "Epoch [5/5], Step [10216/10336], Loss: 0.3631\n",
      "Epoch [5/5], Step [10218/10336], Loss: 4.3630\n",
      "Epoch [5/5], Step [10220/10336], Loss: 0.1069\n",
      "Epoch [5/5], Step [10222/10336], Loss: 0.0721\n",
      "Epoch [5/5], Step [10224/10336], Loss: 0.0145\n",
      "Epoch [5/5], Step [10226/10336], Loss: 0.1178\n",
      "Epoch [5/5], Step [10228/10336], Loss: 0.1519\n",
      "Epoch [5/5], Step [10230/10336], Loss: 0.2428\n",
      "Epoch [5/5], Step [10232/10336], Loss: 0.7564\n",
      "Epoch [5/5], Step [10234/10336], Loss: 0.2717\n",
      "Epoch [5/5], Step [10236/10336], Loss: 0.0090\n",
      "Epoch [5/5], Step [10238/10336], Loss: 0.0602\n",
      "Epoch [5/5], Step [10240/10336], Loss: 0.0623\n",
      "Epoch [5/5], Step [10242/10336], Loss: 0.5132\n",
      "Epoch [5/5], Step [10244/10336], Loss: 0.4272\n",
      "Epoch [5/5], Step [10246/10336], Loss: 0.5663\n",
      "Epoch [5/5], Step [10248/10336], Loss: 4.4431\n",
      "Epoch [5/5], Step [10250/10336], Loss: 0.1927\n",
      "Epoch [5/5], Step [10252/10336], Loss: 1.7505\n",
      "Epoch [5/5], Step [10254/10336], Loss: 0.0212\n",
      "Epoch [5/5], Step [10256/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [10258/10336], Loss: 0.0246\n",
      "Epoch [5/5], Step [10260/10336], Loss: 1.0175\n",
      "Epoch [5/5], Step [10262/10336], Loss: 0.1895\n",
      "Epoch [5/5], Step [10264/10336], Loss: 0.4486\n",
      "Epoch [5/5], Step [10266/10336], Loss: 0.0755\n",
      "Epoch [5/5], Step [10268/10336], Loss: 1.6133\n",
      "Epoch [5/5], Step [10270/10336], Loss: 0.0508\n",
      "Epoch [5/5], Step [10272/10336], Loss: 0.3495\n",
      "Epoch [5/5], Step [10274/10336], Loss: 0.0273\n",
      "Epoch [5/5], Step [10276/10336], Loss: 0.7214\n",
      "Epoch [5/5], Step [10278/10336], Loss: 4.8930\n",
      "Epoch [5/5], Step [10280/10336], Loss: 0.3262\n",
      "Epoch [5/5], Step [10282/10336], Loss: 0.2115\n",
      "Epoch [5/5], Step [10284/10336], Loss: 0.5641\n",
      "Epoch [5/5], Step [10286/10336], Loss: 0.1159\n",
      "Epoch [5/5], Step [10288/10336], Loss: 0.9593\n",
      "Epoch [5/5], Step [10290/10336], Loss: 1.1138\n",
      "Epoch [5/5], Step [10292/10336], Loss: 0.2664\n",
      "Epoch [5/5], Step [10294/10336], Loss: 0.2825\n",
      "Epoch [5/5], Step [10296/10336], Loss: 0.0568\n",
      "Epoch [5/5], Step [10298/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [10300/10336], Loss: 0.1747\n",
      "Epoch [5/5], Step [10302/10336], Loss: 1.7228\n",
      "Epoch [5/5], Step [10304/10336], Loss: 0.0060\n",
      "Epoch [5/5], Step [10306/10336], Loss: 0.5034\n",
      "Epoch [5/5], Step [10308/10336], Loss: 2.0383\n",
      "Epoch [5/5], Step [10310/10336], Loss: 0.2569\n",
      "Epoch [5/5], Step [10312/10336], Loss: 0.2297\n",
      "Epoch [5/5], Step [10314/10336], Loss: 0.3954\n",
      "Epoch [5/5], Step [10316/10336], Loss: 0.0150\n",
      "Epoch [5/5], Step [10318/10336], Loss: 0.0063\n",
      "Epoch [5/5], Step [10320/10336], Loss: 0.7772\n",
      "Epoch [5/5], Step [10322/10336], Loss: 0.8823\n",
      "Epoch [5/5], Step [10324/10336], Loss: 0.3414\n",
      "Epoch [5/5], Step [10326/10336], Loss: 0.1101\n",
      "Epoch [5/5], Step [10328/10336], Loss: 0.0252\n",
      "Epoch [5/5], Step [10330/10336], Loss: 0.2970\n",
      "Epoch [5/5], Step [10332/10336], Loss: 1.3020\n",
      "Epoch [5/5], Step [10334/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [10336/10336], Loss: 1.8535\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAFcJLVYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f72903e7890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31min 20s, sys: 5min 26s, total: 36min 47s\n",
      "Wall time: 33min 33s\n"
     ]
    }
   ],
   "source": [
    "%time arya_train()\n",
    "torch.save(resnet18.state_dict(), 'resnet18.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nThe NVIDIA driver on your system is too old (found version 6050).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org/binaries to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d857ec302981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresnet18\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resnet18.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclassification_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Write loops for testing the model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# You should also print out the accuracy of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[0;32m--> 350\u001b[0;31m                     data_type(size), location)\n\u001b[0m\u001b[1;32m    351\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/serialization.pyc\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mdevice_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m         raise RuntimeError(\n\u001b[1;32m     83\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_sparse_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/cuda/__init__.pyc\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mAlternatively\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbinaries\u001b[0m \u001b[0mto\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0ma\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0myour\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m of the CUDA driver.\"\"\".format(str(torch._C._cuda_getDriverVersion())))\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: \nThe NVIDIA driver on your system is too old (found version 6050).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org/binaries to install\na PyTorch version that has been compiled with your version\nof the CUDA driver."
     ]
    }
   ],
   "source": [
    "resnet18.load_state_dict(torch.load('resnet18.pkl'))\n",
    "def classification_test(model):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels,_ in test_loader:\n",
    "        images = Variable(images)\n",
    "        \n",
    "        if(use_gpu):\n",
    "            images = images.cuda()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "    print('Accuracy of the network on the ' + str(total) +' test images: %d %%' % (100 * correct / total))\n",
    "    \n",
    "classification_test(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "Jorah then asks a question, how is this a detection task?<br/>\n",
    "As everybody wonders, Theon Greyjoy suggests a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "\"We take some windows of varying size and aspect ratios\", he mumbled, \"and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value!\". \"He is right\", says Samwell, \"I read a similar approach in the paper -Faster RCNN by Ross Girshick in the library, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide\". You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theon_sliding_window(image,aspect_ratio,size,slide_amount):\n",
    "    # Begin\n",
    "    windows = []\n",
    "    labels = []\n",
    "    threshold = 0\n",
    "    for x in xrange(0,image.size[0],slide_amount):\n",
    "        for y in xrange(0,image.size[1],slide_amount):\n",
    "            box = [x,y,int(x+size*aspect_ratio),y+size]\n",
    "            crop_img = image.crop(box).convert('RGB')\n",
    "            crop_img = composed_transform(crop_img)\n",
    "            images = Variable(crop_img)\n",
    "            images = images.unsqueeze(0)\n",
    "            if(use_gpu):\n",
    "                images = images.cuda()\n",
    "            output = resnet18(images)\n",
    "            output = torch.nn.functional.softmax(output)\n",
    "            prob,label = torch.topk(output.data,1)\n",
    "#             print (prob[0][0])\n",
    "#             print (label)\n",
    "            if prob[0][0] > threshold and label[0][0] != 0:\n",
    "                windows.append(box)\n",
    "                labels.append(label)\n",
    "    return windows,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Wait\", says <b>Jon Snow</b>, \"The predicted boxes may be too many and we can't deal with all of them. So, I myself will go and apply non_maximum_supression to reduce the number of boxes\". You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aegon_targaryen_non_maximum_supression(boxes,labels,threshold = 0.3):\n",
    "    boxes = np.array(boxes)\n",
    "    nms_boxes = []\n",
    "    nms_labels = []\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "#     if boxes.dtype.kind == \"i\":\n",
    "#         boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # initialize the list of picked indexes\t\n",
    "    pick = []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "    # grab the last index in the indexes list and add the\n",
    "    # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "        nms_boxes.append(boxes[i])\n",
    "        nms_labels.append(labels[i])\n",
    "        \n",
    "        # find the largest (x, y) coordinates for the start of\n",
    "        # the bounding box and the smallest (x, y) coordinates\n",
    "        # for the end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > threshold)[0])))\n",
    "\n",
    "    # return only the bounding boxes that were picked using the\n",
    "    # integer data type\n",
    "#     return boxes[pick].astype(\"int\") , labels[pick]\n",
    "    return nms_boxes , nms_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 170, 448, 618], [0, 180, 448, 628], [0, 190, 448, 638], [0, 200, 448, 648], [0, 210, 448, 658], [0, 230, 448, 678], [0, 240, 448, 688], [0, 280, 448, 728], [0, 320, 448, 768], [10, 210, 458, 658], [10, 250, 458, 698], [10, 350, 458, 798], [20, 190, 468, 638], [20, 220, 468, 668], [20, 230, 468, 678], [20, 260, 468, 708], [20, 270, 468, 718], [30, 230, 478, 678], [30, 270, 478, 718], [30, 320, 478, 768], [40, 200, 488, 648], [40, 240, 488, 688], [40, 270, 488, 718], [50, 190, 498, 638], [50, 240, 498, 688], [50, 250, 498, 698], [50, 270, 498, 718], [60, 240, 508, 688], [60, 250, 508, 698], [70, 240, 518, 688], [70, 260, 518, 708], [70, 270, 518, 718], [100, 250, 548, 698]] [\n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      ", \n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "image = PIL.Image.open('/home/deepak/CS698O/Assignments/Assignment2/test/VOCdevkit/VOC2007/JPEGImages/000002.jpg')\n",
    "windows,labels = theon_sliding_window(image,1,448,10)\n",
    "print (windows,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 10, 350, 458, 798])] [\n",
      " 15\n",
      "[torch.cuda.LongTensor of size 1x1 (GPU 0)]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "windows, labels = aegon_targaryen_non_maximum_supression(windows,labels,0.3)\n",
    "print (windows,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daenerys, the queen, then orders her army to test out the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def daenerys_test(resnet18):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # Also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    aspect_ratio = [1,0.75,1.33,0.5]\n",
    "    size = [224,128,64]\n",
    "    slide_amount = 10\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for images, orig_labels, img_whole in test_loader:\n",
    "        images = Variable(images)\n",
    "        img_whole = Variable(img_whole)\n",
    "        \n",
    "        if(use_gpu):\n",
    "            images = images.cuda()\n",
    "            img_whole = img_whole.cuda()\n",
    "        \n",
    "        for i in range(len(aspect_ratio)):\n",
    "            for j in range(len(size)):\n",
    "                windows, labels = theon_sliding_window(img_whole,aspect_ratio(i),size(j),slide_amount)\n",
    "                boxes.extend()\n",
    "                labels.extend()\n",
    "        boxes, labels = aegon_targaryen_non_maximum_supression(boxes,labels,0.3)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time daenerys_test(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Showdown\n",
    "After covering all the steps and passing the accuracy value to the talking crystal, they all pass through to the land of the living, with a wounded Jon Snow armed with the Dragon-axe. After a fierce battle, Jon Snow manages to go face to face with the Night king. Surrounded by battling men and falling bodies, they engage in a ferocious battle, a battle of spear and axe. After a raging fight, Jon manages to sink the axe into the Night king's heart, but not before he gets wounded by the spear. As dead men fall to bones, Daenerys and others rush to his aid, but it is too late. Everyone is in tears as they look towards the man of honour, Jon Snow, lying in Daenerys's arms when he says his last words: \"The night has ended. Winter is finally over!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"The night has ended. Winter is finally over!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
