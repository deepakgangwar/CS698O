{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: The Winter is here\n",
    "##### This works best with epic battle music. No spoilers present.\n",
    "<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tywin Lannister was right when he said: \"The great war is between death and life, ice and fire. If we loose, the night will never end\"<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It has been six months since the white walkers' army marched into the north, led by the night king himself on a dead dragon. It has been a battle like never before: never before have men faced such an enemy in battle, never before have men fought so bravely against a united threat, and never before have they been so gravely defeated.<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; While Cersei is in King's landing, brave men have died fighting the great war. Among others, Tyrion is dead, Arya is dead and Jon Snow is dead, again. In a desperate battle, Daenerys leads all her forces in a final stand-off with the dead just south of Winterfell. <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Her army defeated, she is now on the run on her dragon in an air battle, being chased by two of her own dragons, the Night king and a dead Jon Snow. Suddenly, the Night king's spear hits Danny's dragon, who, raining blood and fire, falls into ice, taking the lost queen, with him. <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Daenerys opens her eyes in a strange place, a place which does not follow the rules of space and time, where the dead souls killed by the dead men are trapped, forever. But who woke her up? There stands near her, Tyrion, with Jorah, Davos, Jon Snow, and everybody else. They all indulge in a heartfelt reunion when someone yells- \"But how do we get out?<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Varys sees a talking crystal close by, who asks them of completing a task, which on completion would allow them to go back to the land of the living, with the ultimate tool to defeat the white-walkers and kills the night king, the Dragon-axe. They have summoned you for help, as the task is out of their expertise, to apply a modified CNN to solve the object detection problem on the PASCAL VOC dataset. Varys, the master of whisperers, has used his talents to import the following for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# You can ask Varys to get you more if you desire\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "import PIL.Image\n",
    "import PIL.ImageChops\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import skimage.measure\n",
    "import skimage.morphology\n",
    "\n",
    "resnet_input = 224#size of resnet18 input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cersei chose violence, you choose your hyper-parameters wisely using validation data!\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "learning_rate =  0.001\n",
    "hyp_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "The hound who was in charge for getting the data, brought you the following links:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>He also told you that the dataset(datascrolls :P) consists of images from of 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, ie. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the given 20 classes(aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, TV). For parsing the xml file, you can ask Varys to import xml.etree.ElementTree for you. <br/>\n",
    "<br/> You can then ask Bronn and Jamie to organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be 21. This is important for applying the sliding window method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "           'cow', 'diningtable', 'dog', 'horse',\n",
    "           'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jamie_bronn_build_dataset(dir,img_path):            \n",
    "    image = PIL.Image.open(img_path)\n",
    "    img_name = img_path.split(\"/\")[-1].split(\".\")[-2]\n",
    "    print(img_name)\n",
    "    xml_path = dir+'/Annotations/'+img_name+'.xml'\n",
    "    xml_tree = ET.parse(xml_path)\n",
    "    xml_root = xml_tree.getroot()\n",
    "\n",
    "    temp_img = PIL.Image.new('RGB',image.size,0)\n",
    "#     image.show()\n",
    "    \n",
    "    location = []\n",
    "    labels = []\n",
    "    object_img = []\n",
    "    for object in xml_root.findall('object'):\n",
    "        name = object.find('name').text\n",
    "        position = [int(object.find('bndbox').find('xmin').text), int(object.find('bndbox').find('ymin').text),\n",
    "                    int(object.find('bndbox').find('xmax').text), int(object.find('bndbox').find('ymax').text)]\n",
    "        location.append(position)\n",
    "        crop_img = image.crop(position).convert('RGB')\n",
    "        object_img.append(crop_img)\n",
    "        labels.append(classes.index(name))\n",
    "        \n",
    "        temp_img.paste(crop_img,position)\n",
    "\n",
    "    temp_img = PIL.ImageChops.subtract(image,temp_img)\n",
    "    l = skimage.morphology.label(np.array(image.convert('L')))\n",
    "    regions = skimage.measure.regionprops(l)\n",
    "    max_area = 0\n",
    "    for region in regions:\n",
    "        if region.area >= max_area:\n",
    "            position = region.bbox\n",
    "            max_area = region.area\n",
    "            \n",
    "    location.append(position)        \n",
    "    crop_img = image.crop(position).convert('RGB')\n",
    "#     crop_img.show()\n",
    "    object_img.append(crop_img)\n",
    "    labels.append(classes.index('__background__'))\n",
    "        \n",
    "    return object_img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hound_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        if(train):\n",
    "            dir = root_dir + '/train/VOCdevkit/VOC2007'\n",
    "        else :\n",
    "            dir = root_dir + '/test/VOCdevkit/VOC2007'\n",
    "        self.transform = transform\n",
    "        self.img = [];\n",
    "        self.label = [];\n",
    "        i = 0\n",
    "        for img_path in glob.glob(dir+'/JPEGImages/*.jpg'):\n",
    "            object_img, name = jamie_bronn_build_dataset(dir,img_path)\n",
    "            self.img.extend(object_img)\n",
    "            self.label.extend(name)\n",
    "#             i = i+1\n",
    "#             if i == 7:\n",
    "#                 break\n",
    "                       \n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is None:\n",
    "            return (self.img[idx],self.label[idx])\n",
    "        else:\n",
    "            img_transformed = self.transform(self.img[idx])\n",
    "            return (img_transformed,self.label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = hound_dataset(root_dir='.', train=False, transform=None) # Supply proper root_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the netwok\n",
    "<br/>You can ask Arya to train the network on the created dataset. This will yield a classification network on the 21 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "008310\n",
      "001492\n",
      "004514\n",
      "004611\n",
      "003239\n",
      "004685\n",
      "000806\n",
      "008390\n",
      "002933\n",
      "003699\n",
      "001137\n",
      "002520\n",
      "007803\n",
      "007330\n",
      "002056\n",
      "005173\n",
      "004512\n",
      "006346\n",
      "008100\n",
      "007311\n",
      "002036\n",
      "003382\n",
      "007997\n",
      "005058\n",
      "009764\n",
      "006209\n",
      "003790\n",
      "003280\n",
      "001486\n",
      "003618\n",
      "003646\n",
      "002253\n",
      "008213\n",
      "006357\n",
      "007141\n",
      "008771\n",
      "000977\n",
      "007970\n",
      "008076\n",
      "009745\n",
      "009905\n",
      "004111\n",
      "002272\n",
      "004186\n",
      "000524\n",
      "006398\n",
      "000352\n",
      "006424\n",
      "002180\n",
      "004228\n",
      "003783\n",
      "005217\n",
      "000164\n",
      "006468\n",
      "008360\n",
      "004019\n",
      "003337\n",
      "008982\n",
      "008529\n",
      "007897\n",
      "005845\n",
      "009532\n",
      "003299\n",
      "007292\n",
      "001755\n",
      "000270\n",
      "002899\n",
      "005851\n",
      "004140\n",
      "004607\n",
      "004926\n",
      "003135\n",
      "009299\n",
      "004423\n",
      "006725\n",
      "006852\n",
      "007544\n",
      "001442\n",
      "001504\n",
      "000477\n",
      "006687\n",
      "005186\n",
      "003039\n",
      "003824\n",
      "009834\n",
      "007208\n",
      "007132\n",
      "001522\n",
      "002345\n",
      "005487\n",
      "000435\n",
      "005086\n",
      "006355\n",
      "006179\n",
      "002140\n",
      "002858\n",
      "005930\n",
      "008426\n",
      "007138\n",
      "004033\n",
      "003821\n",
      "008318\n",
      "005358\n",
      "000867\n",
      "008069\n",
      "000215\n",
      "000608\n",
      "003654\n",
      "005094\n",
      "001677\n",
      "002969\n",
      "007745\n",
      "007370\n",
      "001523\n",
      "004649\n",
      "006035\n",
      "003094\n",
      "006133\n",
      "006304\n",
      "005517\n",
      "002405\n",
      "000308\n",
      "007474\n",
      "006275\n",
      "007398\n",
      "000726\n",
      "000851\n",
      "001068\n",
      "003017\n",
      "001012\n",
      "006919\n",
      "009285\n",
      "009945\n",
      "008253\n",
      "003260\n",
      "008268\n",
      "005897\n",
      "006314\n",
      "005169\n",
      "001239\n",
      "004999\n",
      "001290\n",
      "005668\n",
      "002699\n",
      "005343\n",
      "001164\n",
      "000949\n",
      "002803\n",
      "006269\n",
      "003554\n",
      "008926\n",
      "004814\n",
      "004815\n",
      "000305\n",
      "003042\n",
      "002129\n",
      "006375\n",
      "000156\n",
      "005790\n",
      "001453\n",
      "003284\n",
      "006699\n",
      "001554\n",
      "006588\n",
      "005838\n",
      "007768\n",
      "005028\n",
      "008026\n",
      "003349\n",
      "002916\n",
      "001475\n",
      "007200\n",
      "004023\n",
      "007971\n",
      "009411\n",
      "007503\n",
      "001950\n",
      "001102\n",
      "000224\n",
      "001614\n",
      "002343\n",
      "002112\n",
      "006291\n",
      "009676\n",
      "002257\n",
      "002589\n",
      "005318\n",
      "005803\n",
      "002896\n",
      "006474\n",
      "005210\n",
      "007258\n",
      "009703\n",
      "001717\n",
      "003376\n",
      "004152\n",
      "002135\n",
      "001860\n",
      "002459\n",
      "003214\n",
      "002287\n",
      "000699\n",
      "003961\n",
      "000849\n",
      "009205\n",
      "000219\n",
      "004974\n",
      "006866\n",
      "009144\n",
      "005457\n",
      "002410\n",
      "000518\n",
      "008342\n",
      "007767\n",
      "006260\n",
      "001617\n",
      "003002\n",
      "005029\n",
      "002460\n",
      "008960\n",
      "009194\n",
      "001858\n",
      "002722\n",
      "002006\n",
      "005220\n",
      "008364\n",
      "008951\n",
      "001203\n",
      "005145\n",
      "004885\n",
      "009018\n",
      "003262\n",
      "005741\n",
      "001185\n",
      "006038\n",
      "008607\n",
      "005828\n",
      "007275\n",
      "004135\n",
      "004984\n",
      "003991\n",
      "006803\n",
      "005483\n",
      "007919\n",
      "003356\n",
      "004950\n",
      "005719\n",
      "005027\n",
      "000312\n",
      "006702\n",
      "009617\n",
      "009155\n",
      "009312\n",
      "009686\n",
      "008985\n",
      "007721\n",
      "000958\n",
      "008012\n",
      "003885\n",
      "001119\n",
      "009496\n",
      "008191\n",
      "000823\n",
      "003912\n",
      "000454\n",
      "004956\n",
      "001166\n",
      "008061\n",
      "002276\n",
      "001082\n",
      "005368\n",
      "008831\n",
      "007519\n",
      "004391\n",
      "005085\n",
      "007877\n",
      "006243\n",
      "005885\n",
      "004632\n",
      "003638\n",
      "007038\n",
      "007381\n",
      "007084\n",
      "000374\n",
      "009860\n",
      "002563\n",
      "005878\n",
      "001933\n",
      "002116\n",
      "006309\n",
      "004566\n",
      "000245\n",
      "002493\n",
      "007884\n",
      "007046\n",
      "008810\n",
      "003300\n",
      "007683\n",
      "001559\n",
      "007663\n",
      "004271\n",
      "004705\n",
      "007048\n",
      "006023\n",
      "007236\n",
      "007883\n",
      "003923\n",
      "009215\n",
      "003147\n",
      "000519\n",
      "001607\n",
      "006587\n",
      "004364\n",
      "007389\n",
      "001902\n",
      "008756\n",
      "004562\n",
      "005185\n",
      "000282\n",
      "001444\n",
      "003754\n",
      "001260\n",
      "003090\n",
      "000680\n",
      "000967\n",
      "006289\n",
      "000929\n",
      "001472\n",
      "001151\n",
      "009289\n",
      "007900\n",
      "000859\n",
      "003732\n",
      "006550\n",
      "008859\n",
      "004671\n",
      "004722\n",
      "009512\n",
      "007482\n",
      "008720\n",
      "000482\n",
      "006674\n",
      "004189\n",
      "003311\n",
      "003150\n",
      "007172\n",
      "005061\n",
      "005360\n",
      "003803\n",
      "005023\n",
      "006466\n",
      "007772\n",
      "000469\n",
      "004093\n",
      "002826\n",
      "002192\n",
      "005985\n",
      "002300\n",
      "003461\n",
      "003497\n",
      "002372\n",
      "007974\n",
      "003054\n",
      "009958\n",
      "003110\n",
      "008958\n",
      "006262\n",
      "000888\n",
      "007535\n",
      "007928\n",
      "003072\n",
      "006026\n",
      "005179\n",
      "009523\n",
      "004991\n",
      "005789\n",
      "009085\n",
      "001160\n",
      "009368\n",
      "006159\n",
      "009859\n",
      "005908\n",
      "009531\n",
      "005471\n",
      "007425\n",
      "006534\n",
      "000829\n",
      "009528\n",
      "004457\n",
      "003868\n",
      "004495\n",
      "008236\n",
      "003407\n",
      "004459\n",
      "003667\n",
      "007421\n",
      "004209\n",
      "006153\n",
      "009698\n",
      "005884\n",
      "006536\n",
      "005812\n",
      "008316\n",
      "007351\n",
      "003781\n",
      "000760\n",
      "007847\n",
      "008760\n",
      "007068\n",
      "008766\n",
      "005424\n",
      "008275\n",
      "004841\n",
      "008883\n",
      "001582\n",
      "008036\n",
      "003511\n",
      "005685\n",
      "007093\n",
      "003675\n",
      "006858\n",
      "003395\n",
      "007263\n",
      "008168\n",
      "005586\n",
      "004274\n",
      "001995\n",
      "007795\n",
      "009197\n",
      "003688\n",
      "006261\n",
      "008876\n",
      "001777\n",
      "002034\n",
      "008482\n",
      "008209\n",
      "007586\n",
      "009614\n",
      "009255\n",
      "002420\n",
      "002366\n",
      "009481\n",
      "000494\n",
      "005956\n",
      "008553\n",
      "005702\n",
      "004876\n",
      "008049\n",
      "000552\n",
      "004539\n",
      "007840\n",
      "007285\n",
      "009007\n",
      "000545\n",
      "009959\n",
      "009587\n",
      "008307\n",
      "006350\n",
      "004117\n",
      "009250\n",
      "002645\n",
      "009600\n",
      "002704\n",
      "000816\n",
      "009778\n",
      "001186\n",
      "008122\n",
      "008186\n",
      "000225\n",
      "005177\n",
      "000908\n",
      "004535\n",
      "004857\n",
      "006124\n",
      "000707\n",
      "007427\n",
      "005430\n",
      "001036\n",
      "008442\n",
      "006878\n",
      "003213\n",
      "007668\n",
      "002807\n",
      "002508\n",
      "003011\n",
      "005566\n",
      "004295\n",
      "000033\n",
      "009537\n",
      "003694\n",
      "008968\n",
      "008422\n",
      "002847\n",
      "002153\n",
      "002367\n",
      "006396\n",
      "000802\n",
      "003714\n",
      "004339\n",
      "006381\n",
      "008032\n",
      "002404\n",
      "001298\n",
      "000555\n",
      "008410\n",
      "002867\n",
      "009721\n",
      "002194\n",
      "001498\n",
      "005736\n",
      "003510\n",
      "008048\n",
      "001501\n",
      "002594\n",
      "005146\n",
      "003259\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((resnet_input,resnet_input)),\n",
    "                                         transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.ToTensor()])\n",
    "train_dataset = hound_dataset(root_dir='.', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = hound_dataset(root_dir='.', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Litlefinger has brought you a pre-trained network. Fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 21)\n",
    "\n",
    "# Add code for using CUDA here\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    vgg16.cuda()\n",
    "    resnet18.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Update if any errors occur\n",
    "optimizer = torch.optim.SGD(resnet18.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arya_train():\n",
    "    # Begin\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if(use_gpu):\n",
    "                images=images.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer_resnet18.zero_grad()  # zero the gradient buffer\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_resnet18.step()\n",
    "            loss_arr.append(loss.data[0])\n",
    "            if (i+1) % batch_size == 0:\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time arya_train()\n",
    "torch.save(resnet18.state_dict(), 'resnet18.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "Jorah then asks a question, how is this a detection task?<br/>\n",
    "As everybody wonders, Theon Greyjoy suggests a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "\"We take some windows of varying size and aspect ratios\", he mumbled, \"and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value!\". \"He is right\", says Samwell, \"I read a similar approach in the paper -Faster RCNN by Ross Girshick in the library, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide\". You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theon_sliding_window():\n",
    "    # Begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Wait\", says <b>Jon Snow</b>, \"The predicted boxes may be too many and we can't deal with all of them. So, I myself will go and apply non_maximum_supression to reduce the number of boxes\". You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aegon_targaryen_non_maximum_supression(boxes,threshold = 0.3):\n",
    "    # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daenerys, the queen, then orders her army to test out the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def daenerys_test(resnet18):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # Also print out the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time daenerys_test(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Showdown\n",
    "After covering all the steps and passing the accuracy value to the talking crystal, they all pass through to the land of the living, with a wounded Jon Snow armed with the Dragon-axe. After a fierce battle, Jon Snow manages to go face to face with the Night king. Surrounded by battling men and falling bodies, they engage in a ferocious battle, a battle of spear and axe. After a raging fight, Jon manages to sink the axe into the Night king's heart, but not before he gets wounded by the spear. As dead men fall to bones, Daenerys and others rush to his aid, but it is too late. Everyone is in tears as they look towards the man of honour, Jon Snow, lying in Daenerys's arms when he says his last words: \"The night has ended. Winter is finally over!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
