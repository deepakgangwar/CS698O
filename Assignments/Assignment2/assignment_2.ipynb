{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: The Winter is here\n",
    "##### This works best with epic battle music. No spoilers present.\n",
    "<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Tywin Lannister was right when he said: \"The great war is between death and life, ice and fire. If we loose, the night will never end\"<br/>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It has been six months since the white walkers' army marched into the north, led by the night king himself on a dead dragon. It has been a battle like never before: never before have men faced such an enemy in battle, never before have men fought so bravely against a united threat, and never before have they been so gravely defeated.<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; While Cersei is in King's landing, brave men have died fighting the great war. Among others, Tyrion is dead, Arya is dead and Jon Snow is dead, again. In a desperate battle, Daenerys leads all her forces in a final stand-off with the dead just south of Winterfell. <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Her army defeated, she is now on the run on her dragon in an air battle, being chased by two of her own dragons, the Night king and a dead Jon Snow. Suddenly, the Night king's spear hits Danny's dragon, who, raining blood and fire, falls into ice, taking the lost queen, with him. <br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Daenerys opens her eyes in a strange place, a place which does not follow the rules of space and time, where the dead souls killed by the dead men are trapped, forever. But who woke her up? There stands near her, Tyrion, with Jorah, Davos, Jon Snow, and everybody else. They all indulge in a heartfelt reunion when someone yells- \"But how do we get out?<br />\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Varys sees a talking crystal close by, who asks them of completing a task, which on completion would allow them to go back to the land of the living, with the ultimate tool to defeat the white-walkers and kills the night king, the Dragon-axe. They have summoned you for help, as the task is out of their expertise, to apply a modified CNN to solve the object detection problem on the PASCAL VOC dataset. Varys, the master of whisperers, has used his talents to import the following for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "# You can ask Varys to get you more if you desire\n",
    "import xml.etree.ElementTree as ET\n",
    "import glob\n",
    "import PIL.Image\n",
    "import PIL.ImageChops\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import skimage.measure\n",
    "import skimage.morphology\n",
    "from random import randint\n",
    "\n",
    "resnet_input = 224#size of resnet18 input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cersei chose violence, you choose your hyper-parameters wisely using validation data!\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "learning_rate =  0.001\n",
    "hyp_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Build the data\n",
    "The hound who was in charge for getting the data, brought you the following links:\n",
    "<br/>Training and validation:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar\n",
    "<br/>Testing data:\n",
    "<br/>http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar\n",
    "<br/>He also told you that the dataset(datascrolls :P) consists of images from of 20 classes, with detection annotations included. The JPEGImages folder houses the images, and the Annotations folder has the object-wise labels for the objects in one xml file per image. You have to extract the object information, ie. the [xmin, ymin] (the top left x,y co-ordinates) and the [xmax, ymax] (the bottom right x,y co-ordinates) of only the objects belonging to the given 20 classes(aeroplane, bicycle, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, train, TV). For parsing the xml file, you can ask Varys to import xml.etree.ElementTree for you. <br/>\n",
    "<br/> You can then ask Bronn and Jamie to organize the data as follows:\n",
    "<br/> For every image in the dataset, extract/crop the object patch from the image one by one using their respective co-ordinates:[xmin, ymin, xmax, ymax], resize the image to resnet_input, and store it with its class label information. Do the same for training/validation and test datasets. <br/>\n",
    "##### Important\n",
    "You also have to collect data for an extra background class which stands for the class of an object which is not a part of any of the 20 classes. For this, you can crop and resize any random patches from an image. A good idea is to extract patches that have low \"intersection over union\" with any object present in the image frame from the 20 Pascal VOC classes. The number of background images should be roughly around those of other class objects' images. Hence the total classes turn out to be 21. This is important for applying the sliding window method later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = ('__background__',\n",
    "           'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "           'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "           'cow', 'diningtable', 'dog', 'horse',\n",
    "           'motorbike', 'person', 'pottedplant',\n",
    "           'sheep', 'sofa', 'train', 'tvmonitor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taken form pyimagesearch for calculating intersection over union\n",
    "def bb_intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    " \n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = (xB - xA + 1) * (yB - yA + 1)\n",
    " \n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    " \n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea + np.finfo(float).eps)\n",
    " \n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Taken from stackoverflow for drawing random bounding boxes\n",
    "def random_bbox(bbox):\n",
    "    v = [randint(0, v) for v in bbox]\n",
    "    left = min(v[0], v[2])\n",
    "    upper = min(v[1], v[3])\n",
    "    right = max(v[0], v[2])\n",
    "    lower = max(v[1], v[3])\n",
    "    return [left, upper, right, lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jamie_bronn_build_dataset(dir,img_path):            \n",
    "    image = PIL.Image.open(img_path)\n",
    "    img_name = img_path.split(\"/\")[-1].split(\".\")[-2]\n",
    "    print(img_name)\n",
    "    xml_path = dir+'/Annotations/'+img_name+'.xml'\n",
    "    xml_tree = ET.parse(xml_path)\n",
    "    xml_root = xml_tree.getroot()\n",
    "\n",
    "    temp_img = PIL.Image.new('RGB',image.size,0)\n",
    "#     image.show()\n",
    "    \n",
    "    location = []\n",
    "    labels = []\n",
    "    object_img = []\n",
    "    for object in xml_root.findall('object'):\n",
    "        name = object.find('name').text\n",
    "        position = [int(object.find('bndbox').find('xmin').text), int(object.find('bndbox').find('ymin').text),\n",
    "                    int(object.find('bndbox').find('xmax').text), int(object.find('bndbox').find('ymax').text)]\n",
    "        location.append(position)\n",
    "        crop_img = image.crop(position).convert('RGB')\n",
    "        object_img.append(crop_img)\n",
    "        labels.append(classes.index(name))\n",
    "        \n",
    "#         temp_img.paste(crop_img,position)\n",
    "\n",
    "#     temp_img = PIL.ImageChops.subtract(image,temp_img)\n",
    "#     l = skimage.morphology.label(np.array(image.convert('L')))\n",
    "#     regions = skimage.measure.regionprops(l)\n",
    "#     max_area = 0\n",
    "#     for region in regions:\n",
    "#         if region.area >= max_area:\n",
    "#             position = region.bbox\n",
    "#             max_area = region.area\n",
    "            \n",
    "#     location.append(position)        \n",
    "#     crop_img = image.crop(position).convert('RGB')\n",
    "# #     crop_img.show()\n",
    "#     object_img.append(crop_img)\n",
    "#     labels.append(classes.index('__background__'))\n",
    "\n",
    "    iou_threshold = 0.3\n",
    "    num = 0\n",
    "    while(num < 1):\n",
    "        bbox = image.getbbox()\n",
    "        boxA = random_bbox(bbox)\n",
    "        mscore = 0\n",
    "        for boxB in location:\n",
    "              score = (bb_intersection_over_union(boxA, boxB))\n",
    "              if (score > mscore):\n",
    "                  mscore = score\n",
    "        if (mscore < iou_threshold):\n",
    "            object_img.append(image.crop(boxA).convert('RGB'))\n",
    "            labels.append(classes.index('__background__'))\n",
    "            num = num + 1\n",
    "        \n",
    "    return object_img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hound_dataset(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # Begin\n",
    "        print(train)\n",
    "        if(train):\n",
    "            dir = root_dir + '/train/VOCdevkit/VOC2007'\n",
    "        else :\n",
    "            dir = root_dir + '/test/VOCdevkit/VOC2007'\n",
    "        self.transform = transform\n",
    "        self.img = [];\n",
    "        self.label = [];\n",
    "        i = 0\n",
    "        for img_path in glob.glob(dir+'/JPEGImages/*.jpg'):\n",
    "            object_img, name = jamie_bronn_build_dataset(dir,img_path)\n",
    "            self.img.extend(object_img)\n",
    "            self.label.extend(name)\n",
    "#             i = i+1\n",
    "#             if i == 7:\n",
    "#                 break\n",
    "                       \n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform is None:\n",
    "            return (self.img[idx],self.label[idx])\n",
    "        else:\n",
    "            img_transformed = self.transform(self.img[idx])\n",
    "            return (img_transformed,self.label[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_dataset = hound_dataset(root_dir='.', train=False, transform=None) # Supply proper root_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the netwok\n",
    "<br/>You can ask Arya to train the network on the created dataset. This will yield a classification network on the 21 classes of the VOC dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "007451\n",
      "000270\n",
      "008144\n",
      "006174\n",
      "008438\n",
      "005669\n",
      "007876\n",
      "002677\n",
      "001214\n",
      "001045\n",
      "005599\n",
      "008051\n",
      "002417\n",
      "001164\n",
      "007021\n",
      "002293\n",
      "001083\n",
      "006190\n",
      "007388\n",
      "008391\n",
      "006617\n",
      "006421\n",
      "009469\n",
      "001768\n",
      "007133\n",
      "000850\n",
      "006129\n",
      "004075\n",
      "007527\n",
      "006947\n",
      "006806\n",
      "004145\n",
      "004686\n",
      "009845\n",
      "007575\n",
      "006300\n",
      "005159\n",
      "008645\n",
      "000020\n",
      "000220\n",
      "008468\n",
      "009954\n",
      "006972\n",
      "007857\n",
      "006841\n",
      "001899\n",
      "008372\n",
      "003946\n",
      "009686\n",
      "003477\n",
      "002689\n",
      "003913\n",
      "001784\n",
      "002767\n",
      "007189\n",
      "006355\n",
      "004091\n",
      "001460\n",
      "009816\n",
      "003327\n",
      "007439\n",
      "003830\n",
      "001911\n",
      "003082\n",
      "009772\n",
      "006455\n",
      "003589\n",
      "009839\n",
      "002267\n",
      "007723\n",
      "007390\n",
      "009308\n",
      "005138\n",
      "009524\n",
      "000772\n",
      "008326\n",
      "006139\n",
      "000048\n",
      "005755\n",
      "009000\n",
      "009584\n",
      "006172\n",
      "008738\n",
      "004351\n",
      "008338\n",
      "005209\n",
      "003433\n",
      "006222\n",
      "002039\n",
      "005222\n",
      "007325\n",
      "003259\n",
      "000163\n",
      "007732\n",
      "007185\n",
      "009316\n",
      "003685\n",
      "002772\n",
      "001073\n",
      "001828\n",
      "004352\n",
      "001084\n",
      "006572\n",
      "002249\n",
      "005016\n",
      "000828\n",
      "001678\n",
      "002027\n",
      "006916\n",
      "009860\n",
      "007389\n",
      "003705\n",
      "006645\n",
      "005430\n",
      "000498\n",
      "000797\n",
      "006714\n",
      "002480\n",
      "007330\n",
      "008550\n",
      "000892\n",
      "009499\n",
      "000476\n",
      "006968\n",
      "006463\n",
      "006391\n",
      "009282\n",
      "008220\n",
      "008970\n",
      "001541\n",
      "006206\n",
      "003788\n",
      "007838\n",
      "004974\n",
      "005791\n",
      "006892\n",
      "008727\n",
      "005600\n",
      "005566\n",
      "002471\n",
      "008772\n",
      "000929\n",
      "009870\n",
      "005542\n",
      "006819\n",
      "002786\n",
      "006133\n",
      "001314\n",
      "005397\n",
      "006240\n",
      "000435\n",
      "001855\n",
      "004191\n",
      "008494\n",
      "009249\n",
      "008951\n",
      "006004\n",
      "000228\n",
      "006166\n",
      "000268\n",
      "009807\n",
      "002392\n",
      "005536\n",
      "009420\n",
      "000446\n",
      "009114\n",
      "001390\n",
      "005769\n",
      "008040\n",
      "009464\n",
      "001326\n",
      "001733\n",
      "007049\n",
      "007449\n",
      "005786\n",
      "003678\n",
      "003838\n",
      "005258\n",
      "006556\n",
      "001907\n",
      "000380\n",
      "003609\n",
      "003017\n",
      "002835\n",
      "001941\n",
      "002633\n",
      "003629\n",
      "005710\n",
      "004168\n",
      "008301\n",
      "005693\n",
      "007786\n",
      "004135\n",
      "007068\n",
      "006731\n",
      "009085\n",
      "002545\n",
      "004831\n",
      "005588\n",
      "003521\n",
      "005783\n",
      "004912\n",
      "009072\n",
      "003362\n",
      "005499\n",
      "001414\n",
      "003242\n",
      "003292\n",
      "001752\n",
      "003759\n",
      "000192\n",
      "005215\n",
      "001849\n",
      "009644\n",
      "004318\n",
      "009348\n",
      "009358\n",
      "002751\n",
      "003092\n",
      "006939\n",
      "005705\n",
      "009947\n",
      "002706\n",
      "003576\n",
      "007958\n",
      "002943\n",
      "000862\n",
      "000336\n",
      "006886\n",
      "003135\n",
      "007855\n",
      "001653\n",
      "006862\n",
      "009566\n",
      "006610\n",
      "004879\n",
      "009598\n",
      "002838\n",
      "000657\n",
      "004634\n",
      "000370\n",
      "000958\n",
      "008482\n",
      "009550\n",
      "002776\n",
      "002988\n",
      "007629\n",
      "004242\n",
      "004390\n",
      "009022\n",
      "007344\n",
      "000461\n",
      "009955\n",
      "001332\n",
      "009508\n",
      "006931\n",
      "006001\n",
      "004047\n",
      "007457\n",
      "003605\n",
      "004459\n",
      "001598\n",
      "001186\n",
      "006805\n",
      "001388\n",
      "002886\n",
      "001384\n",
      "007165\n",
      "005559\n",
      "001426\n",
      "008755\n",
      "004003\n",
      "001675\n",
      "009637\n",
      "002337\n",
      "001940\n",
      "001311\n",
      "008865\n",
      "002958\n",
      "006547\n",
      "003200\n",
      "004683\n",
      "004396\n",
      "003129\n",
      "000949\n",
      "008624\n",
      "008615\n",
      "009472\n",
      "006532\n",
      "002854\n",
      "002914\n",
      "004367\n",
      "006187\n",
      "005873\n",
      "001361\n",
      "001610\n",
      "006899\n",
      "002382\n",
      "006484\n",
      "000207\n",
      "001833\n",
      "005331\n",
      "004873\n",
      "003444\n",
      "005036\n",
      "008315\n",
      "006933\n",
      "000131\n",
      "004174\n",
      "005143\n",
      "000799\n",
      "000030\n",
      "007040\n",
      "008060\n",
      "009825\n",
      "009004\n",
      "009087\n",
      "006448\n",
      "006443\n",
      "002558\n",
      "006822\n",
      "008216\n",
      "008275\n",
      "000563\n",
      "005838\n",
      "003107\n",
      "004423\n",
      "005968\n",
      "000236\n",
      "003587\n",
      "007799\n",
      "003774\n",
      "007064\n",
      "004487\n",
      "001964\n",
      "006621\n",
      "002120\n",
      "004584\n",
      "005923\n",
      "008512\n",
      "001018\n",
      "001539\n",
      "005274\n",
      "005508\n",
      "003622\n",
      "006183\n",
      "007327\n",
      "004777\n",
      "008385\n",
      "001741\n",
      "001845\n",
      "004732\n",
      "008318\n",
      "008756\n",
      "008799\n",
      "001690\n",
      "005270\n",
      "006666\n",
      "009445\n",
      "006418\n",
      "002142\n",
      "009859\n",
      "000805\n",
      "005239\n",
      "006225\n",
      "003699\n",
      "007954\n",
      "007121\n",
      "006483\n",
      "001809\n",
      "005434\n",
      "004954\n",
      "009880\n",
      "006428\n",
      "009567\n",
      "005345\n",
      "005831\n",
      "003110\n",
      "004691\n",
      "003499\n",
      "004967\n",
      "006827\n",
      "003648\n",
      "003704\n",
      "001524\n",
      "007833\n",
      "004652\n",
      "002969\n",
      "001174\n",
      "001963\n",
      "000750\n",
      "000359\n",
      "000843\n",
      "001408\n",
      "001532\n",
      "005231\n",
      "005199\n",
      "004010\n",
      "003645\n",
      "009371\n",
      "007383\n",
      "009417\n",
      "004674\n",
      "005248\n",
      "008190\n",
      "007174\n",
      "005032\n",
      "004956\n",
      "003721\n",
      "007605\n",
      "002978\n",
      "005416\n",
      "002595\n",
      "007657\n",
      "008026\n",
      "001294\n",
      "009339\n",
      "008838\n",
      "007023\n",
      "004229\n",
      "001580\n",
      "001557\n",
      "009773\n",
      "008814\n",
      "001793\n",
      "002124\n",
      "000871\n",
      "007775\n",
      "008462\n",
      "008647\n",
      "002940\n",
      "003554\n",
      "008942\n",
      "005817\n",
      "005860\n",
      "009351\n",
      "006813\n",
      "002266\n",
      "008295\n",
      "002139\n",
      "008095\n",
      "005956\n",
      "004474\n",
      "009684\n",
      "004257\n",
      "006411\n",
      "008368\n",
      "005704\n",
      "005917\n",
      "001818\n",
      "005970\n",
      "000544\n",
      "000125\n",
      "000256\n",
      "000524\n",
      "001345\n",
      "009790\n",
      "002069\n",
      "008558\n",
      "004479\n",
      "003912\n",
      "009477\n",
      "007709\n",
      "003330\n",
      "001140\n",
      "005024\n",
      "005404\n",
      "007803\n",
      "004859\n",
      "008173\n",
      "004015\n",
      "005467\n",
      "001053\n",
      "002820\n",
      "003798\n",
      "004643\n",
      "009291\n",
      "008521\n",
      "009163\n",
      "008197\n",
      "000516\n",
      "008106\n",
      "000667\n",
      "002468\n",
      "007088\n",
      "000918\n",
      "007558\n",
      "004354\n",
      "009018\n",
      "008142\n",
      "004548\n",
      "002523\n",
      "005130\n",
      "003872\n",
      "000911\n",
      "001782\n",
      "001588\n",
      "006512\n",
      "000885\n",
      "003608\n",
      "001400\n",
      "001738\n",
      "005067\n",
      "007054\n",
      "004852\n",
      "006962\n",
      "008376\n",
      "008995\n",
      "009106\n",
      "009250\n",
      "001337\n",
      "004380\n",
      "009517\n",
      "003397\n",
      "002586\n",
      "009863\n",
      "008083\n",
      "008454\n",
      "008968\n",
      "001258\n",
      "001237\n",
      "004424\n",
      "002321\n",
      "002291\n",
      "004597\n",
      "002305\n",
      "009325\n",
      "005212\n",
      "005815\n",
      "005283\n",
      "000860\n",
      "003407\n",
      "000143\n",
      "001161\n",
      "009796\n",
      "001928\n",
      "004296\n",
      "002534\n",
      "005550\n",
      "004405\n",
      "007772\n",
      "004814\n",
      "007247\n",
      "009647\n",
      "002437\n",
      "004434\n",
      "006330\n",
      "008932\n",
      "004710\n",
      "003784\n",
      "006864\n",
      "003796\n",
      "003667\n",
      "009926\n",
      "005191\n",
      "006965\n",
      "006628\n",
      "004138\n",
      "002067\n",
      "003809\n",
      "000438\n",
      "006918\n",
      "001881\n",
      "005510\n",
      "000999\n",
      "006934\n",
      "000164\n",
      "009437\n",
      "000601\n",
      "006146\n",
      "002178\n",
      "009007\n",
      "003380\n",
      "009141\n",
      "007002\n",
      "006789\n",
      "007731\n",
      "009254\n",
      "008948\n",
      "008098\n",
      "000158\n",
      "003656\n",
      "005253\n",
      "008582\n",
      "006922\n",
      "003211\n",
      "008053\n",
      "001104\n",
      "007090\n",
      "003623\n",
      "005662\n",
      "000296\n",
      "007517\n",
      "000889\n",
      "001858\n",
      "005368\n",
      "003550\n",
      "000325\n",
      "001014\n",
      "001725\n",
      "001582\n",
      "007074\n",
      "002855\n",
      "007793\n",
      "003817\n",
      "000592\n",
      "005358\n",
      "002201\n",
      "000699\n",
      "006810\n",
      "006319\n",
      "003057\n",
      "003093\n",
      "007649\n",
      "005563\n",
      "002759\n",
      "002179\n",
      "002975\n",
      "000663\n",
      "004962\n",
      "005439\n",
      "003848\n",
      "002450\n",
      "007997\n",
      "009368\n",
      "006909\n",
      "007640\n",
      "001229\n",
      "005696\n",
      "007924\n",
      "003085\n",
      "009869\n",
      "005203\n",
      "000486\n",
      "002881\n",
      "007565\n",
      "002448\n",
      "000017\n",
      "005338\n",
      "007525\n",
      "007486\n",
      "007011\n",
      "003145\n",
      "009342\n",
      "007889\n",
      "003688\n",
      "005964\n",
      "008477\n",
      "008222\n",
      "005613\n",
      "000134\n",
      "001512\n",
      "000728\n",
      "006472\n",
      "001523\n",
      "003284\n",
      "008810\n",
      "006859\n",
      "000241\n",
      "004585\n",
      "009064\n",
      "004708\n",
      "005549\n",
      "003924\n",
      "005981\n",
      "006549\n",
      "004673\n",
      "006203\n",
      "003462\n",
      "008806\n",
      "007979\n",
      "000774\n",
      "008229\n",
      "002637\n",
      "007713\n",
      "004768\n",
      "007928\n",
      "005068\n",
      "001612\n",
      "002946\n",
      "000222\n",
      "003218\n",
      "006706\n",
      "005947\n",
      "005989\n",
      "007685\n",
      "002410\n",
      "002621\n",
      "009659\n",
      "004866\n",
      "007216\n",
      "004195\n",
      "002320\n",
      "000250\n",
      "009596\n",
      "009868\n",
      "009374\n",
      "006201\n",
      "006012\n",
      "003250\n",
      "005530\n",
      "004905\n",
      "001611\n",
      "003640\n",
      "007150\n",
      "001149\n",
      "005554\n",
      "008971\n",
      "003154\n",
      "006631\n",
      "002352\n",
      "001207\n",
      "000904\n",
      "002427\n",
      "005181\n",
      "008475\n",
      "004490\n",
      "001041\n",
      "006840\n",
      "003655\n",
      "003974\n",
      "004606\n",
      "002108\n",
      "005093\n",
      "005741\n",
      "000322\n",
      "000622\n",
      "006503\n",
      "009603\n",
      "000221\n",
      "005264\n",
      "005697\n",
      "003244\n",
      "007748\n",
      "000763\n",
      "008299\n",
      "001156\n",
      "000980\n",
      "007261\n",
      "008005\n",
      "003886\n",
      "002942\n",
      "001484\n",
      "003360\n",
      "007672\n",
      "006043\n",
      "007194\n",
      "004148\n",
      "006994\n",
      "007020\n",
      "000967\n",
      "009794\n",
      "008132\n",
      "008592\n",
      "001470\n",
      "008760\n",
      "004436\n",
      "002609\n",
      "009408\n",
      "005811\n",
      "009537\n",
      "001576\n",
      "003214\n",
      "009940\n",
      "009649\n",
      "003669\n",
      "006125\n",
      "006073\n",
      "005440\n",
      "004455\n",
      "008296\n",
      "008012\n",
      "001980\n",
      "007105\n",
      "006880\n",
      "007109\n",
      "002807\n",
      "004368\n",
      "007540\n",
      "008413\n",
      "002762\n",
      "007533\n",
      "001810\n",
      "000400\n",
      "003937\n",
      "007878\n",
      "008848\n",
      "000249\n",
      "004636\n",
      "002481\n",
      "006105\n",
      "006603\n",
      "005821\n",
      "002476\n",
      "004058\n",
      "002937\n",
      "006061\n",
      "006657\n",
      "003818\n",
      "007959\n",
      "007323\n",
      "005094\n",
      "002765\n",
      "009724\n",
      "008189\n",
      "001441\n",
      "001312\n",
      "007289\n",
      "004304\n",
      "008586\n",
      "009721\n",
      "004312\n",
      "006238\n",
      "007365\n",
      "008437\n",
      "003127\n",
      "005611\n",
      "008208\n",
      "007132\n",
      "005700\n",
      "006786\n",
      "000554\n",
      "006275\n",
      "001526\n",
      "008725\n",
      "006612\n",
      "004886\n",
      "008879\n",
      "004327\n",
      "006392\n",
      "005780\n",
      "005547\n",
      "002213\n",
      "004034\n",
      "006799\n",
      "002460\n",
      "008297\n",
      "000407\n",
      "004654\n",
      "005714\n",
      "007821\n",
      "008294\n",
      "007622\n",
      "002197\n",
      "007696\n",
      "006689\n",
      "003415\n",
      "003772\n",
      "001160\n",
      "000263\n",
      "000583\n",
      "006104\n",
      "005676\n",
      "008048\n",
      "002613\n",
      "000519\n",
      "000780\n",
      "009502\n",
      "000448\n",
      "003293\n",
      "007072\n",
      "008833\n",
      "001872\n",
      "002411\n",
      "000705\n",
      "003709\n",
      "005760\n",
      "001948\n",
      "000847\n",
      "001772\n",
      "009634\n",
      "009209\n",
      "003530\n",
      "000710\n",
      "001395\n",
      "003717\n",
      "004169\n",
      "004799\n",
      "001920\n",
      "001205\n",
      "002364\n",
      "002181\n",
      "007285\n",
      "004535\n",
      "008351\n",
      "001050\n",
      "004816\n",
      "000827\n",
      "006486\n",
      "003947\n",
      "008927\n",
      "009833\n",
      "003301\n",
      "005327\n",
      "000996\n",
      "007932\n",
      "000702\n",
      "000761\n",
      "002362\n",
      "003377\n",
      "001485\n",
      "009205\n",
      "004322\n",
      "002957\n",
      "008105\n",
      "000849\n",
      "000372\n",
      "006865\n",
      "004576\n",
      "008091\n",
      "004369\n",
      "009481\n",
      "005647\n",
      "008397\n",
      "008955\n",
      "002054\n",
      "005701\n",
      "003986\n",
      "003845\n",
      "002452\n",
      "003391\n",
      "002912\n",
      "000072\n",
      "007294\n",
      "001413\n",
      "007831\n",
      "002995\n",
      "002647\n",
      "005813\n",
      "003164\n",
      "008732\n",
      "008817\n",
      "005438\n",
      "007009\n",
      "005340\n",
      "006911\n",
      "004770\n",
      "004008\n",
      "004150\n",
      "008470\n",
      "008596\n",
      "008581\n",
      "006377\n",
      "003907\n",
      "004470\n",
      "006855\n",
      "003926\n",
      "003923\n",
      "005380\n",
      "008750\n",
      "001409\n",
      "002578\n",
      "007601\n",
      "006261\n",
      "000091\n",
      "006374\n",
      "006321\n",
      "001383\n",
      "008433\n",
      "005185\n",
      "008766\n",
      "009526\n",
      "008878\n",
      "008911\n",
      "009629\n",
      "006576\n",
      "008826\n",
      "003003\n",
      "001537\n",
      "003194\n",
      "005996\n",
      "003337\n",
      "008509\n",
      "003122\n",
      "009048\n",
      "009778\n",
      "003859\n",
      "002454\n",
      "007038\n",
      "005757\n",
      "004943\n",
      "005202\n",
      "009679\n",
      "001171\n",
      "001902\n",
      "006587\n",
      "004832\n",
      "009797\n",
      "006130\n",
      "001479\n",
      "002176\n",
      "001847\n",
      "002456\n",
      "003539\n",
      "009333\n",
      "002606\n",
      "007768\n",
      "001490\n",
      "009218\n",
      "000688\n",
      "000107\n",
      "006025\n",
      "003051\n",
      "008534\n",
      "004526\n",
      "001901\n",
      "002415\n",
      "004264\n",
      "007910\n",
      "001755\n",
      "003497\n",
      "005740\n",
      "009695\n",
      "009848\n",
      "008461\n",
      "007256\n",
      "008032\n",
      "005781\n",
      "007899\n",
      "000740\n",
      "009586\n",
      "000232\n",
      "000579\n",
      "009691\n",
      "000906\n",
      "009191\n",
      "000120\n",
      "009507\n",
      "002049\n",
      "005743\n",
      "000771\n",
      "004386\n",
      "001325\n",
      "002774\n",
      "000599\n",
      "003752\n",
      "000842\n",
      "007999\n",
      "007489\n",
      "008962\n",
      "005995\n",
      "003424\n",
      "009242\n",
      "002263\n",
      "001960\n",
      "007536\n",
      "008966\n",
      "000374\n",
      "004212\n",
      "007673\n",
      "007042\n",
      "003155\n",
      "002990\n",
      "000165\n",
      "000416\n",
      "002960\n",
      "007579\n",
      "003997\n",
      "006677\n",
      "002491\n",
      "001124\n",
      "008612\n",
      "007570\n",
      "000121\n",
      "000235\n",
      "005620\n",
      "002378\n",
      "002649\n",
      "006523\n",
      "006260\n",
      "005224\n",
      "005680\n",
      "001247\n",
      "001092\n",
      "004196\n",
      "008426\n",
      "009874\n",
      "003489\n",
      "002385\n",
      "003425\n",
      "008062\n",
      "004530\n",
      "004039\n",
      "008815\n",
      "004790\n",
      "007826\n",
      "009650\n",
      "007297\n",
      "008425\n",
      "002165\n",
      "007045\n",
      "006852\n",
      "003856\n",
      "000900\n",
      "002361\n",
      "001375\n",
      "007683\n",
      "009446\n",
      "003586\n",
      "007947\n",
      "001004\n",
      "001288\n",
      "009324\n",
      "008241\n",
      "009800\n",
      "006524\n",
      "005586\n",
      "007211\n",
      "002114\n",
      "008602\n",
      "005110\n",
      "006465\n",
      "002248\n",
      "009086\n",
      "009898\n",
      "009373\n",
      "008706\n",
      "001711\n",
      "002102\n",
      "006045\n",
      "009413\n",
      "008919\n",
      "003038\n",
      "002477\n",
      "008332\n",
      "003219\n",
      "002251\n",
      "004149\n",
      "005812\n",
      "001405\n",
      "009315\n",
      "001521\n",
      "000964\n",
      "006550\n",
      "000063\n",
      "009138\n",
      "009623\n",
      "002174\n",
      "002433\n",
      "009520\n",
      "009551\n",
      "008905\n",
      "001443\n",
      "007568\n",
      "008841\n",
      "007758\n",
      "001714\n",
      "009455\n",
      "003957\n",
      "005398\n",
      "001647\n",
      "005764\n",
      "001636\n",
      "007396\n",
      "005552\n",
      "005914\n",
      "008733\n",
      "009755\n",
      "004951\n",
      "007466\n",
      "007621\n",
      "005568\n",
      "002565\n",
      "000298\n",
      "007746\n",
      "002590\n",
      "004076\n",
      "001386\n",
      "004484\n",
      "008559\n",
      "001680\n",
      "009459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "003671\n",
      "007048\n",
      "003453\n",
      "009747\n",
      "009490\n",
      "001036\n",
      "003954\n",
      "007729\n",
      "002308\n",
      "009424\n",
      "009480\n",
      "009195\n",
      "003890\n",
      "001397\n",
      "005940\n",
      "005840\n",
      "003708\n",
      "004992\n",
      "005176\n",
      "009197\n",
      "006611\n",
      "005487\n",
      "004013\n",
      "008723\n",
      "007149\n",
      "004215\n",
      "007065\n",
      "008485\n",
      "001230\n",
      "008374\n",
      "003874\n",
      "000619\n",
      "006848\n",
      "009148\n",
      "006847\n",
      "003525\n",
      "001515\n",
      "008926\n",
      "004735\n",
      "002659\n",
      "008049\n",
      "004105\n",
      "008341\n",
      "000146\n",
      "008873\n",
      "009655\n",
      "007883\n",
      "005648\n",
      "001176\n",
      "001191\n",
      "001892\n",
      "000021\n",
      "000857\n",
      "005992\n",
      "003593\n",
      "008576\n",
      "007224\n",
      "003137\n",
      "005824\n",
      "001739\n",
      "004273\n",
      "003691\n",
      "001225\n",
      "008442\n",
      "001071\n",
      "007578\n",
      "008936\n",
      "000041\n",
      "003865\n",
      "007314\n",
      "004982\n",
      "002741\n",
      "002938\n",
      "005664\n",
      "005307\n",
      "004828\n",
      "007624\n",
      "008564\n",
      "009886\n",
      "002935\n",
      "005579\n",
      "008023\n",
      "006117\n",
      "009738\n",
      "001209\n",
      "005244\n",
      "004625\n",
      "002030\n",
      "004117\n",
      "007408\n",
      "003509\n",
      "005519\n",
      "006520\n",
      "006781\n",
      "009904\n",
      "003847\n",
      "004649\n",
      "006814\n",
      "001402\n",
      "007078\n",
      "003763\n",
      "003885\n",
      "003285\n",
      "006769\n",
      "002670\n",
      "000782\n",
      "009117\n",
      "004868\n",
      "001341\n",
      "005378\n",
      "007585\n",
      "008909\n",
      "004527\n",
      "005483\n",
      "004009\n",
      "006301\n",
      "008983\n",
      "009789\n",
      "003496\n",
      "001684\n",
      "000531\n",
      "002376\n",
      "003429\n",
      "007130\n",
      "004255\n",
      "000153\n",
      "003105\n",
      "000099\n",
      "006747\n",
      "007363\n",
      "009438\n",
      "007372\n",
      "002848\n",
      "001977\n",
      "001780\n",
      "006773\n",
      "004439\n",
      "004514\n",
      "005988\n",
      "004519\n",
      "001289\n",
      "007230\n",
      "000171\n",
      "003773\n",
      "007868\n",
      "001685\n",
      "009884\n",
      "007039\n",
      "008982\n",
      "003536\n",
      "008082\n",
      "002718\n",
      "006350\n",
      "008752\n",
      "003965\n",
      "000920\n",
      "002514\n",
      "001184\n",
      "007004\n",
      "009654\n",
      "000686\n",
      "006874\n",
      "009533\n",
      "000387\n",
      "003876\n",
      "008244\n",
      "003002\n",
      "002136\n",
      "006436\n",
      "009767\n",
      "007414\n",
      "004998\n",
      "001816\n",
      "006519\n",
      "002704\n",
      "004120\n",
      "006314\n",
      "009466\n",
      "003877\n",
      "000173\n",
      "007668\n",
      "004539\n",
      "006694\n",
      "004999\n",
      "003654\n",
      "007480\n",
      "008312\n",
      "005413\n",
      "004551\n",
      "003971\n",
      "004292\n",
      "000483\n",
      "005173\n",
      "009212\n",
      "006867\n",
      "002844\n",
      "005577\n",
      "001234\n",
      "005455\n",
      "007920\n",
      "003537\n",
      "006667\n",
      "001494\n",
      "004376\n",
      "001593\n",
      "004364\n",
      "001577\n",
      "003146\n",
      "009614\n",
      "009620\n",
      "007523\n",
      "000591\n",
      "004902\n",
      "001299\n",
      "001618\n",
      "001504\n",
      "009780\n",
      "001421\n",
      "007398\n",
      "002330\n",
      "003790\n",
      "002191\n",
      "003700\n",
      "004672\n",
      "000707\n",
      "001927\n",
      "007322\n",
      "006740\n",
      "007271\n",
      "005257\n",
      "009126\n",
      "002723\n",
      "004929\n",
      "000552\n",
      "002842\n",
      "008809\n",
      "001281\n",
      "009382\n",
      "002372\n",
      "009349\n",
      "002859\n",
      "002328\n",
      "004060\n",
      "007639\n",
      "006836\n",
      "009161\n",
      "002193\n",
      "009422\n",
      "002646\n",
      "007497\n",
      "001017\n",
      "001309\n",
      "000078\n",
      "008639\n",
      "002140\n",
      "000508\n",
      "000917\n",
      "004387\n",
      "004221\n",
      "006033\n",
      "004171\n",
      "006492\n",
      "005407\n",
      "002810\n",
      "004500\n",
      "007853\n",
      "009877\n",
      "009160\n",
      "000923\n",
      "003299\n",
      "003826\n",
      "008076\n",
      "006908\n",
      "006652\n",
      "004082\n",
      "001553\n",
      "004707\n",
      "005984\n",
      "009246\n",
      "006536\n",
      "000541\n",
      "008390\n",
      "009762\n",
      "000559\n",
      "006254\n",
      "000198\n",
      "009882\n",
      "005145\n",
      "004983\n",
      "005861\n",
      "006284\n",
      "000714\n",
      "007843\n",
      "005918\n",
      "005144\n",
      "008920\n",
      "007436\n",
      "003007\n",
      "008311\n",
      "001466\n",
      "007123\n",
      "004185\n",
      "000304\n",
      "009546\n",
      "008254\n",
      "004066\n",
      "006719\n",
      "008416\n",
      "002443\n",
      "002603\n",
      "007663\n",
      "005897\n",
      "007631\n",
      "000061\n",
      "003555\n",
      "006682\n",
      "006782\n",
      "001498\n",
      "006722\n",
      "005419\n",
      "004605\n",
      "001999\n",
      "002371\n",
      "005928\n",
      "005826\n",
      "000367\n",
      "008943\n",
      "007467\n",
      "009006\n",
      "005818\n",
      "008742\n",
      "001147\n",
      "006309\n",
      "000882\n",
      "008876\n",
      "001323\n",
      "000635\n",
      "003176\n",
      "007915\n",
      "008607\n",
      "001727\n",
      "007375\n",
      "003749\n",
      "005749\n",
      "009470\n",
      "006067\n",
      "001903\n",
      "008268\n",
      "003294\n",
      "004347\n",
      "002125\n",
      "001011\n",
      "005363\n",
      "002276\n",
      "005027\n",
      "005878\n",
      "002775\n",
      "006366\n",
      "000482\n",
      "003013\n",
      "001127\n",
      "006223\n",
      "004910\n",
      "007890\n",
      "000288\n",
      "005343\n",
      "001691\n",
      "008427\n",
      "003992\n",
      "003102\n",
      "007241\n",
      "002042\n",
      "002407\n",
      "002470\n",
      "003159\n",
      "002333\n",
      "002710\n",
      "002287\n",
      "006564\n",
      "004727\n",
      "003290\n",
      "008606\n",
      "002866\n",
      "008202\n",
      "005471\n",
      "001393\n",
      "005829\n",
      "002804\n",
      "002284\n",
      "000077\n",
      "005761\n",
      "001654\n",
      "008483\n",
      "006108\n",
      "001130\n",
      "009591\n",
      "009920\n",
      "007600\n",
      "005360\n",
      "006726\n",
      "009129\n",
      "008029\n",
      "000888\n",
      "004848\n",
      "009852\n",
      "000419\n",
      "001688\n",
      "000526\n",
      "002559\n",
      "003936\n",
      "003247\n",
      "009281\n",
      "005975\n",
      "005057\n",
      "002405\n",
      "009307\n",
      "001614\n",
      "007720\n",
      "000306\n",
      "007877\n",
      "002218\n",
      "008127\n",
      "000257\n",
      "003031\n",
      "009950\n",
      "007031\n",
      "008200\n",
      "000876\n",
      "004836\n",
      "007678\n",
      "006236\n",
      "007385\n",
      "007745\n",
      "001718\n",
      "002963\n",
      "003915\n",
      "003435\n",
      "002334\n",
      "007812\n",
      "005938\n",
      "001106\n",
      "002012\n",
      "006305\n",
      "003458\n",
      "001352\n",
      "005732\n",
      "002944\n",
      "007445\n",
      "000450\n",
      "006619\n",
      "005910\n",
      "005045\n",
      "002540\n",
      "008336\n",
      "001043\n",
      "006286\n",
      "001211\n",
      "007513\n",
      "006495\n",
      "000499\n",
      "001430\n",
      "004005\n",
      "003991\n",
      "008523\n",
      "003410\n",
      "001064\n",
      "004142\n",
      "004244\n",
      "001142\n",
      "003239\n",
      "004570\n",
      "003492\n",
      "003395\n",
      "001651\n",
      "003398\n",
      "005160\n",
      "008573\n",
      "000073\n",
      "007923\n",
      "008883\n",
      "007544\n",
      "009496\n",
      "006734\n",
      "004146\n",
      "001877\n",
      "005079\n",
      "003032\n",
      "006529\n",
      "002056\n",
      "001693\n",
      "008976\n",
      "003866\n",
      "000613\n",
      "005229\n",
      "008683\n",
      "003441\n",
      "007460\n",
      "006066\n",
      "005259\n",
      "002800\n",
      "008322\n",
      "005625\n",
      "009865\n",
      "009091\n",
      "009187\n",
      "008452\n",
      "000190\n",
      "007122\n",
      "005026\n",
      "001483\n",
      "003308\n",
      "004890\n",
      "001628\n",
      "005153\n",
      "007084\n",
      "006030\n",
      "007138\n",
      "005980\n",
      "002539\n",
      "003487\n",
      "003628\n",
      "008823\n",
      "008513\n",
      "008764\n",
      "001853\n",
      "001450\n",
      "006736\n",
      "003468\n",
      "006543\n",
      "006387\n",
      "000269\n",
      "002864\n",
      "003382\n",
      "005262\n",
      "005584\n",
      "003646\n",
      "002965\n",
      "000518\n",
      "007493\n",
      "005054\n",
      "004895\n",
      "007245\n",
      "004685\n",
      "006910\n",
      "000689\n",
      "008618\n",
      "005736\n",
      "004789\n",
      "006940\n",
      "001860\n",
      "001931\n",
      "008621\n",
      "008655\n",
      "001730\n",
      "007008\n",
      "009059\n",
      "001062\n",
      "009098\n",
      "005879\n",
      "001597\n",
      "006829\n",
      "007274\n",
      "007343\n",
      "006369\n",
      "007222\n",
      "009147\n",
      "009743\n",
      "006777\n",
      "009194\n",
      "003199\n",
      "000656\n",
      "001721\n",
      "006884\n",
      "004953\n",
      "005971\n",
      "009175\n",
      "006797\n",
      "009887\n",
      "006958\n",
      "006214\n",
      "000899\n",
      "002458\n",
      "008310\n",
      "005856\n",
      "005478\n",
      "007227\n",
      "006363\n",
      "008973\n",
      "000816\n",
      "005369\n",
      "006341\n",
      "004025\n",
      "006281\n",
      "002023\n",
      "009029\n",
      "007139\n",
      "003855\n",
      "008717\n",
      "009177\n",
      "009706\n",
      "002492\n",
      "006046\n",
      "000118\n",
      "004046\n",
      "004524\n",
      "005367\n",
      "008960\n",
      "003401\n",
      "000598\n",
      "000334\n",
      "008159\n",
      "009900\n",
      "000159\n",
      "007443\n",
      "009710\n",
      "003994\n",
      "002413\n",
      "009732\n",
      "001555\n",
      "005236\n",
      "004201\n",
      "003365\n",
      "009192\n",
      "001777\n",
      "005001\n",
      "005590\n",
      "005509\n",
      "003779\n",
      "003064\n",
      "007680\n",
      "004863\n",
      "005190\n",
      "009080\n",
      "004438\n",
      "003149\n",
      "008024\n",
      "000340\n",
      "008888\n",
      "008557\n",
      "008079\n",
      "006091\n",
      "001475\n",
      "006956\n",
      "000162\n",
      "001024\n",
      "000879\n",
      "002445\n",
      "007058\n",
      "009208\n",
      "003783\n",
      "001346\n",
      "004122\n",
      "009185\n",
      "000713\n",
      "003614\n",
      "009186\n",
      "000408\n",
      "003223\n",
      "007446\n",
      "006866\n",
      "002668\n",
      "004648\n",
      "006216\n",
      "009412\n",
      "006188\n",
      "000484\n",
      "001236\n",
      "008248\n",
      "002519\n",
      "002520\n",
      "008342\n",
      "001789\n",
      "002261\n",
      "009032\n",
      "002260\n",
      "005245\n",
      "001201\n",
      "008434\n",
      "006783\n",
      "003066\n",
      "004141\n",
      "003844\n",
      "006755\n",
      "003567\n",
      "000543\n",
      "005230\n",
      "000760\n",
      "009560\n",
      "001517\n",
      "009809\n",
      "003207\n",
      "000915\n",
      "003837\n",
      "003493\n",
      "002196\n",
      "000123\n",
      "008360\n",
      "009917\n",
      "008262\n",
      "002525\n",
      "008302\n",
      "004073\n",
      "006474\n",
      "009913\n",
      "004651\n",
      "002834\n",
      "003998\n",
      "003008\n",
      "009166\n",
      "009454\n",
      "001677\n",
      "003921\n",
      "002238\n",
      "007869\n",
      "008043\n",
      "002585\n",
      "006868\n",
      "003077\n",
      "000908\n",
      "005450\n",
      "002815\n",
      "000294\n",
      "000712\n",
      "003751\n",
      "009079\n",
      "000496\n",
      "007987\n",
      "002931\n",
      "009828\n",
      "007900\n",
      "000756\n",
      "005420\n",
      "008720\n",
      "008057\n",
      "003651\n",
      "005716\n",
      "007350\n",
      "001945\n",
      "001093\n",
      "004231\n",
      "002367\n",
      "006703\n",
      "007369\n",
      "006949\n",
      "001633\n",
      "003919\n",
      "005699\n",
      "008100\n",
      "004286\n",
      "006159\n",
      "006725\n",
      "001632\n",
      "000308\n",
      "000590\n",
      "004496\n",
      "000462\n",
      "008251\n",
      "004987\n",
      "007671\n",
      "006430\n",
      "000863\n",
      "007373\n",
      "003169\n",
      "001418\n",
      "006198\n",
      "001101\n",
      "001894\n",
      "008886\n",
      "000935\n",
      "008349\n",
      "008163\n",
      "001509\n",
      "000150\n",
      "002215\n",
      "002795\n",
      "008345\n",
      "006963\n",
      "004714\n",
      "005183\n",
      "001330\n",
      "009323\n",
      "009557\n",
      "007863\n",
      "001938\n",
      "008939\n",
      "006987\n",
      "007152\n",
      "007901\n",
      "004702\n",
      "006983\n",
      "005337\n",
      "007056\n",
      "000786\n",
      "003134\n",
      "000406\n",
      "000660\n",
      "001052\n",
      "002324\n",
      "009392\n",
      "005762\n",
      "007704\n",
      "008831\n",
      "007465\n",
      "000854\n",
      "000113\n",
      "006762\n",
      "008121\n",
      "000950\n",
      "003240\n",
      "001707\n",
      "004723\n",
      "006606\n",
      "007197\n",
      "006120\n",
      "004307\n",
      "003565\n",
      "004632\n",
      "001682\n",
      "006560\n",
      "008940\n",
      "003056\n",
      "009116\n",
      "007563\n",
      "004630\n",
      "006844\n",
      "009615\n",
      "009034\n",
      "000394\n",
      "004087\n",
      "002393\n",
      "001548\n",
      "005004\n",
      "000951\n",
      "003546\n",
      "001870\n",
      "003273\n",
      "001284\n",
      "008913\n",
      "001836\n",
      "009734\n",
      "001661\n",
      "009343\n",
      "007715\n",
      "003466\n",
      "005081\n",
      "002068\n",
      "002094\n",
      "009681\n",
      "003611\n",
      "000694\n",
      "000746\n",
      "009440\n",
      "007777\n",
      "006442\n",
      "000046\n",
      "009540\n",
      "009465\n",
      "002932\n",
      "008085\n",
      "004137\n",
      "003889\n",
      "006808\n",
      "001669\n",
      "001627\n",
      "002086\n",
      "003011\n",
      "006618\n",
      "005373\n",
      "007902\n",
      "004968\n",
      "006163\n",
      "003966\n",
      "004507\n",
      "003351\n",
      "001074\n",
      "006562\n",
      "007908\n",
      "005782\n",
      "008213\n",
      "003205\n",
      "005853\n",
      "001954\n",
      "002956\n",
      "005039\n",
      "002625\n",
      "003632\n",
      "004450\n",
      "003754\n",
      "009476\n",
      "001137\n",
      "002152\n",
      "003090\n",
      "003635\n",
      "002783\n",
      "006948\n",
      "007996\n",
      "003024\n",
      "001404\n",
      "000215\n",
      "002006\n",
      "008403\n",
      "000684\n",
      "002366\n",
      "003797\n",
      "000251\n",
      "009279\n",
      "005895\n",
      "009573\n",
      "009429\n",
      "006935\n",
      "003620\n",
      "009939\n",
      "007006\n",
      "008556\n",
      "008002\n",
      "000934\n",
      "004786\n",
      "007519\n",
      "004761\n",
      "009131\n",
      "007374\n",
      "001952\n",
      "003604\n",
      "004494\n",
      "007718\n",
      "006976\n",
      "002247\n",
      "004037\n",
      "008116\n",
      "005254\n",
      "000671\n",
      "000954\n",
      "001304\n",
      "006509\n",
      "005728\n",
      "000338\n",
      "006131\n",
      "005306\n",
      "005526\n",
      "006000\n",
      "007950\n",
      "001840\n",
      "006252\n",
      "004644\n",
      "007836\n",
      "008443\n",
      "003189\n",
      "004488\n",
      "007586\n",
      "008518\n",
      "001406\n",
      "009627\n",
      "006476\n",
      "004131\n",
      "004687\n",
      "007433\n",
      "001915\n",
      "007964\n",
      "009151\n",
      "001453\n",
      "009236\n",
      "005906\n",
      "004446\n",
      "000464\n",
      "009958\n",
      "007275\n",
      "002803\n",
      "004471\n",
      "001154\n",
      "000439\n",
      "002335\n",
      "000793\n",
      "001878\n",
      "000709\n",
      "008365\n",
      "003827\n",
      "004016\n",
      "009287\n",
      "001455\n",
      "007416\n",
      "008667\n",
      "009702\n",
      "003663\n",
      "008759\n",
      "004020\n",
      "001102\n",
      "001934\n",
      "002924\n",
      "008300\n",
      "000742\n",
      "007935\n",
      "005784\n",
      "006208\n",
      "004232\n",
      "006945\n",
      "005738\n",
      "005814\n",
      "000468\n",
      "004571\n",
      "002542\n",
      "002504\n",
      "006042\n",
      "007864\n",
      "001239\n",
      "000919\n",
      "002611\n",
      "005719\n",
      "001427\n",
      "008859\n",
      "007763\n",
      "000305\n",
      "009350\n",
      "000381\n",
      "007376\n",
      "000302\n",
      "002401\n",
      "006429\n",
      "007477\n",
      "006595\n",
      "003183\n",
      "007033\n",
      "009334\n",
      "004574\n",
      "005475\n",
      "002375\n",
      "006404\n",
      "008450\n",
      "007943\n",
      "009019\n",
      "006234\n",
      "005393\n",
      "004140\n",
      "008175\n",
      "005527\n",
      "005592\n",
      "000156\n",
      "004985\n",
      "002001\n",
      "009401\n",
      "001151\n",
      "000047\n",
      "004186\n",
      "004136\n",
      "008258\n",
      "009792\n",
      "007498\n",
      "009745\n",
      "000133\n",
      "007077\n",
      "002340\n",
      "007431\n",
      "007840\n",
      "009289\n",
      "007413\n",
      "006647\n",
      "004143\n",
      "007956\n",
      "006285\n",
      "007535\n",
      "007606\n",
      "002796\n",
      "007086\n",
      "005639\n",
      "003918\n",
      "005023\n",
      "000147\n",
      "004284\n",
      "007854\n",
      "003714\n",
      "009692\n",
      "003698\n",
      "004955\n",
      "009365\n",
      "004916\n",
      "005336\n",
      "003988\n",
      "007916\n",
      "002202\n",
      "009414\n",
      "007117\n",
      "008036\n",
      "005292\n",
      "005431\n",
      "001224\n",
      "003549\n",
      "004579\n",
      "005903\n",
      "000016\n",
      "001985\n",
      "002791\n",
      "006698\n",
      "009932\n",
      "006107\n",
      "008280\n",
      "002833\n",
      "005350\n",
      "004748\n",
      "004077\n",
      "000491\n",
      "006583\n",
      "009700\n",
      "000329\n",
      "001015\n",
      "003780\n",
      "002020\n",
      "002329\n",
      "003390\n",
      "000391\n",
      "006184\n",
      "006760\n",
      "001343\n",
      "008847\n",
      "008885\n",
      "007976\n",
      "007200\n",
      "005062\n",
      "009667\n",
      "006161\n",
      "007721\n",
      "009296\n",
      "002036\n",
      "002194\n",
      "004502\n",
      "006433\n",
      "000800\n",
      "009213\n",
      "008224\n",
      "004192\n",
      "007177\n",
      "006883\n",
      "004189\n",
      "009516\n",
      "006860\n",
      "000549\n",
      "004409\n",
      "002219\n",
      "002332\n",
      "006638\n",
      "003098\n",
      "007619\n",
      "002566\n",
      "005912\n",
      "000312\n",
      "003325\n",
      "009299\n",
      "007754\n",
      "007743\n",
      "004631\n",
      "005408\n",
      "003638\n",
      "002483\n",
      "009735\n",
      "008252\n",
      "008728\n",
      "007193\n",
      "004653\n",
      "005868\n",
      "005319\n",
      "001203\n",
      "002345\n",
      "004628\n",
      "003063\n",
      "009270\n",
      "004976\n",
      "008731\n",
      "006468\n",
      "003103\n",
      "009531\n",
      "002827\n",
      "002598\n",
      "008595\n",
      "009935\n",
      "004323\n",
      "008813\n",
      "000289\n",
      "000808\n",
      "000903\n",
      "005789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "005998\n",
      "007511\n",
      "009005\n",
      "006678\n",
      "001729\n",
      "009123\n",
      "008989\n",
      "003320\n",
      "005889\n",
      "009042\n",
      "002593\n",
      "007664\n",
      "007187\n",
      "001200\n",
      "008698\n",
      "000823\n",
      "003272\n",
      "000500\n",
      "006417\n",
      "006648\n",
      "009719\n",
      "006900\n",
      "009456\n",
      "005843\n",
      "004468\n",
      "003185\n",
      "007204\n",
      "005805\n",
      "002738\n",
      "004067\n",
      "005631\n",
      "008209\n",
      "003760\n",
      "004517\n",
      "008115\n",
      "002691\n",
      "003430\n",
      "006337\n",
      "006702\n",
      "008384\n",
      "005061\n",
      "005874\n",
      "000052\n",
      "008953\n",
      "006338\n",
      "009180\n",
      "000122\n",
      "003642\n",
      "007919\n",
      "003556\n",
      "007872\n",
      "006219\n",
      "002439\n",
      "005614\n",
      "000661\n",
      "001061\n",
      "004946\n",
      "007432\n",
      "007735\n",
      "003450\n",
      "007856\n",
      "006444\n",
      "009224\n",
      "006400\n",
      "008633\n",
      "009617\n",
      "005220\n",
      "006708\n",
      "003956\n",
      "001806\n",
      "008410\n",
      "003879\n",
      "005018\n",
      "001212\n",
      "002022\n",
      "006842\n",
      "003834\n",
      "003118\n",
      "009239\n",
      "005351\n",
      "006627\n",
      "004553\n",
      "003961\n",
      "005318\n",
      "007841\n",
      "008064\n",
      "006578\n",
      "002571\n",
      "000582\n",
      "008843\n",
      "008306\n",
      "005370\n",
      "008975\n",
      "000266\n",
      "002090\n",
      "002763\n",
      "002533\n",
      "005421\n",
      "008529\n",
      "002757\n",
      "009193\n",
      "002683\n",
      "006917\n",
      "001821\n",
      "002088\n",
      "009214\n",
      "004626\n",
      "007097\n",
      "009881\n",
      "000443\n",
      "009326\n",
      "005246\n",
      "001365\n",
      "002490\n",
      "006352\n",
      "002967\n",
      "006803\n",
      "002425\n",
      "004508\n",
      "008690\n",
      "006028\n",
      "004220\n",
      "006488\n",
      "005349\n",
      "002208\n",
      "004303\n",
      "009834\n",
      "001436\n",
      "006038\n",
      "008044\n",
      "009099\n",
      "002182\n",
      "001107\n",
      "005863\n",
      "004204\n",
      "005850\n",
      "007059\n",
      "007682\n",
      "008430\n",
      "004928\n",
      "008453\n",
      "009269\n",
      "000089\n",
      "008415\n",
      "000733\n",
      "005384\n",
      "008530\n",
      "002462\n",
      "009419\n",
      "000110\n",
      "002627\n",
      "002915\n",
      "006981\n",
      "005544\n",
      "001263\n",
      "008635\n",
      "002347\n",
      "005674\n",
      "002151\n",
      "005825\n",
      "007356\n",
      "002794\n",
      "004785\n",
      "006950\n",
      "006636\n",
      "001492\n",
      "007214\n",
      "009073\n",
      "005867\n",
      "009918\n",
      "002004\n",
      "003404\n",
      "006029\n",
      "007677\n",
      "000754\n",
      "004718\n",
      "006475\n",
      "005055\n",
      "001864\n",
      "000738\n",
      "004228\n",
      "002662\n",
      "004972\n",
      "004441\n",
      "007925\n",
      "000050\n",
      "009089\n",
      "006482\n",
      "000982\n",
      "004163\n",
      "004898\n",
      "005522\n",
      "006128\n",
      "004867\n",
      "003262\n",
      "005652\n",
      "005773\n",
      "003679\n",
      "005779\n",
      "008031\n",
      "006424\n",
      "005582\n",
      "005312\n",
      "007530\n",
      "008588\n",
      "000233\n",
      "000653\n",
      "006055\n",
      "007753\n",
      "002288\n",
      "002653\n",
      "001608\n",
      "009532\n",
      "006079\n",
      "004019\n",
      "008495\n",
      "005531\n",
      "005630\n",
      "003528\n",
      "009121\n",
      "000912\n",
      "009181\n",
      "005985\n",
      "004812\n",
      "009268\n",
      "001121\n",
      "004283\n",
      "004498\n",
      "004291\n",
      "008232\n",
      "008776\n",
      "005111\n",
      "007699\n",
      "006427\n",
      "005979\n",
      "006570\n",
      "005073\n",
      "004885\n",
      "006506\n",
      "002290\n",
      "004839\n",
      "004108\n",
      "007153\n",
      "000109\n",
      "003229\n",
      "008359\n",
      "004897\n",
      "006335\n",
      "001158\n",
      "002635\n",
      "003694\n",
      "006290\n",
      "000947\n",
      "005090\n",
      "008068\n",
      "007742\n",
      "003178\n",
      "003767\n",
      "005052\n",
      "004190\n",
      "003106\n",
      "008108\n",
      "004882\n",
      "006282\n",
      "005344\n",
      "006395\n",
      "001649\n",
      "009718\n",
      "006759\n",
      "002645\n",
      "000764\n",
      "009290\n",
      "003177\n",
      "005961\n",
      "004360\n",
      "009434\n",
      "001841\n",
      "000262\n",
      "001807\n",
      "006458\n",
      "007424\n",
      "003984\n",
      "009774\n",
      "002547\n",
      "005685\n",
      "009781\n",
      "005189\n",
      "003793\n",
      "001713\n",
      "005007\n",
      "009295\n",
      "000675\n",
      "004743\n",
      "000540\n",
      "005065\n",
      "006673\n",
      "008519\n",
      "006473\n",
      "005854\n",
      "007052\n",
      "002732\n",
      "004111\n",
      "004913\n",
      "008101\n",
      "009678\n",
      "000637\n",
      "009504\n",
      "008837\n",
      "005496\n",
      "003577\n",
      "002778\n",
      "004512\n",
      "008269\n",
      "004656\n",
      "008199\n",
      "006660\n",
      "004742\n",
      "001563\n",
      "003835\n",
      "009497\n",
      "004581\n",
      "007162\n",
      "007461\n",
      "003195\n",
      "009271\n",
      "008604\n",
      "008931\n",
      "003000\n",
      "004549\n",
      "000332\n",
      "000469\n",
      "000278\n",
      "001595\n",
      "000620\n",
      "003580\n",
      "002747\n",
      "000626\n",
      "006318\n",
      "001734\n",
      "003074\n",
      "001501\n",
      "000104\n",
      "007370\n",
      "009571\n",
      "000931\n",
      "000755\n",
      "008961\n",
      "005905\n",
      "002618\n",
      "005830\n",
      "002404\n",
      "008680\n",
      "000276\n",
      "008186\n",
      "002669\n",
      "008997\n",
      "005423\n",
      "000474\n",
      "003253\n",
      "004948\n",
      "003963\n",
      "009283\n",
      "006385\n",
      "007003\n",
      "008862\n",
      "006153\n",
      "000193\n",
      "004544\n",
      "009805\n",
      "001717\n",
      "008084\n",
      "006306\n",
      "007884\n",
      "009600\n",
      "001445\n",
      "006277\n",
      "000489\n",
      "004662\n",
      "008188\n",
      "006381\n",
      "008801\n",
      "009831\n",
      "002112\n",
      "005028\n",
      "004760\n",
      "008999\n",
      "006434\n",
      "007295\n",
      "009618\n",
      "008542\n",
      "005718\n",
      "004705\n",
      "003662\n",
      "005417\n",
      "005731\n",
      "008096\n",
      "002091\n",
      "006098\n",
      "009238\n",
      "003256\n",
      "007637\n",
      "001324\n",
      "009713\n",
      "006709\n",
      "003419\n",
      "000973\n",
      "002129\n",
      "008317\n",
      "002323\n",
      "006027\n",
      "007427\n",
      "004939\n",
      "006349\n",
      "000459\n",
      "002880\n",
      "003120\n",
      "002643\n",
      "001488\n",
      "003674\n",
      "000991\n",
      "008867\n",
      "008323\n",
      "009862\n",
      "009468\n",
      "003140\n",
      "002153\n",
      "000355\n",
      "006162\n",
      "009230\n",
      "005242\n",
      "000731\n",
      "008944\n",
      "004457\n",
      "002584\n",
      "007210\n",
      "008805\n",
      "007018\n",
      "007506\n",
      "009894\n",
      "003461\n",
      "003664\n",
      "006409\n",
      "009905\n",
      "004857\n",
      "001686\n",
      "003803\n",
      "009337\n",
      "000427\n",
      "007113\n",
      "002873\n",
      "000081\n",
      "004023\n",
      "009729\n",
      "000753\n",
      "007093\n",
      "005881\n",
      "005574\n",
      "001125\n",
      "008465\n",
      "003519\n",
      "000946\n",
      "001060\n",
      "000420\n",
      "007865\n",
      "009432\n",
      "006111\n",
      "002098\n",
      "001250\n",
      "008499\n",
      "000403\n",
      "001756\n",
      "006272\n",
      "006398\n",
      "008072\n",
      "000848\n",
      "006069\n",
      "002658\n",
      "006588\n",
      "005960\n",
      "009656\n",
      "006218\n",
      "003939\n",
      "004209\n",
      "003618\n",
      "002025\n",
      "000822\n",
      "004331\n",
      "000177\n",
      "009278\n",
      "007898\n",
      "002784\n",
      "006320\n",
      "004869\n",
      "005524\n",
      "002061\n",
      "008562\n",
      "002342\n",
      "000203\n",
      "000729\n",
      "008177\n",
      "001642\n",
      "009439\n",
      "009227\n",
      "006966\n",
      "006751\n",
      "007524\n",
      "000610\n",
      "006170\n",
      "009060\n",
      "003846\n",
      "003729\n",
      "008033\n",
      "004397\n",
      "002169\n",
      "000509\n",
      "004429\n",
      "006739\n",
      "000244\n",
      "006339\n",
      "006353\n",
      "007095\n",
      "003307\n",
      "000796\n",
      "005278\n",
      "000851\n",
      "007760\n",
      "007079\n",
      "009541\n",
      "001467\n",
      "008753\n",
      "007148\n",
      "002310\n",
      "002354\n",
      "002494\n",
      "005948\n",
      "006020\n",
      "003392\n",
      "004694\n",
      "008293\n",
      "003236\n",
      "008967\n",
      "002745\n",
      "009851\n",
      "002132\n",
      "009136\n",
      "004253\n",
      "007765\n",
      "009942\n",
      "006103\n",
      "003061\n",
      "002657\n",
      "001002\n",
      "008093\n",
      "004872\n",
      "005058\n",
      "001510\n",
      "004194\n",
      "004421\n",
      "007559\n",
      "006959\n",
      "002933\n",
      "000363\n",
      "005102\n",
      "004682\n",
      "001630\n",
      "004794\n",
      "002684\n",
      "001622\n",
      "002391\n",
      "006097\n",
      "007146\n",
      "003439\n",
      "001259\n",
      "009027\n",
      "000060\n",
      "005576\n",
      "009808\n",
      "000501\n",
      "006833\n",
      "006470\n",
      "005640\n",
      "009568\n",
      "006176\n",
      "008784\n",
      "009613\n",
      "001775\n",
      "005930\n",
      "009879\n",
      "003216\n",
      "000859\n",
      "005179\n",
      "006952\n",
      "006765\n",
      "005845\n",
      "007036\n",
      "000814\n",
      "001274\n",
      "003233\n",
      "004783\n",
      "003034\n",
      "007845\n",
      "002636\n",
      "003331\n",
      "007592\n",
      "009303\n",
      "004520\n",
      "009842\n",
      "003354\n",
      "004826\n",
      "000012\n",
      "005328\n",
      "007991\n",
      "005888\n",
      "009902\n",
      "002730\n",
      "000700\n",
      "004747\n",
      "006858\n",
      "003349\n",
      "006869\n",
      "003270\n",
      "000379\n",
      "008835\n",
      "005836\n",
      "000768\n",
      "000214\n",
      "009202\n",
      "001554\n",
      "007626\n",
      "005539\n",
      "007346\n",
      "004361\n",
      "004842\n",
      "009712\n",
      "005365\n",
      "003228\n",
      "000648\n",
      "001028\n",
      "003871\n",
      "006569\n",
      "005518\n",
      "000161\n",
      "009377\n",
      "000065\n",
      "004647\n",
      "008087\n",
      "000337\n",
      "001480\n",
      "003416\n",
      "000064\n",
      "005107\n",
      "007454\n",
      "003339\n",
      "004333\n",
      "001726\n",
      "007612\n",
      "001360\n",
      "001226\n",
      "006156\n",
      "009707\n",
      "006229\n",
      "000530\n",
      "005765\n",
      "008141\n",
      "008988\n",
      "004028\n",
      "003625\n",
      "009463\n",
      "004287\n",
      "008691\n",
      "001057\n",
      "007963\n",
      "009200\n",
      "005217\n",
      "003313\n",
      "007791\n",
      "007223\n",
      "002675\n",
      "002300\n",
      "002272\n",
      "005768\n",
      "009173\n",
      "003452\n",
      "008398\n",
      "009112\n",
      "006753\n",
      "002737\n",
      "003279\n",
      "009923\n",
      "004051\n",
      "001185\n",
      "005448\n",
      "005695\n",
      "001590\n",
      "009515\n",
      "006553\n",
      "008921\n",
      "002034\n",
      "002237\n",
      "003636\n",
      "001937\n",
      "000417\n",
      "006078\n",
      "001898\n",
      "003564\n",
      "002095\n",
      "005410\n",
      "007824\n",
      "008769\n",
      "001875\n",
      "004057\n",
      "007168\n",
      "005655\n",
      "007296\n",
      "001315\n",
      "009867\n",
      "002221\n",
      "009105\n",
      "002307\n",
      "006445\n",
      "006622\n",
      "002465\n",
      "007284\n",
      "002311\n",
      "004984\n",
      "003566\n",
      "002373\n",
      "008980\n",
      "002695\n",
      "001882\n",
      "003412\n",
      "003857\n",
      "007736\n",
      "000730\n",
      "005747\n",
      "003740\n",
      "009527\n",
      "004719\n",
      "005608\n",
      "006893\n",
      "002709\n",
      "007208\n",
      "000545\n",
      "003175\n",
      "006932\n",
      "001268\n",
      "009711\n",
      "009896\n",
      "005735\n",
      "007141\n",
      "007490\n",
      "004779\n",
      "005136\n",
      "008313\n",
      "005864\n",
      "000208\n",
      "001012\n",
      "008930\n",
      "009641\n",
      "005629\n",
      "004676\n",
      "002891\n",
      "001930\n",
      "005686\n",
      "004797\n",
      "007244\n",
      "008872\n",
      "009528\n",
      "001385\n",
      "006825\n",
      "000463\n",
      "003355\n",
      "005135\n",
      "008587\n",
      "007025\n",
      "006196\n",
      "008086\n",
      "007483\n",
      "005352\n",
      "009565\n",
      "005521\n",
      "008524\n",
      "003350\n",
      "005124\n",
      "005920\n",
      "001221\n",
      "003713\n",
      "007859\n",
      "006289\n",
      "009393\n",
      "001978\n",
      "004345\n",
      "004392\n",
      "001617\n",
      "006761\n",
      "001273\n",
      "003161\n",
      "007614\n",
      "008168\n",
      "008260\n",
      "008166\n",
      "002224\n",
      "005875\n",
      "008506\n",
      "002666\n",
      "000826\n",
      "004432\n",
      "006026\n",
      "008478\n",
      "001316\n",
      "002869\n",
      "001072\n",
      "001110\n",
      "006674\n",
      "009113\n",
      "003585\n",
      "008748\n",
      "007814\n",
      "000411\n",
      "001746\n",
      "000887\n",
      "008130\n",
      "006438\n",
      "000597\n",
      "007075\n",
      "002801\n",
      "007080\n",
      "008307\n",
      "002702\n",
      "004815\n",
      "005169\n",
      "004938\n",
      "002070\n",
      "008654\n",
      "003681\n",
      "007062\n",
      "004950\n",
      "001640\n",
      "009386\n",
      "007921\n",
      "001529\n",
      "008107\n",
      "002721\n",
      "002641\n",
      "002435\n",
      "006704\n",
      "003820\n",
      "000726\n",
      "004411\n",
      "009409\n",
      "006924\n",
      "007543\n",
      "003184\n",
      "006944\n",
      "002713\n",
      "003891\n",
      "002209\n",
      "005909\n",
      "002187\n",
      "008467\n",
      "009737\n",
      "005715\n",
      "009668\n",
      "002884\n",
      "004321\n",
      "007654\n",
      "006089\n",
      "008701\n",
      "005461\n",
      "003599\n",
      "006367\n",
      "009389\n",
      "006727\n",
      "002493\n",
      "006626\n",
      "004808\n",
      "008741\n",
      "008796\n",
      "008568\n",
      "003338\n",
      "002436\n",
      "009756\n",
      "008004\n",
      "009858\n",
      "007266\n",
      "009658\n",
      "006748\n",
      "000608\n",
      "007236\n",
      "000555\n",
      "006679\n",
      "006501\n",
      "004692\n",
      "007259\n",
      "008319\n",
      "009039\n",
      "006181\n",
      "008226\n",
      "004224\n",
      "002302\n",
      "009411\n",
      "008388\n",
      "008139\n",
      "009949\n",
      "004926\n",
      "007182\n",
      "003386\n",
      "008355\n",
      "002384\n",
      "006235\n",
      "004069\n",
      "009162\n",
      "005852\n",
      "003396\n",
      "005893\n",
      "000690\n",
      "008770\n",
      "008749\n",
      "006210\n",
      "000428\n",
      "000812\n",
      "008985\n",
      "006507\n",
      "002019\n",
      "003953\n",
      "001241\n",
      "000971\n",
      "002648\n",
      "003575\n",
      "003657\n",
      "005894\n",
      "002501\n",
      "005963\n",
      "006291\n",
      "002512\n",
      "000794\n",
      "007280\n",
      "000898\n",
      "008112\n",
      "003311\n",
      "007270\n",
      "007410\n",
      "001269\n",
      "007528\n",
      "001265\n",
      "002544\n",
      "008620\n",
      "002166\n",
      "000831\n",
      "003356\n",
      "002941\n",
      "000632\n",
      "005470\n",
      "003596\n",
      "003369\n",
      "001068\n",
      "004994\n",
      "006690\n",
      "004263\n",
      "004011\n",
      "004031\n",
      "005071\n",
      "002104\n",
      "005263\n",
      "001166\n",
      "006609\n",
      "000695\n",
      "006658\n",
      "002910\n",
      "008688\n",
      "005954\n",
      "000066\n",
      "006375\n",
      "000174\n",
      "007998\n",
      "005436\n",
      "006018\n",
      "002047\n",
      "004121\n",
      "003660\n",
      "002145\n",
      "001607\n",
      "004876\n",
      "005991\n",
      "002055\n",
      "008584\n",
      "001310\n",
      "007475\n",
      "005445\n",
      "004936\n",
      "006249\n",
      "002253\n",
      "001042\n",
      "008492\n",
      "002714\n",
      "003594\n",
      "004792\n",
      "004093\n",
      "004856\n",
      "002505\n",
      "007521\n",
      "005195\n",
      "000130\n",
      "008103\n",
      "009699\n",
      "004966\n",
      "002183\n",
      "002984\n",
      "004896\n",
      "000513\n",
      "002423\n",
      "004356\n",
      "000523\n",
      "002109\n",
      "007820\n",
      "009285\n",
      "009347\n",
      "007421\n",
      "001896\n",
      "003422\n",
      "003808\n",
      "000676\n",
      "006584\n",
      "004604\n",
      "003210\n",
      "000937\n",
      "006351\n",
      "000343\n",
      "004793\n",
      "008610\n",
      "008771\n",
      "005108\n",
      "002285\n",
      "002096\n",
      "009421\n",
      "009153\n",
      "003170\n",
      "002989\n",
      "003089\n",
      "006323\n",
      "009155\n",
      "005405\n",
      "007484\n",
      "007576\n",
      "000026\n",
      "003869\n",
      "000141\n",
      "006534\n",
      "009398\n",
      "004609\n",
      "002947\n",
      "004404\n",
      "009247\n",
      "002717\n",
      "001008\n",
      "000874\n",
      "005260\n",
      "009577\n",
      "004834\n",
      "000972\n",
      "008472\n",
      "003945\n",
      "001348\n",
      "004271\n",
      "008699\n",
      "002858\n",
      "003058\n",
      "006687\n",
      "008429\n",
      "009184\n",
      "009157\n",
      "000431\n",
      "006943\n",
      "001010\n",
      "005469\n",
      "001231\n",
      "005320\n",
      "004384\n",
      "002051\n",
      "004818\n",
      "005658\n",
      "009375\n",
      "007798\n",
      "007022\n",
      "003508\n",
      "001266\n",
      "000154\n",
      "007419\n",
      "003758\n",
      "006849\n",
      "001056\n",
      "003979\n",
      "009037\n",
      "001976\n",
      "005901\n",
      "002024\n",
      "009545\n",
      "003786\n",
      "008169\n",
      "008790\n",
      "003935\n",
      "006988\n",
      "001699\n",
      "003280\n",
      "005641\n",
      "002497\n",
      "000019\n",
      "002278\n",
      "003078\n",
      "006953\n",
      "005517\n",
      "009754\n",
      "008775\n",
      "001571\n",
      "006247\n",
      "008514\n",
      "000647\n",
      "008204\n",
      "005702\n",
      "004247\n",
      "008001\n",
      "004837\n",
      "000224\n",
      "005311\n",
      "006141\n",
      "000093\n",
      "003142\n",
      "001972\n",
      "001387\n",
      "006425\n",
      "008191\n",
      "003905\n",
      "009388\n",
      "000005\n",
      "001522\n",
      "008250\n",
      "008929\n",
      "002058\n",
      "000039\n",
      "006136\n",
      "000321\n",
      "008008\n",
      "007394\n",
      "005591\n",
      "003165\n",
      "002554\n",
      "006919\n",
      "000024\n",
      "003993\n",
      "005414\n",
      "005983\n",
      "005175\n",
      "008140\n",
      "002368\n",
      "002634\n",
      "009830\n",
      "003911\n",
      "001544\n",
      "008387\n",
      "003138\n",
      "001333\n",
      "003898\n",
      "001481\n",
      "003727\n",
      "008958\n",
      "006551\n",
      "007847\n",
      "002690\n",
      "005161\n",
      "000259\n",
      "005585\n",
      "004935\n",
      "001077\n",
      "007611\n",
      "006295\n",
      "008793\n",
      "009580\n",
      "006538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000810\n",
      "002259\n",
      "001334\n",
      "003748\n",
      "006772\n",
      "005134\n",
      "001982\n",
      "008346\n",
      "000565\n",
      "008695\n",
      "003510\n",
      "001732\n",
      "004518\n",
      "003684\n",
      "002696\n",
      "003465\n",
      "007092\n",
      "004298\n",
      "009500\n",
      "006250\n",
      "009327\n",
      "005297\n",
      "007089\n",
      "000117\n",
      "005951\n",
      "009259\n",
      "008533\n",
      "007809\n",
      "004689\n",
      "000770\n",
      "005395\n",
      "007984\n",
      "006912\n",
      "004170\n",
      "009100\n",
      "006850\n",
      "000720\n",
      "004990\n",
      "001556\n",
      "003255\n",
      "001766\n",
      "001797\n",
      "000424\n",
      "005268\n",
      "003491\n",
      "002064\n",
      "008381\n",
      "003260\n",
      "003072\n",
      "006270\n",
      "008923\n",
      "007697\n",
      "001750\n",
      "009443\n",
      "005654\n",
      "007667\n",
      "002867\n",
      "002870\n",
      "002496\n",
      "002986\n",
      "003258\n",
      "005389\n",
      "007035\n",
      "008880\n",
      "001439\n",
      "000477\n",
      "001603\n",
      "004907\n",
      "006348\n",
      "007751\n",
      "000878\n",
      "006989\n",
      "008692\n",
      "008871\n",
      "007411\n",
      "008628\n",
      "008856\n",
      "008794\n",
      "004205\n",
      "006565\n",
      "001765\n",
      "008636\n",
      "003899\n",
      "001842\n",
      "007779\n",
      "002099\n",
      "000189\n",
      "003690\n",
      "006009\n",
      "001204\n",
      "001170\n",
      "009049\n",
      "007007\n",
      "008773\n",
      "000577\n",
      "001887\n",
      "004326\n",
      "002953\n",
      "000142\n",
      "001862\n",
      "007727\n",
      "001464\n",
      "000246\n",
      "002441\n",
      "003045\n",
      "003181\n",
      "003949\n",
      "005507\n",
      "002816\n",
      "000036\n",
      "000588\n",
      "006635\n",
      "005679\n",
      "005952\n",
      "009306\n",
      "007071\n",
      "008710\n",
      "008670\n",
      "007279\n",
      "004481\n",
      "004701\n",
      "009068\n",
      "006449\n",
      "006251\n",
      "003735\n",
      "004239\n",
      "003806\n",
      "006593\n",
      "004095\n",
      "004178\n",
      "001136\n",
      "002664\n",
      "005657\n",
      "004878\n",
      "005756\n",
      "009448\n",
      "008978\n",
      "009944\n",
      "000948\n",
      "008137\n",
      "000804\n",
      "000492\n",
      "003421\n",
      "008617\n",
      "007914\n",
      "003987\n",
      "000654\n",
      "006845\n",
      "006459\n",
      "004272\n",
      "007300\n",
      "005919\n",
      "002518\n",
      "004379\n",
      "004329\n",
      "005310\n",
      "009758\n",
      "004796\n",
      "003603\n",
      "007933\n",
      "008329\n",
      "003436\n",
      "006530\n",
      "005729\n",
      "004931\n",
      "004274\n",
      "000776\n",
      "005086\n",
      "008917\n",
      "006605\n",
      "006497\n",
      "009671\n",
      "007329\n",
      "009836\n",
      "007526\n",
      "008874\n",
      "008892\n",
      "000194\n",
      "008019\n",
      "005453\n",
      "004565\n",
      "005603\n",
      "008164\n",
      "001097\n",
      "000997\n",
      "003133\n",
      "009196\n",
      "004372\n",
      "006456\n",
      "005003\n",
      "009144\n",
      "009523\n",
      "001109\n",
      "001175\n",
      "004241\n",
      "008281\n",
      "007212\n",
      "007666\n",
      "004715\n",
      "004607\n",
      "007724\n",
      "002749\n",
      "005063\n",
      "005346\n",
      "008709\n",
      "009670\n",
      "009693\n",
      "005042\n",
      "000051\n",
      "009410\n",
      "005371\n",
      "002561\n",
      "009288\n",
      "002682\n",
      "008840\n",
      "004788\n",
      "000820\n",
      "002615\n",
      "003868\n",
      "003781\n",
      "002270\n",
      "000872\n",
      "004750\n",
      "007458\n",
      "002939\n",
      "000855\n",
      "002472\n",
      "002798\n",
      "001676\n",
      "007217\n",
      "004246\n",
      "001918\n",
      "000943\n",
      "006329\n",
      "003753\n",
      "004699\n",
      "006179\n",
      "002508\n",
      "000112\n",
      "006699\n",
      "007163\n",
      "005208\n",
      "007687\n",
      "006684\n",
      "005429\n",
      "000711\n",
      "007931\n",
      "000460\n",
      "000880\n",
      "003254\n",
      "005168\n",
      "004997\n",
      "005128\n",
      "005298\n",
      "009494\n",
      "000395\n",
      "008526\n",
      "008665\n",
      "009542\n",
      "003597\n",
      "009621\n",
      "009015\n",
      "003124\n",
      "003005\n",
      "005908\n",
      "002184\n",
      "005326\n",
      "005391\n",
      "004295\n",
      "002555\n",
      "001286\n",
      "004493\n",
      "003303\n",
      "003420\n",
      "003150\n",
      "007101\n",
      "005819\n",
      "001579\n",
      "001270\n",
      "001465\n",
      "008503\n",
      "006180\n",
      "001723\n",
      "001463\n",
      "005485\n",
      "007675\n",
      "005186\n",
      "003117\n",
      "004612\n",
      "000129\n",
      "007679\n",
      "000023\n",
      "003675\n",
      "001683\n",
      "004106\n",
      "003996\n",
      "009336\n",
      "009512\n",
      "001785\n",
      "005514\n",
      "000307\n",
      "003621\n",
      "001827\n",
      "002116\n",
      "004089\n",
      "008900\n",
      "009002\n",
      "002785\n",
      "007154\n",
      "002444\n",
      "002359\n",
      "002934\n",
      "003126\n",
      "000034\n",
      "008279\n",
      "009638\n",
      "006175\n",
      "007276\n",
      "009801\n",
      "007776\n",
      "008987\n",
      "004995\n",
      "000865\n",
      "003157\n",
      "005682\n",
      "003015\n",
      "007125\n",
      "006123\n",
      "004430\n",
      "005293\n",
      "002180\n",
      "006258\n",
      "005304\n",
      "009576\n",
      "003522\n",
      "004466\n",
      "005433\n",
      "005101\n",
      "004452\n",
      "006995\n",
      "002722\n",
      "007670\n",
      "000868\n",
      "001543\n",
      "004200\n",
      "005085\n",
      "005583\n",
      "002889\n",
      "004737\n",
      "002343\n",
      "008702\n",
      "008370\n",
      "007909\n",
      "002257\n",
      "004258\n",
      "003455\n",
      "007939\n",
      "001933\n",
      "009748\n",
      "003849\n",
      "005303\n",
      "008150\n",
      "000138\n",
      "000323\n",
      "003188\n",
      "002906\n",
      "007905\n",
      "004562\n",
      "003518\n",
      "005615\n",
      "002002\n",
      "001545\n",
      "002977\n",
      "006661\n",
      "007692\n",
      "004509\n",
      "001442\n",
      "007749\n",
      "006267\n",
      "005601\n",
      "004281\n",
      "009938\n",
      "007815\n",
      "002212\n",
      "007381\n",
      "002735\n",
      "000033\n",
      "006766\n",
      "002678\n",
      "003261\n",
      "002632\n",
      "005803\n",
      "000717\n",
      "005146\n",
      "005033\n",
      "000095\n",
      "004237\n",
      "004359\n",
      "002011\n",
      "008285\n",
      "003703\n",
      "005535\n",
      "008272\n",
      "006707\n",
      "005637\n",
      "007263\n",
      "001787\n",
      "003516\n",
      "007299\n",
      "009405\n",
      "009911\n",
      "005267\n",
      "005593\n",
      "003722\n",
      "006696\n",
      "004558\n",
      "001514\n",
      "000535\n",
      "001499\n",
      "006597\n",
      "009378\n",
      "002913\n",
      "006241\n",
      "006262\n",
      "003750\n",
      "008638\n",
      "006447\n",
      "003413\n",
      "008498\n",
      "004341\n",
      "002461\n",
      "009619\n",
      "000229\n",
      "008203\n",
      "001834\n",
      "004841\n",
      "006681\n",
      "005072\n",
      "002579\n",
      "005374\n",
      "007334\n",
      "006212\n",
      "001944\n",
      "004746\n",
      "009058\n",
      "003807\n",
      "001970\n",
      "003374\n",
      "006784\n",
      "005387\n",
      "009945\n",
      "001528\n",
      "006800\n",
      "004830\n",
      "004693\n",
      "007159\n",
      "000382\n",
      "006202\n",
      "007298\n",
      "007712\n",
      "001906\n",
      "000767\n",
      "002466\n",
      "005645\n",
      "002513\n",
      "008687\n",
      "003282\n",
      "003204\n",
      "008424\n",
      "008601\n",
      "001172\n",
      "004433\n",
      "003344\n",
      "008653\n",
      "008364\n",
      "002158\n",
      "005288\n",
      "003990\n",
      "008730\n",
      "009189\n",
      "007166\n",
      "009562\n",
      "004110\n",
      "004100\n",
      "005790\n",
      "002744\n",
      "006632\n",
      "009897\n",
      "003163\n",
      "005672\n",
      "008320\n",
      "003121\n",
      "007351\n",
      "005084\n",
      "001749\n",
      "002226\n",
      "006440\n",
      "004279\n",
      "009168\n",
      "000331\n",
      "006382\n",
      "002546\n",
      "005606\n",
      "001378\n",
      "008747\n",
      "006695\n",
      "002172\n",
      "008713\n",
      "002043\n",
      "003821\n",
      "006542\n",
      "004092\n",
      "003300\n",
      "002146\n",
      "002255\n",
      "005114\n",
      "004349\n",
      "001861\n",
      "005214\n",
      "004600\n",
      "000042\n",
      "006088\n",
      "007205\n",
      "000895\n",
      "003054\n",
      "004464\n",
      "002727\n",
      "001079\n",
      "001962\n",
      "008502\n",
      "009433\n",
      "005269\n",
      "008757\n",
      "009763\n",
      "007740\n",
      "000966\n",
      "000083\n",
      "004849\n",
      "000525\n",
      "007128\n",
      "002976\n",
      "006071\n",
      "004595\n",
      "006914\n",
      "006062\n",
      "009024\n",
      "002667\n",
      "003027\n",
      "003047\n",
      "003065\n",
      "007781\n",
      "006259\n",
      "006140\n",
      "006185\n",
      "005609\n",
      "001434\n",
      "004300\n",
      "003335\n",
      "002355\n",
      "007243\n",
      "004256\n",
      "007104\n",
      "003828\n",
      "009558\n",
      "006224\n",
      "000503\n",
      "007795\n",
      "005605\n",
      "002186\n",
      "004280\n",
      "001904\n",
      "001187\n",
      "001958\n",
      "003983\n",
      "002318\n",
      "008409\n",
      "004963\n",
      "006124\n",
      "009036\n",
      "001091\n",
      "003941\n",
      "007100\n",
      "002228\n",
      "001472\n",
      "006158\n",
      "003588\n",
      "009318\n",
      "006670\n",
      "003960\n",
      "009579\n",
      "007318\n",
      "005515\n",
      "005851\n",
      "009078\n",
      "009549\n",
      "006134\n",
      "005990\n",
      "003696\n",
      "005424\n",
      "008160\n",
      "009394\n",
      "004977\n",
      "001298\n",
      "008218\n",
      "000404\n",
      "009733\n",
      "009785\n",
      "003086\n",
      "008261\n",
      "007547\n",
      "008811\n",
      "004033\n",
      "004259\n",
      "005713\n",
      "005687\n",
      "007249\n",
      "008585\n",
      "002564\n",
      "007215\n",
      "002779\n",
      "003500\n",
      "009636\n",
      "004463\n",
      "008768\n",
      "003969\n",
      "001194\n",
      "006896\n",
      "005451\n",
      "005406\n",
      "004592\n",
      "000628\n",
      "002899\n",
      "008017\n",
      "009810\n",
      "003403\n",
      "008061\n",
      "001069\n",
      "007479\n",
      "006023\n",
      "001561\n",
      "000354\n",
      "005029\n",
      "001290\n",
      "002171\n",
      "003271\n",
      "005150\n",
      "005899\n",
      "005796\n",
      "001971\n",
      "006887\n",
      "007361\n",
      "002315\n",
      "007873\n",
      "003243\n",
      "000739\n",
      "004754\n",
      "000454\n",
      "008327\n",
      "007813\n",
      "005273\n",
      "006575\n",
      "004017\n",
      "005116\n",
      "006135\n",
      "002135\n",
      "002170\n",
      "005020\n",
      "002992\n",
      "001082\n",
      "002502\n",
      "004991\n",
      "002199\n",
      "007438\n",
      "005839\n",
      "007702\n",
      "007147\n",
      "001078\n",
      "002734\n",
      "003363\n",
      "008444\n",
      "005219\n",
      "003551\n",
      "006344\n",
      "009066\n",
      "008423\n",
      "003202\n",
      "002916\n",
      "006299\n",
      "001758\n",
      "009491\n",
      "007250\n",
      "000832\n",
      "007655\n",
      "002812\n",
      "009133\n",
      "003948\n",
      "006718\n",
      "008849\n",
      "007974\n",
      "000494\n",
      "007491\n",
      "009609\n",
      "005541\n",
      "005037\n",
      "008063\n",
      "007953\n",
      "004563\n",
      "002134\n",
      "001888\n",
      "005573\n",
      "000802\n",
      "009761\n",
      "001801\n",
      "006100\n",
      "002962\n",
      "009519\n",
      "000867\n",
      "008744\n",
      "003147\n",
      "005129\n",
      "001594\n",
      "000396\n",
      "001643\n",
      "000317\n",
      "006643\n",
      "004566\n",
      "000926\n",
      "004365\n",
      "005618\n",
      "008965\n",
      "006585\n",
      "000550\n",
      "002479\n",
      "005383\n",
      "003548\n",
      "005742\n",
      "006357\n",
      "000373\n",
      "001432\n",
      "009908\n",
      "003451\n",
      "000993\n",
      "004622\n",
      "009819\n",
      "009312\n",
      "002192\n",
      "002419\n",
      "001144\n",
      "000902\n",
      "009709\n",
      "002256\n",
      "006035\n",
      "006730\n",
      "000896\n",
      "001754\n",
      "002420\n",
      "008069\n",
      "006828\n",
      "007180\n",
      "001995\n",
      "006930\n",
      "002875\n",
      "007603\n",
      "006971\n",
      "007773\n",
      "001009\n",
      "001932\n",
      "007046\n",
      "001795\n",
      "007834\n",
      "001254\n",
      "007305\n",
      "001565\n",
      "000791\n",
      "004552\n",
      "002045\n",
      "005668\n",
      "007448\n",
      "005314\n",
      "002220\n",
      "005859\n",
      "003376\n",
      "007481\n",
      "000685\n",
      "003484\n",
      "003639\n",
      "008819\n",
      "002994\n",
      "005281\n",
      "001168\n",
      "007650\n",
      "001724\n",
      "005355\n",
      "005325\n",
      "001854\n",
      "008522\n",
      "002693\n",
      "000752\n",
      "007615\n",
      "004129\n",
      "006041\n",
      "003367\n",
      "007219\n",
      "004805\n",
      "005285\n",
      "001143\n",
      "005077\n",
      "004338\n",
      "008386\n",
      "003469\n",
      "006148\n",
      "004437\n",
      "004660\n",
      "008335\n",
      "002000\n",
      "004164\n",
      "004679\n",
      "006325\n",
      "002966\n",
      "005014\n",
      "003706\n",
      "004510\n",
      "007468\n",
      "000989\n",
      "004158\n",
      "002847\n",
      "009331\n",
      "005047\n",
      "001747\n",
      "002755\n",
      "003116\n",
      "009855\n",
      "001497\n",
      "004537\n",
      "006876\n",
      "003213\n",
      "001206\n",
      "004310\n",
      "000682\n",
      "000977\n",
      "003274\n",
      "004499\n",
      "001559\n",
      "003524\n",
      "007435\n",
      "002244\n",
      "002766\n",
      "003009\n",
      "000514\n",
      "003627\n",
      "000211\n",
      "004627\n",
      "009703\n",
      "007311\n",
      "002845\n",
      "007073\n",
      "001638\n",
      "002277\n",
      "003634\n",
      "001192\n",
      "003336\n",
      "002563\n",
      "001586\n",
      "003373\n",
      "001287\n",
      "008572\n",
      "004850\n",
      "009822\n",
      "001066\n",
      "006177\n",
      "006839\n",
      "001327\n",
      "009518\n",
      "004014\n",
      "006548\n",
      "000184\n",
      "009354\n",
      "009959\n",
      "004986\n",
      "008456\n",
      "006243\n",
      "002954\n",
      "006437\n",
      "003408\n",
      "004911\n",
      "005486\n",
      "002478\n",
      "008608\n",
      "006011\n",
      "005396\n",
      "001761\n",
      "007691\n",
      "007885\n",
      "007694\n",
      "008663\n",
      "007184\n",
      "006824\n",
      "000275\n",
      "006515\n",
      "006903\n",
      "007470\n",
      "005489\n",
      "000612\n",
      "000311\n",
      "000140\n",
      "001371\n",
      "007551\n",
      "006189\n",
      "008263\n",
      "006419\n",
      "002241\n",
      "004389\n",
      "003443\n",
      "003644\n",
      "000170\n",
      "009488\n",
      "007292\n",
      "002680\n",
      "000101\n",
      "008445\n",
      "003887\n",
      "000007\n",
      "004339\n",
      "000480\n",
      "005653\n",
      "003112\n",
      "005104\n",
      "008517\n",
      "002529\n",
      "006821\n",
      "002841\n",
      "008292\n",
      "005828\n",
      "000200\n",
      "005305\n",
      "007897\n",
      "009128\n",
      "007656\n",
      "001293\n",
      "000470\n",
      "002879\n",
      "009179\n",
      "004532\n",
      "005884\n",
      "003449\n",
      "004601\n",
      "007260\n",
      "009585\n",
      "005056\n",
      "003606\n",
      "009051\n",
      "000225\n",
      "002594\n",
      "002599\n",
      "009461\n",
      "000987\n",
      "002600\n",
      "004346\n",
      "008644\n",
      "000818\n",
      "002280\n",
      "001981\n",
      "009717\n",
      "009053\n",
      "005078\n",
      "002306\n",
      "008151\n",
      "005177\n",
      "000035\n",
      "007213\n",
      "007537\n",
      "002233\n",
      "003792\n",
      "006637\n",
      "006738\n",
      "003186\n",
      "000303\n",
      "004587\n",
      "005841\n",
      "007417\n",
      "001457\n",
      "008122\n",
      "008422\n",
      "001650\n",
      "003359\n",
      "008739\n",
      "006095\n",
      "002083\n",
      "003023\n",
      "006005\n",
      "001279\n",
      "009878\n",
      "007980\n",
      "004555\n",
      "008933\n",
      "007555\n",
      "006654\n",
      "004655\n",
      "009016\n",
      "003053\n",
      "005794\n",
      "009359\n",
      "006264\n",
      "002156\n",
      "003506\n",
      "005388\n",
      "000829\n",
      "002760\n",
      "001830\n",
      "007302\n",
      "003743\n",
      "008449\n",
      "001800\n",
      "002350\n",
      "002387\n",
      "002403\n",
      "004315\n",
      "006768\n",
      "002836\n",
      "008484\n",
      "002234\n",
      "004618\n",
      "000285\n",
      "002117\n",
      "009611\n",
      "003296\n",
      "007050\n",
      "008067\n",
      "006480\n",
      "004528\n",
      "005660\n",
      "004776\n",
      "007594\n",
      "005911\n",
      "001950\n",
      "001531\n",
      "009872\n",
      "008541\n",
      "000962\n",
      "009252\n",
      "006462\n",
      "009457\n",
      "001272\n",
      "003711\n",
      "000344\n",
      "005385\n",
      "009178\n",
      "004133\n",
      "000515\n",
      "009309\n",
      "004823\n",
      "009946\n",
      "004012\n",
      "008890\n",
      "008884\n",
      "000815\n",
      "001451\n",
      "003370\n",
      "009666\n",
      "004371\n",
      "004370\n",
      "008009\n",
      "005511\n",
      "007129\n",
      "007070\n",
      "001182\n",
      "002893\n",
      "006304\n",
      "001486\n",
      "002063\n",
      "007016\n",
      "003470\n",
      "001364\n",
      "004542\n",
      "004203\n",
      "005636\n",
      "001689\n",
      "008138\n",
      "001468\n",
      "008718\n",
      "007819\n",
      "000044\n",
      "000677\n",
      "009605\n",
      "007425\n",
      "004113\n",
      "004782\n",
      "009832\n",
      "004269\n",
      "003863\n",
      "006096\n",
      "004591\n",
      "002699\n",
      "006065\n",
      "006671\n",
      "006215\n",
      "002459\n",
      "006878\n",
      "005290\n",
      "009244\n",
      "001362\n",
      "003824\n",
      "006233\n",
      "001989\n",
      "003406\n",
      "006220\n",
      "008783\n",
      "008075\n",
      "005418\n",
      "007647\n",
      "007572\n",
      "002537\n",
      "009749\n",
      "005171\n",
      "008466\n",
      "000748\n",
      "008716\n",
      "002015\n",
      "009286\n",
      "002101\n",
      "009108\n",
      "001837\n",
      "004102\n",
      "001145\n",
      "005122\n",
      "008235\n",
      "005465\n",
      "005097\n",
      "007705\n",
      "007566\n",
      "007114\n",
      "006269\n",
      "004052\n",
      "008722\n",
      "001843\n",
      "007911\n",
      "009418\n",
      "002987\n",
      "002082\n",
      "007642\n",
      "001248\n",
      "001799\n",
      "006835\n",
      "006362\n",
      "003695\n",
      "001112\n",
      "008117\n",
      "000169\n",
      "007767\n",
      "008282\n",
      "004801\n",
      "004611\n",
      "009664\n",
      "004675\n",
      "000328\n",
      "005723\n",
      "004495\n",
      "003044\n",
      "004840\n",
      "009330\n",
      "009035\n",
      "000537\n",
      "000433\n",
      "001260\n",
      "009776\n",
      "003811\n",
      "005064\n",
      "003088\n",
      "006074\n",
      "000645\n",
      "000528\n",
      "008211\n",
      "006209\n",
      "009215\n",
      "001708\n",
      "000787\n",
      "009597\n",
      "001113\n",
      "004588\n",
      "009362\n",
      "007172\n",
      "009094\n",
      "002868\n",
      "000936\n",
      "005121\n",
      "001001\n",
      "007688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "009698\n",
      "009460\n",
      "003791\n",
      "006279\n",
      "000187\n",
      "006499\n",
      "009255\n",
      "004152\n",
      "009841\n",
      "000320\n",
      "001129\n",
      "000633\n",
      "000347\n",
      "000282\n",
      "006668\n",
      "002919\n",
      "007590\n",
      "003814\n",
      "002155\n",
      "008225\n",
      "008858\n",
      "007191\n",
      "002273\n",
      "003379\n",
      "004265\n",
      "000520\n",
      "003895\n",
      "009543\n",
      "009676\n",
      "006084\n",
      "006230\n",
      "006396\n",
      "000180\n",
      "004623\n",
      "004903\n",
      "002281\n",
      "001493\n",
      "003973\n",
      "002021\n",
      "008284\n",
      "004958\n",
      "000318\n",
      "000845\n",
      "007810\n",
      "000834\n",
      "004230\n",
      "000680\n",
      "001673\n",
      "006697\n",
      "007503\n",
      "005454\n",
      "000218\n",
      "002190\n",
      "002896\n",
      "000806\n",
      "007234\n",
      "002697\n",
      "006664\n",
      "005624\n",
      "001277\n",
      "001148\n",
      "008979\n",
      "007422\n",
      "003028\n",
      "002605\n",
      "002348\n",
      "009174\n",
      "005644\n",
      "003162\n",
      "005730\n",
      "007482\n",
      "005223\n",
      "001936\n",
      "003417\n",
      "002782\n",
      "009685\n",
      "000209\n",
      "009020\n",
      "006450\n",
      "008822\n",
      "005799\n",
      "001759\n",
      "008125\n",
      "007762\n",
      "004825\n",
      "004973\n",
      "008836\n",
      "003511\n",
      "005885\n",
      "009687\n",
      "001444\n",
      "001778\n",
      "005441\n",
      "004722\n",
      "008037\n",
      "002377\n",
      "000625\n",
      "000830\n",
      "007946\n",
      "008316\n",
      "003108\n",
      "009150\n",
      "004960\n",
      "006150\n",
      "007108\n",
      "004753\n",
      "004270\n",
      "001240\n",
      "002268\n",
      "003083\n",
      "006296\n",
      "003004\n",
      "004325\n",
      "004540\n",
      "001152\n",
      "002572\n",
      "007926\n",
      "006276\n",
      "005348\n",
      "007653\n",
      "002369\n",
      "003039\n",
      "005315\n",
      "003100\n",
      "000242\n",
      "009063\n",
      "005457\n",
      "004846\n",
      "007474\n",
      "002524\n",
      "003658\n",
      "002901\n",
      "007140\n",
      "005806\n",
      "003860\n",
      "004293\n",
      "006151\n",
      "000352\n",
      "004706\n",
      "002917\n",
      "000009\n",
      "009587\n",
      "008223\n",
      "006602\n",
      "003021\n",
      "005877\n",
      "007968\n",
      "000589\n",
      "007970\n",
      "003231\n",
      "003343\n",
      "008676\n",
      "009484\n",
      "000245\n",
      "003932\n",
      "005752\n",
      "001292\n",
      "000210\n",
      "008549\n",
      "005788\n",
      "004085\n",
      "006346\n",
      "008180\n",
      "003316\n",
      "005131\n",
      "000430\n",
      "007336\n",
      "002037\n",
      "006599\n",
      "002826\n",
      "009764\n",
      "000605\n",
      "007029\n",
      "009159\n",
      "007144\n",
      "001420\n",
      "006625\n",
      "000522\n",
      "001604\n",
      "009045\n",
      "006221\n",
      "007308\n",
      "008042\n",
      "009407\n",
      "007940\n",
      "000349\n",
      "008969\n",
      "008171\n",
      "005006\n",
      "008236\n",
      "002442\n",
      "009245\n",
      "009746\n",
      "006802\n",
      "007546\n",
      "004193\n",
      "001825\n",
      "002163\n",
      "007283\n",
      "002500\n",
      "003464\n",
      "001832\n",
      "007359\n",
      "009479\n",
      "003861\n",
      "002567\n",
      "005481\n",
      "001027\n",
      "003042\n",
      "006171\n",
      "009221\n",
      "009251\n",
      "001922\n",
      "007437\n",
      "009823\n",
      "000921\n",
      "004961\n",
      "004223\n",
      "002279\n",
      "009458\n",
      "008536\n",
      "006058\n",
      "006794\n",
      "002265\n",
      "002952\n",
      "009273\n",
      "009406\n",
      "008253\n",
      "009726\n",
      "003673\n",
      "000102\n",
      "000219\n",
      "007258\n",
      "006990\n",
      "007538\n",
      "009588\n",
      "004671\n",
      "007971\n",
      "001350\n",
      "002374\n",
      "004275\n",
      "006371\n",
      "008553\n",
      "009961\n",
      "000777\n",
      "001771\n",
      "009813\n",
      "007790\n",
      "009272\n",
      "005210\n",
      "006466\n",
      "007618\n",
      "003269\n",
      "000564\n",
      "002549\n",
      "003529\n",
      "008174\n",
      "007662\n",
      "001199\n",
      "001233\n",
      "006838\n",
      "004391\n",
      "000581\n",
      "008891\n",
      "002214\n",
      "003970\n",
      "000672\n",
      "007167\n",
      "004773\n",
      "001536\n",
      "003732\n",
      "000032\n",
      "005379\n",
      "001119\n",
      "002569\n",
      "008854\n",
      "007886\n",
      "005497\n",
      "001215\n",
      "000132\n",
      "006070\n",
      "002817\n",
      "008535\n",
      "007633\n",
      "002126\n",
      "008914\n",
      "002589\n",
      "004035\n",
      "000609\n",
      "005156\n",
      "001662\n",
      "007571\n",
      "003094\n",
      "000965\n",
      "006735\n",
      "002715\n"
     ]
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((resnet_input,resnet_input)),\n",
    "                                         transforms.RandomHorizontalFlip(),\n",
    "                                         transforms.ToTensor()])\n",
    "train_dataset = hound_dataset(root_dir='.', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "# test_dataset = hound_dataset(root_dir='.', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning\n",
    "Litlefinger has brought you a pre-trained network. Fine-tune the network in the following section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 21)\n",
    "\n",
    "# Add code for using CUDA here\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    resnet18.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Update if any errors occur\n",
    "optimizer = torch.optim.SGD(resnet18.parameters(), learning_rate, hyp_momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def arya_train():\n",
    "    # Begin\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if(use_gpu):\n",
    "                images=images.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_arr.append(loss.data[0])\n",
    "            if (i+1) % batch_size == 0:\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2/10336], Loss: 3.2987\n",
      "Epoch [1/5], Step [4/10336], Loss: 2.7368\n",
      "Epoch [1/5], Step [6/10336], Loss: 2.4968\n",
      "Epoch [1/5], Step [8/10336], Loss: 2.5038\n",
      "Epoch [1/5], Step [10/10336], Loss: 1.4645\n",
      "Epoch [1/5], Step [12/10336], Loss: 0.7685\n",
      "Epoch [1/5], Step [14/10336], Loss: 0.8538\n",
      "Epoch [1/5], Step [16/10336], Loss: 2.7035\n",
      "Epoch [1/5], Step [18/10336], Loss: 4.9910\n",
      "Epoch [1/5], Step [20/10336], Loss: 0.3253\n",
      "Epoch [1/5], Step [22/10336], Loss: 1.1724\n",
      "Epoch [1/5], Step [24/10336], Loss: 5.2462\n",
      "Epoch [1/5], Step [26/10336], Loss: 3.7815\n",
      "Epoch [1/5], Step [28/10336], Loss: 5.9995\n",
      "Epoch [1/5], Step [30/10336], Loss: 3.2110\n",
      "Epoch [1/5], Step [32/10336], Loss: 4.5494\n",
      "Epoch [1/5], Step [34/10336], Loss: 3.1467\n",
      "Epoch [1/5], Step [36/10336], Loss: 4.9604\n",
      "Epoch [1/5], Step [38/10336], Loss: 3.0776\n",
      "Epoch [1/5], Step [40/10336], Loss: 2.2289\n",
      "Epoch [1/5], Step [42/10336], Loss: 1.9813\n",
      "Epoch [1/5], Step [44/10336], Loss: 3.1018\n",
      "Epoch [1/5], Step [46/10336], Loss: 4.0856\n",
      "Epoch [1/5], Step [48/10336], Loss: 1.2990\n",
      "Epoch [1/5], Step [50/10336], Loss: 1.8065\n",
      "Epoch [1/5], Step [52/10336], Loss: 3.7032\n",
      "Epoch [1/5], Step [54/10336], Loss: 4.6397\n",
      "Epoch [1/5], Step [56/10336], Loss: 1.1320\n",
      "Epoch [1/5], Step [58/10336], Loss: 1.0828\n",
      "Epoch [1/5], Step [60/10336], Loss: 3.6856\n",
      "Epoch [1/5], Step [62/10336], Loss: 3.3078\n",
      "Epoch [1/5], Step [64/10336], Loss: 1.8466\n",
      "Epoch [1/5], Step [66/10336], Loss: 1.6002\n",
      "Epoch [1/5], Step [68/10336], Loss: 2.4112\n",
      "Epoch [1/5], Step [70/10336], Loss: 0.5063\n",
      "Epoch [1/5], Step [72/10336], Loss: 4.1379\n",
      "Epoch [1/5], Step [74/10336], Loss: 4.0716\n",
      "Epoch [1/5], Step [76/10336], Loss: 1.4765\n",
      "Epoch [1/5], Step [78/10336], Loss: 2.0702\n",
      "Epoch [1/5], Step [80/10336], Loss: 3.3746\n",
      "Epoch [1/5], Step [82/10336], Loss: 5.5892\n",
      "Epoch [1/5], Step [84/10336], Loss: 3.0355\n",
      "Epoch [1/5], Step [86/10336], Loss: 3.2507\n",
      "Epoch [1/5], Step [88/10336], Loss: 0.7403\n",
      "Epoch [1/5], Step [90/10336], Loss: 1.7113\n",
      "Epoch [1/5], Step [92/10336], Loss: 2.0102\n",
      "Epoch [1/5], Step [94/10336], Loss: 2.4047\n",
      "Epoch [1/5], Step [96/10336], Loss: 5.3778\n",
      "Epoch [1/5], Step [98/10336], Loss: 2.4888\n",
      "Epoch [1/5], Step [100/10336], Loss: 5.5233\n",
      "Epoch [1/5], Step [102/10336], Loss: 2.9651\n",
      "Epoch [1/5], Step [104/10336], Loss: 0.6074\n",
      "Epoch [1/5], Step [106/10336], Loss: 4.1178\n",
      "Epoch [1/5], Step [108/10336], Loss: 3.1429\n",
      "Epoch [1/5], Step [110/10336], Loss: 1.3937\n",
      "Epoch [1/5], Step [112/10336], Loss: 0.6733\n",
      "Epoch [1/5], Step [114/10336], Loss: 0.9762\n",
      "Epoch [1/5], Step [116/10336], Loss: 4.1690\n",
      "Epoch [1/5], Step [118/10336], Loss: 1.3468\n",
      "Epoch [1/5], Step [120/10336], Loss: 2.7516\n",
      "Epoch [1/5], Step [122/10336], Loss: 0.9665\n",
      "Epoch [1/5], Step [124/10336], Loss: 4.2844\n",
      "Epoch [1/5], Step [126/10336], Loss: 6.6064\n",
      "Epoch [1/5], Step [128/10336], Loss: 2.0584\n",
      "Epoch [1/5], Step [130/10336], Loss: 2.7633\n",
      "Epoch [1/5], Step [132/10336], Loss: 4.1743\n",
      "Epoch [1/5], Step [134/10336], Loss: 2.4333\n",
      "Epoch [1/5], Step [136/10336], Loss: 4.0655\n",
      "Epoch [1/5], Step [138/10336], Loss: 2.3880\n",
      "Epoch [1/5], Step [140/10336], Loss: 2.7190\n",
      "Epoch [1/5], Step [142/10336], Loss: 1.5422\n",
      "Epoch [1/5], Step [144/10336], Loss: 2.6452\n",
      "Epoch [1/5], Step [146/10336], Loss: 1.7475\n",
      "Epoch [1/5], Step [148/10336], Loss: 1.9449\n",
      "Epoch [1/5], Step [150/10336], Loss: 4.5418\n",
      "Epoch [1/5], Step [152/10336], Loss: 0.3290\n",
      "Epoch [1/5], Step [154/10336], Loss: 3.4885\n",
      "Epoch [1/5], Step [156/10336], Loss: 2.6364\n",
      "Epoch [1/5], Step [158/10336], Loss: 2.3407\n",
      "Epoch [1/5], Step [160/10336], Loss: 1.2809\n",
      "Epoch [1/5], Step [162/10336], Loss: 1.8738\n",
      "Epoch [1/5], Step [164/10336], Loss: 1.3809\n",
      "Epoch [1/5], Step [166/10336], Loss: 2.3839\n",
      "Epoch [1/5], Step [168/10336], Loss: 0.5443\n",
      "Epoch [1/5], Step [170/10336], Loss: 2.0600\n",
      "Epoch [1/5], Step [172/10336], Loss: 2.2144\n",
      "Epoch [1/5], Step [174/10336], Loss: 4.4386\n",
      "Epoch [1/5], Step [176/10336], Loss: 2.6958\n",
      "Epoch [1/5], Step [178/10336], Loss: 1.8397\n",
      "Epoch [1/5], Step [180/10336], Loss: 1.6839\n",
      "Epoch [1/5], Step [182/10336], Loss: 3.5525\n",
      "Epoch [1/5], Step [184/10336], Loss: 3.3272\n",
      "Epoch [1/5], Step [186/10336], Loss: 1.9456\n",
      "Epoch [1/5], Step [188/10336], Loss: 0.7450\n",
      "Epoch [1/5], Step [190/10336], Loss: 4.4939\n",
      "Epoch [1/5], Step [192/10336], Loss: 0.9048\n",
      "Epoch [1/5], Step [194/10336], Loss: 1.1326\n",
      "Epoch [1/5], Step [196/10336], Loss: 3.4209\n",
      "Epoch [1/5], Step [198/10336], Loss: 2.9142\n",
      "Epoch [1/5], Step [200/10336], Loss: 2.4188\n",
      "Epoch [1/5], Step [202/10336], Loss: 2.9539\n",
      "Epoch [1/5], Step [204/10336], Loss: 0.9449\n",
      "Epoch [1/5], Step [206/10336], Loss: 3.1485\n",
      "Epoch [1/5], Step [208/10336], Loss: 0.2086\n",
      "Epoch [1/5], Step [210/10336], Loss: 1.3617\n",
      "Epoch [1/5], Step [212/10336], Loss: 2.3052\n",
      "Epoch [1/5], Step [214/10336], Loss: 3.3676\n",
      "Epoch [1/5], Step [216/10336], Loss: 2.8371\n",
      "Epoch [1/5], Step [218/10336], Loss: 3.1485\n",
      "Epoch [1/5], Step [220/10336], Loss: 3.1654\n",
      "Epoch [1/5], Step [222/10336], Loss: 2.9887\n",
      "Epoch [1/5], Step [224/10336], Loss: 3.2097\n",
      "Epoch [1/5], Step [226/10336], Loss: 1.5131\n",
      "Epoch [1/5], Step [228/10336], Loss: 3.6728\n",
      "Epoch [1/5], Step [230/10336], Loss: 2.1071\n",
      "Epoch [1/5], Step [232/10336], Loss: 1.9434\n",
      "Epoch [1/5], Step [234/10336], Loss: 3.5126\n",
      "Epoch [1/5], Step [236/10336], Loss: 2.3592\n",
      "Epoch [1/5], Step [238/10336], Loss: 2.2261\n",
      "Epoch [1/5], Step [240/10336], Loss: 2.3969\n",
      "Epoch [1/5], Step [242/10336], Loss: 3.4799\n",
      "Epoch [1/5], Step [244/10336], Loss: 1.7271\n",
      "Epoch [1/5], Step [246/10336], Loss: 2.5326\n",
      "Epoch [1/5], Step [248/10336], Loss: 2.0343\n",
      "Epoch [1/5], Step [250/10336], Loss: 2.9860\n",
      "Epoch [1/5], Step [252/10336], Loss: 1.5678\n",
      "Epoch [1/5], Step [254/10336], Loss: 2.1232\n",
      "Epoch [1/5], Step [256/10336], Loss: 1.5633\n",
      "Epoch [1/5], Step [258/10336], Loss: 2.1059\n",
      "Epoch [1/5], Step [260/10336], Loss: 3.2146\n",
      "Epoch [1/5], Step [262/10336], Loss: 2.3956\n",
      "Epoch [1/5], Step [264/10336], Loss: 4.8215\n",
      "Epoch [1/5], Step [266/10336], Loss: 2.4836\n",
      "Epoch [1/5], Step [268/10336], Loss: 2.7704\n",
      "Epoch [1/5], Step [270/10336], Loss: 2.2581\n",
      "Epoch [1/5], Step [272/10336], Loss: 2.0023\n",
      "Epoch [1/5], Step [274/10336], Loss: 2.4728\n",
      "Epoch [1/5], Step [276/10336], Loss: 2.2847\n",
      "Epoch [1/5], Step [278/10336], Loss: 1.3542\n",
      "Epoch [1/5], Step [280/10336], Loss: 1.4955\n",
      "Epoch [1/5], Step [282/10336], Loss: 1.2189\n",
      "Epoch [1/5], Step [284/10336], Loss: 2.8538\n",
      "Epoch [1/5], Step [286/10336], Loss: 1.5309\n",
      "Epoch [1/5], Step [288/10336], Loss: 2.7927\n",
      "Epoch [1/5], Step [290/10336], Loss: 1.5751\n",
      "Epoch [1/5], Step [292/10336], Loss: 0.1546\n",
      "Epoch [1/5], Step [294/10336], Loss: 3.3592\n",
      "Epoch [1/5], Step [296/10336], Loss: 1.5160\n",
      "Epoch [1/5], Step [298/10336], Loss: 0.7348\n",
      "Epoch [1/5], Step [300/10336], Loss: 3.6894\n",
      "Epoch [1/5], Step [302/10336], Loss: 2.2464\n",
      "Epoch [1/5], Step [304/10336], Loss: 0.8371\n",
      "Epoch [1/5], Step [306/10336], Loss: 1.6918\n",
      "Epoch [1/5], Step [308/10336], Loss: 2.1024\n",
      "Epoch [1/5], Step [310/10336], Loss: 2.8071\n",
      "Epoch [1/5], Step [312/10336], Loss: 1.5120\n",
      "Epoch [1/5], Step [314/10336], Loss: 0.2201\n",
      "Epoch [1/5], Step [316/10336], Loss: 2.3602\n",
      "Epoch [1/5], Step [318/10336], Loss: 0.1283\n",
      "Epoch [1/5], Step [320/10336], Loss: 0.7275\n",
      "Epoch [1/5], Step [322/10336], Loss: 2.5312\n",
      "Epoch [1/5], Step [324/10336], Loss: 1.5051\n",
      "Epoch [1/5], Step [326/10336], Loss: 2.4675\n",
      "Epoch [1/5], Step [328/10336], Loss: 3.1837\n",
      "Epoch [1/5], Step [330/10336], Loss: 0.3159\n",
      "Epoch [1/5], Step [332/10336], Loss: 1.3786\n",
      "Epoch [1/5], Step [334/10336], Loss: 0.7875\n",
      "Epoch [1/5], Step [336/10336], Loss: 2.1055\n",
      "Epoch [1/5], Step [338/10336], Loss: 1.0963\n",
      "Epoch [1/5], Step [340/10336], Loss: 6.8471\n",
      "Epoch [1/5], Step [342/10336], Loss: 2.3731\n",
      "Epoch [1/5], Step [344/10336], Loss: 0.4889\n",
      "Epoch [1/5], Step [346/10336], Loss: 3.8847\n",
      "Epoch [1/5], Step [348/10336], Loss: 1.5147\n",
      "Epoch [1/5], Step [350/10336], Loss: 5.2546\n",
      "Epoch [1/5], Step [352/10336], Loss: 3.8198\n",
      "Epoch [1/5], Step [354/10336], Loss: 5.8300\n",
      "Epoch [1/5], Step [356/10336], Loss: 3.2924\n",
      "Epoch [1/5], Step [358/10336], Loss: 1.3769\n",
      "Epoch [1/5], Step [360/10336], Loss: 2.9861\n",
      "Epoch [1/5], Step [362/10336], Loss: 2.3398\n",
      "Epoch [1/5], Step [364/10336], Loss: 0.5774\n",
      "Epoch [1/5], Step [366/10336], Loss: 0.6712\n",
      "Epoch [1/5], Step [368/10336], Loss: 7.2697\n",
      "Epoch [1/5], Step [370/10336], Loss: 0.7470\n",
      "Epoch [1/5], Step [372/10336], Loss: 5.2563\n",
      "Epoch [1/5], Step [374/10336], Loss: 3.1535\n",
      "Epoch [1/5], Step [376/10336], Loss: 1.0587\n",
      "Epoch [1/5], Step [378/10336], Loss: 2.3589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [380/10336], Loss: 1.4662\n",
      "Epoch [1/5], Step [382/10336], Loss: 2.8443\n",
      "Epoch [1/5], Step [384/10336], Loss: 4.2801\n",
      "Epoch [1/5], Step [386/10336], Loss: 1.3774\n",
      "Epoch [1/5], Step [388/10336], Loss: 5.2698\n",
      "Epoch [1/5], Step [390/10336], Loss: 5.5772\n",
      "Epoch [1/5], Step [392/10336], Loss: 3.5044\n",
      "Epoch [1/5], Step [394/10336], Loss: 1.7633\n",
      "Epoch [1/5], Step [396/10336], Loss: 1.4021\n",
      "Epoch [1/5], Step [398/10336], Loss: 5.0964\n",
      "Epoch [1/5], Step [400/10336], Loss: 5.7908\n",
      "Epoch [1/5], Step [402/10336], Loss: 4.5891\n",
      "Epoch [1/5], Step [404/10336], Loss: 3.5112\n",
      "Epoch [1/5], Step [406/10336], Loss: 2.1924\n",
      "Epoch [1/5], Step [408/10336], Loss: 5.0596\n",
      "Epoch [1/5], Step [410/10336], Loss: 5.3193\n",
      "Epoch [1/5], Step [412/10336], Loss: 1.2918\n",
      "Epoch [1/5], Step [414/10336], Loss: 5.4769\n",
      "Epoch [1/5], Step [416/10336], Loss: 4.4012\n",
      "Epoch [1/5], Step [418/10336], Loss: 2.8205\n",
      "Epoch [1/5], Step [420/10336], Loss: 1.4667\n",
      "Epoch [1/5], Step [422/10336], Loss: 4.4899\n",
      "Epoch [1/5], Step [424/10336], Loss: 3.6767\n",
      "Epoch [1/5], Step [426/10336], Loss: 1.8073\n",
      "Epoch [1/5], Step [428/10336], Loss: 1.7007\n",
      "Epoch [1/5], Step [430/10336], Loss: 4.6091\n",
      "Epoch [1/5], Step [432/10336], Loss: 1.2609\n",
      "Epoch [1/5], Step [434/10336], Loss: 4.4895\n",
      "Epoch [1/5], Step [436/10336], Loss: 0.8189\n",
      "Epoch [1/5], Step [438/10336], Loss: 1.7569\n",
      "Epoch [1/5], Step [440/10336], Loss: 2.4728\n",
      "Epoch [1/5], Step [442/10336], Loss: 0.1000\n",
      "Epoch [1/5], Step [444/10336], Loss: 2.0841\n",
      "Epoch [1/5], Step [446/10336], Loss: 0.6861\n",
      "Epoch [1/5], Step [448/10336], Loss: 5.3434\n",
      "Epoch [1/5], Step [450/10336], Loss: 2.4086\n",
      "Epoch [1/5], Step [452/10336], Loss: 0.8075\n",
      "Epoch [1/5], Step [454/10336], Loss: 4.9100\n",
      "Epoch [1/5], Step [456/10336], Loss: 2.9165\n",
      "Epoch [1/5], Step [458/10336], Loss: 0.4382\n",
      "Epoch [1/5], Step [460/10336], Loss: 1.6821\n",
      "Epoch [1/5], Step [462/10336], Loss: 3.2508\n",
      "Epoch [1/5], Step [464/10336], Loss: 0.3914\n",
      "Epoch [1/5], Step [466/10336], Loss: 1.9562\n",
      "Epoch [1/5], Step [468/10336], Loss: 3.0927\n",
      "Epoch [1/5], Step [470/10336], Loss: 1.4987\n",
      "Epoch [1/5], Step [472/10336], Loss: 2.3062\n",
      "Epoch [1/5], Step [474/10336], Loss: 2.4377\n",
      "Epoch [1/5], Step [476/10336], Loss: 2.4260\n",
      "Epoch [1/5], Step [478/10336], Loss: 3.1537\n",
      "Epoch [1/5], Step [480/10336], Loss: 0.7547\n",
      "Epoch [1/5], Step [482/10336], Loss: 4.1156\n",
      "Epoch [1/5], Step [484/10336], Loss: 0.0687\n",
      "Epoch [1/5], Step [486/10336], Loss: 2.2226\n",
      "Epoch [1/5], Step [488/10336], Loss: 2.4641\n",
      "Epoch [1/5], Step [490/10336], Loss: 1.7288\n",
      "Epoch [1/5], Step [492/10336], Loss: 4.2508\n",
      "Epoch [1/5], Step [494/10336], Loss: 2.6080\n",
      "Epoch [1/5], Step [496/10336], Loss: 1.3353\n",
      "Epoch [1/5], Step [498/10336], Loss: 1.9900\n",
      "Epoch [1/5], Step [500/10336], Loss: 2.0881\n",
      "Epoch [1/5], Step [502/10336], Loss: 1.9457\n",
      "Epoch [1/5], Step [504/10336], Loss: 1.1037\n",
      "Epoch [1/5], Step [506/10336], Loss: 2.9622\n",
      "Epoch [1/5], Step [508/10336], Loss: 2.2976\n",
      "Epoch [1/5], Step [510/10336], Loss: 2.4670\n",
      "Epoch [1/5], Step [512/10336], Loss: 2.8531\n",
      "Epoch [1/5], Step [514/10336], Loss: 4.0057\n",
      "Epoch [1/5], Step [516/10336], Loss: 1.2946\n",
      "Epoch [1/5], Step [518/10336], Loss: 3.4239\n",
      "Epoch [1/5], Step [520/10336], Loss: 4.5429\n",
      "Epoch [1/5], Step [522/10336], Loss: 3.0995\n",
      "Epoch [1/5], Step [524/10336], Loss: 5.8209\n",
      "Epoch [1/5], Step [526/10336], Loss: 2.6264\n",
      "Epoch [1/5], Step [528/10336], Loss: 4.6113\n",
      "Epoch [1/5], Step [530/10336], Loss: 2.3817\n",
      "Epoch [1/5], Step [532/10336], Loss: 1.7437\n",
      "Epoch [1/5], Step [534/10336], Loss: 1.2349\n",
      "Epoch [1/5], Step [536/10336], Loss: 0.6090\n",
      "Epoch [1/5], Step [538/10336], Loss: 1.7394\n",
      "Epoch [1/5], Step [540/10336], Loss: 0.8984\n",
      "Epoch [1/5], Step [542/10336], Loss: 4.5926\n",
      "Epoch [1/5], Step [544/10336], Loss: 0.4170\n",
      "Epoch [1/5], Step [546/10336], Loss: 2.3856\n",
      "Epoch [1/5], Step [548/10336], Loss: 2.4232\n",
      "Epoch [1/5], Step [550/10336], Loss: 1.8168\n",
      "Epoch [1/5], Step [552/10336], Loss: 5.1391\n",
      "Epoch [1/5], Step [554/10336], Loss: 3.1843\n",
      "Epoch [1/5], Step [556/10336], Loss: 0.4701\n",
      "Epoch [1/5], Step [558/10336], Loss: 3.0684\n",
      "Epoch [1/5], Step [560/10336], Loss: 1.1795\n",
      "Epoch [1/5], Step [562/10336], Loss: 2.7877\n",
      "Epoch [1/5], Step [564/10336], Loss: 4.0823\n",
      "Epoch [1/5], Step [566/10336], Loss: 3.0911\n",
      "Epoch [1/5], Step [568/10336], Loss: 4.3908\n",
      "Epoch [1/5], Step [570/10336], Loss: 1.1949\n",
      "Epoch [1/5], Step [572/10336], Loss: 2.8104\n",
      "Epoch [1/5], Step [574/10336], Loss: 1.1774\n",
      "Epoch [1/5], Step [576/10336], Loss: 4.9912\n",
      "Epoch [1/5], Step [578/10336], Loss: 3.2242\n",
      "Epoch [1/5], Step [580/10336], Loss: 3.4004\n",
      "Epoch [1/5], Step [582/10336], Loss: 2.0877\n",
      "Epoch [1/5], Step [584/10336], Loss: 3.7274\n",
      "Epoch [1/5], Step [586/10336], Loss: 2.0021\n",
      "Epoch [1/5], Step [588/10336], Loss: 2.1893\n",
      "Epoch [1/5], Step [590/10336], Loss: 0.1812\n",
      "Epoch [1/5], Step [592/10336], Loss: 3.3785\n",
      "Epoch [1/5], Step [594/10336], Loss: 0.7868\n",
      "Epoch [1/5], Step [596/10336], Loss: 2.4099\n",
      "Epoch [1/5], Step [598/10336], Loss: 2.8532\n",
      "Epoch [1/5], Step [600/10336], Loss: 1.7484\n",
      "Epoch [1/5], Step [602/10336], Loss: 4.8554\n",
      "Epoch [1/5], Step [604/10336], Loss: 3.0307\n",
      "Epoch [1/5], Step [606/10336], Loss: 2.1325\n",
      "Epoch [1/5], Step [608/10336], Loss: 1.1368\n",
      "Epoch [1/5], Step [610/10336], Loss: 3.4417\n",
      "Epoch [1/5], Step [612/10336], Loss: 1.9243\n",
      "Epoch [1/5], Step [614/10336], Loss: 2.5779\n",
      "Epoch [1/5], Step [616/10336], Loss: 1.2678\n",
      "Epoch [1/5], Step [618/10336], Loss: 3.7523\n",
      "Epoch [1/5], Step [620/10336], Loss: 1.7114\n",
      "Epoch [1/5], Step [622/10336], Loss: 3.9437\n",
      "Epoch [1/5], Step [624/10336], Loss: 1.4918\n",
      "Epoch [1/5], Step [626/10336], Loss: 3.7147\n",
      "Epoch [1/5], Step [628/10336], Loss: 1.0227\n",
      "Epoch [1/5], Step [630/10336], Loss: 2.4163\n",
      "Epoch [1/5], Step [632/10336], Loss: 0.6442\n",
      "Epoch [1/5], Step [634/10336], Loss: 0.7921\n",
      "Epoch [1/5], Step [636/10336], Loss: 1.3781\n",
      "Epoch [1/5], Step [638/10336], Loss: 0.7196\n",
      "Epoch [1/5], Step [640/10336], Loss: 4.8922\n",
      "Epoch [1/5], Step [642/10336], Loss: 2.1123\n",
      "Epoch [1/5], Step [644/10336], Loss: 2.3933\n",
      "Epoch [1/5], Step [646/10336], Loss: 3.9379\n",
      "Epoch [1/5], Step [648/10336], Loss: 3.2331\n",
      "Epoch [1/5], Step [650/10336], Loss: 2.8711\n",
      "Epoch [1/5], Step [652/10336], Loss: 3.6246\n",
      "Epoch [1/5], Step [654/10336], Loss: 1.5676\n",
      "Epoch [1/5], Step [656/10336], Loss: 1.3947\n",
      "Epoch [1/5], Step [658/10336], Loss: 0.3184\n",
      "Epoch [1/5], Step [660/10336], Loss: 1.9043\n",
      "Epoch [1/5], Step [662/10336], Loss: 1.5995\n",
      "Epoch [1/5], Step [664/10336], Loss: 3.1739\n",
      "Epoch [1/5], Step [666/10336], Loss: 3.5491\n",
      "Epoch [1/5], Step [668/10336], Loss: 0.1916\n",
      "Epoch [1/5], Step [670/10336], Loss: 3.4491\n",
      "Epoch [1/5], Step [672/10336], Loss: 1.6014\n",
      "Epoch [1/5], Step [674/10336], Loss: 1.3778\n",
      "Epoch [1/5], Step [676/10336], Loss: 4.4523\n",
      "Epoch [1/5], Step [678/10336], Loss: 5.3073\n",
      "Epoch [1/5], Step [680/10336], Loss: 2.7997\n",
      "Epoch [1/5], Step [682/10336], Loss: 2.4833\n",
      "Epoch [1/5], Step [684/10336], Loss: 1.0063\n",
      "Epoch [1/5], Step [686/10336], Loss: 1.7632\n",
      "Epoch [1/5], Step [688/10336], Loss: 2.8737\n",
      "Epoch [1/5], Step [690/10336], Loss: 3.5243\n",
      "Epoch [1/5], Step [692/10336], Loss: 5.6490\n",
      "Epoch [1/5], Step [694/10336], Loss: 3.9981\n",
      "Epoch [1/5], Step [696/10336], Loss: 3.8526\n",
      "Epoch [1/5], Step [698/10336], Loss: 3.1875\n",
      "Epoch [1/5], Step [700/10336], Loss: 2.0569\n",
      "Epoch [1/5], Step [702/10336], Loss: 3.0477\n",
      "Epoch [1/5], Step [704/10336], Loss: 1.6931\n",
      "Epoch [1/5], Step [706/10336], Loss: 1.7776\n",
      "Epoch [1/5], Step [708/10336], Loss: 4.8657\n",
      "Epoch [1/5], Step [710/10336], Loss: 2.6208\n",
      "Epoch [1/5], Step [712/10336], Loss: 4.2521\n",
      "Epoch [1/5], Step [714/10336], Loss: 2.4588\n",
      "Epoch [1/5], Step [716/10336], Loss: 0.5591\n",
      "Epoch [1/5], Step [718/10336], Loss: 2.4142\n",
      "Epoch [1/5], Step [720/10336], Loss: 3.0771\n",
      "Epoch [1/5], Step [722/10336], Loss: 1.8172\n",
      "Epoch [1/5], Step [724/10336], Loss: 3.0364\n",
      "Epoch [1/5], Step [726/10336], Loss: 1.3422\n",
      "Epoch [1/5], Step [728/10336], Loss: 1.1261\n",
      "Epoch [1/5], Step [730/10336], Loss: 4.3154\n",
      "Epoch [1/5], Step [732/10336], Loss: 2.5125\n",
      "Epoch [1/5], Step [734/10336], Loss: 1.7306\n",
      "Epoch [1/5], Step [736/10336], Loss: 1.1886\n",
      "Epoch [1/5], Step [738/10336], Loss: 3.7854\n",
      "Epoch [1/5], Step [740/10336], Loss: 0.4900\n",
      "Epoch [1/5], Step [742/10336], Loss: 3.5601\n",
      "Epoch [1/5], Step [744/10336], Loss: 2.0641\n",
      "Epoch [1/5], Step [746/10336], Loss: 0.5803\n",
      "Epoch [1/5], Step [748/10336], Loss: 1.3545\n",
      "Epoch [1/5], Step [750/10336], Loss: 0.8466\n",
      "Epoch [1/5], Step [752/10336], Loss: 2.5880\n",
      "Epoch [1/5], Step [754/10336], Loss: 1.4294\n",
      "Epoch [1/5], Step [756/10336], Loss: 0.6847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [758/10336], Loss: 2.0646\n",
      "Epoch [1/5], Step [760/10336], Loss: 1.1979\n",
      "Epoch [1/5], Step [762/10336], Loss: 0.3550\n",
      "Epoch [1/5], Step [764/10336], Loss: 4.4631\n",
      "Epoch [1/5], Step [766/10336], Loss: 0.2523\n",
      "Epoch [1/5], Step [768/10336], Loss: 1.6880\n",
      "Epoch [1/5], Step [770/10336], Loss: 5.3302\n",
      "Epoch [1/5], Step [772/10336], Loss: 4.9637\n",
      "Epoch [1/5], Step [774/10336], Loss: 1.7278\n",
      "Epoch [1/5], Step [776/10336], Loss: 1.6262\n",
      "Epoch [1/5], Step [778/10336], Loss: 2.4174\n",
      "Epoch [1/5], Step [780/10336], Loss: 3.5063\n",
      "Epoch [1/5], Step [782/10336], Loss: 3.3543\n",
      "Epoch [1/5], Step [784/10336], Loss: 2.7870\n",
      "Epoch [1/5], Step [786/10336], Loss: 3.1415\n",
      "Epoch [1/5], Step [788/10336], Loss: 1.7970\n",
      "Epoch [1/5], Step [790/10336], Loss: 2.4209\n",
      "Epoch [1/5], Step [792/10336], Loss: 2.2618\n",
      "Epoch [1/5], Step [794/10336], Loss: 0.4532\n",
      "Epoch [1/5], Step [796/10336], Loss: 2.4343\n",
      "Epoch [1/5], Step [798/10336], Loss: 2.2771\n",
      "Epoch [1/5], Step [800/10336], Loss: 2.0967\n",
      "Epoch [1/5], Step [802/10336], Loss: 1.4834\n",
      "Epoch [1/5], Step [804/10336], Loss: 2.4148\n",
      "Epoch [1/5], Step [806/10336], Loss: 3.8470\n",
      "Epoch [1/5], Step [808/10336], Loss: 2.1901\n",
      "Epoch [1/5], Step [810/10336], Loss: 3.6651\n",
      "Epoch [1/5], Step [812/10336], Loss: 0.4609\n",
      "Epoch [1/5], Step [814/10336], Loss: 2.5012\n",
      "Epoch [1/5], Step [816/10336], Loss: 0.8698\n",
      "Epoch [1/5], Step [818/10336], Loss: 0.5654\n",
      "Epoch [1/5], Step [820/10336], Loss: 1.2787\n",
      "Epoch [1/5], Step [822/10336], Loss: 1.5294\n",
      "Epoch [1/5], Step [824/10336], Loss: 1.2845\n",
      "Epoch [1/5], Step [826/10336], Loss: 5.6043\n",
      "Epoch [1/5], Step [828/10336], Loss: 5.1699\n",
      "Epoch [1/5], Step [830/10336], Loss: 4.9120\n",
      "Epoch [1/5], Step [832/10336], Loss: 2.5736\n",
      "Epoch [1/5], Step [834/10336], Loss: 0.9664\n",
      "Epoch [1/5], Step [836/10336], Loss: 2.8465\n",
      "Epoch [1/5], Step [838/10336], Loss: 1.8529\n",
      "Epoch [1/5], Step [840/10336], Loss: 4.0140\n",
      "Epoch [1/5], Step [842/10336], Loss: 2.5288\n",
      "Epoch [1/5], Step [844/10336], Loss: 2.0131\n",
      "Epoch [1/5], Step [846/10336], Loss: 0.6537\n",
      "Epoch [1/5], Step [848/10336], Loss: 5.7210\n",
      "Epoch [1/5], Step [850/10336], Loss: 3.7374\n",
      "Epoch [1/5], Step [852/10336], Loss: 1.5626\n",
      "Epoch [1/5], Step [854/10336], Loss: 2.7437\n",
      "Epoch [1/5], Step [856/10336], Loss: 3.7541\n",
      "Epoch [1/5], Step [858/10336], Loss: 4.4079\n",
      "Epoch [1/5], Step [860/10336], Loss: 2.9288\n",
      "Epoch [1/5], Step [862/10336], Loss: 2.5489\n",
      "Epoch [1/5], Step [864/10336], Loss: 1.7599\n",
      "Epoch [1/5], Step [866/10336], Loss: 1.0740\n",
      "Epoch [1/5], Step [868/10336], Loss: 1.1130\n",
      "Epoch [1/5], Step [870/10336], Loss: 2.7255\n",
      "Epoch [1/5], Step [872/10336], Loss: 3.9069\n",
      "Epoch [1/5], Step [874/10336], Loss: 0.9328\n",
      "Epoch [1/5], Step [876/10336], Loss: 1.9110\n",
      "Epoch [1/5], Step [878/10336], Loss: 1.4795\n",
      "Epoch [1/5], Step [880/10336], Loss: 3.5883\n",
      "Epoch [1/5], Step [882/10336], Loss: 1.1372\n",
      "Epoch [1/5], Step [884/10336], Loss: 2.6052\n",
      "Epoch [1/5], Step [886/10336], Loss: 1.4366\n",
      "Epoch [1/5], Step [888/10336], Loss: 2.7459\n",
      "Epoch [1/5], Step [890/10336], Loss: 2.5850\n",
      "Epoch [1/5], Step [892/10336], Loss: 3.6061\n",
      "Epoch [1/5], Step [894/10336], Loss: 2.3289\n",
      "Epoch [1/5], Step [896/10336], Loss: 0.4521\n",
      "Epoch [1/5], Step [898/10336], Loss: 3.0379\n",
      "Epoch [1/5], Step [900/10336], Loss: 2.2058\n",
      "Epoch [1/5], Step [902/10336], Loss: 2.1656\n",
      "Epoch [1/5], Step [904/10336], Loss: 5.2894\n",
      "Epoch [1/5], Step [906/10336], Loss: 4.7849\n",
      "Epoch [1/5], Step [908/10336], Loss: 0.1359\n",
      "Epoch [1/5], Step [910/10336], Loss: 3.1949\n",
      "Epoch [1/5], Step [912/10336], Loss: 2.4101\n",
      "Epoch [1/5], Step [914/10336], Loss: 2.5092\n",
      "Epoch [1/5], Step [916/10336], Loss: 0.2528\n",
      "Epoch [1/5], Step [918/10336], Loss: 0.9076\n",
      "Epoch [1/5], Step [920/10336], Loss: 4.0864\n",
      "Epoch [1/5], Step [922/10336], Loss: 3.3536\n",
      "Epoch [1/5], Step [924/10336], Loss: 3.5683\n",
      "Epoch [1/5], Step [926/10336], Loss: 0.2013\n",
      "Epoch [1/5], Step [928/10336], Loss: 3.2468\n",
      "Epoch [1/5], Step [930/10336], Loss: 2.7950\n",
      "Epoch [1/5], Step [932/10336], Loss: 2.1052\n",
      "Epoch [1/5], Step [934/10336], Loss: 1.0685\n",
      "Epoch [1/5], Step [936/10336], Loss: 0.1446\n",
      "Epoch [1/5], Step [938/10336], Loss: 1.0872\n",
      "Epoch [1/5], Step [940/10336], Loss: 3.0537\n",
      "Epoch [1/5], Step [942/10336], Loss: 3.6677\n",
      "Epoch [1/5], Step [944/10336], Loss: 1.8602\n",
      "Epoch [1/5], Step [946/10336], Loss: 2.9199\n",
      "Epoch [1/5], Step [948/10336], Loss: 1.9441\n",
      "Epoch [1/5], Step [950/10336], Loss: 2.8263\n",
      "Epoch [1/5], Step [952/10336], Loss: 0.6301\n",
      "Epoch [1/5], Step [954/10336], Loss: 0.8653\n",
      "Epoch [1/5], Step [956/10336], Loss: 1.9055\n",
      "Epoch [1/5], Step [958/10336], Loss: 2.5601\n",
      "Epoch [1/5], Step [960/10336], Loss: 1.4911\n",
      "Epoch [1/5], Step [962/10336], Loss: 2.9322\n",
      "Epoch [1/5], Step [964/10336], Loss: 1.5975\n",
      "Epoch [1/5], Step [966/10336], Loss: 3.7735\n",
      "Epoch [1/5], Step [968/10336], Loss: 2.5330\n",
      "Epoch [1/5], Step [970/10336], Loss: 4.0983\n",
      "Epoch [1/5], Step [972/10336], Loss: 5.2143\n",
      "Epoch [1/5], Step [974/10336], Loss: 1.4665\n",
      "Epoch [1/5], Step [976/10336], Loss: 2.0087\n",
      "Epoch [1/5], Step [978/10336], Loss: 0.4698\n",
      "Epoch [1/5], Step [980/10336], Loss: 3.7134\n",
      "Epoch [1/5], Step [982/10336], Loss: 3.4014\n",
      "Epoch [1/5], Step [984/10336], Loss: 2.6197\n",
      "Epoch [1/5], Step [986/10336], Loss: 1.5098\n",
      "Epoch [1/5], Step [988/10336], Loss: 2.6182\n",
      "Epoch [1/5], Step [990/10336], Loss: 3.1538\n",
      "Epoch [1/5], Step [992/10336], Loss: 2.2506\n",
      "Epoch [1/5], Step [994/10336], Loss: 4.3783\n",
      "Epoch [1/5], Step [996/10336], Loss: 2.2918\n",
      "Epoch [1/5], Step [998/10336], Loss: 2.7133\n",
      "Epoch [1/5], Step [1000/10336], Loss: 2.3835\n",
      "Epoch [1/5], Step [1002/10336], Loss: 0.5383\n",
      "Epoch [1/5], Step [1004/10336], Loss: 0.8373\n",
      "Epoch [1/5], Step [1006/10336], Loss: 0.6724\n",
      "Epoch [1/5], Step [1008/10336], Loss: 4.3750\n",
      "Epoch [1/5], Step [1010/10336], Loss: 2.8287\n",
      "Epoch [1/5], Step [1012/10336], Loss: 2.0570\n",
      "Epoch [1/5], Step [1014/10336], Loss: 1.9009\n",
      "Epoch [1/5], Step [1016/10336], Loss: 1.4755\n",
      "Epoch [1/5], Step [1018/10336], Loss: 0.5872\n",
      "Epoch [1/5], Step [1020/10336], Loss: 2.5728\n",
      "Epoch [1/5], Step [1022/10336], Loss: 3.5894\n",
      "Epoch [1/5], Step [1024/10336], Loss: 1.1244\n",
      "Epoch [1/5], Step [1026/10336], Loss: 1.4066\n",
      "Epoch [1/5], Step [1028/10336], Loss: 0.8562\n",
      "Epoch [1/5], Step [1030/10336], Loss: 1.7695\n",
      "Epoch [1/5], Step [1032/10336], Loss: 2.3125\n",
      "Epoch [1/5], Step [1034/10336], Loss: 0.5085\n",
      "Epoch [1/5], Step [1036/10336], Loss: 1.2738\n",
      "Epoch [1/5], Step [1038/10336], Loss: 1.2451\n",
      "Epoch [1/5], Step [1040/10336], Loss: 2.7902\n",
      "Epoch [1/5], Step [1042/10336], Loss: 3.6817\n",
      "Epoch [1/5], Step [1044/10336], Loss: 0.6176\n",
      "Epoch [1/5], Step [1046/10336], Loss: 0.4261\n",
      "Epoch [1/5], Step [1048/10336], Loss: 2.7467\n",
      "Epoch [1/5], Step [1050/10336], Loss: 3.3823\n",
      "Epoch [1/5], Step [1052/10336], Loss: 2.4234\n",
      "Epoch [1/5], Step [1054/10336], Loss: 4.5559\n",
      "Epoch [1/5], Step [1056/10336], Loss: 2.2690\n",
      "Epoch [1/5], Step [1058/10336], Loss: 3.8256\n",
      "Epoch [1/5], Step [1060/10336], Loss: 3.6678\n",
      "Epoch [1/5], Step [1062/10336], Loss: 3.1989\n",
      "Epoch [1/5], Step [1064/10336], Loss: 1.3369\n",
      "Epoch [1/5], Step [1066/10336], Loss: 2.2996\n",
      "Epoch [1/5], Step [1068/10336], Loss: 2.4092\n",
      "Epoch [1/5], Step [1070/10336], Loss: 3.4185\n",
      "Epoch [1/5], Step [1072/10336], Loss: 1.9808\n",
      "Epoch [1/5], Step [1074/10336], Loss: 1.9040\n",
      "Epoch [1/5], Step [1076/10336], Loss: 2.2090\n",
      "Epoch [1/5], Step [1078/10336], Loss: 2.3889\n",
      "Epoch [1/5], Step [1080/10336], Loss: 6.3839\n",
      "Epoch [1/5], Step [1082/10336], Loss: 3.9310\n",
      "Epoch [1/5], Step [1084/10336], Loss: 1.5550\n",
      "Epoch [1/5], Step [1086/10336], Loss: 2.5468\n",
      "Epoch [1/5], Step [1088/10336], Loss: 2.3940\n",
      "Epoch [1/5], Step [1090/10336], Loss: 2.6369\n",
      "Epoch [1/5], Step [1092/10336], Loss: 0.4987\n",
      "Epoch [1/5], Step [1094/10336], Loss: 2.8464\n",
      "Epoch [1/5], Step [1096/10336], Loss: 0.1403\n",
      "Epoch [1/5], Step [1098/10336], Loss: 1.6330\n",
      "Epoch [1/5], Step [1100/10336], Loss: 1.5668\n",
      "Epoch [1/5], Step [1102/10336], Loss: 5.5077\n",
      "Epoch [1/5], Step [1104/10336], Loss: 1.3776\n",
      "Epoch [1/5], Step [1106/10336], Loss: 1.7602\n",
      "Epoch [1/5], Step [1108/10336], Loss: 2.5709\n",
      "Epoch [1/5], Step [1110/10336], Loss: 0.5684\n",
      "Epoch [1/5], Step [1112/10336], Loss: 0.4366\n",
      "Epoch [1/5], Step [1114/10336], Loss: 2.6920\n",
      "Epoch [1/5], Step [1116/10336], Loss: 1.9722\n",
      "Epoch [1/5], Step [1118/10336], Loss: 3.5097\n",
      "Epoch [1/5], Step [1120/10336], Loss: 3.9621\n",
      "Epoch [1/5], Step [1122/10336], Loss: 3.9983\n",
      "Epoch [1/5], Step [1124/10336], Loss: 2.1936\n",
      "Epoch [1/5], Step [1126/10336], Loss: 1.5846\n",
      "Epoch [1/5], Step [1128/10336], Loss: 2.8620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1130/10336], Loss: 1.7966\n",
      "Epoch [1/5], Step [1132/10336], Loss: 1.7662\n",
      "Epoch [1/5], Step [1134/10336], Loss: 2.2348\n",
      "Epoch [1/5], Step [1136/10336], Loss: 2.9688\n",
      "Epoch [1/5], Step [1138/10336], Loss: 2.8623\n",
      "Epoch [1/5], Step [1140/10336], Loss: 0.8515\n",
      "Epoch [1/5], Step [1142/10336], Loss: 0.6376\n",
      "Epoch [1/5], Step [1144/10336], Loss: 2.0939\n",
      "Epoch [1/5], Step [1146/10336], Loss: 5.6804\n",
      "Epoch [1/5], Step [1148/10336], Loss: 0.4146\n",
      "Epoch [1/5], Step [1150/10336], Loss: 1.5443\n",
      "Epoch [1/5], Step [1152/10336], Loss: 0.2501\n",
      "Epoch [1/5], Step [1154/10336], Loss: 5.7726\n",
      "Epoch [1/5], Step [1156/10336], Loss: 2.6893\n",
      "Epoch [1/5], Step [1158/10336], Loss: 1.1135\n",
      "Epoch [1/5], Step [1160/10336], Loss: 4.3362\n",
      "Epoch [1/5], Step [1162/10336], Loss: 3.4621\n",
      "Epoch [1/5], Step [1164/10336], Loss: 1.6888\n",
      "Epoch [1/5], Step [1166/10336], Loss: 1.8994\n",
      "Epoch [1/5], Step [1168/10336], Loss: 3.2794\n",
      "Epoch [1/5], Step [1170/10336], Loss: 2.4267\n",
      "Epoch [1/5], Step [1172/10336], Loss: 1.6511\n",
      "Epoch [1/5], Step [1174/10336], Loss: 0.6257\n",
      "Epoch [1/5], Step [1176/10336], Loss: 6.0612\n",
      "Epoch [1/5], Step [1178/10336], Loss: 0.4288\n",
      "Epoch [1/5], Step [1180/10336], Loss: 1.5097\n",
      "Epoch [1/5], Step [1182/10336], Loss: 3.0744\n",
      "Epoch [1/5], Step [1184/10336], Loss: 3.0299\n",
      "Epoch [1/5], Step [1186/10336], Loss: 1.0107\n",
      "Epoch [1/5], Step [1188/10336], Loss: 1.9149\n",
      "Epoch [1/5], Step [1190/10336], Loss: 3.9749\n",
      "Epoch [1/5], Step [1192/10336], Loss: 1.5431\n",
      "Epoch [1/5], Step [1194/10336], Loss: 1.3844\n",
      "Epoch [1/5], Step [1196/10336], Loss: 0.7204\n",
      "Epoch [1/5], Step [1198/10336], Loss: 1.4170\n",
      "Epoch [1/5], Step [1200/10336], Loss: 0.9132\n",
      "Epoch [1/5], Step [1202/10336], Loss: 0.6431\n",
      "Epoch [1/5], Step [1204/10336], Loss: 0.2374\n",
      "Epoch [1/5], Step [1206/10336], Loss: 4.3202\n",
      "Epoch [1/5], Step [1208/10336], Loss: 1.7705\n",
      "Epoch [1/5], Step [1210/10336], Loss: 0.8390\n",
      "Epoch [1/5], Step [1212/10336], Loss: 4.5084\n",
      "Epoch [1/5], Step [1214/10336], Loss: 2.8175\n",
      "Epoch [1/5], Step [1216/10336], Loss: 2.9611\n",
      "Epoch [1/5], Step [1218/10336], Loss: 1.2418\n",
      "Epoch [1/5], Step [1220/10336], Loss: 3.7532\n",
      "Epoch [1/5], Step [1222/10336], Loss: 4.0314\n",
      "Epoch [1/5], Step [1224/10336], Loss: 3.2723\n",
      "Epoch [1/5], Step [1226/10336], Loss: 1.8124\n",
      "Epoch [1/5], Step [1228/10336], Loss: 1.9270\n",
      "Epoch [1/5], Step [1230/10336], Loss: 1.1380\n",
      "Epoch [1/5], Step [1232/10336], Loss: 1.3891\n",
      "Epoch [1/5], Step [1234/10336], Loss: 0.1355\n",
      "Epoch [1/5], Step [1236/10336], Loss: 2.1098\n",
      "Epoch [1/5], Step [1238/10336], Loss: 4.8374\n",
      "Epoch [1/5], Step [1240/10336], Loss: 2.1831\n",
      "Epoch [1/5], Step [1242/10336], Loss: 2.3883\n",
      "Epoch [1/5], Step [1244/10336], Loss: 1.8433\n",
      "Epoch [1/5], Step [1246/10336], Loss: 4.0111\n",
      "Epoch [1/5], Step [1248/10336], Loss: 3.9335\n",
      "Epoch [1/5], Step [1250/10336], Loss: 2.9717\n",
      "Epoch [1/5], Step [1252/10336], Loss: 4.7651\n",
      "Epoch [1/5], Step [1254/10336], Loss: 2.4669\n",
      "Epoch [1/5], Step [1256/10336], Loss: 3.8130\n",
      "Epoch [1/5], Step [1258/10336], Loss: 3.0395\n",
      "Epoch [1/5], Step [1260/10336], Loss: 1.7598\n",
      "Epoch [1/5], Step [1262/10336], Loss: 2.6622\n",
      "Epoch [1/5], Step [1264/10336], Loss: 3.5328\n",
      "Epoch [1/5], Step [1266/10336], Loss: 2.5425\n",
      "Epoch [1/5], Step [1268/10336], Loss: 4.0293\n",
      "Epoch [1/5], Step [1270/10336], Loss: 4.0407\n",
      "Epoch [1/5], Step [1272/10336], Loss: 5.5574\n",
      "Epoch [1/5], Step [1274/10336], Loss: 1.9386\n",
      "Epoch [1/5], Step [1276/10336], Loss: 3.8349\n",
      "Epoch [1/5], Step [1278/10336], Loss: 0.9252\n",
      "Epoch [1/5], Step [1280/10336], Loss: 2.6699\n",
      "Epoch [1/5], Step [1282/10336], Loss: 2.1654\n",
      "Epoch [1/5], Step [1284/10336], Loss: 0.9205\n",
      "Epoch [1/5], Step [1286/10336], Loss: 1.2423\n",
      "Epoch [1/5], Step [1288/10336], Loss: 1.2547\n",
      "Epoch [1/5], Step [1290/10336], Loss: 3.8028\n",
      "Epoch [1/5], Step [1292/10336], Loss: 3.6593\n",
      "Epoch [1/5], Step [1294/10336], Loss: 1.5935\n",
      "Epoch [1/5], Step [1296/10336], Loss: 4.6993\n",
      "Epoch [1/5], Step [1298/10336], Loss: 2.7960\n",
      "Epoch [1/5], Step [1300/10336], Loss: 0.2070\n",
      "Epoch [1/5], Step [1302/10336], Loss: 1.8925\n",
      "Epoch [1/5], Step [1304/10336], Loss: 1.4905\n",
      "Epoch [1/5], Step [1306/10336], Loss: 1.9395\n",
      "Epoch [1/5], Step [1308/10336], Loss: 1.3250\n",
      "Epoch [1/5], Step [1310/10336], Loss: 3.2475\n",
      "Epoch [1/5], Step [1312/10336], Loss: 1.3752\n",
      "Epoch [1/5], Step [1314/10336], Loss: 0.3032\n",
      "Epoch [1/5], Step [1316/10336], Loss: 2.0282\n",
      "Epoch [1/5], Step [1318/10336], Loss: 0.9979\n",
      "Epoch [1/5], Step [1320/10336], Loss: 1.8849\n",
      "Epoch [1/5], Step [1322/10336], Loss: 6.6387\n",
      "Epoch [1/5], Step [1324/10336], Loss: 0.3835\n",
      "Epoch [1/5], Step [1326/10336], Loss: 0.3713\n",
      "Epoch [1/5], Step [1328/10336], Loss: 2.6485\n",
      "Epoch [1/5], Step [1330/10336], Loss: 1.9857\n",
      "Epoch [1/5], Step [1332/10336], Loss: 2.2454\n",
      "Epoch [1/5], Step [1334/10336], Loss: 1.0348\n",
      "Epoch [1/5], Step [1336/10336], Loss: 2.2542\n",
      "Epoch [1/5], Step [1338/10336], Loss: 4.0892\n",
      "Epoch [1/5], Step [1340/10336], Loss: 2.6342\n",
      "Epoch [1/5], Step [1342/10336], Loss: 0.2603\n",
      "Epoch [1/5], Step [1344/10336], Loss: 1.0695\n",
      "Epoch [1/5], Step [1346/10336], Loss: 0.2081\n",
      "Epoch [1/5], Step [1348/10336], Loss: 2.9777\n",
      "Epoch [1/5], Step [1350/10336], Loss: 0.8605\n",
      "Epoch [1/5], Step [1352/10336], Loss: 1.9317\n",
      "Epoch [1/5], Step [1354/10336], Loss: 0.4028\n",
      "Epoch [1/5], Step [1356/10336], Loss: 2.6634\n",
      "Epoch [1/5], Step [1358/10336], Loss: 1.0876\n",
      "Epoch [1/5], Step [1360/10336], Loss: 1.3848\n",
      "Epoch [1/5], Step [1362/10336], Loss: 0.9277\n",
      "Epoch [1/5], Step [1364/10336], Loss: 0.4584\n",
      "Epoch [1/5], Step [1366/10336], Loss: 2.3334\n",
      "Epoch [1/5], Step [1368/10336], Loss: 1.8419\n",
      "Epoch [1/5], Step [1370/10336], Loss: 3.4748\n",
      "Epoch [1/5], Step [1372/10336], Loss: 2.7169\n",
      "Epoch [1/5], Step [1374/10336], Loss: 1.4338\n",
      "Epoch [1/5], Step [1376/10336], Loss: 2.4546\n",
      "Epoch [1/5], Step [1378/10336], Loss: 1.3379\n",
      "Epoch [1/5], Step [1380/10336], Loss: 1.0511\n",
      "Epoch [1/5], Step [1382/10336], Loss: 1.6362\n",
      "Epoch [1/5], Step [1384/10336], Loss: 1.3383\n",
      "Epoch [1/5], Step [1386/10336], Loss: 3.7442\n",
      "Epoch [1/5], Step [1388/10336], Loss: 2.4601\n",
      "Epoch [1/5], Step [1390/10336], Loss: 2.2331\n",
      "Epoch [1/5], Step [1392/10336], Loss: 3.3694\n",
      "Epoch [1/5], Step [1394/10336], Loss: 0.2016\n",
      "Epoch [1/5], Step [1396/10336], Loss: 0.8863\n",
      "Epoch [1/5], Step [1398/10336], Loss: 4.2776\n",
      "Epoch [1/5], Step [1400/10336], Loss: 0.3571\n",
      "Epoch [1/5], Step [1402/10336], Loss: 0.1330\n",
      "Epoch [1/5], Step [1404/10336], Loss: 2.3686\n",
      "Epoch [1/5], Step [1406/10336], Loss: 2.8572\n",
      "Epoch [1/5], Step [1408/10336], Loss: 4.4013\n",
      "Epoch [1/5], Step [1410/10336], Loss: 1.2281\n",
      "Epoch [1/5], Step [1412/10336], Loss: 1.4657\n",
      "Epoch [1/5], Step [1414/10336], Loss: 2.6771\n",
      "Epoch [1/5], Step [1416/10336], Loss: 1.5386\n",
      "Epoch [1/5], Step [1418/10336], Loss: 1.9812\n",
      "Epoch [1/5], Step [1420/10336], Loss: 4.5863\n",
      "Epoch [1/5], Step [1422/10336], Loss: 2.4262\n",
      "Epoch [1/5], Step [1424/10336], Loss: 2.3723\n",
      "Epoch [1/5], Step [1426/10336], Loss: 3.8379\n",
      "Epoch [1/5], Step [1428/10336], Loss: 2.0510\n",
      "Epoch [1/5], Step [1430/10336], Loss: 0.8059\n",
      "Epoch [1/5], Step [1432/10336], Loss: 2.0705\n",
      "Epoch [1/5], Step [1434/10336], Loss: 1.0640\n",
      "Epoch [1/5], Step [1436/10336], Loss: 4.1756\n",
      "Epoch [1/5], Step [1438/10336], Loss: 0.9525\n",
      "Epoch [1/5], Step [1440/10336], Loss: 0.9850\n",
      "Epoch [1/5], Step [1442/10336], Loss: 0.0275\n",
      "Epoch [1/5], Step [1444/10336], Loss: 1.8765\n",
      "Epoch [1/5], Step [1446/10336], Loss: 4.4842\n",
      "Epoch [1/5], Step [1448/10336], Loss: 1.7858\n",
      "Epoch [1/5], Step [1450/10336], Loss: 3.8455\n",
      "Epoch [1/5], Step [1452/10336], Loss: 1.8010\n",
      "Epoch [1/5], Step [1454/10336], Loss: 2.9955\n",
      "Epoch [1/5], Step [1456/10336], Loss: 1.0445\n",
      "Epoch [1/5], Step [1458/10336], Loss: 4.3319\n",
      "Epoch [1/5], Step [1460/10336], Loss: 2.2457\n",
      "Epoch [1/5], Step [1462/10336], Loss: 0.6367\n",
      "Epoch [1/5], Step [1464/10336], Loss: 1.1008\n",
      "Epoch [1/5], Step [1466/10336], Loss: 1.1112\n",
      "Epoch [1/5], Step [1468/10336], Loss: 3.5192\n",
      "Epoch [1/5], Step [1470/10336], Loss: 0.5976\n",
      "Epoch [1/5], Step [1472/10336], Loss: 1.2355\n",
      "Epoch [1/5], Step [1474/10336], Loss: 0.7026\n",
      "Epoch [1/5], Step [1476/10336], Loss: 0.6798\n",
      "Epoch [1/5], Step [1478/10336], Loss: 3.1087\n",
      "Epoch [1/5], Step [1480/10336], Loss: 4.8652\n",
      "Epoch [1/5], Step [1482/10336], Loss: 0.8655\n",
      "Epoch [1/5], Step [1484/10336], Loss: 0.2729\n",
      "Epoch [1/5], Step [1486/10336], Loss: 0.5588\n",
      "Epoch [1/5], Step [1488/10336], Loss: 1.9984\n",
      "Epoch [1/5], Step [1490/10336], Loss: 0.2570\n",
      "Epoch [1/5], Step [1492/10336], Loss: 1.0876\n",
      "Epoch [1/5], Step [1494/10336], Loss: 1.6251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1496/10336], Loss: 2.4838\n",
      "Epoch [1/5], Step [1498/10336], Loss: 3.2022\n",
      "Epoch [1/5], Step [1500/10336], Loss: 2.6433\n",
      "Epoch [1/5], Step [1502/10336], Loss: 1.0678\n",
      "Epoch [1/5], Step [1504/10336], Loss: 2.1443\n",
      "Epoch [1/5], Step [1506/10336], Loss: 2.1382\n",
      "Epoch [1/5], Step [1508/10336], Loss: 4.7892\n",
      "Epoch [1/5], Step [1510/10336], Loss: 2.0143\n",
      "Epoch [1/5], Step [1512/10336], Loss: 0.0206\n",
      "Epoch [1/5], Step [1514/10336], Loss: 1.2010\n",
      "Epoch [1/5], Step [1516/10336], Loss: 2.9708\n",
      "Epoch [1/5], Step [1518/10336], Loss: 1.9445\n",
      "Epoch [1/5], Step [1520/10336], Loss: 4.3432\n",
      "Epoch [1/5], Step [1522/10336], Loss: 1.0317\n",
      "Epoch [1/5], Step [1524/10336], Loss: 2.4791\n",
      "Epoch [1/5], Step [1526/10336], Loss: 2.2227\n",
      "Epoch [1/5], Step [1528/10336], Loss: 6.1019\n",
      "Epoch [1/5], Step [1530/10336], Loss: 1.2331\n",
      "Epoch [1/5], Step [1532/10336], Loss: 0.9453\n",
      "Epoch [1/5], Step [1534/10336], Loss: 0.7871\n",
      "Epoch [1/5], Step [1536/10336], Loss: 3.8902\n",
      "Epoch [1/5], Step [1538/10336], Loss: 1.1393\n",
      "Epoch [1/5], Step [1540/10336], Loss: 3.3910\n",
      "Epoch [1/5], Step [1542/10336], Loss: 0.6095\n",
      "Epoch [1/5], Step [1544/10336], Loss: 5.3693\n",
      "Epoch [1/5], Step [1546/10336], Loss: 4.1916\n",
      "Epoch [1/5], Step [1548/10336], Loss: 0.3293\n",
      "Epoch [1/5], Step [1550/10336], Loss: 2.2020\n",
      "Epoch [1/5], Step [1552/10336], Loss: 2.5532\n",
      "Epoch [1/5], Step [1554/10336], Loss: 4.2552\n",
      "Epoch [1/5], Step [1556/10336], Loss: 2.5113\n",
      "Epoch [1/5], Step [1558/10336], Loss: 2.5187\n",
      "Epoch [1/5], Step [1560/10336], Loss: 4.0852\n",
      "Epoch [1/5], Step [1562/10336], Loss: 4.0154\n",
      "Epoch [1/5], Step [1564/10336], Loss: 0.2667\n",
      "Epoch [1/5], Step [1566/10336], Loss: 2.8796\n",
      "Epoch [1/5], Step [1568/10336], Loss: 0.5973\n",
      "Epoch [1/5], Step [1570/10336], Loss: 0.1880\n",
      "Epoch [1/5], Step [1572/10336], Loss: 4.3833\n",
      "Epoch [1/5], Step [1574/10336], Loss: 3.3477\n",
      "Epoch [1/5], Step [1576/10336], Loss: 2.7988\n",
      "Epoch [1/5], Step [1578/10336], Loss: 4.2575\n",
      "Epoch [1/5], Step [1580/10336], Loss: 0.8846\n",
      "Epoch [1/5], Step [1582/10336], Loss: 4.7153\n",
      "Epoch [1/5], Step [1584/10336], Loss: 1.7408\n",
      "Epoch [1/5], Step [1586/10336], Loss: 2.0484\n",
      "Epoch [1/5], Step [1588/10336], Loss: 2.1140\n",
      "Epoch [1/5], Step [1590/10336], Loss: 2.7947\n",
      "Epoch [1/5], Step [1592/10336], Loss: 1.8519\n",
      "Epoch [1/5], Step [1594/10336], Loss: 0.2860\n",
      "Epoch [1/5], Step [1596/10336], Loss: 1.6600\n",
      "Epoch [1/5], Step [1598/10336], Loss: 0.8610\n",
      "Epoch [1/5], Step [1600/10336], Loss: 0.5267\n",
      "Epoch [1/5], Step [1602/10336], Loss: 1.6449\n",
      "Epoch [1/5], Step [1604/10336], Loss: 1.5688\n",
      "Epoch [1/5], Step [1606/10336], Loss: 3.7592\n",
      "Epoch [1/5], Step [1608/10336], Loss: 0.6354\n",
      "Epoch [1/5], Step [1610/10336], Loss: 0.5909\n",
      "Epoch [1/5], Step [1612/10336], Loss: 2.8331\n",
      "Epoch [1/5], Step [1614/10336], Loss: 2.5653\n",
      "Epoch [1/5], Step [1616/10336], Loss: 1.2190\n",
      "Epoch [1/5], Step [1618/10336], Loss: 2.5611\n",
      "Epoch [1/5], Step [1620/10336], Loss: 3.0322\n",
      "Epoch [1/5], Step [1622/10336], Loss: 1.3735\n",
      "Epoch [1/5], Step [1624/10336], Loss: 2.8080\n",
      "Epoch [1/5], Step [1626/10336], Loss: 1.2326\n",
      "Epoch [1/5], Step [1628/10336], Loss: 0.8753\n",
      "Epoch [1/5], Step [1630/10336], Loss: 2.8635\n",
      "Epoch [1/5], Step [1632/10336], Loss: 2.0691\n",
      "Epoch [1/5], Step [1634/10336], Loss: 3.9803\n",
      "Epoch [1/5], Step [1636/10336], Loss: 3.5512\n",
      "Epoch [1/5], Step [1638/10336], Loss: 1.7635\n",
      "Epoch [1/5], Step [1640/10336], Loss: 2.1877\n",
      "Epoch [1/5], Step [1642/10336], Loss: 2.4862\n",
      "Epoch [1/5], Step [1644/10336], Loss: 0.0866\n",
      "Epoch [1/5], Step [1646/10336], Loss: 4.3310\n",
      "Epoch [1/5], Step [1648/10336], Loss: 2.3006\n",
      "Epoch [1/5], Step [1650/10336], Loss: 0.1185\n",
      "Epoch [1/5], Step [1652/10336], Loss: 1.4319\n",
      "Epoch [1/5], Step [1654/10336], Loss: 2.6678\n",
      "Epoch [1/5], Step [1656/10336], Loss: 2.0572\n",
      "Epoch [1/5], Step [1658/10336], Loss: 4.8820\n",
      "Epoch [1/5], Step [1660/10336], Loss: 2.6787\n",
      "Epoch [1/5], Step [1662/10336], Loss: 0.2101\n",
      "Epoch [1/5], Step [1664/10336], Loss: 3.7508\n",
      "Epoch [1/5], Step [1666/10336], Loss: 5.8449\n",
      "Epoch [1/5], Step [1668/10336], Loss: 2.5150\n",
      "Epoch [1/5], Step [1670/10336], Loss: 1.5722\n",
      "Epoch [1/5], Step [1672/10336], Loss: 2.4952\n",
      "Epoch [1/5], Step [1674/10336], Loss: 2.2124\n",
      "Epoch [1/5], Step [1676/10336], Loss: 1.1045\n",
      "Epoch [1/5], Step [1678/10336], Loss: 0.5928\n",
      "Epoch [1/5], Step [1680/10336], Loss: 2.0273\n",
      "Epoch [1/5], Step [1682/10336], Loss: 1.4452\n",
      "Epoch [1/5], Step [1684/10336], Loss: 1.9525\n",
      "Epoch [1/5], Step [1686/10336], Loss: 1.9830\n",
      "Epoch [1/5], Step [1688/10336], Loss: 2.7587\n",
      "Epoch [1/5], Step [1690/10336], Loss: 2.1835\n",
      "Epoch [1/5], Step [1692/10336], Loss: 1.4307\n",
      "Epoch [1/5], Step [1694/10336], Loss: 0.1343\n",
      "Epoch [1/5], Step [1696/10336], Loss: 3.3953\n",
      "Epoch [1/5], Step [1698/10336], Loss: 1.3473\n",
      "Epoch [1/5], Step [1700/10336], Loss: 2.8957\n",
      "Epoch [1/5], Step [1702/10336], Loss: 1.3351\n",
      "Epoch [1/5], Step [1704/10336], Loss: 0.0955\n",
      "Epoch [1/5], Step [1706/10336], Loss: 2.4190\n",
      "Epoch [1/5], Step [1708/10336], Loss: 0.2118\n",
      "Epoch [1/5], Step [1710/10336], Loss: 0.5069\n",
      "Epoch [1/5], Step [1712/10336], Loss: 3.8016\n",
      "Epoch [1/5], Step [1714/10336], Loss: 1.7564\n",
      "Epoch [1/5], Step [1716/10336], Loss: 3.8280\n",
      "Epoch [1/5], Step [1718/10336], Loss: 3.3105\n",
      "Epoch [1/5], Step [1720/10336], Loss: 0.7688\n",
      "Epoch [1/5], Step [1722/10336], Loss: 0.1407\n",
      "Epoch [1/5], Step [1724/10336], Loss: 4.8172\n",
      "Epoch [1/5], Step [1726/10336], Loss: 1.0495\n",
      "Epoch [1/5], Step [1728/10336], Loss: 4.6796\n",
      "Epoch [1/5], Step [1730/10336], Loss: 0.7193\n",
      "Epoch [1/5], Step [1732/10336], Loss: 4.1433\n",
      "Epoch [1/5], Step [1734/10336], Loss: 1.3515\n",
      "Epoch [1/5], Step [1736/10336], Loss: 3.6798\n",
      "Epoch [1/5], Step [1738/10336], Loss: 0.6580\n",
      "Epoch [1/5], Step [1740/10336], Loss: 0.3908\n",
      "Epoch [1/5], Step [1742/10336], Loss: 2.4855\n",
      "Epoch [1/5], Step [1744/10336], Loss: 0.7017\n",
      "Epoch [1/5], Step [1746/10336], Loss: 1.3206\n",
      "Epoch [1/5], Step [1748/10336], Loss: 1.0158\n",
      "Epoch [1/5], Step [1750/10336], Loss: 1.2012\n",
      "Epoch [1/5], Step [1752/10336], Loss: 4.0026\n",
      "Epoch [1/5], Step [1754/10336], Loss: 1.9942\n",
      "Epoch [1/5], Step [1756/10336], Loss: 2.2974\n",
      "Epoch [1/5], Step [1758/10336], Loss: 1.4163\n",
      "Epoch [1/5], Step [1760/10336], Loss: 3.1968\n",
      "Epoch [1/5], Step [1762/10336], Loss: 1.9633\n",
      "Epoch [1/5], Step [1764/10336], Loss: 2.8328\n",
      "Epoch [1/5], Step [1766/10336], Loss: 1.7800\n",
      "Epoch [1/5], Step [1768/10336], Loss: 1.2986\n",
      "Epoch [1/5], Step [1770/10336], Loss: 4.8543\n",
      "Epoch [1/5], Step [1772/10336], Loss: 0.7350\n",
      "Epoch [1/5], Step [1774/10336], Loss: 4.5462\n",
      "Epoch [1/5], Step [1776/10336], Loss: 0.2094\n",
      "Epoch [1/5], Step [1778/10336], Loss: 2.4863\n",
      "Epoch [1/5], Step [1780/10336], Loss: 0.5079\n",
      "Epoch [1/5], Step [1782/10336], Loss: 0.0687\n",
      "Epoch [1/5], Step [1784/10336], Loss: 3.1415\n",
      "Epoch [1/5], Step [1786/10336], Loss: 2.9106\n",
      "Epoch [1/5], Step [1788/10336], Loss: 1.7113\n",
      "Epoch [1/5], Step [1790/10336], Loss: 2.3166\n",
      "Epoch [1/5], Step [1792/10336], Loss: 2.3898\n",
      "Epoch [1/5], Step [1794/10336], Loss: 3.2040\n",
      "Epoch [1/5], Step [1796/10336], Loss: 3.3023\n",
      "Epoch [1/5], Step [1798/10336], Loss: 1.5485\n",
      "Epoch [1/5], Step [1800/10336], Loss: 3.7782\n",
      "Epoch [1/5], Step [1802/10336], Loss: 4.4886\n",
      "Epoch [1/5], Step [1804/10336], Loss: 1.1316\n",
      "Epoch [1/5], Step [1806/10336], Loss: 4.8400\n",
      "Epoch [1/5], Step [1808/10336], Loss: 1.0563\n",
      "Epoch [1/5], Step [1810/10336], Loss: 1.8281\n",
      "Epoch [1/5], Step [1812/10336], Loss: 1.6099\n",
      "Epoch [1/5], Step [1814/10336], Loss: 1.4238\n",
      "Epoch [1/5], Step [1816/10336], Loss: 2.1520\n",
      "Epoch [1/5], Step [1818/10336], Loss: 6.2116\n",
      "Epoch [1/5], Step [1820/10336], Loss: 1.3130\n",
      "Epoch [1/5], Step [1822/10336], Loss: 1.6891\n",
      "Epoch [1/5], Step [1824/10336], Loss: 2.5119\n",
      "Epoch [1/5], Step [1826/10336], Loss: 1.4286\n",
      "Epoch [1/5], Step [1828/10336], Loss: 0.6032\n",
      "Epoch [1/5], Step [1830/10336], Loss: 2.4422\n",
      "Epoch [1/5], Step [1832/10336], Loss: 2.6840\n",
      "Epoch [1/5], Step [1834/10336], Loss: 0.5749\n",
      "Epoch [1/5], Step [1836/10336], Loss: 1.8548\n",
      "Epoch [1/5], Step [1838/10336], Loss: 3.3395\n",
      "Epoch [1/5], Step [1840/10336], Loss: 4.3083\n",
      "Epoch [1/5], Step [1842/10336], Loss: 0.1292\n",
      "Epoch [1/5], Step [1844/10336], Loss: 0.9324\n",
      "Epoch [1/5], Step [1846/10336], Loss: 2.0492\n",
      "Epoch [1/5], Step [1848/10336], Loss: 5.6829\n",
      "Epoch [1/5], Step [1850/10336], Loss: 0.3130\n",
      "Epoch [1/5], Step [1852/10336], Loss: 1.4471\n",
      "Epoch [1/5], Step [1854/10336], Loss: 0.6645\n",
      "Epoch [1/5], Step [1856/10336], Loss: 1.5992\n",
      "Epoch [1/5], Step [1858/10336], Loss: 0.4972\n",
      "Epoch [1/5], Step [1860/10336], Loss: 0.8395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1862/10336], Loss: 0.2273\n",
      "Epoch [1/5], Step [1864/10336], Loss: 4.3924\n",
      "Epoch [1/5], Step [1866/10336], Loss: 1.3553\n",
      "Epoch [1/5], Step [1868/10336], Loss: 4.4408\n",
      "Epoch [1/5], Step [1870/10336], Loss: 2.7066\n",
      "Epoch [1/5], Step [1872/10336], Loss: 4.1709\n",
      "Epoch [1/5], Step [1874/10336], Loss: 1.7009\n",
      "Epoch [1/5], Step [1876/10336], Loss: 1.0857\n",
      "Epoch [1/5], Step [1878/10336], Loss: 3.8988\n",
      "Epoch [1/5], Step [1880/10336], Loss: 0.7960\n",
      "Epoch [1/5], Step [1882/10336], Loss: 3.2067\n",
      "Epoch [1/5], Step [1884/10336], Loss: 1.5555\n",
      "Epoch [1/5], Step [1886/10336], Loss: 1.0411\n",
      "Epoch [1/5], Step [1888/10336], Loss: 2.0782\n",
      "Epoch [1/5], Step [1890/10336], Loss: 1.5710\n",
      "Epoch [1/5], Step [1892/10336], Loss: 0.5889\n",
      "Epoch [1/5], Step [1894/10336], Loss: 0.1989\n",
      "Epoch [1/5], Step [1896/10336], Loss: 1.4890\n",
      "Epoch [1/5], Step [1898/10336], Loss: 4.1115\n",
      "Epoch [1/5], Step [1900/10336], Loss: 3.7507\n",
      "Epoch [1/5], Step [1902/10336], Loss: 1.4636\n",
      "Epoch [1/5], Step [1904/10336], Loss: 2.0353\n",
      "Epoch [1/5], Step [1906/10336], Loss: 0.9350\n",
      "Epoch [1/5], Step [1908/10336], Loss: 0.3485\n",
      "Epoch [1/5], Step [1910/10336], Loss: 3.7081\n",
      "Epoch [1/5], Step [1912/10336], Loss: 3.8878\n",
      "Epoch [1/5], Step [1914/10336], Loss: 4.5539\n",
      "Epoch [1/5], Step [1916/10336], Loss: 0.5149\n",
      "Epoch [1/5], Step [1918/10336], Loss: 1.1019\n",
      "Epoch [1/5], Step [1920/10336], Loss: 4.9878\n",
      "Epoch [1/5], Step [1922/10336], Loss: 3.9412\n",
      "Epoch [1/5], Step [1924/10336], Loss: 2.7021\n",
      "Epoch [1/5], Step [1926/10336], Loss: 2.1846\n",
      "Epoch [1/5], Step [1928/10336], Loss: 2.1489\n",
      "Epoch [1/5], Step [1930/10336], Loss: 1.9363\n",
      "Epoch [1/5], Step [1932/10336], Loss: 5.7224\n",
      "Epoch [1/5], Step [1934/10336], Loss: 2.9545\n",
      "Epoch [1/5], Step [1936/10336], Loss: 2.0188\n",
      "Epoch [1/5], Step [1938/10336], Loss: 1.7662\n",
      "Epoch [1/5], Step [1940/10336], Loss: 2.4074\n",
      "Epoch [1/5], Step [1942/10336], Loss: 0.5659\n",
      "Epoch [1/5], Step [1944/10336], Loss: 3.6922\n",
      "Epoch [1/5], Step [1946/10336], Loss: 0.1591\n",
      "Epoch [1/5], Step [1948/10336], Loss: 0.9906\n",
      "Epoch [1/5], Step [1950/10336], Loss: 0.7259\n",
      "Epoch [1/5], Step [1952/10336], Loss: 0.7554\n",
      "Epoch [1/5], Step [1954/10336], Loss: 0.5539\n",
      "Epoch [1/5], Step [1956/10336], Loss: 4.2058\n",
      "Epoch [1/5], Step [1958/10336], Loss: 2.0908\n",
      "Epoch [1/5], Step [1960/10336], Loss: 2.2789\n",
      "Epoch [1/5], Step [1962/10336], Loss: 2.8943\n",
      "Epoch [1/5], Step [1964/10336], Loss: 1.0434\n",
      "Epoch [1/5], Step [1966/10336], Loss: 2.8380\n",
      "Epoch [1/5], Step [1968/10336], Loss: 2.8821\n",
      "Epoch [1/5], Step [1970/10336], Loss: 0.1077\n",
      "Epoch [1/5], Step [1972/10336], Loss: 0.0861\n",
      "Epoch [1/5], Step [1974/10336], Loss: 2.0674\n",
      "Epoch [1/5], Step [1976/10336], Loss: 1.4179\n",
      "Epoch [1/5], Step [1978/10336], Loss: 2.1325\n",
      "Epoch [1/5], Step [1980/10336], Loss: 2.9474\n",
      "Epoch [1/5], Step [1982/10336], Loss: 0.7743\n",
      "Epoch [1/5], Step [1984/10336], Loss: 1.7974\n",
      "Epoch [1/5], Step [1986/10336], Loss: 3.1283\n",
      "Epoch [1/5], Step [1988/10336], Loss: 1.4167\n",
      "Epoch [1/5], Step [1990/10336], Loss: 0.8007\n",
      "Epoch [1/5], Step [1992/10336], Loss: 2.8244\n",
      "Epoch [1/5], Step [1994/10336], Loss: 1.6275\n",
      "Epoch [1/5], Step [1996/10336], Loss: 2.9783\n",
      "Epoch [1/5], Step [1998/10336], Loss: 1.6795\n",
      "Epoch [1/5], Step [2000/10336], Loss: 0.6475\n",
      "Epoch [1/5], Step [2002/10336], Loss: 2.4965\n",
      "Epoch [1/5], Step [2004/10336], Loss: 1.0971\n",
      "Epoch [1/5], Step [2006/10336], Loss: 1.9857\n",
      "Epoch [1/5], Step [2008/10336], Loss: 0.2158\n",
      "Epoch [1/5], Step [2010/10336], Loss: 0.1848\n",
      "Epoch [1/5], Step [2012/10336], Loss: 3.9626\n",
      "Epoch [1/5], Step [2014/10336], Loss: 1.5349\n",
      "Epoch [1/5], Step [2016/10336], Loss: 2.4012\n",
      "Epoch [1/5], Step [2018/10336], Loss: 0.7985\n",
      "Epoch [1/5], Step [2020/10336], Loss: 2.9769\n",
      "Epoch [1/5], Step [2022/10336], Loss: 2.3454\n",
      "Epoch [1/5], Step [2024/10336], Loss: 4.8484\n",
      "Epoch [1/5], Step [2026/10336], Loss: 1.6463\n",
      "Epoch [1/5], Step [2028/10336], Loss: 2.4457\n",
      "Epoch [1/5], Step [2030/10336], Loss: 2.5387\n",
      "Epoch [1/5], Step [2032/10336], Loss: 2.2806\n",
      "Epoch [1/5], Step [2034/10336], Loss: 2.2998\n",
      "Epoch [1/5], Step [2036/10336], Loss: 3.8392\n",
      "Epoch [1/5], Step [2038/10336], Loss: 3.6655\n",
      "Epoch [1/5], Step [2040/10336], Loss: 1.9217\n",
      "Epoch [1/5], Step [2042/10336], Loss: 2.6480\n",
      "Epoch [1/5], Step [2044/10336], Loss: 2.4431\n",
      "Epoch [1/5], Step [2046/10336], Loss: 2.9780\n",
      "Epoch [1/5], Step [2048/10336], Loss: 0.5408\n",
      "Epoch [1/5], Step [2050/10336], Loss: 2.7013\n",
      "Epoch [1/5], Step [2052/10336], Loss: 1.6634\n",
      "Epoch [1/5], Step [2054/10336], Loss: 2.0659\n",
      "Epoch [1/5], Step [2056/10336], Loss: 1.5923\n",
      "Epoch [1/5], Step [2058/10336], Loss: 1.0988\n",
      "Epoch [1/5], Step [2060/10336], Loss: 1.0939\n",
      "Epoch [1/5], Step [2062/10336], Loss: 3.7779\n",
      "Epoch [1/5], Step [2064/10336], Loss: 2.0984\n",
      "Epoch [1/5], Step [2066/10336], Loss: 1.6103\n",
      "Epoch [1/5], Step [2068/10336], Loss: 0.3535\n",
      "Epoch [1/5], Step [2070/10336], Loss: 2.2157\n",
      "Epoch [1/5], Step [2072/10336], Loss: 3.0852\n",
      "Epoch [1/5], Step [2074/10336], Loss: 0.4717\n",
      "Epoch [1/5], Step [2076/10336], Loss: 3.3079\n",
      "Epoch [1/5], Step [2078/10336], Loss: 1.7672\n",
      "Epoch [1/5], Step [2080/10336], Loss: 2.2184\n",
      "Epoch [1/5], Step [2082/10336], Loss: 2.5215\n",
      "Epoch [1/5], Step [2084/10336], Loss: 4.8686\n",
      "Epoch [1/5], Step [2086/10336], Loss: 2.7965\n",
      "Epoch [1/5], Step [2088/10336], Loss: 1.8443\n",
      "Epoch [1/5], Step [2090/10336], Loss: 0.4897\n",
      "Epoch [1/5], Step [2092/10336], Loss: 0.8354\n",
      "Epoch [1/5], Step [2094/10336], Loss: 1.9770\n",
      "Epoch [1/5], Step [2096/10336], Loss: 1.0110\n",
      "Epoch [1/5], Step [2098/10336], Loss: 6.2693\n",
      "Epoch [1/5], Step [2100/10336], Loss: 1.7693\n",
      "Epoch [1/5], Step [2102/10336], Loss: 1.8357\n",
      "Epoch [1/5], Step [2104/10336], Loss: 0.9633\n",
      "Epoch [1/5], Step [2106/10336], Loss: 1.9478\n",
      "Epoch [1/5], Step [2108/10336], Loss: 0.5170\n",
      "Epoch [1/5], Step [2110/10336], Loss: 5.2345\n",
      "Epoch [1/5], Step [2112/10336], Loss: 0.5198\n",
      "Epoch [1/5], Step [2114/10336], Loss: 2.1419\n",
      "Epoch [1/5], Step [2116/10336], Loss: 1.2660\n",
      "Epoch [1/5], Step [2118/10336], Loss: 1.1531\n",
      "Epoch [1/5], Step [2120/10336], Loss: 3.7744\n",
      "Epoch [1/5], Step [2122/10336], Loss: 3.3485\n",
      "Epoch [1/5], Step [2124/10336], Loss: 0.1868\n",
      "Epoch [1/5], Step [2126/10336], Loss: 1.5733\n",
      "Epoch [1/5], Step [2128/10336], Loss: 1.7898\n",
      "Epoch [1/5], Step [2130/10336], Loss: 4.9764\n",
      "Epoch [1/5], Step [2132/10336], Loss: 3.0712\n",
      "Epoch [1/5], Step [2134/10336], Loss: 2.2920\n",
      "Epoch [1/5], Step [2136/10336], Loss: 2.4005\n",
      "Epoch [1/5], Step [2138/10336], Loss: 1.3328\n",
      "Epoch [1/5], Step [2140/10336], Loss: 3.5212\n",
      "Epoch [1/5], Step [2142/10336], Loss: 3.3077\n",
      "Epoch [1/5], Step [2144/10336], Loss: 1.2984\n",
      "Epoch [1/5], Step [2146/10336], Loss: 0.2528\n",
      "Epoch [1/5], Step [2148/10336], Loss: 2.1040\n",
      "Epoch [1/5], Step [2150/10336], Loss: 2.2490\n",
      "Epoch [1/5], Step [2152/10336], Loss: 3.5242\n",
      "Epoch [1/5], Step [2154/10336], Loss: 1.9149\n",
      "Epoch [1/5], Step [2156/10336], Loss: 1.6493\n",
      "Epoch [1/5], Step [2158/10336], Loss: 0.9148\n",
      "Epoch [1/5], Step [2160/10336], Loss: 1.0964\n",
      "Epoch [1/5], Step [2162/10336], Loss: 2.1750\n",
      "Epoch [1/5], Step [2164/10336], Loss: 2.6104\n",
      "Epoch [1/5], Step [2166/10336], Loss: 1.8088\n",
      "Epoch [1/5], Step [2168/10336], Loss: 2.2022\n",
      "Epoch [1/5], Step [2170/10336], Loss: 1.3715\n",
      "Epoch [1/5], Step [2172/10336], Loss: 2.2855\n",
      "Epoch [1/5], Step [2174/10336], Loss: 1.3036\n",
      "Epoch [1/5], Step [2176/10336], Loss: 0.7111\n",
      "Epoch [1/5], Step [2178/10336], Loss: 0.5081\n",
      "Epoch [1/5], Step [2180/10336], Loss: 2.0842\n",
      "Epoch [1/5], Step [2182/10336], Loss: 2.3216\n",
      "Epoch [1/5], Step [2184/10336], Loss: 1.5251\n",
      "Epoch [1/5], Step [2186/10336], Loss: 3.1078\n",
      "Epoch [1/5], Step [2188/10336], Loss: 1.8788\n",
      "Epoch [1/5], Step [2190/10336], Loss: 2.1469\n",
      "Epoch [1/5], Step [2192/10336], Loss: 1.3700\n",
      "Epoch [1/5], Step [2194/10336], Loss: 2.8714\n",
      "Epoch [1/5], Step [2196/10336], Loss: 1.9422\n",
      "Epoch [1/5], Step [2198/10336], Loss: 3.9339\n",
      "Epoch [1/5], Step [2200/10336], Loss: 2.1576\n",
      "Epoch [1/5], Step [2202/10336], Loss: 4.9338\n",
      "Epoch [1/5], Step [2204/10336], Loss: 1.9018\n",
      "Epoch [1/5], Step [2206/10336], Loss: 4.1107\n",
      "Epoch [1/5], Step [2208/10336], Loss: 0.4344\n",
      "Epoch [1/5], Step [2210/10336], Loss: 1.9867\n",
      "Epoch [1/5], Step [2212/10336], Loss: 0.7946\n",
      "Epoch [1/5], Step [2214/10336], Loss: 5.3980\n",
      "Epoch [1/5], Step [2216/10336], Loss: 2.8576\n",
      "Epoch [1/5], Step [2218/10336], Loss: 3.0545\n",
      "Epoch [1/5], Step [2220/10336], Loss: 1.0021\n",
      "Epoch [1/5], Step [2222/10336], Loss: 1.4741\n",
      "Epoch [1/5], Step [2224/10336], Loss: 1.3944\n",
      "Epoch [1/5], Step [2226/10336], Loss: 1.5653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2228/10336], Loss: 0.9310\n",
      "Epoch [1/5], Step [2230/10336], Loss: 0.5649\n",
      "Epoch [1/5], Step [2232/10336], Loss: 1.8514\n",
      "Epoch [1/5], Step [2234/10336], Loss: 3.0737\n",
      "Epoch [1/5], Step [2236/10336], Loss: 2.0490\n",
      "Epoch [1/5], Step [2238/10336], Loss: 0.3082\n",
      "Epoch [1/5], Step [2240/10336], Loss: 1.1624\n",
      "Epoch [1/5], Step [2242/10336], Loss: 1.7615\n",
      "Epoch [1/5], Step [2244/10336], Loss: 1.5381\n",
      "Epoch [1/5], Step [2246/10336], Loss: 6.2132\n",
      "Epoch [1/5], Step [2248/10336], Loss: 3.0813\n",
      "Epoch [1/5], Step [2250/10336], Loss: 5.4131\n",
      "Epoch [1/5], Step [2252/10336], Loss: 0.2594\n",
      "Epoch [1/5], Step [2254/10336], Loss: 2.3320\n",
      "Epoch [1/5], Step [2256/10336], Loss: 0.2497\n",
      "Epoch [1/5], Step [2258/10336], Loss: 1.9474\n",
      "Epoch [1/5], Step [2260/10336], Loss: 1.7866\n",
      "Epoch [1/5], Step [2262/10336], Loss: 2.8885\n",
      "Epoch [1/5], Step [2264/10336], Loss: 1.3501\n",
      "Epoch [1/5], Step [2266/10336], Loss: 1.4394\n",
      "Epoch [1/5], Step [2268/10336], Loss: 1.8678\n",
      "Epoch [1/5], Step [2270/10336], Loss: 0.6338\n",
      "Epoch [1/5], Step [2272/10336], Loss: 1.6403\n",
      "Epoch [1/5], Step [2274/10336], Loss: 5.3997\n",
      "Epoch [1/5], Step [2276/10336], Loss: 3.5160\n",
      "Epoch [1/5], Step [2278/10336], Loss: 5.0540\n",
      "Epoch [1/5], Step [2280/10336], Loss: 0.2564\n",
      "Epoch [1/5], Step [2282/10336], Loss: 2.5378\n",
      "Epoch [1/5], Step [2284/10336], Loss: 1.9797\n",
      "Epoch [1/5], Step [2286/10336], Loss: 1.6361\n",
      "Epoch [1/5], Step [2288/10336], Loss: 1.0303\n",
      "Epoch [1/5], Step [2290/10336], Loss: 3.0104\n",
      "Epoch [1/5], Step [2292/10336], Loss: 1.3371\n",
      "Epoch [1/5], Step [2294/10336], Loss: 4.1515\n",
      "Epoch [1/5], Step [2296/10336], Loss: 2.8397\n",
      "Epoch [1/5], Step [2298/10336], Loss: 1.3769\n",
      "Epoch [1/5], Step [2300/10336], Loss: 0.6389\n",
      "Epoch [1/5], Step [2302/10336], Loss: 1.0410\n",
      "Epoch [1/5], Step [2304/10336], Loss: 2.3024\n",
      "Epoch [1/5], Step [2306/10336], Loss: 2.2758\n",
      "Epoch [1/5], Step [2308/10336], Loss: 2.5930\n",
      "Epoch [1/5], Step [2310/10336], Loss: 2.3660\n",
      "Epoch [1/5], Step [2312/10336], Loss: 1.8179\n",
      "Epoch [1/5], Step [2314/10336], Loss: 1.6103\n",
      "Epoch [1/5], Step [2316/10336], Loss: 3.2020\n",
      "Epoch [1/5], Step [2318/10336], Loss: 2.2697\n",
      "Epoch [1/5], Step [2320/10336], Loss: 1.2663\n",
      "Epoch [1/5], Step [2322/10336], Loss: 0.1124\n",
      "Epoch [1/5], Step [2324/10336], Loss: 5.0139\n",
      "Epoch [1/5], Step [2326/10336], Loss: 2.4797\n",
      "Epoch [1/5], Step [2328/10336], Loss: 1.0920\n",
      "Epoch [1/5], Step [2330/10336], Loss: 3.9601\n",
      "Epoch [1/5], Step [2332/10336], Loss: 1.5904\n",
      "Epoch [1/5], Step [2334/10336], Loss: 1.4898\n",
      "Epoch [1/5], Step [2336/10336], Loss: 1.5674\n",
      "Epoch [1/5], Step [2338/10336], Loss: 1.7106\n",
      "Epoch [1/5], Step [2340/10336], Loss: 4.1673\n",
      "Epoch [1/5], Step [2342/10336], Loss: 3.1732\n",
      "Epoch [1/5], Step [2344/10336], Loss: 2.2215\n",
      "Epoch [1/5], Step [2346/10336], Loss: 2.3939\n",
      "Epoch [1/5], Step [2348/10336], Loss: 2.4510\n",
      "Epoch [1/5], Step [2350/10336], Loss: 1.4364\n",
      "Epoch [1/5], Step [2352/10336], Loss: 1.3967\n",
      "Epoch [1/5], Step [2354/10336], Loss: 0.9022\n",
      "Epoch [1/5], Step [2356/10336], Loss: 0.5086\n",
      "Epoch [1/5], Step [2358/10336], Loss: 1.8310\n",
      "Epoch [1/5], Step [2360/10336], Loss: 2.1265\n",
      "Epoch [1/5], Step [2362/10336], Loss: 1.6133\n",
      "Epoch [1/5], Step [2364/10336], Loss: 0.6912\n",
      "Epoch [1/5], Step [2366/10336], Loss: 1.0774\n",
      "Epoch [1/5], Step [2368/10336], Loss: 1.1836\n",
      "Epoch [1/5], Step [2370/10336], Loss: 2.7308\n",
      "Epoch [1/5], Step [2372/10336], Loss: 3.1080\n",
      "Epoch [1/5], Step [2374/10336], Loss: 3.5017\n",
      "Epoch [1/5], Step [2376/10336], Loss: 1.2111\n",
      "Epoch [1/5], Step [2378/10336], Loss: 4.2452\n",
      "Epoch [1/5], Step [2380/10336], Loss: 3.4516\n",
      "Epoch [1/5], Step [2382/10336], Loss: 1.9985\n",
      "Epoch [1/5], Step [2384/10336], Loss: 1.7489\n",
      "Epoch [1/5], Step [2386/10336], Loss: 3.3261\n",
      "Epoch [1/5], Step [2388/10336], Loss: 3.3787\n",
      "Epoch [1/5], Step [2390/10336], Loss: 0.2058\n",
      "Epoch [1/5], Step [2392/10336], Loss: 1.5914\n",
      "Epoch [1/5], Step [2394/10336], Loss: 0.9493\n",
      "Epoch [1/5], Step [2396/10336], Loss: 0.0436\n",
      "Epoch [1/5], Step [2398/10336], Loss: 0.8662\n",
      "Epoch [1/5], Step [2400/10336], Loss: 3.2662\n",
      "Epoch [1/5], Step [2402/10336], Loss: 0.5239\n",
      "Epoch [1/5], Step [2404/10336], Loss: 1.1274\n",
      "Epoch [1/5], Step [2406/10336], Loss: 1.2220\n",
      "Epoch [1/5], Step [2408/10336], Loss: 3.5914\n",
      "Epoch [1/5], Step [2410/10336], Loss: 0.0367\n",
      "Epoch [1/5], Step [2412/10336], Loss: 2.3920\n",
      "Epoch [1/5], Step [2414/10336], Loss: 0.6123\n",
      "Epoch [1/5], Step [2416/10336], Loss: 2.4796\n",
      "Epoch [1/5], Step [2418/10336], Loss: 0.9484\n",
      "Epoch [1/5], Step [2420/10336], Loss: 1.6255\n",
      "Epoch [1/5], Step [2422/10336], Loss: 1.9719\n",
      "Epoch [1/5], Step [2424/10336], Loss: 0.5303\n",
      "Epoch [1/5], Step [2426/10336], Loss: 2.4308\n",
      "Epoch [1/5], Step [2428/10336], Loss: 1.3773\n",
      "Epoch [1/5], Step [2430/10336], Loss: 1.5676\n",
      "Epoch [1/5], Step [2432/10336], Loss: 1.9978\n",
      "Epoch [1/5], Step [2434/10336], Loss: 1.1615\n",
      "Epoch [1/5], Step [2436/10336], Loss: 0.8771\n",
      "Epoch [1/5], Step [2438/10336], Loss: 3.4856\n",
      "Epoch [1/5], Step [2440/10336], Loss: 1.9475\n",
      "Epoch [1/5], Step [2442/10336], Loss: 4.7080\n",
      "Epoch [1/5], Step [2444/10336], Loss: 2.3877\n",
      "Epoch [1/5], Step [2446/10336], Loss: 0.5368\n",
      "Epoch [1/5], Step [2448/10336], Loss: 1.7302\n",
      "Epoch [1/5], Step [2450/10336], Loss: 0.6922\n",
      "Epoch [1/5], Step [2452/10336], Loss: 0.4577\n",
      "Epoch [1/5], Step [2454/10336], Loss: 0.9146\n",
      "Epoch [1/5], Step [2456/10336], Loss: 1.5663\n",
      "Epoch [1/5], Step [2458/10336], Loss: 0.5933\n",
      "Epoch [1/5], Step [2460/10336], Loss: 0.1302\n",
      "Epoch [1/5], Step [2462/10336], Loss: 0.1702\n",
      "Epoch [1/5], Step [2464/10336], Loss: 1.7806\n",
      "Epoch [1/5], Step [2466/10336], Loss: 0.0845\n",
      "Epoch [1/5], Step [2468/10336], Loss: 0.2857\n",
      "Epoch [1/5], Step [2470/10336], Loss: 2.0071\n",
      "Epoch [1/5], Step [2472/10336], Loss: 3.3881\n",
      "Epoch [1/5], Step [2474/10336], Loss: 0.9458\n",
      "Epoch [1/5], Step [2476/10336], Loss: 5.1581\n",
      "Epoch [1/5], Step [2478/10336], Loss: 1.2795\n",
      "Epoch [1/5], Step [2480/10336], Loss: 0.3930\n",
      "Epoch [1/5], Step [2482/10336], Loss: 1.1927\n",
      "Epoch [1/5], Step [2484/10336], Loss: 4.8214\n",
      "Epoch [1/5], Step [2486/10336], Loss: 1.4549\n",
      "Epoch [1/5], Step [2488/10336], Loss: 1.4045\n",
      "Epoch [1/5], Step [2490/10336], Loss: 1.0611\n",
      "Epoch [1/5], Step [2492/10336], Loss: 0.2974\n",
      "Epoch [1/5], Step [2494/10336], Loss: 2.1793\n",
      "Epoch [1/5], Step [2496/10336], Loss: 0.9590\n",
      "Epoch [1/5], Step [2498/10336], Loss: 1.3742\n",
      "Epoch [1/5], Step [2500/10336], Loss: 1.4495\n",
      "Epoch [1/5], Step [2502/10336], Loss: 2.6797\n",
      "Epoch [1/5], Step [2504/10336], Loss: 1.8287\n",
      "Epoch [1/5], Step [2506/10336], Loss: 2.2353\n",
      "Epoch [1/5], Step [2508/10336], Loss: 0.7534\n",
      "Epoch [1/5], Step [2510/10336], Loss: 1.6815\n",
      "Epoch [1/5], Step [2512/10336], Loss: 1.1646\n",
      "Epoch [1/5], Step [2514/10336], Loss: 2.7055\n",
      "Epoch [1/5], Step [2516/10336], Loss: 6.9597\n",
      "Epoch [1/5], Step [2518/10336], Loss: 4.9009\n",
      "Epoch [1/5], Step [2520/10336], Loss: 6.5845\n",
      "Epoch [1/5], Step [2522/10336], Loss: 2.5659\n",
      "Epoch [1/5], Step [2524/10336], Loss: 1.2671\n",
      "Epoch [1/5], Step [2526/10336], Loss: 3.4091\n",
      "Epoch [1/5], Step [2528/10336], Loss: 1.4001\n",
      "Epoch [1/5], Step [2530/10336], Loss: 2.2045\n",
      "Epoch [1/5], Step [2532/10336], Loss: 0.8005\n",
      "Epoch [1/5], Step [2534/10336], Loss: 2.8964\n",
      "Epoch [1/5], Step [2536/10336], Loss: 4.1614\n",
      "Epoch [1/5], Step [2538/10336], Loss: 0.2253\n",
      "Epoch [1/5], Step [2540/10336], Loss: 0.8974\n",
      "Epoch [1/5], Step [2542/10336], Loss: 0.7026\n",
      "Epoch [1/5], Step [2544/10336], Loss: 2.6363\n",
      "Epoch [1/5], Step [2546/10336], Loss: 2.9013\n",
      "Epoch [1/5], Step [2548/10336], Loss: 1.5486\n",
      "Epoch [1/5], Step [2550/10336], Loss: 2.5602\n",
      "Epoch [1/5], Step [2552/10336], Loss: 3.6380\n",
      "Epoch [1/5], Step [2554/10336], Loss: 1.8405\n",
      "Epoch [1/5], Step [2556/10336], Loss: 1.2846\n",
      "Epoch [1/5], Step [2558/10336], Loss: 2.2959\n",
      "Epoch [1/5], Step [2560/10336], Loss: 1.3931\n",
      "Epoch [1/5], Step [2562/10336], Loss: 1.7371\n",
      "Epoch [1/5], Step [2564/10336], Loss: 0.9905\n",
      "Epoch [1/5], Step [2566/10336], Loss: 0.8749\n",
      "Epoch [1/5], Step [2568/10336], Loss: 4.5081\n",
      "Epoch [1/5], Step [2570/10336], Loss: 1.9082\n",
      "Epoch [1/5], Step [2572/10336], Loss: 1.2293\n",
      "Epoch [1/5], Step [2574/10336], Loss: 1.3284\n",
      "Epoch [1/5], Step [2576/10336], Loss: 3.3746\n",
      "Epoch [1/5], Step [2578/10336], Loss: 0.7669\n",
      "Epoch [1/5], Step [2580/10336], Loss: 1.6042\n",
      "Epoch [1/5], Step [2582/10336], Loss: 3.6799\n",
      "Epoch [1/5], Step [2584/10336], Loss: 0.2517\n",
      "Epoch [1/5], Step [2586/10336], Loss: 2.9366\n",
      "Epoch [1/5], Step [2588/10336], Loss: 2.5083\n",
      "Epoch [1/5], Step [2590/10336], Loss: 2.8841\n",
      "Epoch [1/5], Step [2592/10336], Loss: 0.3663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2594/10336], Loss: 1.4639\n",
      "Epoch [1/5], Step [2596/10336], Loss: 2.6887\n",
      "Epoch [1/5], Step [2598/10336], Loss: 1.2209\n",
      "Epoch [1/5], Step [2600/10336], Loss: 0.5399\n",
      "Epoch [1/5], Step [2602/10336], Loss: 5.3091\n",
      "Epoch [1/5], Step [2604/10336], Loss: 1.2775\n",
      "Epoch [1/5], Step [2606/10336], Loss: 4.8856\n",
      "Epoch [1/5], Step [2608/10336], Loss: 1.0407\n",
      "Epoch [1/5], Step [2610/10336], Loss: 3.6395\n",
      "Epoch [1/5], Step [2612/10336], Loss: 1.5719\n",
      "Epoch [1/5], Step [2614/10336], Loss: 3.7172\n",
      "Epoch [1/5], Step [2616/10336], Loss: 2.8424\n",
      "Epoch [1/5], Step [2618/10336], Loss: 1.9060\n",
      "Epoch [1/5], Step [2620/10336], Loss: 4.4523\n",
      "Epoch [1/5], Step [2622/10336], Loss: 3.2362\n",
      "Epoch [1/5], Step [2624/10336], Loss: 1.0341\n",
      "Epoch [1/5], Step [2626/10336], Loss: 0.9982\n",
      "Epoch [1/5], Step [2628/10336], Loss: 0.8614\n",
      "Epoch [1/5], Step [2630/10336], Loss: 3.1077\n",
      "Epoch [1/5], Step [2632/10336], Loss: 3.3304\n",
      "Epoch [1/5], Step [2634/10336], Loss: 0.5964\n",
      "Epoch [1/5], Step [2636/10336], Loss: 0.5155\n",
      "Epoch [1/5], Step [2638/10336], Loss: 0.7582\n",
      "Epoch [1/5], Step [2640/10336], Loss: 2.5651\n",
      "Epoch [1/5], Step [2642/10336], Loss: 1.1673\n",
      "Epoch [1/5], Step [2644/10336], Loss: 0.3841\n",
      "Epoch [1/5], Step [2646/10336], Loss: 1.1958\n",
      "Epoch [1/5], Step [2648/10336], Loss: 0.7954\n",
      "Epoch [1/5], Step [2650/10336], Loss: 0.4032\n",
      "Epoch [1/5], Step [2652/10336], Loss: 0.2568\n",
      "Epoch [1/5], Step [2654/10336], Loss: 1.0600\n",
      "Epoch [1/5], Step [2656/10336], Loss: 1.2509\n",
      "Epoch [1/5], Step [2658/10336], Loss: 0.8397\n",
      "Epoch [1/5], Step [2660/10336], Loss: 2.4674\n",
      "Epoch [1/5], Step [2662/10336], Loss: 1.4698\n",
      "Epoch [1/5], Step [2664/10336], Loss: 2.1544\n",
      "Epoch [1/5], Step [2666/10336], Loss: 1.3151\n",
      "Epoch [1/5], Step [2668/10336], Loss: 1.0321\n",
      "Epoch [1/5], Step [2670/10336], Loss: 2.0065\n",
      "Epoch [1/5], Step [2672/10336], Loss: 0.7327\n",
      "Epoch [1/5], Step [2674/10336], Loss: 2.3195\n",
      "Epoch [1/5], Step [2676/10336], Loss: 0.9939\n",
      "Epoch [1/5], Step [2678/10336], Loss: 1.7216\n",
      "Epoch [1/5], Step [2680/10336], Loss: 1.6086\n",
      "Epoch [1/5], Step [2682/10336], Loss: 2.5801\n",
      "Epoch [1/5], Step [2684/10336], Loss: 1.2239\n",
      "Epoch [1/5], Step [2686/10336], Loss: 1.5958\n",
      "Epoch [1/5], Step [2688/10336], Loss: 0.7061\n",
      "Epoch [1/5], Step [2690/10336], Loss: 0.3959\n",
      "Epoch [1/5], Step [2692/10336], Loss: 2.3391\n",
      "Epoch [1/5], Step [2694/10336], Loss: 2.7071\n",
      "Epoch [1/5], Step [2696/10336], Loss: 2.9310\n",
      "Epoch [1/5], Step [2698/10336], Loss: 0.8280\n",
      "Epoch [1/5], Step [2700/10336], Loss: 0.5780\n",
      "Epoch [1/5], Step [2702/10336], Loss: 1.9314\n",
      "Epoch [1/5], Step [2704/10336], Loss: 2.6084\n",
      "Epoch [1/5], Step [2706/10336], Loss: 5.8295\n",
      "Epoch [1/5], Step [2708/10336], Loss: 1.0986\n",
      "Epoch [1/5], Step [2710/10336], Loss: 4.7882\n",
      "Epoch [1/5], Step [2712/10336], Loss: 3.5988\n",
      "Epoch [1/5], Step [2714/10336], Loss: 1.5538\n",
      "Epoch [1/5], Step [2716/10336], Loss: 3.9852\n",
      "Epoch [1/5], Step [2718/10336], Loss: 1.4022\n",
      "Epoch [1/5], Step [2720/10336], Loss: 2.2472\n",
      "Epoch [1/5], Step [2722/10336], Loss: 0.2037\n",
      "Epoch [1/5], Step [2724/10336], Loss: 5.2785\n",
      "Epoch [1/5], Step [2726/10336], Loss: 0.3401\n",
      "Epoch [1/5], Step [2728/10336], Loss: 3.4727\n",
      "Epoch [1/5], Step [2730/10336], Loss: 2.1829\n",
      "Epoch [1/5], Step [2732/10336], Loss: 2.6723\n",
      "Epoch [1/5], Step [2734/10336], Loss: 0.1630\n",
      "Epoch [1/5], Step [2736/10336], Loss: 3.7047\n",
      "Epoch [1/5], Step [2738/10336], Loss: 0.1231\n",
      "Epoch [1/5], Step [2740/10336], Loss: 3.4606\n",
      "Epoch [1/5], Step [2742/10336], Loss: 1.4628\n",
      "Epoch [1/5], Step [2744/10336], Loss: 1.4716\n",
      "Epoch [1/5], Step [2746/10336], Loss: 0.8661\n",
      "Epoch [1/5], Step [2748/10336], Loss: 2.2374\n",
      "Epoch [1/5], Step [2750/10336], Loss: 0.9074\n",
      "Epoch [1/5], Step [2752/10336], Loss: 1.2398\n",
      "Epoch [1/5], Step [2754/10336], Loss: 0.7066\n",
      "Epoch [1/5], Step [2756/10336], Loss: 3.0937\n",
      "Epoch [1/5], Step [2758/10336], Loss: 2.4642\n",
      "Epoch [1/5], Step [2760/10336], Loss: 1.6110\n",
      "Epoch [1/5], Step [2762/10336], Loss: 0.8705\n",
      "Epoch [1/5], Step [2764/10336], Loss: 2.0531\n",
      "Epoch [1/5], Step [2766/10336], Loss: 0.5564\n",
      "Epoch [1/5], Step [2768/10336], Loss: 1.1152\n",
      "Epoch [1/5], Step [2770/10336], Loss: 0.6670\n",
      "Epoch [1/5], Step [2772/10336], Loss: 2.7877\n",
      "Epoch [1/5], Step [2774/10336], Loss: 0.2107\n",
      "Epoch [1/5], Step [2776/10336], Loss: 3.5320\n",
      "Epoch [1/5], Step [2778/10336], Loss: 2.7835\n",
      "Epoch [1/5], Step [2780/10336], Loss: 0.9981\n",
      "Epoch [1/5], Step [2782/10336], Loss: 0.5700\n",
      "Epoch [1/5], Step [2784/10336], Loss: 2.0758\n",
      "Epoch [1/5], Step [2786/10336], Loss: 2.5230\n",
      "Epoch [1/5], Step [2788/10336], Loss: 1.3506\n",
      "Epoch [1/5], Step [2790/10336], Loss: 3.4124\n",
      "Epoch [1/5], Step [2792/10336], Loss: 1.1635\n",
      "Epoch [1/5], Step [2794/10336], Loss: 0.9877\n",
      "Epoch [1/5], Step [2796/10336], Loss: 5.1931\n",
      "Epoch [1/5], Step [2798/10336], Loss: 1.8898\n",
      "Epoch [1/5], Step [2800/10336], Loss: 0.7625\n",
      "Epoch [1/5], Step [2802/10336], Loss: 0.8277\n",
      "Epoch [1/5], Step [2804/10336], Loss: 3.2436\n",
      "Epoch [1/5], Step [2806/10336], Loss: 0.2027\n",
      "Epoch [1/5], Step [2808/10336], Loss: 1.4016\n",
      "Epoch [1/5], Step [2810/10336], Loss: 2.4909\n",
      "Epoch [1/5], Step [2812/10336], Loss: 0.7113\n",
      "Epoch [1/5], Step [2814/10336], Loss: 2.0826\n",
      "Epoch [1/5], Step [2816/10336], Loss: 0.3086\n",
      "Epoch [1/5], Step [2818/10336], Loss: 2.9497\n",
      "Epoch [1/5], Step [2820/10336], Loss: 1.4645\n",
      "Epoch [1/5], Step [2822/10336], Loss: 1.5229\n",
      "Epoch [1/5], Step [2824/10336], Loss: 1.1581\n",
      "Epoch [1/5], Step [2826/10336], Loss: 2.1824\n",
      "Epoch [1/5], Step [2828/10336], Loss: 0.0873\n",
      "Epoch [1/5], Step [2830/10336], Loss: 0.6090\n",
      "Epoch [1/5], Step [2832/10336], Loss: 0.3047\n",
      "Epoch [1/5], Step [2834/10336], Loss: 0.5427\n",
      "Epoch [1/5], Step [2836/10336], Loss: 0.4370\n",
      "Epoch [1/5], Step [2838/10336], Loss: 2.0710\n",
      "Epoch [1/5], Step [2840/10336], Loss: 0.0369\n",
      "Epoch [1/5], Step [2842/10336], Loss: 0.0536\n",
      "Epoch [1/5], Step [2844/10336], Loss: 4.5620\n",
      "Epoch [1/5], Step [2846/10336], Loss: 3.8983\n",
      "Epoch [1/5], Step [2848/10336], Loss: 5.4027\n",
      "Epoch [1/5], Step [2850/10336], Loss: 0.4815\n",
      "Epoch [1/5], Step [2852/10336], Loss: 0.9584\n",
      "Epoch [1/5], Step [2854/10336], Loss: 2.2553\n",
      "Epoch [1/5], Step [2856/10336], Loss: 1.8038\n",
      "Epoch [1/5], Step [2858/10336], Loss: 1.2114\n",
      "Epoch [1/5], Step [2860/10336], Loss: 3.1197\n",
      "Epoch [1/5], Step [2862/10336], Loss: 2.7095\n",
      "Epoch [1/5], Step [2864/10336], Loss: 2.0594\n",
      "Epoch [1/5], Step [2866/10336], Loss: 2.3254\n",
      "Epoch [1/5], Step [2868/10336], Loss: 2.7962\n",
      "Epoch [1/5], Step [2870/10336], Loss: 1.9497\n",
      "Epoch [1/5], Step [2872/10336], Loss: 1.1322\n",
      "Epoch [1/5], Step [2874/10336], Loss: 1.7896\n",
      "Epoch [1/5], Step [2876/10336], Loss: 3.5183\n",
      "Epoch [1/5], Step [2878/10336], Loss: 0.8502\n",
      "Epoch [1/5], Step [2880/10336], Loss: 3.3983\n",
      "Epoch [1/5], Step [2882/10336], Loss: 3.1326\n",
      "Epoch [1/5], Step [2884/10336], Loss: 1.5843\n",
      "Epoch [1/5], Step [2886/10336], Loss: 1.2595\n",
      "Epoch [1/5], Step [2888/10336], Loss: 3.4539\n",
      "Epoch [1/5], Step [2890/10336], Loss: 2.0874\n",
      "Epoch [1/5], Step [2892/10336], Loss: 1.8568\n",
      "Epoch [1/5], Step [2894/10336], Loss: 0.0699\n",
      "Epoch [1/5], Step [2896/10336], Loss: 2.5470\n",
      "Epoch [1/5], Step [2898/10336], Loss: 2.3293\n",
      "Epoch [1/5], Step [2900/10336], Loss: 3.0758\n",
      "Epoch [1/5], Step [2902/10336], Loss: 2.1936\n",
      "Epoch [1/5], Step [2904/10336], Loss: 4.4357\n",
      "Epoch [1/5], Step [2906/10336], Loss: 3.0231\n",
      "Epoch [1/5], Step [2908/10336], Loss: 0.6330\n",
      "Epoch [1/5], Step [2910/10336], Loss: 1.4858\n",
      "Epoch [1/5], Step [2912/10336], Loss: 0.3806\n",
      "Epoch [1/5], Step [2914/10336], Loss: 3.3056\n",
      "Epoch [1/5], Step [2916/10336], Loss: 0.6564\n",
      "Epoch [1/5], Step [2918/10336], Loss: 1.6055\n",
      "Epoch [1/5], Step [2920/10336], Loss: 1.0772\n",
      "Epoch [1/5], Step [2922/10336], Loss: 3.9144\n",
      "Epoch [1/5], Step [2924/10336], Loss: 0.6725\n",
      "Epoch [1/5], Step [2926/10336], Loss: 1.3044\n",
      "Epoch [1/5], Step [2928/10336], Loss: 2.7837\n",
      "Epoch [1/5], Step [2930/10336], Loss: 1.0806\n",
      "Epoch [1/5], Step [2932/10336], Loss: 1.2035\n",
      "Epoch [1/5], Step [2934/10336], Loss: 1.8819\n",
      "Epoch [1/5], Step [2936/10336], Loss: 1.1471\n",
      "Epoch [1/5], Step [2938/10336], Loss: 1.1765\n",
      "Epoch [1/5], Step [2940/10336], Loss: 0.0260\n",
      "Epoch [1/5], Step [2942/10336], Loss: 0.1594\n",
      "Epoch [1/5], Step [2944/10336], Loss: 2.5581\n",
      "Epoch [1/5], Step [2946/10336], Loss: 1.7165\n",
      "Epoch [1/5], Step [2948/10336], Loss: 0.9382\n",
      "Epoch [1/5], Step [2950/10336], Loss: 1.6714\n",
      "Epoch [1/5], Step [2952/10336], Loss: 0.6381\n",
      "Epoch [1/5], Step [2954/10336], Loss: 0.5518\n",
      "Epoch [1/5], Step [2956/10336], Loss: 0.6575\n",
      "Epoch [1/5], Step [2958/10336], Loss: 2.2367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2960/10336], Loss: 1.5894\n",
      "Epoch [1/5], Step [2962/10336], Loss: 2.5258\n",
      "Epoch [1/5], Step [2964/10336], Loss: 4.4167\n",
      "Epoch [1/5], Step [2966/10336], Loss: 0.0125\n",
      "Epoch [1/5], Step [2968/10336], Loss: 3.6666\n",
      "Epoch [1/5], Step [2970/10336], Loss: 2.0513\n",
      "Epoch [1/5], Step [2972/10336], Loss: 0.4219\n",
      "Epoch [1/5], Step [2974/10336], Loss: 0.4099\n",
      "Epoch [1/5], Step [2976/10336], Loss: 1.3737\n",
      "Epoch [1/5], Step [2978/10336], Loss: 1.6565\n",
      "Epoch [1/5], Step [2980/10336], Loss: 4.3718\n",
      "Epoch [1/5], Step [2982/10336], Loss: 1.9020\n",
      "Epoch [1/5], Step [2984/10336], Loss: 2.0547\n",
      "Epoch [1/5], Step [2986/10336], Loss: 0.4361\n",
      "Epoch [1/5], Step [2988/10336], Loss: 2.0060\n",
      "Epoch [1/5], Step [2990/10336], Loss: 1.9171\n",
      "Epoch [1/5], Step [2992/10336], Loss: 2.0687\n",
      "Epoch [1/5], Step [2994/10336], Loss: 1.5367\n",
      "Epoch [1/5], Step [2996/10336], Loss: 5.4504\n",
      "Epoch [1/5], Step [2998/10336], Loss: 0.1957\n",
      "Epoch [1/5], Step [3000/10336], Loss: 0.2372\n",
      "Epoch [1/5], Step [3002/10336], Loss: 0.2722\n",
      "Epoch [1/5], Step [3004/10336], Loss: 1.2278\n",
      "Epoch [1/5], Step [3006/10336], Loss: 2.2889\n",
      "Epoch [1/5], Step [3008/10336], Loss: 1.4906\n",
      "Epoch [1/5], Step [3010/10336], Loss: 1.0412\n",
      "Epoch [1/5], Step [3012/10336], Loss: 1.8517\n",
      "Epoch [1/5], Step [3014/10336], Loss: 1.3336\n",
      "Epoch [1/5], Step [3016/10336], Loss: 3.8568\n",
      "Epoch [1/5], Step [3018/10336], Loss: 0.3475\n",
      "Epoch [1/5], Step [3020/10336], Loss: 0.8110\n",
      "Epoch [1/5], Step [3022/10336], Loss: 1.6724\n",
      "Epoch [1/5], Step [3024/10336], Loss: 1.8163\n",
      "Epoch [1/5], Step [3026/10336], Loss: 0.9088\n",
      "Epoch [1/5], Step [3028/10336], Loss: 1.4395\n",
      "Epoch [1/5], Step [3030/10336], Loss: 1.2225\n",
      "Epoch [1/5], Step [3032/10336], Loss: 1.1432\n",
      "Epoch [1/5], Step [3034/10336], Loss: 5.3685\n",
      "Epoch [1/5], Step [3036/10336], Loss: 0.7841\n",
      "Epoch [1/5], Step [3038/10336], Loss: 4.2568\n",
      "Epoch [1/5], Step [3040/10336], Loss: 2.6922\n",
      "Epoch [1/5], Step [3042/10336], Loss: 2.7919\n",
      "Epoch [1/5], Step [3044/10336], Loss: 1.0097\n",
      "Epoch [1/5], Step [3046/10336], Loss: 0.1350\n",
      "Epoch [1/5], Step [3048/10336], Loss: 1.5699\n",
      "Epoch [1/5], Step [3050/10336], Loss: 0.7925\n",
      "Epoch [1/5], Step [3052/10336], Loss: 1.0128\n",
      "Epoch [1/5], Step [3054/10336], Loss: 1.0380\n",
      "Epoch [1/5], Step [3056/10336], Loss: 3.4185\n",
      "Epoch [1/5], Step [3058/10336], Loss: 6.5565\n",
      "Epoch [1/5], Step [3060/10336], Loss: 1.9272\n",
      "Epoch [1/5], Step [3062/10336], Loss: 0.6620\n",
      "Epoch [1/5], Step [3064/10336], Loss: 1.3342\n",
      "Epoch [1/5], Step [3066/10336], Loss: 2.3870\n",
      "Epoch [1/5], Step [3068/10336], Loss: 1.2260\n",
      "Epoch [1/5], Step [3070/10336], Loss: 4.0654\n",
      "Epoch [1/5], Step [3072/10336], Loss: 0.8186\n",
      "Epoch [1/5], Step [3074/10336], Loss: 0.3210\n",
      "Epoch [1/5], Step [3076/10336], Loss: 3.0465\n",
      "Epoch [1/5], Step [3078/10336], Loss: 0.3653\n",
      "Epoch [1/5], Step [3080/10336], Loss: 1.9496\n",
      "Epoch [1/5], Step [3082/10336], Loss: 3.4487\n",
      "Epoch [1/5], Step [3084/10336], Loss: 1.4685\n",
      "Epoch [1/5], Step [3086/10336], Loss: 1.9037\n",
      "Epoch [1/5], Step [3088/10336], Loss: 2.0275\n",
      "Epoch [1/5], Step [3090/10336], Loss: 0.6973\n",
      "Epoch [1/5], Step [3092/10336], Loss: 0.8419\n",
      "Epoch [1/5], Step [3094/10336], Loss: 0.5926\n",
      "Epoch [1/5], Step [3096/10336], Loss: 3.1875\n",
      "Epoch [1/5], Step [3098/10336], Loss: 5.8817\n",
      "Epoch [1/5], Step [3100/10336], Loss: 0.3432\n",
      "Epoch [1/5], Step [3102/10336], Loss: 0.2756\n",
      "Epoch [1/5], Step [3104/10336], Loss: 3.4954\n",
      "Epoch [1/5], Step [3106/10336], Loss: 0.5749\n",
      "Epoch [1/5], Step [3108/10336], Loss: 1.7764\n",
      "Epoch [1/5], Step [3110/10336], Loss: 3.1063\n",
      "Epoch [1/5], Step [3112/10336], Loss: 2.5023\n",
      "Epoch [1/5], Step [3114/10336], Loss: 2.1735\n",
      "Epoch [1/5], Step [3116/10336], Loss: 0.4554\n",
      "Epoch [1/5], Step [3118/10336], Loss: 1.9561\n",
      "Epoch [1/5], Step [3120/10336], Loss: 1.6535\n",
      "Epoch [1/5], Step [3122/10336], Loss: 6.2076\n",
      "Epoch [1/5], Step [3124/10336], Loss: 3.2868\n",
      "Epoch [1/5], Step [3126/10336], Loss: 2.0729\n",
      "Epoch [1/5], Step [3128/10336], Loss: 2.0772\n",
      "Epoch [1/5], Step [3130/10336], Loss: 0.9668\n",
      "Epoch [1/5], Step [3132/10336], Loss: 1.6589\n",
      "Epoch [1/5], Step [3134/10336], Loss: 5.0634\n",
      "Epoch [1/5], Step [3136/10336], Loss: 2.3046\n",
      "Epoch [1/5], Step [3138/10336], Loss: 1.9753\n",
      "Epoch [1/5], Step [3140/10336], Loss: 1.2004\n",
      "Epoch [1/5], Step [3142/10336], Loss: 1.2741\n",
      "Epoch [1/5], Step [3144/10336], Loss: 0.6339\n",
      "Epoch [1/5], Step [3146/10336], Loss: 1.7129\n",
      "Epoch [1/5], Step [3148/10336], Loss: 1.5479\n",
      "Epoch [1/5], Step [3150/10336], Loss: 1.4691\n",
      "Epoch [1/5], Step [3152/10336], Loss: 4.6191\n",
      "Epoch [1/5], Step [3154/10336], Loss: 2.2527\n",
      "Epoch [1/5], Step [3156/10336], Loss: 0.3025\n",
      "Epoch [1/5], Step [3158/10336], Loss: 3.3677\n",
      "Epoch [1/5], Step [3160/10336], Loss: 2.7682\n",
      "Epoch [1/5], Step [3162/10336], Loss: 1.2722\n",
      "Epoch [1/5], Step [3164/10336], Loss: 1.2478\n",
      "Epoch [1/5], Step [3166/10336], Loss: 2.4428\n",
      "Epoch [1/5], Step [3168/10336], Loss: 2.3959\n",
      "Epoch [1/5], Step [3170/10336], Loss: 2.0570\n",
      "Epoch [1/5], Step [3172/10336], Loss: 0.2771\n",
      "Epoch [1/5], Step [3174/10336], Loss: 2.7376\n",
      "Epoch [1/5], Step [3176/10336], Loss: 4.2643\n",
      "Epoch [1/5], Step [3178/10336], Loss: 1.3498\n",
      "Epoch [1/5], Step [3180/10336], Loss: 0.8156\n",
      "Epoch [1/5], Step [3182/10336], Loss: 2.0643\n",
      "Epoch [1/5], Step [3184/10336], Loss: 1.6727\n",
      "Epoch [1/5], Step [3186/10336], Loss: 1.0343\n",
      "Epoch [1/5], Step [3188/10336], Loss: 0.3549\n",
      "Epoch [1/5], Step [3190/10336], Loss: 3.1095\n",
      "Epoch [1/5], Step [3192/10336], Loss: 1.3025\n",
      "Epoch [1/5], Step [3194/10336], Loss: 0.6028\n",
      "Epoch [1/5], Step [3196/10336], Loss: 1.5847\n",
      "Epoch [1/5], Step [3198/10336], Loss: 1.2903\n",
      "Epoch [1/5], Step [3200/10336], Loss: 2.7475\n",
      "Epoch [1/5], Step [3202/10336], Loss: 2.6312\n",
      "Epoch [1/5], Step [3204/10336], Loss: 1.0789\n",
      "Epoch [1/5], Step [3206/10336], Loss: 1.7660\n",
      "Epoch [1/5], Step [3208/10336], Loss: 1.5882\n",
      "Epoch [1/5], Step [3210/10336], Loss: 0.0153\n",
      "Epoch [1/5], Step [3212/10336], Loss: 1.9383\n",
      "Epoch [1/5], Step [3214/10336], Loss: 1.5534\n",
      "Epoch [1/5], Step [3216/10336], Loss: 1.2404\n",
      "Epoch [1/5], Step [3218/10336], Loss: 1.6183\n",
      "Epoch [1/5], Step [3220/10336], Loss: 1.4109\n",
      "Epoch [1/5], Step [3222/10336], Loss: 0.3991\n",
      "Epoch [1/5], Step [3224/10336], Loss: 0.0422\n",
      "Epoch [1/5], Step [3226/10336], Loss: 2.2275\n",
      "Epoch [1/5], Step [3228/10336], Loss: 1.0197\n",
      "Epoch [1/5], Step [3230/10336], Loss: 2.5732\n",
      "Epoch [1/5], Step [3232/10336], Loss: 1.9825\n",
      "Epoch [1/5], Step [3234/10336], Loss: 2.4387\n",
      "Epoch [1/5], Step [3236/10336], Loss: 2.9791\n",
      "Epoch [1/5], Step [3238/10336], Loss: 2.2716\n",
      "Epoch [1/5], Step [3240/10336], Loss: 0.4000\n",
      "Epoch [1/5], Step [3242/10336], Loss: 1.5462\n",
      "Epoch [1/5], Step [3244/10336], Loss: 3.9586\n",
      "Epoch [1/5], Step [3246/10336], Loss: 2.4593\n",
      "Epoch [1/5], Step [3248/10336], Loss: 2.9882\n",
      "Epoch [1/5], Step [3250/10336], Loss: 1.8203\n",
      "Epoch [1/5], Step [3252/10336], Loss: 1.8510\n",
      "Epoch [1/5], Step [3254/10336], Loss: 0.9593\n",
      "Epoch [1/5], Step [3256/10336], Loss: 0.8733\n",
      "Epoch [1/5], Step [3258/10336], Loss: 3.7322\n",
      "Epoch [1/5], Step [3260/10336], Loss: 3.6304\n",
      "Epoch [1/5], Step [3262/10336], Loss: 0.2627\n",
      "Epoch [1/5], Step [3264/10336], Loss: 0.0638\n",
      "Epoch [1/5], Step [3266/10336], Loss: 1.8119\n",
      "Epoch [1/5], Step [3268/10336], Loss: 0.6206\n",
      "Epoch [1/5], Step [3270/10336], Loss: 2.8812\n",
      "Epoch [1/5], Step [3272/10336], Loss: 2.1087\n",
      "Epoch [1/5], Step [3274/10336], Loss: 1.4289\n",
      "Epoch [1/5], Step [3276/10336], Loss: 2.0009\n",
      "Epoch [1/5], Step [3278/10336], Loss: 0.7296\n",
      "Epoch [1/5], Step [3280/10336], Loss: 0.3108\n",
      "Epoch [1/5], Step [3282/10336], Loss: 0.6485\n",
      "Epoch [1/5], Step [3284/10336], Loss: 0.3513\n",
      "Epoch [1/5], Step [3286/10336], Loss: 1.7702\n",
      "Epoch [1/5], Step [3288/10336], Loss: 2.0742\n",
      "Epoch [1/5], Step [3290/10336], Loss: 1.3249\n",
      "Epoch [1/5], Step [3292/10336], Loss: 0.6159\n",
      "Epoch [1/5], Step [3294/10336], Loss: 2.8315\n",
      "Epoch [1/5], Step [3296/10336], Loss: 2.6937\n",
      "Epoch [1/5], Step [3298/10336], Loss: 2.4688\n",
      "Epoch [1/5], Step [3300/10336], Loss: 2.9003\n",
      "Epoch [1/5], Step [3302/10336], Loss: 1.2601\n",
      "Epoch [1/5], Step [3304/10336], Loss: 1.1990\n",
      "Epoch [1/5], Step [3306/10336], Loss: 0.1827\n",
      "Epoch [1/5], Step [3308/10336], Loss: 1.7743\n",
      "Epoch [1/5], Step [3310/10336], Loss: 1.9995\n",
      "Epoch [1/5], Step [3312/10336], Loss: 1.7401\n",
      "Epoch [1/5], Step [3314/10336], Loss: 0.5006\n",
      "Epoch [1/5], Step [3316/10336], Loss: 0.7126\n",
      "Epoch [1/5], Step [3318/10336], Loss: 2.3573\n",
      "Epoch [1/5], Step [3320/10336], Loss: 2.5367\n",
      "Epoch [1/5], Step [3322/10336], Loss: 2.4568\n",
      "Epoch [1/5], Step [3324/10336], Loss: 2.0719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [3326/10336], Loss: 0.1214\n",
      "Epoch [1/5], Step [3328/10336], Loss: 4.8966\n",
      "Epoch [1/5], Step [3330/10336], Loss: 1.9828\n",
      "Epoch [1/5], Step [3332/10336], Loss: 3.9453\n",
      "Epoch [1/5], Step [3334/10336], Loss: 0.5905\n",
      "Epoch [1/5], Step [3336/10336], Loss: 1.0128\n",
      "Epoch [1/5], Step [3338/10336], Loss: 1.3123\n",
      "Epoch [1/5], Step [3340/10336], Loss: 0.5884\n",
      "Epoch [1/5], Step [3342/10336], Loss: 4.2429\n",
      "Epoch [1/5], Step [3344/10336], Loss: 2.9204\n",
      "Epoch [1/5], Step [3346/10336], Loss: 1.3496\n",
      "Epoch [1/5], Step [3348/10336], Loss: 2.1410\n",
      "Epoch [1/5], Step [3350/10336], Loss: 2.2467\n",
      "Epoch [1/5], Step [3352/10336], Loss: 1.9636\n",
      "Epoch [1/5], Step [3354/10336], Loss: 0.5350\n",
      "Epoch [1/5], Step [3356/10336], Loss: 0.2124\n",
      "Epoch [1/5], Step [3358/10336], Loss: 0.6426\n",
      "Epoch [1/5], Step [3360/10336], Loss: 0.3119\n",
      "Epoch [1/5], Step [3362/10336], Loss: 2.4165\n",
      "Epoch [1/5], Step [3364/10336], Loss: 1.4685\n",
      "Epoch [1/5], Step [3366/10336], Loss: 1.7070\n",
      "Epoch [1/5], Step [3368/10336], Loss: 2.4031\n",
      "Epoch [1/5], Step [3370/10336], Loss: 0.1014\n",
      "Epoch [1/5], Step [3372/10336], Loss: 0.0935\n",
      "Epoch [1/5], Step [3374/10336], Loss: 1.0011\n",
      "Epoch [1/5], Step [3376/10336], Loss: 3.6872\n",
      "Epoch [1/5], Step [3378/10336], Loss: 0.7955\n",
      "Epoch [1/5], Step [3380/10336], Loss: 1.5636\n",
      "Epoch [1/5], Step [3382/10336], Loss: 1.4939\n",
      "Epoch [1/5], Step [3384/10336], Loss: 1.0272\n",
      "Epoch [1/5], Step [3386/10336], Loss: 1.4785\n",
      "Epoch [1/5], Step [3388/10336], Loss: 3.4237\n",
      "Epoch [1/5], Step [3390/10336], Loss: 3.0901\n",
      "Epoch [1/5], Step [3392/10336], Loss: 0.4555\n",
      "Epoch [1/5], Step [3394/10336], Loss: 1.4197\n",
      "Epoch [1/5], Step [3396/10336], Loss: 2.3957\n",
      "Epoch [1/5], Step [3398/10336], Loss: 0.8428\n",
      "Epoch [1/5], Step [3400/10336], Loss: 0.0453\n",
      "Epoch [1/5], Step [3402/10336], Loss: 1.2506\n",
      "Epoch [1/5], Step [3404/10336], Loss: 0.7468\n",
      "Epoch [1/5], Step [3406/10336], Loss: 0.3475\n",
      "Epoch [1/5], Step [3408/10336], Loss: 0.9204\n",
      "Epoch [1/5], Step [3410/10336], Loss: 4.2922\n",
      "Epoch [1/5], Step [3412/10336], Loss: 1.9048\n",
      "Epoch [1/5], Step [3414/10336], Loss: 0.0549\n",
      "Epoch [1/5], Step [3416/10336], Loss: 1.1129\n",
      "Epoch [1/5], Step [3418/10336], Loss: 0.1255\n",
      "Epoch [1/5], Step [3420/10336], Loss: 0.4553\n",
      "Epoch [1/5], Step [3422/10336], Loss: 1.2659\n",
      "Epoch [1/5], Step [3424/10336], Loss: 1.3311\n",
      "Epoch [1/5], Step [3426/10336], Loss: 0.3146\n",
      "Epoch [1/5], Step [3428/10336], Loss: 1.2291\n",
      "Epoch [1/5], Step [3430/10336], Loss: 0.0890\n",
      "Epoch [1/5], Step [3432/10336], Loss: 2.1031\n",
      "Epoch [1/5], Step [3434/10336], Loss: 1.3985\n",
      "Epoch [1/5], Step [3436/10336], Loss: 2.6397\n",
      "Epoch [1/5], Step [3438/10336], Loss: 1.6896\n",
      "Epoch [1/5], Step [3440/10336], Loss: 0.1130\n",
      "Epoch [1/5], Step [3442/10336], Loss: 0.3718\n",
      "Epoch [1/5], Step [3444/10336], Loss: 2.6073\n",
      "Epoch [1/5], Step [3446/10336], Loss: 0.2304\n",
      "Epoch [1/5], Step [3448/10336], Loss: 2.9543\n",
      "Epoch [1/5], Step [3450/10336], Loss: 0.7538\n",
      "Epoch [1/5], Step [3452/10336], Loss: 1.6049\n",
      "Epoch [1/5], Step [3454/10336], Loss: 0.5568\n",
      "Epoch [1/5], Step [3456/10336], Loss: 4.4393\n",
      "Epoch [1/5], Step [3458/10336], Loss: 3.3827\n",
      "Epoch [1/5], Step [3460/10336], Loss: 1.8589\n",
      "Epoch [1/5], Step [3462/10336], Loss: 0.3189\n",
      "Epoch [1/5], Step [3464/10336], Loss: 1.9811\n",
      "Epoch [1/5], Step [3466/10336], Loss: 0.1946\n",
      "Epoch [1/5], Step [3468/10336], Loss: 1.3927\n",
      "Epoch [1/5], Step [3470/10336], Loss: 2.9487\n",
      "Epoch [1/5], Step [3472/10336], Loss: 4.2366\n",
      "Epoch [1/5], Step [3474/10336], Loss: 0.4919\n",
      "Epoch [1/5], Step [3476/10336], Loss: 2.3701\n",
      "Epoch [1/5], Step [3478/10336], Loss: 2.0558\n",
      "Epoch [1/5], Step [3480/10336], Loss: 0.4522\n",
      "Epoch [1/5], Step [3482/10336], Loss: 0.1557\n",
      "Epoch [1/5], Step [3484/10336], Loss: 0.7355\n",
      "Epoch [1/5], Step [3486/10336], Loss: 0.6767\n",
      "Epoch [1/5], Step [3488/10336], Loss: 2.0276\n",
      "Epoch [1/5], Step [3490/10336], Loss: 2.0279\n",
      "Epoch [1/5], Step [3492/10336], Loss: 1.0258\n",
      "Epoch [1/5], Step [3494/10336], Loss: 0.0490\n",
      "Epoch [1/5], Step [3496/10336], Loss: 0.0222\n",
      "Epoch [1/5], Step [3498/10336], Loss: 3.0263\n",
      "Epoch [1/5], Step [3500/10336], Loss: 1.2240\n",
      "Epoch [1/5], Step [3502/10336], Loss: 1.3585\n",
      "Epoch [1/5], Step [3504/10336], Loss: 1.4609\n",
      "Epoch [1/5], Step [3506/10336], Loss: 0.9586\n",
      "Epoch [1/5], Step [3508/10336], Loss: 0.0976\n",
      "Epoch [1/5], Step [3510/10336], Loss: 1.9312\n",
      "Epoch [1/5], Step [3512/10336], Loss: 1.6605\n",
      "Epoch [1/5], Step [3514/10336], Loss: 1.3280\n",
      "Epoch [1/5], Step [3516/10336], Loss: 5.2507\n",
      "Epoch [1/5], Step [3518/10336], Loss: 2.7783\n",
      "Epoch [1/5], Step [3520/10336], Loss: 0.9229\n",
      "Epoch [1/5], Step [3522/10336], Loss: 4.3907\n",
      "Epoch [1/5], Step [3524/10336], Loss: 6.7735\n",
      "Epoch [1/5], Step [3526/10336], Loss: 0.1963\n",
      "Epoch [1/5], Step [3528/10336], Loss: 1.8860\n",
      "Epoch [1/5], Step [3530/10336], Loss: 4.7014\n",
      "Epoch [1/5], Step [3532/10336], Loss: 2.7959\n",
      "Epoch [1/5], Step [3534/10336], Loss: 0.2071\n",
      "Epoch [1/5], Step [3536/10336], Loss: 1.4228\n",
      "Epoch [1/5], Step [3538/10336], Loss: 1.8796\n",
      "Epoch [1/5], Step [3540/10336], Loss: 1.8743\n",
      "Epoch [1/5], Step [3542/10336], Loss: 0.7255\n",
      "Epoch [1/5], Step [3544/10336], Loss: 4.7250\n",
      "Epoch [1/5], Step [3546/10336], Loss: 1.1658\n",
      "Epoch [1/5], Step [3548/10336], Loss: 0.2696\n",
      "Epoch [1/5], Step [3550/10336], Loss: 1.0104\n",
      "Epoch [1/5], Step [3552/10336], Loss: 0.9751\n",
      "Epoch [1/5], Step [3554/10336], Loss: 1.9739\n",
      "Epoch [1/5], Step [3556/10336], Loss: 2.8575\n",
      "Epoch [1/5], Step [3558/10336], Loss: 1.1263\n",
      "Epoch [1/5], Step [3560/10336], Loss: 1.3016\n",
      "Epoch [1/5], Step [3562/10336], Loss: 1.6376\n",
      "Epoch [1/5], Step [3564/10336], Loss: 2.3864\n",
      "Epoch [1/5], Step [3566/10336], Loss: 3.5303\n",
      "Epoch [1/5], Step [3568/10336], Loss: 2.2043\n",
      "Epoch [1/5], Step [3570/10336], Loss: 1.5704\n",
      "Epoch [1/5], Step [3572/10336], Loss: 2.1615\n",
      "Epoch [1/5], Step [3574/10336], Loss: 3.1398\n",
      "Epoch [1/5], Step [3576/10336], Loss: 0.2203\n",
      "Epoch [1/5], Step [3578/10336], Loss: 0.9625\n",
      "Epoch [1/5], Step [3580/10336], Loss: 1.3921\n",
      "Epoch [1/5], Step [3582/10336], Loss: 1.4013\n",
      "Epoch [1/5], Step [3584/10336], Loss: 0.6328\n",
      "Epoch [1/5], Step [3586/10336], Loss: 2.5647\n",
      "Epoch [1/5], Step [3588/10336], Loss: 1.5374\n",
      "Epoch [1/5], Step [3590/10336], Loss: 2.2326\n",
      "Epoch [1/5], Step [3592/10336], Loss: 4.5515\n",
      "Epoch [1/5], Step [3594/10336], Loss: 3.6290\n",
      "Epoch [1/5], Step [3596/10336], Loss: 3.6159\n",
      "Epoch [1/5], Step [3598/10336], Loss: 0.9035\n",
      "Epoch [1/5], Step [3600/10336], Loss: 2.3477\n",
      "Epoch [1/5], Step [3602/10336], Loss: 0.6646\n",
      "Epoch [1/5], Step [3604/10336], Loss: 0.8931\n",
      "Epoch [1/5], Step [3606/10336], Loss: 2.6535\n",
      "Epoch [1/5], Step [3608/10336], Loss: 1.4357\n",
      "Epoch [1/5], Step [3610/10336], Loss: 0.6882\n",
      "Epoch [1/5], Step [3612/10336], Loss: 1.7086\n",
      "Epoch [1/5], Step [3614/10336], Loss: 2.2973\n",
      "Epoch [1/5], Step [3616/10336], Loss: 2.1477\n",
      "Epoch [1/5], Step [3618/10336], Loss: 1.8972\n",
      "Epoch [1/5], Step [3620/10336], Loss: 2.6563\n",
      "Epoch [1/5], Step [3622/10336], Loss: 0.1178\n",
      "Epoch [1/5], Step [3624/10336], Loss: 0.4123\n",
      "Epoch [1/5], Step [3626/10336], Loss: 5.7374\n",
      "Epoch [1/5], Step [3628/10336], Loss: 1.5263\n",
      "Epoch [1/5], Step [3630/10336], Loss: 0.9328\n",
      "Epoch [1/5], Step [3632/10336], Loss: 1.3970\n",
      "Epoch [1/5], Step [3634/10336], Loss: 4.0064\n",
      "Epoch [1/5], Step [3636/10336], Loss: 0.0909\n",
      "Epoch [1/5], Step [3638/10336], Loss: 1.8717\n",
      "Epoch [1/5], Step [3640/10336], Loss: 3.6036\n",
      "Epoch [1/5], Step [3642/10336], Loss: 1.4976\n",
      "Epoch [1/5], Step [3644/10336], Loss: 2.6992\n",
      "Epoch [1/5], Step [3646/10336], Loss: 1.1856\n",
      "Epoch [1/5], Step [3648/10336], Loss: 0.8249\n",
      "Epoch [1/5], Step [3650/10336], Loss: 0.1752\n",
      "Epoch [1/5], Step [3652/10336], Loss: 1.7239\n",
      "Epoch [1/5], Step [3654/10336], Loss: 1.7838\n",
      "Epoch [1/5], Step [3656/10336], Loss: 0.6604\n",
      "Epoch [1/5], Step [3658/10336], Loss: 2.3047\n",
      "Epoch [1/5], Step [3660/10336], Loss: 0.6653\n",
      "Epoch [1/5], Step [3662/10336], Loss: 0.7621\n",
      "Epoch [1/5], Step [3664/10336], Loss: 0.8708\n",
      "Epoch [1/5], Step [3666/10336], Loss: 2.4581\n",
      "Epoch [1/5], Step [3668/10336], Loss: 2.6483\n",
      "Epoch [1/5], Step [3670/10336], Loss: 0.7130\n",
      "Epoch [1/5], Step [3672/10336], Loss: 1.4652\n",
      "Epoch [1/5], Step [3674/10336], Loss: 0.1090\n",
      "Epoch [1/5], Step [3676/10336], Loss: 4.0001\n",
      "Epoch [1/5], Step [3678/10336], Loss: 0.4316\n",
      "Epoch [1/5], Step [3680/10336], Loss: 1.0774\n",
      "Epoch [1/5], Step [3682/10336], Loss: 2.6857\n",
      "Epoch [1/5], Step [3684/10336], Loss: 3.3030\n",
      "Epoch [1/5], Step [3686/10336], Loss: 3.6298\n",
      "Epoch [1/5], Step [3688/10336], Loss: 0.7296\n",
      "Epoch [1/5], Step [3690/10336], Loss: 0.4839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [3692/10336], Loss: 4.9367\n",
      "Epoch [1/5], Step [3694/10336], Loss: 1.6853\n",
      "Epoch [1/5], Step [3696/10336], Loss: 0.1593\n",
      "Epoch [1/5], Step [3698/10336], Loss: 0.3086\n",
      "Epoch [1/5], Step [3700/10336], Loss: 3.8371\n",
      "Epoch [1/5], Step [3702/10336], Loss: 1.7951\n",
      "Epoch [1/5], Step [3704/10336], Loss: 0.6348\n",
      "Epoch [1/5], Step [3706/10336], Loss: 3.2815\n",
      "Epoch [1/5], Step [3708/10336], Loss: 0.6473\n",
      "Epoch [1/5], Step [3710/10336], Loss: 0.5666\n",
      "Epoch [1/5], Step [3712/10336], Loss: 1.3649\n",
      "Epoch [1/5], Step [3714/10336], Loss: 1.4552\n",
      "Epoch [1/5], Step [3716/10336], Loss: 2.5921\n",
      "Epoch [1/5], Step [3718/10336], Loss: 0.2779\n",
      "Epoch [1/5], Step [3720/10336], Loss: 1.9909\n",
      "Epoch [1/5], Step [3722/10336], Loss: 3.8189\n",
      "Epoch [1/5], Step [3724/10336], Loss: 0.8523\n",
      "Epoch [1/5], Step [3726/10336], Loss: 2.8083\n",
      "Epoch [1/5], Step [3728/10336], Loss: 0.6308\n",
      "Epoch [1/5], Step [3730/10336], Loss: 1.0640\n",
      "Epoch [1/5], Step [3732/10336], Loss: 1.0102\n",
      "Epoch [1/5], Step [3734/10336], Loss: 1.2908\n",
      "Epoch [1/5], Step [3736/10336], Loss: 2.7669\n",
      "Epoch [1/5], Step [3738/10336], Loss: 0.2205\n",
      "Epoch [1/5], Step [3740/10336], Loss: 2.4753\n",
      "Epoch [1/5], Step [3742/10336], Loss: 0.9811\n",
      "Epoch [1/5], Step [3744/10336], Loss: 2.3717\n",
      "Epoch [1/5], Step [3746/10336], Loss: 1.0420\n",
      "Epoch [1/5], Step [3748/10336], Loss: 0.7848\n",
      "Epoch [1/5], Step [3750/10336], Loss: 3.4774\n",
      "Epoch [1/5], Step [3752/10336], Loss: 0.2928\n",
      "Epoch [1/5], Step [3754/10336], Loss: 0.1206\n",
      "Epoch [1/5], Step [3756/10336], Loss: 3.2978\n",
      "Epoch [1/5], Step [3758/10336], Loss: 1.7830\n",
      "Epoch [1/5], Step [3760/10336], Loss: 0.7762\n",
      "Epoch [1/5], Step [3762/10336], Loss: 0.3830\n",
      "Epoch [1/5], Step [3764/10336], Loss: 0.0086\n",
      "Epoch [1/5], Step [3766/10336], Loss: 2.5880\n",
      "Epoch [1/5], Step [3768/10336], Loss: 2.6481\n",
      "Epoch [1/5], Step [3770/10336], Loss: 2.5082\n",
      "Epoch [1/5], Step [3772/10336], Loss: 3.7961\n",
      "Epoch [1/5], Step [3774/10336], Loss: 2.8210\n",
      "Epoch [1/5], Step [3776/10336], Loss: 1.3576\n",
      "Epoch [1/5], Step [3778/10336], Loss: 1.3683\n",
      "Epoch [1/5], Step [3780/10336], Loss: 3.2408\n",
      "Epoch [1/5], Step [3782/10336], Loss: 2.3119\n",
      "Epoch [1/5], Step [3784/10336], Loss: 0.4491\n",
      "Epoch [1/5], Step [3786/10336], Loss: 2.0011\n",
      "Epoch [1/5], Step [3788/10336], Loss: 1.9632\n",
      "Epoch [1/5], Step [3790/10336], Loss: 3.9295\n",
      "Epoch [1/5], Step [3792/10336], Loss: 0.8520\n",
      "Epoch [1/5], Step [3794/10336], Loss: 0.7861\n",
      "Epoch [1/5], Step [3796/10336], Loss: 1.2471\n",
      "Epoch [1/5], Step [3798/10336], Loss: 3.9665\n",
      "Epoch [1/5], Step [3800/10336], Loss: 0.2899\n",
      "Epoch [1/5], Step [3802/10336], Loss: 1.6966\n",
      "Epoch [1/5], Step [3804/10336], Loss: 0.2419\n",
      "Epoch [1/5], Step [3806/10336], Loss: 0.9801\n",
      "Epoch [1/5], Step [3808/10336], Loss: 0.1723\n",
      "Epoch [1/5], Step [3810/10336], Loss: 3.5624\n",
      "Epoch [1/5], Step [3812/10336], Loss: 1.8496\n",
      "Epoch [1/5], Step [3814/10336], Loss: 1.6313\n",
      "Epoch [1/5], Step [3816/10336], Loss: 6.0630\n",
      "Epoch [1/5], Step [3818/10336], Loss: 1.9280\n",
      "Epoch [1/5], Step [3820/10336], Loss: 0.5262\n",
      "Epoch [1/5], Step [3822/10336], Loss: 2.1860\n",
      "Epoch [1/5], Step [3824/10336], Loss: 2.3320\n",
      "Epoch [1/5], Step [3826/10336], Loss: 2.0794\n",
      "Epoch [1/5], Step [3828/10336], Loss: 2.1066\n",
      "Epoch [1/5], Step [3830/10336], Loss: 0.9985\n",
      "Epoch [1/5], Step [3832/10336], Loss: 1.0369\n",
      "Epoch [1/5], Step [3834/10336], Loss: 3.3970\n",
      "Epoch [1/5], Step [3836/10336], Loss: 1.4797\n",
      "Epoch [1/5], Step [3838/10336], Loss: 1.1022\n",
      "Epoch [1/5], Step [3840/10336], Loss: 1.9087\n",
      "Epoch [1/5], Step [3842/10336], Loss: 1.5307\n",
      "Epoch [1/5], Step [3844/10336], Loss: 1.5487\n",
      "Epoch [1/5], Step [3846/10336], Loss: 1.4994\n",
      "Epoch [1/5], Step [3848/10336], Loss: 0.1729\n",
      "Epoch [1/5], Step [3850/10336], Loss: 0.7867\n",
      "Epoch [1/5], Step [3852/10336], Loss: 1.2034\n",
      "Epoch [1/5], Step [3854/10336], Loss: 0.5748\n",
      "Epoch [1/5], Step [3856/10336], Loss: 0.8249\n",
      "Epoch [1/5], Step [3858/10336], Loss: 5.5971\n",
      "Epoch [1/5], Step [3860/10336], Loss: 0.6038\n",
      "Epoch [1/5], Step [3862/10336], Loss: 0.6936\n",
      "Epoch [1/5], Step [3864/10336], Loss: 2.4133\n",
      "Epoch [1/5], Step [3866/10336], Loss: 2.0748\n",
      "Epoch [1/5], Step [3868/10336], Loss: 0.4151\n",
      "Epoch [1/5], Step [3870/10336], Loss: 1.7928\n",
      "Epoch [1/5], Step [3872/10336], Loss: 1.0197\n",
      "Epoch [1/5], Step [3874/10336], Loss: 1.7765\n",
      "Epoch [1/5], Step [3876/10336], Loss: 2.4850\n",
      "Epoch [1/5], Step [3878/10336], Loss: 1.1382\n",
      "Epoch [1/5], Step [3880/10336], Loss: 1.7556\n",
      "Epoch [1/5], Step [3882/10336], Loss: 0.5525\n",
      "Epoch [1/5], Step [3884/10336], Loss: 2.9382\n",
      "Epoch [1/5], Step [3886/10336], Loss: 0.1428\n",
      "Epoch [1/5], Step [3888/10336], Loss: 3.8258\n",
      "Epoch [1/5], Step [3890/10336], Loss: 1.9973\n",
      "Epoch [1/5], Step [3892/10336], Loss: 0.6177\n",
      "Epoch [1/5], Step [3894/10336], Loss: 1.4464\n",
      "Epoch [1/5], Step [3896/10336], Loss: 0.3210\n",
      "Epoch [1/5], Step [3898/10336], Loss: 0.3754\n",
      "Epoch [1/5], Step [3900/10336], Loss: 1.5172\n",
      "Epoch [1/5], Step [3902/10336], Loss: 2.3351\n",
      "Epoch [1/5], Step [3904/10336], Loss: 4.0661\n",
      "Epoch [1/5], Step [3906/10336], Loss: 2.6120\n",
      "Epoch [1/5], Step [3908/10336], Loss: 0.4335\n",
      "Epoch [1/5], Step [3910/10336], Loss: 1.5428\n",
      "Epoch [1/5], Step [3912/10336], Loss: 0.8111\n",
      "Epoch [1/5], Step [3914/10336], Loss: 5.3122\n",
      "Epoch [1/5], Step [3916/10336], Loss: 1.4894\n",
      "Epoch [1/5], Step [3918/10336], Loss: 5.4687\n",
      "Epoch [1/5], Step [3920/10336], Loss: 1.3352\n",
      "Epoch [1/5], Step [3922/10336], Loss: 1.6513\n",
      "Epoch [1/5], Step [3924/10336], Loss: 1.5632\n",
      "Epoch [1/5], Step [3926/10336], Loss: 0.6846\n",
      "Epoch [1/5], Step [3928/10336], Loss: 2.9212\n",
      "Epoch [1/5], Step [3930/10336], Loss: 0.9977\n",
      "Epoch [1/5], Step [3932/10336], Loss: 1.4627\n",
      "Epoch [1/5], Step [3934/10336], Loss: 1.7384\n",
      "Epoch [1/5], Step [3936/10336], Loss: 3.3381\n",
      "Epoch [1/5], Step [3938/10336], Loss: 2.2999\n",
      "Epoch [1/5], Step [3940/10336], Loss: 3.2332\n",
      "Epoch [1/5], Step [3942/10336], Loss: 0.9517\n",
      "Epoch [1/5], Step [3944/10336], Loss: 1.9467\n",
      "Epoch [1/5], Step [3946/10336], Loss: 0.6705\n",
      "Epoch [1/5], Step [3948/10336], Loss: 1.0537\n",
      "Epoch [1/5], Step [3950/10336], Loss: 1.3948\n",
      "Epoch [1/5], Step [3952/10336], Loss: 0.7432\n",
      "Epoch [1/5], Step [3954/10336], Loss: 4.2696\n",
      "Epoch [1/5], Step [3956/10336], Loss: 1.4693\n",
      "Epoch [1/5], Step [3958/10336], Loss: 2.2815\n",
      "Epoch [1/5], Step [3960/10336], Loss: 1.9033\n",
      "Epoch [1/5], Step [3962/10336], Loss: 1.1899\n",
      "Epoch [1/5], Step [3964/10336], Loss: 2.7239\n",
      "Epoch [1/5], Step [3966/10336], Loss: 0.5647\n",
      "Epoch [1/5], Step [3968/10336], Loss: 1.3637\n",
      "Epoch [1/5], Step [3970/10336], Loss: 1.0177\n",
      "Epoch [1/5], Step [3972/10336], Loss: 1.0389\n",
      "Epoch [1/5], Step [3974/10336], Loss: 1.4094\n",
      "Epoch [1/5], Step [3976/10336], Loss: 3.5557\n",
      "Epoch [1/5], Step [3978/10336], Loss: 2.9792\n",
      "Epoch [1/5], Step [3980/10336], Loss: 2.0318\n",
      "Epoch [1/5], Step [3982/10336], Loss: 1.7305\n",
      "Epoch [1/5], Step [3984/10336], Loss: 2.3579\n",
      "Epoch [1/5], Step [3986/10336], Loss: 1.0681\n",
      "Epoch [1/5], Step [3988/10336], Loss: 4.8835\n",
      "Epoch [1/5], Step [3990/10336], Loss: 2.0905\n",
      "Epoch [1/5], Step [3992/10336], Loss: 1.2722\n",
      "Epoch [1/5], Step [3994/10336], Loss: 2.3949\n",
      "Epoch [1/5], Step [3996/10336], Loss: 1.8073\n",
      "Epoch [1/5], Step [3998/10336], Loss: 0.9341\n",
      "Epoch [1/5], Step [4000/10336], Loss: 3.0885\n",
      "Epoch [1/5], Step [4002/10336], Loss: 0.8578\n",
      "Epoch [1/5], Step [4004/10336], Loss: 2.5418\n",
      "Epoch [1/5], Step [4006/10336], Loss: 1.9944\n",
      "Epoch [1/5], Step [4008/10336], Loss: 1.1167\n",
      "Epoch [1/5], Step [4010/10336], Loss: 3.4526\n",
      "Epoch [1/5], Step [4012/10336], Loss: 1.7150\n",
      "Epoch [1/5], Step [4014/10336], Loss: 0.2095\n",
      "Epoch [1/5], Step [4016/10336], Loss: 2.7198\n",
      "Epoch [1/5], Step [4018/10336], Loss: 2.4453\n",
      "Epoch [1/5], Step [4020/10336], Loss: 1.2332\n",
      "Epoch [1/5], Step [4022/10336], Loss: 3.7454\n",
      "Epoch [1/5], Step [4024/10336], Loss: 3.3288\n",
      "Epoch [1/5], Step [4026/10336], Loss: 0.2138\n",
      "Epoch [1/5], Step [4028/10336], Loss: 0.5317\n",
      "Epoch [1/5], Step [4030/10336], Loss: 2.1877\n",
      "Epoch [1/5], Step [4032/10336], Loss: 4.8314\n",
      "Epoch [1/5], Step [4034/10336], Loss: 0.4734\n",
      "Epoch [1/5], Step [4036/10336], Loss: 1.5124\n",
      "Epoch [1/5], Step [4038/10336], Loss: 2.1659\n",
      "Epoch [1/5], Step [4040/10336], Loss: 0.3571\n",
      "Epoch [1/5], Step [4042/10336], Loss: 0.7752\n",
      "Epoch [1/5], Step [4044/10336], Loss: 0.9309\n",
      "Epoch [1/5], Step [4046/10336], Loss: 2.6056\n",
      "Epoch [1/5], Step [4048/10336], Loss: 1.2449\n",
      "Epoch [1/5], Step [4050/10336], Loss: 0.2449\n",
      "Epoch [1/5], Step [4052/10336], Loss: 1.8397\n",
      "Epoch [1/5], Step [4054/10336], Loss: 1.8525\n",
      "Epoch [1/5], Step [4056/10336], Loss: 1.7465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4058/10336], Loss: 0.7212\n",
      "Epoch [1/5], Step [4060/10336], Loss: 2.5021\n",
      "Epoch [1/5], Step [4062/10336], Loss: 3.6813\n",
      "Epoch [1/5], Step [4064/10336], Loss: 3.4836\n",
      "Epoch [1/5], Step [4066/10336], Loss: 0.7909\n",
      "Epoch [1/5], Step [4068/10336], Loss: 0.7198\n",
      "Epoch [1/5], Step [4070/10336], Loss: 4.1820\n",
      "Epoch [1/5], Step [4072/10336], Loss: 0.4435\n",
      "Epoch [1/5], Step [4074/10336], Loss: 2.0914\n",
      "Epoch [1/5], Step [4076/10336], Loss: 0.2379\n",
      "Epoch [1/5], Step [4078/10336], Loss: 0.4185\n",
      "Epoch [1/5], Step [4080/10336], Loss: 0.3413\n",
      "Epoch [1/5], Step [4082/10336], Loss: 1.4417\n",
      "Epoch [1/5], Step [4084/10336], Loss: 0.0730\n",
      "Epoch [1/5], Step [4086/10336], Loss: 2.0151\n",
      "Epoch [1/5], Step [4088/10336], Loss: 2.2479\n",
      "Epoch [1/5], Step [4090/10336], Loss: 1.5814\n",
      "Epoch [1/5], Step [4092/10336], Loss: 1.7455\n",
      "Epoch [1/5], Step [4094/10336], Loss: 2.5814\n",
      "Epoch [1/5], Step [4096/10336], Loss: 2.7733\n",
      "Epoch [1/5], Step [4098/10336], Loss: 4.4074\n",
      "Epoch [1/5], Step [4100/10336], Loss: 1.9466\n",
      "Epoch [1/5], Step [4102/10336], Loss: 3.4950\n",
      "Epoch [1/5], Step [4104/10336], Loss: 1.7353\n",
      "Epoch [1/5], Step [4106/10336], Loss: 0.5201\n",
      "Epoch [1/5], Step [4108/10336], Loss: 0.1450\n",
      "Epoch [1/5], Step [4110/10336], Loss: 2.2608\n",
      "Epoch [1/5], Step [4112/10336], Loss: 2.3486\n",
      "Epoch [1/5], Step [4114/10336], Loss: 0.3362\n",
      "Epoch [1/5], Step [4116/10336], Loss: 1.0562\n",
      "Epoch [1/5], Step [4118/10336], Loss: 2.9359\n",
      "Epoch [1/5], Step [4120/10336], Loss: 0.1641\n",
      "Epoch [1/5], Step [4122/10336], Loss: 1.7387\n",
      "Epoch [1/5], Step [4124/10336], Loss: 2.3157\n",
      "Epoch [1/5], Step [4126/10336], Loss: 2.4820\n",
      "Epoch [1/5], Step [4128/10336], Loss: 1.9946\n",
      "Epoch [1/5], Step [4130/10336], Loss: 2.4197\n",
      "Epoch [1/5], Step [4132/10336], Loss: 1.6291\n",
      "Epoch [1/5], Step [4134/10336], Loss: 2.2820\n",
      "Epoch [1/5], Step [4136/10336], Loss: 1.3070\n",
      "Epoch [1/5], Step [4138/10336], Loss: 3.9703\n",
      "Epoch [1/5], Step [4140/10336], Loss: 0.3009\n",
      "Epoch [1/5], Step [4142/10336], Loss: 0.1610\n",
      "Epoch [1/5], Step [4144/10336], Loss: 2.0157\n",
      "Epoch [1/5], Step [4146/10336], Loss: 2.5401\n",
      "Epoch [1/5], Step [4148/10336], Loss: 0.1886\n",
      "Epoch [1/5], Step [4150/10336], Loss: 2.1766\n",
      "Epoch [1/5], Step [4152/10336], Loss: 1.0242\n",
      "Epoch [1/5], Step [4154/10336], Loss: 1.6331\n",
      "Epoch [1/5], Step [4156/10336], Loss: 1.1762\n",
      "Epoch [1/5], Step [4158/10336], Loss: 1.6036\n",
      "Epoch [1/5], Step [4160/10336], Loss: 0.2887\n",
      "Epoch [1/5], Step [4162/10336], Loss: 0.1442\n",
      "Epoch [1/5], Step [4164/10336], Loss: 1.3946\n",
      "Epoch [1/5], Step [4166/10336], Loss: 0.0351\n",
      "Epoch [1/5], Step [4168/10336], Loss: 4.8266\n",
      "Epoch [1/5], Step [4170/10336], Loss: 2.8960\n",
      "Epoch [1/5], Step [4172/10336], Loss: 2.4647\n",
      "Epoch [1/5], Step [4174/10336], Loss: 2.1308\n",
      "Epoch [1/5], Step [4176/10336], Loss: 2.2141\n",
      "Epoch [1/5], Step [4178/10336], Loss: 0.4735\n",
      "Epoch [1/5], Step [4180/10336], Loss: 0.2273\n",
      "Epoch [1/5], Step [4182/10336], Loss: 3.0344\n",
      "Epoch [1/5], Step [4184/10336], Loss: 2.3432\n",
      "Epoch [1/5], Step [4186/10336], Loss: 2.5374\n",
      "Epoch [1/5], Step [4188/10336], Loss: 1.5463\n",
      "Epoch [1/5], Step [4190/10336], Loss: 2.4802\n",
      "Epoch [1/5], Step [4192/10336], Loss: 1.7402\n",
      "Epoch [1/5], Step [4194/10336], Loss: 0.3714\n",
      "Epoch [1/5], Step [4196/10336], Loss: 1.3554\n",
      "Epoch [1/5], Step [4198/10336], Loss: 1.7900\n",
      "Epoch [1/5], Step [4200/10336], Loss: 0.3271\n",
      "Epoch [1/5], Step [4202/10336], Loss: 0.4983\n",
      "Epoch [1/5], Step [4204/10336], Loss: 1.4920\n",
      "Epoch [1/5], Step [4206/10336], Loss: 0.2377\n",
      "Epoch [1/5], Step [4208/10336], Loss: 2.0313\n",
      "Epoch [1/5], Step [4210/10336], Loss: 4.0396\n",
      "Epoch [1/5], Step [4212/10336], Loss: 2.2860\n",
      "Epoch [1/5], Step [4214/10336], Loss: 4.0488\n",
      "Epoch [1/5], Step [4216/10336], Loss: 2.2901\n",
      "Epoch [1/5], Step [4218/10336], Loss: 3.8556\n",
      "Epoch [1/5], Step [4220/10336], Loss: 0.9147\n",
      "Epoch [1/5], Step [4222/10336], Loss: 1.0445\n",
      "Epoch [1/5], Step [4224/10336], Loss: 1.8521\n",
      "Epoch [1/5], Step [4226/10336], Loss: 2.8564\n",
      "Epoch [1/5], Step [4228/10336], Loss: 1.6040\n",
      "Epoch [1/5], Step [4230/10336], Loss: 3.0862\n",
      "Epoch [1/5], Step [4232/10336], Loss: 2.4383\n",
      "Epoch [1/5], Step [4234/10336], Loss: 3.3515\n",
      "Epoch [1/5], Step [4236/10336], Loss: 1.7522\n",
      "Epoch [1/5], Step [4238/10336], Loss: 4.2799\n",
      "Epoch [1/5], Step [4240/10336], Loss: 1.3319\n",
      "Epoch [1/5], Step [4242/10336], Loss: 2.3933\n",
      "Epoch [1/5], Step [4244/10336], Loss: 0.3415\n",
      "Epoch [1/5], Step [4246/10336], Loss: 0.1687\n",
      "Epoch [1/5], Step [4248/10336], Loss: 0.8479\n",
      "Epoch [1/5], Step [4250/10336], Loss: 5.7253\n",
      "Epoch [1/5], Step [4252/10336], Loss: 0.1508\n",
      "Epoch [1/5], Step [4254/10336], Loss: 2.8631\n",
      "Epoch [1/5], Step [4256/10336], Loss: 0.7243\n",
      "Epoch [1/5], Step [4258/10336], Loss: 0.7560\n",
      "Epoch [1/5], Step [4260/10336], Loss: 1.0410\n",
      "Epoch [1/5], Step [4262/10336], Loss: 0.0923\n",
      "Epoch [1/5], Step [4264/10336], Loss: 1.3383\n",
      "Epoch [1/5], Step [4266/10336], Loss: 0.3233\n",
      "Epoch [1/5], Step [4268/10336], Loss: 0.9081\n",
      "Epoch [1/5], Step [4270/10336], Loss: 0.2490\n",
      "Epoch [1/5], Step [4272/10336], Loss: 1.1662\n",
      "Epoch [1/5], Step [4274/10336], Loss: 3.1938\n",
      "Epoch [1/5], Step [4276/10336], Loss: 1.6975\n",
      "Epoch [1/5], Step [4278/10336], Loss: 2.9049\n",
      "Epoch [1/5], Step [4280/10336], Loss: 1.0425\n",
      "Epoch [1/5], Step [4282/10336], Loss: 1.1169\n",
      "Epoch [1/5], Step [4284/10336], Loss: 0.3539\n",
      "Epoch [1/5], Step [4286/10336], Loss: 2.1525\n",
      "Epoch [1/5], Step [4288/10336], Loss: 2.2317\n",
      "Epoch [1/5], Step [4290/10336], Loss: 2.0862\n",
      "Epoch [1/5], Step [4292/10336], Loss: 5.5344\n",
      "Epoch [1/5], Step [4294/10336], Loss: 0.7318\n",
      "Epoch [1/5], Step [4296/10336], Loss: 2.5027\n",
      "Epoch [1/5], Step [4298/10336], Loss: 0.1639\n",
      "Epoch [1/5], Step [4300/10336], Loss: 1.4107\n",
      "Epoch [1/5], Step [4302/10336], Loss: 1.3150\n",
      "Epoch [1/5], Step [4304/10336], Loss: 1.3221\n",
      "Epoch [1/5], Step [4306/10336], Loss: 1.0895\n",
      "Epoch [1/5], Step [4308/10336], Loss: 0.2679\n",
      "Epoch [1/5], Step [4310/10336], Loss: 0.9711\n",
      "Epoch [1/5], Step [4312/10336], Loss: 1.4439\n",
      "Epoch [1/5], Step [4314/10336], Loss: 3.2471\n",
      "Epoch [1/5], Step [4316/10336], Loss: 0.4890\n",
      "Epoch [1/5], Step [4318/10336], Loss: 1.4166\n",
      "Epoch [1/5], Step [4320/10336], Loss: 0.4663\n",
      "Epoch [1/5], Step [4322/10336], Loss: 0.7011\n",
      "Epoch [1/5], Step [4324/10336], Loss: 2.0852\n",
      "Epoch [1/5], Step [4326/10336], Loss: 1.8206\n",
      "Epoch [1/5], Step [4328/10336], Loss: 0.3100\n",
      "Epoch [1/5], Step [4330/10336], Loss: 1.3286\n",
      "Epoch [1/5], Step [4332/10336], Loss: 1.7619\n",
      "Epoch [1/5], Step [4334/10336], Loss: 1.0577\n",
      "Epoch [1/5], Step [4336/10336], Loss: 3.6710\n",
      "Epoch [1/5], Step [4338/10336], Loss: 0.0807\n",
      "Epoch [1/5], Step [4340/10336], Loss: 2.6018\n",
      "Epoch [1/5], Step [4342/10336], Loss: 0.5085\n",
      "Epoch [1/5], Step [4344/10336], Loss: 2.3797\n",
      "Epoch [1/5], Step [4346/10336], Loss: 1.3420\n",
      "Epoch [1/5], Step [4348/10336], Loss: 1.6164\n",
      "Epoch [1/5], Step [4350/10336], Loss: 3.3279\n",
      "Epoch [1/5], Step [4352/10336], Loss: 2.2792\n",
      "Epoch [1/5], Step [4354/10336], Loss: 1.2546\n",
      "Epoch [1/5], Step [4356/10336], Loss: 1.1631\n",
      "Epoch [1/5], Step [4358/10336], Loss: 0.6987\n",
      "Epoch [1/5], Step [4360/10336], Loss: 2.9202\n",
      "Epoch [1/5], Step [4362/10336], Loss: 1.2578\n",
      "Epoch [1/5], Step [4364/10336], Loss: 1.0718\n",
      "Epoch [1/5], Step [4366/10336], Loss: 0.3774\n",
      "Epoch [1/5], Step [4368/10336], Loss: 0.5984\n",
      "Epoch [1/5], Step [4370/10336], Loss: 3.0554\n",
      "Epoch [1/5], Step [4372/10336], Loss: 1.5190\n",
      "Epoch [1/5], Step [4374/10336], Loss: 2.4984\n",
      "Epoch [1/5], Step [4376/10336], Loss: 0.1260\n",
      "Epoch [1/5], Step [4378/10336], Loss: 2.2330\n",
      "Epoch [1/5], Step [4380/10336], Loss: 0.2928\n",
      "Epoch [1/5], Step [4382/10336], Loss: 2.2336\n",
      "Epoch [1/5], Step [4384/10336], Loss: 0.2994\n",
      "Epoch [1/5], Step [4386/10336], Loss: 0.9223\n",
      "Epoch [1/5], Step [4388/10336], Loss: 1.3465\n",
      "Epoch [1/5], Step [4390/10336], Loss: 0.9105\n",
      "Epoch [1/5], Step [4392/10336], Loss: 3.6550\n",
      "Epoch [1/5], Step [4394/10336], Loss: 0.5025\n",
      "Epoch [1/5], Step [4396/10336], Loss: 1.7828\n",
      "Epoch [1/5], Step [4398/10336], Loss: 1.4167\n",
      "Epoch [1/5], Step [4400/10336], Loss: 2.2380\n",
      "Epoch [1/5], Step [4402/10336], Loss: 2.6429\n",
      "Epoch [1/5], Step [4404/10336], Loss: 4.2985\n",
      "Epoch [1/5], Step [4406/10336], Loss: 0.8088\n",
      "Epoch [1/5], Step [4408/10336], Loss: 0.4112\n",
      "Epoch [1/5], Step [4410/10336], Loss: 0.1880\n",
      "Epoch [1/5], Step [4412/10336], Loss: 2.2662\n",
      "Epoch [1/5], Step [4414/10336], Loss: 0.6117\n",
      "Epoch [1/5], Step [4416/10336], Loss: 2.4336\n",
      "Epoch [1/5], Step [4418/10336], Loss: 1.7479\n",
      "Epoch [1/5], Step [4420/10336], Loss: 4.5653\n",
      "Epoch [1/5], Step [4422/10336], Loss: 2.1004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4424/10336], Loss: 2.5572\n",
      "Epoch [1/5], Step [4426/10336], Loss: 1.3633\n",
      "Epoch [1/5], Step [4428/10336], Loss: 2.2353\n",
      "Epoch [1/5], Step [4430/10336], Loss: 1.7930\n",
      "Epoch [1/5], Step [4432/10336], Loss: 0.2665\n",
      "Epoch [1/5], Step [4434/10336], Loss: 2.2273\n",
      "Epoch [1/5], Step [4436/10336], Loss: 1.4403\n",
      "Epoch [1/5], Step [4438/10336], Loss: 2.6314\n",
      "Epoch [1/5], Step [4440/10336], Loss: 1.8998\n",
      "Epoch [1/5], Step [4442/10336], Loss: 1.2998\n",
      "Epoch [1/5], Step [4444/10336], Loss: 0.6807\n",
      "Epoch [1/5], Step [4446/10336], Loss: 1.4422\n",
      "Epoch [1/5], Step [4448/10336], Loss: 2.4495\n",
      "Epoch [1/5], Step [4450/10336], Loss: 0.8664\n",
      "Epoch [1/5], Step [4452/10336], Loss: 2.2150\n",
      "Epoch [1/5], Step [4454/10336], Loss: 0.2078\n",
      "Epoch [1/5], Step [4456/10336], Loss: 2.3653\n",
      "Epoch [1/5], Step [4458/10336], Loss: 1.8137\n",
      "Epoch [1/5], Step [4460/10336], Loss: 0.3496\n",
      "Epoch [1/5], Step [4462/10336], Loss: 3.2689\n",
      "Epoch [1/5], Step [4464/10336], Loss: 0.8622\n",
      "Epoch [1/5], Step [4466/10336], Loss: 0.5086\n",
      "Epoch [1/5], Step [4468/10336], Loss: 1.9462\n",
      "Epoch [1/5], Step [4470/10336], Loss: 1.9417\n",
      "Epoch [1/5], Step [4472/10336], Loss: 0.6797\n",
      "Epoch [1/5], Step [4474/10336], Loss: 4.3491\n",
      "Epoch [1/5], Step [4476/10336], Loss: 1.5868\n",
      "Epoch [1/5], Step [4478/10336], Loss: 0.9720\n",
      "Epoch [1/5], Step [4480/10336], Loss: 1.1950\n",
      "Epoch [1/5], Step [4482/10336], Loss: 0.4834\n",
      "Epoch [1/5], Step [4484/10336], Loss: 0.8561\n",
      "Epoch [1/5], Step [4486/10336], Loss: 0.7376\n",
      "Epoch [1/5], Step [4488/10336], Loss: 0.8898\n",
      "Epoch [1/5], Step [4490/10336], Loss: 2.1949\n",
      "Epoch [1/5], Step [4492/10336], Loss: 1.4702\n",
      "Epoch [1/5], Step [4494/10336], Loss: 1.1021\n",
      "Epoch [1/5], Step [4496/10336], Loss: 0.6352\n",
      "Epoch [1/5], Step [4498/10336], Loss: 1.7676\n",
      "Epoch [1/5], Step [4500/10336], Loss: 0.4805\n",
      "Epoch [1/5], Step [4502/10336], Loss: 0.3257\n",
      "Epoch [1/5], Step [4504/10336], Loss: 0.2511\n",
      "Epoch [1/5], Step [4506/10336], Loss: 0.6815\n",
      "Epoch [1/5], Step [4508/10336], Loss: 1.3489\n",
      "Epoch [1/5], Step [4510/10336], Loss: 0.5881\n",
      "Epoch [1/5], Step [4512/10336], Loss: 0.5136\n",
      "Epoch [1/5], Step [4514/10336], Loss: 1.1804\n",
      "Epoch [1/5], Step [4516/10336], Loss: 0.7846\n",
      "Epoch [1/5], Step [4518/10336], Loss: 1.5400\n",
      "Epoch [1/5], Step [4520/10336], Loss: 3.8154\n",
      "Epoch [1/5], Step [4522/10336], Loss: 6.7797\n",
      "Epoch [1/5], Step [4524/10336], Loss: 1.2572\n",
      "Epoch [1/5], Step [4526/10336], Loss: 1.1803\n",
      "Epoch [1/5], Step [4528/10336], Loss: 3.8470\n",
      "Epoch [1/5], Step [4530/10336], Loss: 2.4956\n",
      "Epoch [1/5], Step [4532/10336], Loss: 3.3979\n",
      "Epoch [1/5], Step [4534/10336], Loss: 1.1375\n",
      "Epoch [1/5], Step [4536/10336], Loss: 0.6226\n",
      "Epoch [1/5], Step [4538/10336], Loss: 3.9266\n",
      "Epoch [1/5], Step [4540/10336], Loss: 2.2482\n",
      "Epoch [1/5], Step [4542/10336], Loss: 1.1807\n",
      "Epoch [1/5], Step [4544/10336], Loss: 1.1786\n",
      "Epoch [1/5], Step [4546/10336], Loss: 6.1122\n",
      "Epoch [1/5], Step [4548/10336], Loss: 0.7430\n",
      "Epoch [1/5], Step [4550/10336], Loss: 0.9915\n",
      "Epoch [1/5], Step [4552/10336], Loss: 0.3414\n",
      "Epoch [1/5], Step [4554/10336], Loss: 1.3505\n",
      "Epoch [1/5], Step [4556/10336], Loss: 0.4252\n",
      "Epoch [1/5], Step [4558/10336], Loss: 2.0218\n",
      "Epoch [1/5], Step [4560/10336], Loss: 4.0857\n",
      "Epoch [1/5], Step [4562/10336], Loss: 1.0346\n",
      "Epoch [1/5], Step [4564/10336], Loss: 2.6123\n",
      "Epoch [1/5], Step [4566/10336], Loss: 1.9446\n",
      "Epoch [1/5], Step [4568/10336], Loss: 0.0634\n",
      "Epoch [1/5], Step [4570/10336], Loss: 2.4626\n",
      "Epoch [1/5], Step [4572/10336], Loss: 2.0627\n",
      "Epoch [1/5], Step [4574/10336], Loss: 1.5732\n",
      "Epoch [1/5], Step [4576/10336], Loss: 0.5703\n",
      "Epoch [1/5], Step [4578/10336], Loss: 0.6554\n",
      "Epoch [1/5], Step [4580/10336], Loss: 4.2333\n",
      "Epoch [1/5], Step [4582/10336], Loss: 1.3408\n",
      "Epoch [1/5], Step [4584/10336], Loss: 0.5043\n",
      "Epoch [1/5], Step [4586/10336], Loss: 1.7678\n",
      "Epoch [1/5], Step [4588/10336], Loss: 4.6259\n",
      "Epoch [1/5], Step [4590/10336], Loss: 0.7050\n",
      "Epoch [1/5], Step [4592/10336], Loss: 2.1819\n",
      "Epoch [1/5], Step [4594/10336], Loss: 0.5097\n",
      "Epoch [1/5], Step [4596/10336], Loss: 0.0545\n",
      "Epoch [1/5], Step [4598/10336], Loss: 0.7317\n",
      "Epoch [1/5], Step [4600/10336], Loss: 1.5614\n",
      "Epoch [1/5], Step [4602/10336], Loss: 1.5993\n",
      "Epoch [1/5], Step [4604/10336], Loss: 0.0537\n",
      "Epoch [1/5], Step [4606/10336], Loss: 1.2247\n",
      "Epoch [1/5], Step [4608/10336], Loss: 0.5311\n",
      "Epoch [1/5], Step [4610/10336], Loss: 1.6115\n",
      "Epoch [1/5], Step [4612/10336], Loss: 0.4001\n",
      "Epoch [1/5], Step [4614/10336], Loss: 2.4502\n",
      "Epoch [1/5], Step [4616/10336], Loss: 4.3470\n",
      "Epoch [1/5], Step [4618/10336], Loss: 0.9693\n",
      "Epoch [1/5], Step [4620/10336], Loss: 2.4741\n",
      "Epoch [1/5], Step [4622/10336], Loss: 1.2700\n",
      "Epoch [1/5], Step [4624/10336], Loss: 0.4029\n",
      "Epoch [1/5], Step [4626/10336], Loss: 2.2322\n",
      "Epoch [1/5], Step [4628/10336], Loss: 1.1823\n",
      "Epoch [1/5], Step [4630/10336], Loss: 1.6434\n",
      "Epoch [1/5], Step [4632/10336], Loss: 0.4318\n",
      "Epoch [1/5], Step [4634/10336], Loss: 0.5561\n",
      "Epoch [1/5], Step [4636/10336], Loss: 4.0355\n",
      "Epoch [1/5], Step [4638/10336], Loss: 0.1113\n",
      "Epoch [1/5], Step [4640/10336], Loss: 2.6530\n",
      "Epoch [1/5], Step [4642/10336], Loss: 1.5556\n",
      "Epoch [1/5], Step [4644/10336], Loss: 2.6225\n",
      "Epoch [1/5], Step [4646/10336], Loss: 0.4321\n",
      "Epoch [1/5], Step [4648/10336], Loss: 1.7473\n",
      "Epoch [1/5], Step [4650/10336], Loss: 0.6613\n",
      "Epoch [1/5], Step [4652/10336], Loss: 3.1585\n",
      "Epoch [1/5], Step [4654/10336], Loss: 4.9635\n",
      "Epoch [1/5], Step [4656/10336], Loss: 1.6219\n",
      "Epoch [1/5], Step [4658/10336], Loss: 1.9216\n",
      "Epoch [1/5], Step [4660/10336], Loss: 0.2599\n",
      "Epoch [1/5], Step [4662/10336], Loss: 1.0865\n",
      "Epoch [1/5], Step [4664/10336], Loss: 1.0007\n",
      "Epoch [1/5], Step [4666/10336], Loss: 3.8295\n",
      "Epoch [1/5], Step [4668/10336], Loss: 1.3645\n",
      "Epoch [1/5], Step [4670/10336], Loss: 3.5295\n",
      "Epoch [1/5], Step [4672/10336], Loss: 1.3925\n",
      "Epoch [1/5], Step [4674/10336], Loss: 1.6776\n",
      "Epoch [1/5], Step [4676/10336], Loss: 2.0706\n",
      "Epoch [1/5], Step [4678/10336], Loss: 1.6357\n",
      "Epoch [1/5], Step [4680/10336], Loss: 0.7355\n",
      "Epoch [1/5], Step [4682/10336], Loss: 2.9639\n",
      "Epoch [1/5], Step [4684/10336], Loss: 0.5515\n",
      "Epoch [1/5], Step [4686/10336], Loss: 0.3471\n",
      "Epoch [1/5], Step [4688/10336], Loss: 1.8695\n",
      "Epoch [1/5], Step [4690/10336], Loss: 0.3634\n",
      "Epoch [1/5], Step [4692/10336], Loss: 2.1362\n",
      "Epoch [1/5], Step [4694/10336], Loss: 0.2600\n",
      "Epoch [1/5], Step [4696/10336], Loss: 0.3710\n",
      "Epoch [1/5], Step [4698/10336], Loss: 3.9106\n",
      "Epoch [1/5], Step [4700/10336], Loss: 1.5778\n",
      "Epoch [1/5], Step [4702/10336], Loss: 0.2499\n",
      "Epoch [1/5], Step [4704/10336], Loss: 0.1842\n",
      "Epoch [1/5], Step [4706/10336], Loss: 0.1942\n",
      "Epoch [1/5], Step [4708/10336], Loss: 0.6251\n",
      "Epoch [1/5], Step [4710/10336], Loss: 2.6397\n",
      "Epoch [1/5], Step [4712/10336], Loss: 0.7741\n",
      "Epoch [1/5], Step [4714/10336], Loss: 0.5641\n",
      "Epoch [1/5], Step [4716/10336], Loss: 2.3352\n",
      "Epoch [1/5], Step [4718/10336], Loss: 1.5197\n",
      "Epoch [1/5], Step [4720/10336], Loss: 3.0186\n",
      "Epoch [1/5], Step [4722/10336], Loss: 2.1164\n",
      "Epoch [1/5], Step [4724/10336], Loss: 1.6490\n",
      "Epoch [1/5], Step [4726/10336], Loss: 1.4751\n",
      "Epoch [1/5], Step [4728/10336], Loss: 0.9630\n",
      "Epoch [1/5], Step [4730/10336], Loss: 1.8035\n",
      "Epoch [1/5], Step [4732/10336], Loss: 3.1771\n",
      "Epoch [1/5], Step [4734/10336], Loss: 2.3352\n",
      "Epoch [1/5], Step [4736/10336], Loss: 4.4318\n",
      "Epoch [1/5], Step [4738/10336], Loss: 3.5607\n",
      "Epoch [1/5], Step [4740/10336], Loss: 0.2764\n",
      "Epoch [1/5], Step [4742/10336], Loss: 2.7160\n",
      "Epoch [1/5], Step [4744/10336], Loss: 0.5918\n",
      "Epoch [1/5], Step [4746/10336], Loss: 1.9609\n",
      "Epoch [1/5], Step [4748/10336], Loss: 1.1188\n",
      "Epoch [1/5], Step [4750/10336], Loss: 0.4669\n",
      "Epoch [1/5], Step [4752/10336], Loss: 3.0953\n",
      "Epoch [1/5], Step [4754/10336], Loss: 0.7974\n",
      "Epoch [1/5], Step [4756/10336], Loss: 0.8464\n",
      "Epoch [1/5], Step [4758/10336], Loss: 0.5170\n",
      "Epoch [1/5], Step [4760/10336], Loss: 0.6503\n",
      "Epoch [1/5], Step [4762/10336], Loss: 0.5718\n",
      "Epoch [1/5], Step [4764/10336], Loss: 1.0760\n",
      "Epoch [1/5], Step [4766/10336], Loss: 4.2574\n",
      "Epoch [1/5], Step [4768/10336], Loss: 2.1757\n",
      "Epoch [1/5], Step [4770/10336], Loss: 0.2604\n",
      "Epoch [1/5], Step [4772/10336], Loss: 1.6587\n",
      "Epoch [1/5], Step [4774/10336], Loss: 1.9989\n",
      "Epoch [1/5], Step [4776/10336], Loss: 2.4329\n",
      "Epoch [1/5], Step [4778/10336], Loss: 2.1283\n",
      "Epoch [1/5], Step [4780/10336], Loss: 2.2859\n",
      "Epoch [1/5], Step [4782/10336], Loss: 1.1025\n",
      "Epoch [1/5], Step [4784/10336], Loss: 0.5649\n",
      "Epoch [1/5], Step [4786/10336], Loss: 2.5424\n",
      "Epoch [1/5], Step [4788/10336], Loss: 1.2346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [4790/10336], Loss: 0.9896\n",
      "Epoch [1/5], Step [4792/10336], Loss: 1.4195\n",
      "Epoch [1/5], Step [4794/10336], Loss: 0.5888\n",
      "Epoch [1/5], Step [4796/10336], Loss: 2.1740\n",
      "Epoch [1/5], Step [4798/10336], Loss: 0.0564\n",
      "Epoch [1/5], Step [4800/10336], Loss: 2.7431\n",
      "Epoch [1/5], Step [4802/10336], Loss: 1.0733\n",
      "Epoch [1/5], Step [4804/10336], Loss: 1.8384\n",
      "Epoch [1/5], Step [4806/10336], Loss: 1.8988\n",
      "Epoch [1/5], Step [4808/10336], Loss: 0.3298\n",
      "Epoch [1/5], Step [4810/10336], Loss: 1.6534\n",
      "Epoch [1/5], Step [4812/10336], Loss: 0.4045\n",
      "Epoch [1/5], Step [4814/10336], Loss: 2.1229\n",
      "Epoch [1/5], Step [4816/10336], Loss: 0.6847\n",
      "Epoch [1/5], Step [4818/10336], Loss: 0.9091\n",
      "Epoch [1/5], Step [4820/10336], Loss: 0.3700\n",
      "Epoch [1/5], Step [4822/10336], Loss: 1.1090\n",
      "Epoch [1/5], Step [4824/10336], Loss: 0.3034\n",
      "Epoch [1/5], Step [4826/10336], Loss: 0.8808\n",
      "Epoch [1/5], Step [4828/10336], Loss: 2.4649\n",
      "Epoch [1/5], Step [4830/10336], Loss: 0.6006\n",
      "Epoch [1/5], Step [4832/10336], Loss: 0.0097\n",
      "Epoch [1/5], Step [4834/10336], Loss: 1.3970\n",
      "Epoch [1/5], Step [4836/10336], Loss: 2.4622\n",
      "Epoch [1/5], Step [4838/10336], Loss: 0.2047\n",
      "Epoch [1/5], Step [4840/10336], Loss: 1.1365\n",
      "Epoch [1/5], Step [4842/10336], Loss: 1.9625\n",
      "Epoch [1/5], Step [4844/10336], Loss: 2.4342\n",
      "Epoch [1/5], Step [4846/10336], Loss: 1.3900\n",
      "Epoch [1/5], Step [4848/10336], Loss: 0.6924\n",
      "Epoch [1/5], Step [4850/10336], Loss: 1.2062\n",
      "Epoch [1/5], Step [4852/10336], Loss: 0.7756\n",
      "Epoch [1/5], Step [4854/10336], Loss: 3.3854\n",
      "Epoch [1/5], Step [4856/10336], Loss: 1.5264\n",
      "Epoch [1/5], Step [4858/10336], Loss: 1.4105\n",
      "Epoch [1/5], Step [4860/10336], Loss: 1.4068\n",
      "Epoch [1/5], Step [4862/10336], Loss: 0.8030\n",
      "Epoch [1/5], Step [4864/10336], Loss: 2.1962\n",
      "Epoch [1/5], Step [4866/10336], Loss: 0.9932\n",
      "Epoch [1/5], Step [4868/10336], Loss: 0.2058\n",
      "Epoch [1/5], Step [4870/10336], Loss: 3.5235\n",
      "Epoch [1/5], Step [4872/10336], Loss: 0.1156\n",
      "Epoch [1/5], Step [4874/10336], Loss: 5.0935\n",
      "Epoch [1/5], Step [4876/10336], Loss: 2.0947\n",
      "Epoch [1/5], Step [4878/10336], Loss: 1.7585\n",
      "Epoch [1/5], Step [4880/10336], Loss: 1.0339\n",
      "Epoch [1/5], Step [4882/10336], Loss: 1.1065\n",
      "Epoch [1/5], Step [4884/10336], Loss: 0.2232\n",
      "Epoch [1/5], Step [4886/10336], Loss: 1.7477\n",
      "Epoch [1/5], Step [4888/10336], Loss: 0.8282\n",
      "Epoch [1/5], Step [4890/10336], Loss: 4.7668\n",
      "Epoch [1/5], Step [4892/10336], Loss: 0.0781\n",
      "Epoch [1/5], Step [4894/10336], Loss: 0.9613\n",
      "Epoch [1/5], Step [4896/10336], Loss: 0.7436\n",
      "Epoch [1/5], Step [4898/10336], Loss: 1.8179\n",
      "Epoch [1/5], Step [4900/10336], Loss: 0.7165\n",
      "Epoch [1/5], Step [4902/10336], Loss: 0.9361\n",
      "Epoch [1/5], Step [4904/10336], Loss: 1.5567\n",
      "Epoch [1/5], Step [4906/10336], Loss: 1.3955\n",
      "Epoch [1/5], Step [4908/10336], Loss: 5.0587\n",
      "Epoch [1/5], Step [4910/10336], Loss: 0.1591\n",
      "Epoch [1/5], Step [4912/10336], Loss: 0.5336\n",
      "Epoch [1/5], Step [4914/10336], Loss: 2.0679\n",
      "Epoch [1/5], Step [4916/10336], Loss: 0.3547\n",
      "Epoch [1/5], Step [4918/10336], Loss: 1.4092\n",
      "Epoch [1/5], Step [4920/10336], Loss: 0.0140\n",
      "Epoch [1/5], Step [4922/10336], Loss: 1.8354\n",
      "Epoch [1/5], Step [4924/10336], Loss: 2.4382\n",
      "Epoch [1/5], Step [4926/10336], Loss: 2.9245\n",
      "Epoch [1/5], Step [4928/10336], Loss: 3.0095\n",
      "Epoch [1/5], Step [4930/10336], Loss: 0.0360\n",
      "Epoch [1/5], Step [4932/10336], Loss: 3.6972\n",
      "Epoch [1/5], Step [4934/10336], Loss: 1.5174\n",
      "Epoch [1/5], Step [4936/10336], Loss: 1.6269\n",
      "Epoch [1/5], Step [4938/10336], Loss: 1.9815\n",
      "Epoch [1/5], Step [4940/10336], Loss: 1.9781\n",
      "Epoch [1/5], Step [4942/10336], Loss: 1.5073\n",
      "Epoch [1/5], Step [4944/10336], Loss: 0.0768\n",
      "Epoch [1/5], Step [4946/10336], Loss: 0.7458\n",
      "Epoch [1/5], Step [4948/10336], Loss: 0.9227\n",
      "Epoch [1/5], Step [4950/10336], Loss: 1.5596\n",
      "Epoch [1/5], Step [4952/10336], Loss: 1.5286\n",
      "Epoch [1/5], Step [4954/10336], Loss: 1.0062\n",
      "Epoch [1/5], Step [4956/10336], Loss: 1.9804\n",
      "Epoch [1/5], Step [4958/10336], Loss: 1.5134\n",
      "Epoch [1/5], Step [4960/10336], Loss: 1.1245\n",
      "Epoch [1/5], Step [4962/10336], Loss: 1.5751\n",
      "Epoch [1/5], Step [4964/10336], Loss: 2.4787\n",
      "Epoch [1/5], Step [4966/10336], Loss: 1.4965\n",
      "Epoch [1/5], Step [4968/10336], Loss: 2.0450\n",
      "Epoch [1/5], Step [4970/10336], Loss: 0.6341\n",
      "Epoch [1/5], Step [4972/10336], Loss: 2.1918\n",
      "Epoch [1/5], Step [4974/10336], Loss: 1.7560\n",
      "Epoch [1/5], Step [4976/10336], Loss: 0.1586\n",
      "Epoch [1/5], Step [4978/10336], Loss: 1.3221\n",
      "Epoch [1/5], Step [4980/10336], Loss: 1.7604\n",
      "Epoch [1/5], Step [4982/10336], Loss: 0.4272\n",
      "Epoch [1/5], Step [4984/10336], Loss: 0.8539\n",
      "Epoch [1/5], Step [4986/10336], Loss: 1.5790\n",
      "Epoch [1/5], Step [4988/10336], Loss: 0.0640\n",
      "Epoch [1/5], Step [4990/10336], Loss: 0.2658\n",
      "Epoch [1/5], Step [4992/10336], Loss: 2.1524\n",
      "Epoch [1/5], Step [4994/10336], Loss: 1.1565\n",
      "Epoch [1/5], Step [4996/10336], Loss: 0.8231\n",
      "Epoch [1/5], Step [4998/10336], Loss: 0.9040\n",
      "Epoch [1/5], Step [5000/10336], Loss: 0.1882\n",
      "Epoch [1/5], Step [5002/10336], Loss: 0.0107\n",
      "Epoch [1/5], Step [5004/10336], Loss: 1.0151\n",
      "Epoch [1/5], Step [5006/10336], Loss: 0.0092\n",
      "Epoch [1/5], Step [5008/10336], Loss: 1.4865\n",
      "Epoch [1/5], Step [5010/10336], Loss: 0.3492\n",
      "Epoch [1/5], Step [5012/10336], Loss: 0.7280\n",
      "Epoch [1/5], Step [5014/10336], Loss: 2.8661\n",
      "Epoch [1/5], Step [5016/10336], Loss: 1.2308\n",
      "Epoch [1/5], Step [5018/10336], Loss: 0.4238\n",
      "Epoch [1/5], Step [5020/10336], Loss: 0.2190\n",
      "Epoch [1/5], Step [5022/10336], Loss: 2.7244\n",
      "Epoch [1/5], Step [5024/10336], Loss: 5.2580\n",
      "Epoch [1/5], Step [5026/10336], Loss: 0.1646\n",
      "Epoch [1/5], Step [5028/10336], Loss: 0.9595\n",
      "Epoch [1/5], Step [5030/10336], Loss: 1.2556\n",
      "Epoch [1/5], Step [5032/10336], Loss: 2.2370\n",
      "Epoch [1/5], Step [5034/10336], Loss: 0.4087\n",
      "Epoch [1/5], Step [5036/10336], Loss: 0.9233\n",
      "Epoch [1/5], Step [5038/10336], Loss: 1.4030\n",
      "Epoch [1/5], Step [5040/10336], Loss: 2.7527\n",
      "Epoch [1/5], Step [5042/10336], Loss: 1.4952\n",
      "Epoch [1/5], Step [5044/10336], Loss: 0.1037\n",
      "Epoch [1/5], Step [5046/10336], Loss: 1.7837\n",
      "Epoch [1/5], Step [5048/10336], Loss: 2.2440\n",
      "Epoch [1/5], Step [5050/10336], Loss: 1.3109\n",
      "Epoch [1/5], Step [5052/10336], Loss: 1.9009\n",
      "Epoch [1/5], Step [5054/10336], Loss: 0.0051\n",
      "Epoch [1/5], Step [5056/10336], Loss: 3.3565\n",
      "Epoch [1/5], Step [5058/10336], Loss: 3.1322\n",
      "Epoch [1/5], Step [5060/10336], Loss: 3.8417\n",
      "Epoch [1/5], Step [5062/10336], Loss: 0.1937\n",
      "Epoch [1/5], Step [5064/10336], Loss: 0.3098\n",
      "Epoch [1/5], Step [5066/10336], Loss: 1.9738\n",
      "Epoch [1/5], Step [5068/10336], Loss: 3.1533\n",
      "Epoch [1/5], Step [5070/10336], Loss: 0.0400\n",
      "Epoch [1/5], Step [5072/10336], Loss: 3.6891\n",
      "Epoch [1/5], Step [5074/10336], Loss: 2.4426\n",
      "Epoch [1/5], Step [5076/10336], Loss: 0.6282\n",
      "Epoch [1/5], Step [5078/10336], Loss: 1.7360\n",
      "Epoch [1/5], Step [5080/10336], Loss: 2.4562\n",
      "Epoch [1/5], Step [5082/10336], Loss: 3.0428\n",
      "Epoch [1/5], Step [5084/10336], Loss: 1.3594\n",
      "Epoch [1/5], Step [5086/10336], Loss: 3.3279\n",
      "Epoch [1/5], Step [5088/10336], Loss: 0.1732\n",
      "Epoch [1/5], Step [5090/10336], Loss: 0.2386\n",
      "Epoch [1/5], Step [5092/10336], Loss: 1.6138\n",
      "Epoch [1/5], Step [5094/10336], Loss: 0.2607\n",
      "Epoch [1/5], Step [5096/10336], Loss: 1.0565\n",
      "Epoch [1/5], Step [5098/10336], Loss: 1.0645\n",
      "Epoch [1/5], Step [5100/10336], Loss: 2.1582\n",
      "Epoch [1/5], Step [5102/10336], Loss: 3.6401\n",
      "Epoch [1/5], Step [5104/10336], Loss: 1.1128\n",
      "Epoch [1/5], Step [5106/10336], Loss: 0.1625\n",
      "Epoch [1/5], Step [5108/10336], Loss: 3.2969\n",
      "Epoch [1/5], Step [5110/10336], Loss: 0.1481\n",
      "Epoch [1/5], Step [5112/10336], Loss: 0.1787\n",
      "Epoch [1/5], Step [5114/10336], Loss: 0.7716\n",
      "Epoch [1/5], Step [5116/10336], Loss: 1.0410\n",
      "Epoch [1/5], Step [5118/10336], Loss: 1.1073\n",
      "Epoch [1/5], Step [5120/10336], Loss: 0.3803\n",
      "Epoch [1/5], Step [5122/10336], Loss: 3.3610\n",
      "Epoch [1/5], Step [5124/10336], Loss: 4.7923\n",
      "Epoch [1/5], Step [5126/10336], Loss: 0.2827\n",
      "Epoch [1/5], Step [5128/10336], Loss: 1.4526\n",
      "Epoch [1/5], Step [5130/10336], Loss: 1.5443\n",
      "Epoch [1/5], Step [5132/10336], Loss: 1.5812\n",
      "Epoch [1/5], Step [5134/10336], Loss: 0.6333\n",
      "Epoch [1/5], Step [5136/10336], Loss: 0.5399\n",
      "Epoch [1/5], Step [5138/10336], Loss: 0.9378\n",
      "Epoch [1/5], Step [5140/10336], Loss: 1.5213\n",
      "Epoch [1/5], Step [5142/10336], Loss: 1.4128\n",
      "Epoch [1/5], Step [5144/10336], Loss: 0.7879\n",
      "Epoch [1/5], Step [5146/10336], Loss: 0.5777\n",
      "Epoch [1/5], Step [5148/10336], Loss: 1.4362\n",
      "Epoch [1/5], Step [5150/10336], Loss: 1.3559\n",
      "Epoch [1/5], Step [5152/10336], Loss: 1.8203\n",
      "Epoch [1/5], Step [5154/10336], Loss: 0.1229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5156/10336], Loss: 3.0829\n",
      "Epoch [1/5], Step [5158/10336], Loss: 2.0957\n",
      "Epoch [1/5], Step [5160/10336], Loss: 1.9121\n",
      "Epoch [1/5], Step [5162/10336], Loss: 0.4006\n",
      "Epoch [1/5], Step [5164/10336], Loss: 0.3520\n",
      "Epoch [1/5], Step [5166/10336], Loss: 1.3366\n",
      "Epoch [1/5], Step [5168/10336], Loss: 1.0109\n",
      "Epoch [1/5], Step [5170/10336], Loss: 0.9356\n",
      "Epoch [1/5], Step [5172/10336], Loss: 0.6974\n",
      "Epoch [1/5], Step [5174/10336], Loss: 0.1271\n",
      "Epoch [1/5], Step [5176/10336], Loss: 1.4570\n",
      "Epoch [1/5], Step [5178/10336], Loss: 0.6804\n",
      "Epoch [1/5], Step [5180/10336], Loss: 0.1376\n",
      "Epoch [1/5], Step [5182/10336], Loss: 1.6239\n",
      "Epoch [1/5], Step [5184/10336], Loss: 1.8899\n",
      "Epoch [1/5], Step [5186/10336], Loss: 0.0500\n",
      "Epoch [1/5], Step [5188/10336], Loss: 0.0654\n",
      "Epoch [1/5], Step [5190/10336], Loss: 1.2681\n",
      "Epoch [1/5], Step [5192/10336], Loss: 0.8730\n",
      "Epoch [1/5], Step [5194/10336], Loss: 0.6946\n",
      "Epoch [1/5], Step [5196/10336], Loss: 2.3606\n",
      "Epoch [1/5], Step [5198/10336], Loss: 0.8707\n",
      "Epoch [1/5], Step [5200/10336], Loss: 0.6756\n",
      "Epoch [1/5], Step [5202/10336], Loss: 0.8378\n",
      "Epoch [1/5], Step [5204/10336], Loss: 0.9804\n",
      "Epoch [1/5], Step [5206/10336], Loss: 0.2428\n",
      "Epoch [1/5], Step [5208/10336], Loss: 0.0789\n",
      "Epoch [1/5], Step [5210/10336], Loss: 0.6081\n",
      "Epoch [1/5], Step [5212/10336], Loss: 0.0557\n",
      "Epoch [1/5], Step [5214/10336], Loss: 1.3664\n",
      "Epoch [1/5], Step [5216/10336], Loss: 2.6281\n",
      "Epoch [1/5], Step [5218/10336], Loss: 0.4000\n",
      "Epoch [1/5], Step [5220/10336], Loss: 4.2250\n",
      "Epoch [1/5], Step [5222/10336], Loss: 0.2498\n",
      "Epoch [1/5], Step [5224/10336], Loss: 6.7489\n",
      "Epoch [1/5], Step [5226/10336], Loss: 1.9684\n",
      "Epoch [1/5], Step [5228/10336], Loss: 0.9009\n",
      "Epoch [1/5], Step [5230/10336], Loss: 2.3506\n",
      "Epoch [1/5], Step [5232/10336], Loss: 0.4836\n",
      "Epoch [1/5], Step [5234/10336], Loss: 0.3836\n",
      "Epoch [1/5], Step [5236/10336], Loss: 1.9275\n",
      "Epoch [1/5], Step [5238/10336], Loss: 0.4965\n",
      "Epoch [1/5], Step [5240/10336], Loss: 4.6799\n",
      "Epoch [1/5], Step [5242/10336], Loss: 1.6661\n",
      "Epoch [1/5], Step [5244/10336], Loss: 1.2140\n",
      "Epoch [1/5], Step [5246/10336], Loss: 4.3119\n",
      "Epoch [1/5], Step [5248/10336], Loss: 1.5100\n",
      "Epoch [1/5], Step [5250/10336], Loss: 0.0849\n",
      "Epoch [1/5], Step [5252/10336], Loss: 0.3478\n",
      "Epoch [1/5], Step [5254/10336], Loss: 0.2933\n",
      "Epoch [1/5], Step [5256/10336], Loss: 3.0527\n",
      "Epoch [1/5], Step [5258/10336], Loss: 1.0863\n",
      "Epoch [1/5], Step [5260/10336], Loss: 1.9333\n",
      "Epoch [1/5], Step [5262/10336], Loss: 0.0261\n",
      "Epoch [1/5], Step [5264/10336], Loss: 0.6515\n",
      "Epoch [1/5], Step [5266/10336], Loss: 1.6995\n",
      "Epoch [1/5], Step [5268/10336], Loss: 0.2743\n",
      "Epoch [1/5], Step [5270/10336], Loss: 0.3503\n",
      "Epoch [1/5], Step [5272/10336], Loss: 2.4755\n",
      "Epoch [1/5], Step [5274/10336], Loss: 1.4804\n",
      "Epoch [1/5], Step [5276/10336], Loss: 5.1591\n",
      "Epoch [1/5], Step [5278/10336], Loss: 5.0612\n",
      "Epoch [1/5], Step [5280/10336], Loss: 0.5570\n",
      "Epoch [1/5], Step [5282/10336], Loss: 0.6594\n",
      "Epoch [1/5], Step [5284/10336], Loss: 0.8567\n",
      "Epoch [1/5], Step [5286/10336], Loss: 0.5139\n",
      "Epoch [1/5], Step [5288/10336], Loss: 2.2551\n",
      "Epoch [1/5], Step [5290/10336], Loss: 1.1635\n",
      "Epoch [1/5], Step [5292/10336], Loss: 1.8319\n",
      "Epoch [1/5], Step [5294/10336], Loss: 1.3310\n",
      "Epoch [1/5], Step [5296/10336], Loss: 0.8907\n",
      "Epoch [1/5], Step [5298/10336], Loss: 0.3922\n",
      "Epoch [1/5], Step [5300/10336], Loss: 0.4987\n",
      "Epoch [1/5], Step [5302/10336], Loss: 0.3842\n",
      "Epoch [1/5], Step [5304/10336], Loss: 2.5656\n",
      "Epoch [1/5], Step [5306/10336], Loss: 0.3835\n",
      "Epoch [1/5], Step [5308/10336], Loss: 2.7625\n",
      "Epoch [1/5], Step [5310/10336], Loss: 0.4595\n",
      "Epoch [1/5], Step [5312/10336], Loss: 0.3484\n",
      "Epoch [1/5], Step [5314/10336], Loss: 2.2899\n",
      "Epoch [1/5], Step [5316/10336], Loss: 0.8355\n",
      "Epoch [1/5], Step [5318/10336], Loss: 3.4586\n",
      "Epoch [1/5], Step [5320/10336], Loss: 0.8406\n",
      "Epoch [1/5], Step [5322/10336], Loss: 2.6685\n",
      "Epoch [1/5], Step [5324/10336], Loss: 0.8257\n",
      "Epoch [1/5], Step [5326/10336], Loss: 1.2098\n",
      "Epoch [1/5], Step [5328/10336], Loss: 1.4381\n",
      "Epoch [1/5], Step [5330/10336], Loss: 2.7523\n",
      "Epoch [1/5], Step [5332/10336], Loss: 3.9050\n",
      "Epoch [1/5], Step [5334/10336], Loss: 0.6021\n",
      "Epoch [1/5], Step [5336/10336], Loss: 2.2186\n",
      "Epoch [1/5], Step [5338/10336], Loss: 2.5969\n",
      "Epoch [1/5], Step [5340/10336], Loss: 1.5806\n",
      "Epoch [1/5], Step [5342/10336], Loss: 1.4336\n",
      "Epoch [1/5], Step [5344/10336], Loss: 1.7440\n",
      "Epoch [1/5], Step [5346/10336], Loss: 1.7608\n",
      "Epoch [1/5], Step [5348/10336], Loss: 0.1241\n",
      "Epoch [1/5], Step [5350/10336], Loss: 0.7755\n",
      "Epoch [1/5], Step [5352/10336], Loss: 1.2861\n",
      "Epoch [1/5], Step [5354/10336], Loss: 0.0921\n",
      "Epoch [1/5], Step [5356/10336], Loss: 1.2450\n",
      "Epoch [1/5], Step [5358/10336], Loss: 2.1234\n",
      "Epoch [1/5], Step [5360/10336], Loss: 4.1655\n",
      "Epoch [1/5], Step [5362/10336], Loss: 2.3476\n",
      "Epoch [1/5], Step [5364/10336], Loss: 4.1828\n",
      "Epoch [1/5], Step [5366/10336], Loss: 1.8386\n",
      "Epoch [1/5], Step [5368/10336], Loss: 2.5342\n",
      "Epoch [1/5], Step [5370/10336], Loss: 1.5400\n",
      "Epoch [1/5], Step [5372/10336], Loss: 1.9251\n",
      "Epoch [1/5], Step [5374/10336], Loss: 0.2877\n",
      "Epoch [1/5], Step [5376/10336], Loss: 2.8148\n",
      "Epoch [1/5], Step [5378/10336], Loss: 0.2834\n",
      "Epoch [1/5], Step [5380/10336], Loss: 0.9322\n",
      "Epoch [1/5], Step [5382/10336], Loss: 0.3156\n",
      "Epoch [1/5], Step [5384/10336], Loss: 0.8182\n",
      "Epoch [1/5], Step [5386/10336], Loss: 2.5032\n",
      "Epoch [1/5], Step [5388/10336], Loss: 0.3636\n",
      "Epoch [1/5], Step [5390/10336], Loss: 2.2483\n",
      "Epoch [1/5], Step [5392/10336], Loss: 1.6490\n",
      "Epoch [1/5], Step [5394/10336], Loss: 0.6285\n",
      "Epoch [1/5], Step [5396/10336], Loss: 0.4284\n",
      "Epoch [1/5], Step [5398/10336], Loss: 2.0707\n",
      "Epoch [1/5], Step [5400/10336], Loss: 0.6260\n",
      "Epoch [1/5], Step [5402/10336], Loss: 2.8061\n",
      "Epoch [1/5], Step [5404/10336], Loss: 1.5479\n",
      "Epoch [1/5], Step [5406/10336], Loss: 2.3486\n",
      "Epoch [1/5], Step [5408/10336], Loss: 3.5293\n",
      "Epoch [1/5], Step [5410/10336], Loss: 0.6632\n",
      "Epoch [1/5], Step [5412/10336], Loss: 1.3401\n",
      "Epoch [1/5], Step [5414/10336], Loss: 0.9931\n",
      "Epoch [1/5], Step [5416/10336], Loss: 2.9977\n",
      "Epoch [1/5], Step [5418/10336], Loss: 0.6756\n",
      "Epoch [1/5], Step [5420/10336], Loss: 1.4358\n",
      "Epoch [1/5], Step [5422/10336], Loss: 1.1434\n",
      "Epoch [1/5], Step [5424/10336], Loss: 1.9369\n",
      "Epoch [1/5], Step [5426/10336], Loss: 0.7234\n",
      "Epoch [1/5], Step [5428/10336], Loss: 0.4922\n",
      "Epoch [1/5], Step [5430/10336], Loss: 1.0460\n",
      "Epoch [1/5], Step [5432/10336], Loss: 0.3137\n",
      "Epoch [1/5], Step [5434/10336], Loss: 0.9584\n",
      "Epoch [1/5], Step [5436/10336], Loss: 0.7068\n",
      "Epoch [1/5], Step [5438/10336], Loss: 2.4115\n",
      "Epoch [1/5], Step [5440/10336], Loss: 1.4270\n",
      "Epoch [1/5], Step [5442/10336], Loss: 1.1898\n",
      "Epoch [1/5], Step [5444/10336], Loss: 0.7616\n",
      "Epoch [1/5], Step [5446/10336], Loss: 0.5955\n",
      "Epoch [1/5], Step [5448/10336], Loss: 1.0916\n",
      "Epoch [1/5], Step [5450/10336], Loss: 1.2913\n",
      "Epoch [1/5], Step [5452/10336], Loss: 2.5320\n",
      "Epoch [1/5], Step [5454/10336], Loss: 2.4268\n",
      "Epoch [1/5], Step [5456/10336], Loss: 0.1975\n",
      "Epoch [1/5], Step [5458/10336], Loss: 1.6932\n",
      "Epoch [1/5], Step [5460/10336], Loss: 2.0762\n",
      "Epoch [1/5], Step [5462/10336], Loss: 0.8058\n",
      "Epoch [1/5], Step [5464/10336], Loss: 2.0626\n",
      "Epoch [1/5], Step [5466/10336], Loss: 1.4762\n",
      "Epoch [1/5], Step [5468/10336], Loss: 1.7322\n",
      "Epoch [1/5], Step [5470/10336], Loss: 2.0997\n",
      "Epoch [1/5], Step [5472/10336], Loss: 2.2409\n",
      "Epoch [1/5], Step [5474/10336], Loss: 2.6224\n",
      "Epoch [1/5], Step [5476/10336], Loss: 1.8568\n",
      "Epoch [1/5], Step [5478/10336], Loss: 2.1502\n",
      "Epoch [1/5], Step [5480/10336], Loss: 0.8194\n",
      "Epoch [1/5], Step [5482/10336], Loss: 1.5065\n",
      "Epoch [1/5], Step [5484/10336], Loss: 0.7261\n",
      "Epoch [1/5], Step [5486/10336], Loss: 0.8291\n",
      "Epoch [1/5], Step [5488/10336], Loss: 0.0556\n",
      "Epoch [1/5], Step [5490/10336], Loss: 2.5657\n",
      "Epoch [1/5], Step [5492/10336], Loss: 1.4880\n",
      "Epoch [1/5], Step [5494/10336], Loss: 0.8719\n",
      "Epoch [1/5], Step [5496/10336], Loss: 0.4922\n",
      "Epoch [1/5], Step [5498/10336], Loss: 1.5018\n",
      "Epoch [1/5], Step [5500/10336], Loss: 3.0612\n",
      "Epoch [1/5], Step [5502/10336], Loss: 3.3069\n",
      "Epoch [1/5], Step [5504/10336], Loss: 3.0591\n",
      "Epoch [1/5], Step [5506/10336], Loss: 2.4302\n",
      "Epoch [1/5], Step [5508/10336], Loss: 1.4190\n",
      "Epoch [1/5], Step [5510/10336], Loss: 0.3929\n",
      "Epoch [1/5], Step [5512/10336], Loss: 0.4796\n",
      "Epoch [1/5], Step [5514/10336], Loss: 1.2494\n",
      "Epoch [1/5], Step [5516/10336], Loss: 1.5675\n",
      "Epoch [1/5], Step [5518/10336], Loss: 0.7072\n",
      "Epoch [1/5], Step [5520/10336], Loss: 3.1344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5522/10336], Loss: 0.4388\n",
      "Epoch [1/5], Step [5524/10336], Loss: 1.5114\n",
      "Epoch [1/5], Step [5526/10336], Loss: 1.3932\n",
      "Epoch [1/5], Step [5528/10336], Loss: 1.3299\n",
      "Epoch [1/5], Step [5530/10336], Loss: 1.6518\n",
      "Epoch [1/5], Step [5532/10336], Loss: 1.1027\n",
      "Epoch [1/5], Step [5534/10336], Loss: 1.9470\n",
      "Epoch [1/5], Step [5536/10336], Loss: 3.0897\n",
      "Epoch [1/5], Step [5538/10336], Loss: 2.7349\n",
      "Epoch [1/5], Step [5540/10336], Loss: 0.2861\n",
      "Epoch [1/5], Step [5542/10336], Loss: 0.0211\n",
      "Epoch [1/5], Step [5544/10336], Loss: 0.0124\n",
      "Epoch [1/5], Step [5546/10336], Loss: 2.3999\n",
      "Epoch [1/5], Step [5548/10336], Loss: 0.9378\n",
      "Epoch [1/5], Step [5550/10336], Loss: 0.6163\n",
      "Epoch [1/5], Step [5552/10336], Loss: 0.1925\n",
      "Epoch [1/5], Step [5554/10336], Loss: 0.8862\n",
      "Epoch [1/5], Step [5556/10336], Loss: 1.6173\n",
      "Epoch [1/5], Step [5558/10336], Loss: 0.8584\n",
      "Epoch [1/5], Step [5560/10336], Loss: 2.9032\n",
      "Epoch [1/5], Step [5562/10336], Loss: 1.5764\n",
      "Epoch [1/5], Step [5564/10336], Loss: 0.4603\n",
      "Epoch [1/5], Step [5566/10336], Loss: 0.6398\n",
      "Epoch [1/5], Step [5568/10336], Loss: 1.1473\n",
      "Epoch [1/5], Step [5570/10336], Loss: 0.3250\n",
      "Epoch [1/5], Step [5572/10336], Loss: 0.0225\n",
      "Epoch [1/5], Step [5574/10336], Loss: 0.1146\n",
      "Epoch [1/5], Step [5576/10336], Loss: 0.4177\n",
      "Epoch [1/5], Step [5578/10336], Loss: 2.6532\n",
      "Epoch [1/5], Step [5580/10336], Loss: 1.7868\n",
      "Epoch [1/5], Step [5582/10336], Loss: 0.9574\n",
      "Epoch [1/5], Step [5584/10336], Loss: 2.3972\n",
      "Epoch [1/5], Step [5586/10336], Loss: 0.4683\n",
      "Epoch [1/5], Step [5588/10336], Loss: 1.6707\n",
      "Epoch [1/5], Step [5590/10336], Loss: 2.1857\n",
      "Epoch [1/5], Step [5592/10336], Loss: 1.7242\n",
      "Epoch [1/5], Step [5594/10336], Loss: 0.6824\n",
      "Epoch [1/5], Step [5596/10336], Loss: 2.0948\n",
      "Epoch [1/5], Step [5598/10336], Loss: 2.4915\n",
      "Epoch [1/5], Step [5600/10336], Loss: 0.3241\n",
      "Epoch [1/5], Step [5602/10336], Loss: 1.1635\n",
      "Epoch [1/5], Step [5604/10336], Loss: 1.2266\n",
      "Epoch [1/5], Step [5606/10336], Loss: 1.2961\n",
      "Epoch [1/5], Step [5608/10336], Loss: 0.6409\n",
      "Epoch [1/5], Step [5610/10336], Loss: 2.0635\n",
      "Epoch [1/5], Step [5612/10336], Loss: 0.0546\n",
      "Epoch [1/5], Step [5614/10336], Loss: 3.8020\n",
      "Epoch [1/5], Step [5616/10336], Loss: 0.4661\n",
      "Epoch [1/5], Step [5618/10336], Loss: 3.0066\n",
      "Epoch [1/5], Step [5620/10336], Loss: 3.1025\n",
      "Epoch [1/5], Step [5622/10336], Loss: 2.2300\n",
      "Epoch [1/5], Step [5624/10336], Loss: 0.6243\n",
      "Epoch [1/5], Step [5626/10336], Loss: 2.4206\n",
      "Epoch [1/5], Step [5628/10336], Loss: 1.5243\n",
      "Epoch [1/5], Step [5630/10336], Loss: 1.7381\n",
      "Epoch [1/5], Step [5632/10336], Loss: 0.2911\n",
      "Epoch [1/5], Step [5634/10336], Loss: 0.0519\n",
      "Epoch [1/5], Step [5636/10336], Loss: 1.3088\n",
      "Epoch [1/5], Step [5638/10336], Loss: 0.4367\n",
      "Epoch [1/5], Step [5640/10336], Loss: 1.9992\n",
      "Epoch [1/5], Step [5642/10336], Loss: 2.4014\n",
      "Epoch [1/5], Step [5644/10336], Loss: 0.2543\n",
      "Epoch [1/5], Step [5646/10336], Loss: 0.5165\n",
      "Epoch [1/5], Step [5648/10336], Loss: 5.2659\n",
      "Epoch [1/5], Step [5650/10336], Loss: 0.0519\n",
      "Epoch [1/5], Step [5652/10336], Loss: 1.7648\n",
      "Epoch [1/5], Step [5654/10336], Loss: 3.5976\n",
      "Epoch [1/5], Step [5656/10336], Loss: 1.9533\n",
      "Epoch [1/5], Step [5658/10336], Loss: 2.6267\n",
      "Epoch [1/5], Step [5660/10336], Loss: 1.4517\n",
      "Epoch [1/5], Step [5662/10336], Loss: 1.3708\n",
      "Epoch [1/5], Step [5664/10336], Loss: 0.3502\n",
      "Epoch [1/5], Step [5666/10336], Loss: 0.3000\n",
      "Epoch [1/5], Step [5668/10336], Loss: 1.1206\n",
      "Epoch [1/5], Step [5670/10336], Loss: 2.2734\n",
      "Epoch [1/5], Step [5672/10336], Loss: 1.7005\n",
      "Epoch [1/5], Step [5674/10336], Loss: 0.7815\n",
      "Epoch [1/5], Step [5676/10336], Loss: 0.5556\n",
      "Epoch [1/5], Step [5678/10336], Loss: 5.3499\n",
      "Epoch [1/5], Step [5680/10336], Loss: 1.7759\n",
      "Epoch [1/5], Step [5682/10336], Loss: 1.0649\n",
      "Epoch [1/5], Step [5684/10336], Loss: 4.4460\n",
      "Epoch [1/5], Step [5686/10336], Loss: 2.8854\n",
      "Epoch [1/5], Step [5688/10336], Loss: 0.8442\n",
      "Epoch [1/5], Step [5690/10336], Loss: 1.2655\n",
      "Epoch [1/5], Step [5692/10336], Loss: 0.8419\n",
      "Epoch [1/5], Step [5694/10336], Loss: 1.6504\n",
      "Epoch [1/5], Step [5696/10336], Loss: 1.4471\n",
      "Epoch [1/5], Step [5698/10336], Loss: 1.3634\n",
      "Epoch [1/5], Step [5700/10336], Loss: 0.1430\n",
      "Epoch [1/5], Step [5702/10336], Loss: 2.0464\n",
      "Epoch [1/5], Step [5704/10336], Loss: 2.4682\n",
      "Epoch [1/5], Step [5706/10336], Loss: 2.3086\n",
      "Epoch [1/5], Step [5708/10336], Loss: 1.0160\n",
      "Epoch [1/5], Step [5710/10336], Loss: 0.6918\n",
      "Epoch [1/5], Step [5712/10336], Loss: 1.7060\n",
      "Epoch [1/5], Step [5714/10336], Loss: 2.4696\n",
      "Epoch [1/5], Step [5716/10336], Loss: 0.5096\n",
      "Epoch [1/5], Step [5718/10336], Loss: 2.2242\n",
      "Epoch [1/5], Step [5720/10336], Loss: 0.9846\n",
      "Epoch [1/5], Step [5722/10336], Loss: 0.5228\n",
      "Epoch [1/5], Step [5724/10336], Loss: 0.2639\n",
      "Epoch [1/5], Step [5726/10336], Loss: 1.5304\n",
      "Epoch [1/5], Step [5728/10336], Loss: 1.6822\n",
      "Epoch [1/5], Step [5730/10336], Loss: 1.1395\n",
      "Epoch [1/5], Step [5732/10336], Loss: 0.1996\n",
      "Epoch [1/5], Step [5734/10336], Loss: 1.2155\n",
      "Epoch [1/5], Step [5736/10336], Loss: 0.6068\n",
      "Epoch [1/5], Step [5738/10336], Loss: 1.9753\n",
      "Epoch [1/5], Step [5740/10336], Loss: 0.1118\n",
      "Epoch [1/5], Step [5742/10336], Loss: 1.3895\n",
      "Epoch [1/5], Step [5744/10336], Loss: 0.0680\n",
      "Epoch [1/5], Step [5746/10336], Loss: 1.4621\n",
      "Epoch [1/5], Step [5748/10336], Loss: 0.6184\n",
      "Epoch [1/5], Step [5750/10336], Loss: 0.1941\n",
      "Epoch [1/5], Step [5752/10336], Loss: 1.3729\n",
      "Epoch [1/5], Step [5754/10336], Loss: 0.8523\n",
      "Epoch [1/5], Step [5756/10336], Loss: 1.8294\n",
      "Epoch [1/5], Step [5758/10336], Loss: 1.2277\n",
      "Epoch [1/5], Step [5760/10336], Loss: 1.2205\n",
      "Epoch [1/5], Step [5762/10336], Loss: 1.4707\n",
      "Epoch [1/5], Step [5764/10336], Loss: 2.1032\n",
      "Epoch [1/5], Step [5766/10336], Loss: 1.8396\n",
      "Epoch [1/5], Step [5768/10336], Loss: 1.2774\n",
      "Epoch [1/5], Step [5770/10336], Loss: 0.6653\n",
      "Epoch [1/5], Step [5772/10336], Loss: 1.9355\n",
      "Epoch [1/5], Step [5774/10336], Loss: 0.9753\n",
      "Epoch [1/5], Step [5776/10336], Loss: 0.6545\n",
      "Epoch [1/5], Step [5778/10336], Loss: 1.8832\n",
      "Epoch [1/5], Step [5780/10336], Loss: 0.2344\n",
      "Epoch [1/5], Step [5782/10336], Loss: 0.5131\n",
      "Epoch [1/5], Step [5784/10336], Loss: 2.6853\n",
      "Epoch [1/5], Step [5786/10336], Loss: 0.5916\n",
      "Epoch [1/5], Step [5788/10336], Loss: 2.6238\n",
      "Epoch [1/5], Step [5790/10336], Loss: 3.3978\n",
      "Epoch [1/5], Step [5792/10336], Loss: 2.0355\n",
      "Epoch [1/5], Step [5794/10336], Loss: 1.0169\n",
      "Epoch [1/5], Step [5796/10336], Loss: 0.7514\n",
      "Epoch [1/5], Step [5798/10336], Loss: 0.8471\n",
      "Epoch [1/5], Step [5800/10336], Loss: 2.1701\n",
      "Epoch [1/5], Step [5802/10336], Loss: 1.4508\n",
      "Epoch [1/5], Step [5804/10336], Loss: 0.7732\n",
      "Epoch [1/5], Step [5806/10336], Loss: 2.8649\n",
      "Epoch [1/5], Step [5808/10336], Loss: 1.2271\n",
      "Epoch [1/5], Step [5810/10336], Loss: 1.4418\n",
      "Epoch [1/5], Step [5812/10336], Loss: 1.1052\n",
      "Epoch [1/5], Step [5814/10336], Loss: 1.4414\n",
      "Epoch [1/5], Step [5816/10336], Loss: 0.5463\n",
      "Epoch [1/5], Step [5818/10336], Loss: 0.7384\n",
      "Epoch [1/5], Step [5820/10336], Loss: 0.1844\n",
      "Epoch [1/5], Step [5822/10336], Loss: 0.2902\n",
      "Epoch [1/5], Step [5824/10336], Loss: 0.3689\n",
      "Epoch [1/5], Step [5826/10336], Loss: 0.1417\n",
      "Epoch [1/5], Step [5828/10336], Loss: 0.2484\n",
      "Epoch [1/5], Step [5830/10336], Loss: 1.1878\n",
      "Epoch [1/5], Step [5832/10336], Loss: 1.9266\n",
      "Epoch [1/5], Step [5834/10336], Loss: 2.2128\n",
      "Epoch [1/5], Step [5836/10336], Loss: 3.0886\n",
      "Epoch [1/5], Step [5838/10336], Loss: 2.6616\n",
      "Epoch [1/5], Step [5840/10336], Loss: 0.2015\n",
      "Epoch [1/5], Step [5842/10336], Loss: 2.1437\n",
      "Epoch [1/5], Step [5844/10336], Loss: 1.6394\n",
      "Epoch [1/5], Step [5846/10336], Loss: 3.4537\n",
      "Epoch [1/5], Step [5848/10336], Loss: 1.9916\n",
      "Epoch [1/5], Step [5850/10336], Loss: 2.0416\n",
      "Epoch [1/5], Step [5852/10336], Loss: 0.0949\n",
      "Epoch [1/5], Step [5854/10336], Loss: 1.0699\n",
      "Epoch [1/5], Step [5856/10336], Loss: 1.4842\n",
      "Epoch [1/5], Step [5858/10336], Loss: 1.0689\n",
      "Epoch [1/5], Step [5860/10336], Loss: 1.4844\n",
      "Epoch [1/5], Step [5862/10336], Loss: 3.3511\n",
      "Epoch [1/5], Step [5864/10336], Loss: 1.9387\n",
      "Epoch [1/5], Step [5866/10336], Loss: 1.0310\n",
      "Epoch [1/5], Step [5868/10336], Loss: 0.3007\n",
      "Epoch [1/5], Step [5870/10336], Loss: 3.2946\n",
      "Epoch [1/5], Step [5872/10336], Loss: 0.6690\n",
      "Epoch [1/5], Step [5874/10336], Loss: 0.4928\n",
      "Epoch [1/5], Step [5876/10336], Loss: 0.1858\n",
      "Epoch [1/5], Step [5878/10336], Loss: 0.1844\n",
      "Epoch [1/5], Step [5880/10336], Loss: 1.2731\n",
      "Epoch [1/5], Step [5882/10336], Loss: 1.5084\n",
      "Epoch [1/5], Step [5884/10336], Loss: 3.7569\n",
      "Epoch [1/5], Step [5886/10336], Loss: 0.9009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5888/10336], Loss: 1.0306\n",
      "Epoch [1/5], Step [5890/10336], Loss: 0.6279\n",
      "Epoch [1/5], Step [5892/10336], Loss: 0.6315\n",
      "Epoch [1/5], Step [5894/10336], Loss: 1.1597\n",
      "Epoch [1/5], Step [5896/10336], Loss: 0.5627\n",
      "Epoch [1/5], Step [5898/10336], Loss: 0.3459\n",
      "Epoch [1/5], Step [5900/10336], Loss: 1.3688\n",
      "Epoch [1/5], Step [5902/10336], Loss: 1.6295\n",
      "Epoch [1/5], Step [5904/10336], Loss: 1.7425\n",
      "Epoch [1/5], Step [5906/10336], Loss: 0.0472\n",
      "Epoch [1/5], Step [5908/10336], Loss: 0.4013\n",
      "Epoch [1/5], Step [5910/10336], Loss: 1.3867\n",
      "Epoch [1/5], Step [5912/10336], Loss: 0.0947\n",
      "Epoch [1/5], Step [5914/10336], Loss: 1.7191\n",
      "Epoch [1/5], Step [5916/10336], Loss: 4.3514\n",
      "Epoch [1/5], Step [5918/10336], Loss: 2.6787\n",
      "Epoch [1/5], Step [5920/10336], Loss: 0.2773\n",
      "Epoch [1/5], Step [5922/10336], Loss: 0.5999\n",
      "Epoch [1/5], Step [5924/10336], Loss: 1.5907\n",
      "Epoch [1/5], Step [5926/10336], Loss: 0.1140\n",
      "Epoch [1/5], Step [5928/10336], Loss: 2.7988\n",
      "Epoch [1/5], Step [5930/10336], Loss: 2.3659\n",
      "Epoch [1/5], Step [5932/10336], Loss: 1.2643\n",
      "Epoch [1/5], Step [5934/10336], Loss: 0.8263\n",
      "Epoch [1/5], Step [5936/10336], Loss: 0.2607\n",
      "Epoch [1/5], Step [5938/10336], Loss: 0.0045\n",
      "Epoch [1/5], Step [5940/10336], Loss: 0.1864\n",
      "Epoch [1/5], Step [5942/10336], Loss: 0.3204\n",
      "Epoch [1/5], Step [5944/10336], Loss: 1.8837\n",
      "Epoch [1/5], Step [5946/10336], Loss: 0.2102\n",
      "Epoch [1/5], Step [5948/10336], Loss: 0.1737\n",
      "Epoch [1/5], Step [5950/10336], Loss: 3.0104\n",
      "Epoch [1/5], Step [5952/10336], Loss: 0.9556\n",
      "Epoch [1/5], Step [5954/10336], Loss: 1.3945\n",
      "Epoch [1/5], Step [5956/10336], Loss: 0.7982\n",
      "Epoch [1/5], Step [5958/10336], Loss: 0.0871\n",
      "Epoch [1/5], Step [5960/10336], Loss: 2.7878\n",
      "Epoch [1/5], Step [5962/10336], Loss: 0.6551\n",
      "Epoch [1/5], Step [5964/10336], Loss: 0.4806\n",
      "Epoch [1/5], Step [5966/10336], Loss: 3.6400\n",
      "Epoch [1/5], Step [5968/10336], Loss: 1.2546\n",
      "Epoch [1/5], Step [5970/10336], Loss: 0.1257\n",
      "Epoch [1/5], Step [5972/10336], Loss: 0.0939\n",
      "Epoch [1/5], Step [5974/10336], Loss: 4.6082\n",
      "Epoch [1/5], Step [5976/10336], Loss: 0.1415\n",
      "Epoch [1/5], Step [5978/10336], Loss: 0.7663\n",
      "Epoch [1/5], Step [5980/10336], Loss: 0.7643\n",
      "Epoch [1/5], Step [5982/10336], Loss: 2.9360\n",
      "Epoch [1/5], Step [5984/10336], Loss: 0.7356\n",
      "Epoch [1/5], Step [5986/10336], Loss: 0.4800\n",
      "Epoch [1/5], Step [5988/10336], Loss: 1.1226\n",
      "Epoch [1/5], Step [5990/10336], Loss: 0.4703\n",
      "Epoch [1/5], Step [5992/10336], Loss: 0.6612\n",
      "Epoch [1/5], Step [5994/10336], Loss: 1.0156\n",
      "Epoch [1/5], Step [5996/10336], Loss: 0.9077\n",
      "Epoch [1/5], Step [5998/10336], Loss: 0.7776\n",
      "Epoch [1/5], Step [6000/10336], Loss: 1.6384\n",
      "Epoch [1/5], Step [6002/10336], Loss: 1.0285\n",
      "Epoch [1/5], Step [6004/10336], Loss: 0.4320\n",
      "Epoch [1/5], Step [6006/10336], Loss: 0.5935\n",
      "Epoch [1/5], Step [6008/10336], Loss: 0.6822\n",
      "Epoch [1/5], Step [6010/10336], Loss: 1.2267\n",
      "Epoch [1/5], Step [6012/10336], Loss: 1.4622\n",
      "Epoch [1/5], Step [6014/10336], Loss: 2.3912\n",
      "Epoch [1/5], Step [6016/10336], Loss: 0.6959\n",
      "Epoch [1/5], Step [6018/10336], Loss: 1.0943\n",
      "Epoch [1/5], Step [6020/10336], Loss: 3.2505\n",
      "Epoch [1/5], Step [6022/10336], Loss: 0.8151\n",
      "Epoch [1/5], Step [6024/10336], Loss: 2.1379\n",
      "Epoch [1/5], Step [6026/10336], Loss: 0.8508\n",
      "Epoch [1/5], Step [6028/10336], Loss: 0.2479\n",
      "Epoch [1/5], Step [6030/10336], Loss: 0.3382\n",
      "Epoch [1/5], Step [6032/10336], Loss: 0.4051\n",
      "Epoch [1/5], Step [6034/10336], Loss: 1.7368\n",
      "Epoch [1/5], Step [6036/10336], Loss: 3.1863\n",
      "Epoch [1/5], Step [6038/10336], Loss: 1.2005\n",
      "Epoch [1/5], Step [6040/10336], Loss: 1.7195\n",
      "Epoch [1/5], Step [6042/10336], Loss: 0.9703\n",
      "Epoch [1/5], Step [6044/10336], Loss: 1.6447\n",
      "Epoch [1/5], Step [6046/10336], Loss: 2.9305\n",
      "Epoch [1/5], Step [6048/10336], Loss: 1.0788\n",
      "Epoch [1/5], Step [6050/10336], Loss: 2.9768\n",
      "Epoch [1/5], Step [6052/10336], Loss: 0.3681\n",
      "Epoch [1/5], Step [6054/10336], Loss: 0.1850\n",
      "Epoch [1/5], Step [6056/10336], Loss: 0.3836\n",
      "Epoch [1/5], Step [6058/10336], Loss: 3.2439\n",
      "Epoch [1/5], Step [6060/10336], Loss: 1.3208\n",
      "Epoch [1/5], Step [6062/10336], Loss: 1.1596\n",
      "Epoch [1/5], Step [6064/10336], Loss: 3.6384\n",
      "Epoch [1/5], Step [6066/10336], Loss: 2.0029\n",
      "Epoch [1/5], Step [6068/10336], Loss: 1.4654\n",
      "Epoch [1/5], Step [6070/10336], Loss: 0.1717\n",
      "Epoch [1/5], Step [6072/10336], Loss: 0.0857\n",
      "Epoch [1/5], Step [6074/10336], Loss: 1.2811\n",
      "Epoch [1/5], Step [6076/10336], Loss: 1.4685\n",
      "Epoch [1/5], Step [6078/10336], Loss: 3.2047\n",
      "Epoch [1/5], Step [6080/10336], Loss: 0.3698\n",
      "Epoch [1/5], Step [6082/10336], Loss: 0.3277\n",
      "Epoch [1/5], Step [6084/10336], Loss: 1.2704\n",
      "Epoch [1/5], Step [6086/10336], Loss: 1.3643\n",
      "Epoch [1/5], Step [6088/10336], Loss: 0.3179\n",
      "Epoch [1/5], Step [6090/10336], Loss: 1.6162\n",
      "Epoch [1/5], Step [6092/10336], Loss: 1.6602\n",
      "Epoch [1/5], Step [6094/10336], Loss: 0.4121\n",
      "Epoch [1/5], Step [6096/10336], Loss: 2.5505\n",
      "Epoch [1/5], Step [6098/10336], Loss: 2.2242\n",
      "Epoch [1/5], Step [6100/10336], Loss: 3.1835\n",
      "Epoch [1/5], Step [6102/10336], Loss: 0.5261\n",
      "Epoch [1/5], Step [6104/10336], Loss: 1.2342\n",
      "Epoch [1/5], Step [6106/10336], Loss: 0.8849\n",
      "Epoch [1/5], Step [6108/10336], Loss: 1.6002\n",
      "Epoch [1/5], Step [6110/10336], Loss: 3.5994\n",
      "Epoch [1/5], Step [6112/10336], Loss: 0.9301\n",
      "Epoch [1/5], Step [6114/10336], Loss: 1.1190\n",
      "Epoch [1/5], Step [6116/10336], Loss: 1.3461\n",
      "Epoch [1/5], Step [6118/10336], Loss: 0.5438\n",
      "Epoch [1/5], Step [6120/10336], Loss: 1.0374\n",
      "Epoch [1/5], Step [6122/10336], Loss: 0.7972\n",
      "Epoch [1/5], Step [6124/10336], Loss: 1.2529\n",
      "Epoch [1/5], Step [6126/10336], Loss: 0.8482\n",
      "Epoch [1/5], Step [6128/10336], Loss: 1.9414\n",
      "Epoch [1/5], Step [6130/10336], Loss: 0.7234\n",
      "Epoch [1/5], Step [6132/10336], Loss: 0.3274\n",
      "Epoch [1/5], Step [6134/10336], Loss: 1.9635\n",
      "Epoch [1/5], Step [6136/10336], Loss: 0.6427\n",
      "Epoch [1/5], Step [6138/10336], Loss: 0.8617\n",
      "Epoch [1/5], Step [6140/10336], Loss: 3.5728\n",
      "Epoch [1/5], Step [6142/10336], Loss: 0.0501\n",
      "Epoch [1/5], Step [6144/10336], Loss: 4.6636\n",
      "Epoch [1/5], Step [6146/10336], Loss: 1.0155\n",
      "Epoch [1/5], Step [6148/10336], Loss: 2.0704\n",
      "Epoch [1/5], Step [6150/10336], Loss: 1.0925\n",
      "Epoch [1/5], Step [6152/10336], Loss: 0.1800\n",
      "Epoch [1/5], Step [6154/10336], Loss: 2.1275\n",
      "Epoch [1/5], Step [6156/10336], Loss: 1.4341\n",
      "Epoch [1/5], Step [6158/10336], Loss: 1.7099\n",
      "Epoch [1/5], Step [6160/10336], Loss: 0.0889\n",
      "Epoch [1/5], Step [6162/10336], Loss: 0.0424\n",
      "Epoch [1/5], Step [6164/10336], Loss: 0.5552\n",
      "Epoch [1/5], Step [6166/10336], Loss: 1.8467\n",
      "Epoch [1/5], Step [6168/10336], Loss: 1.2780\n",
      "Epoch [1/5], Step [6170/10336], Loss: 2.1347\n",
      "Epoch [1/5], Step [6172/10336], Loss: 2.2388\n",
      "Epoch [1/5], Step [6174/10336], Loss: 0.0506\n",
      "Epoch [1/5], Step [6176/10336], Loss: 0.8597\n",
      "Epoch [1/5], Step [6178/10336], Loss: 4.7191\n",
      "Epoch [1/5], Step [6180/10336], Loss: 2.9014\n",
      "Epoch [1/5], Step [6182/10336], Loss: 3.5381\n",
      "Epoch [1/5], Step [6184/10336], Loss: 5.3620\n",
      "Epoch [1/5], Step [6186/10336], Loss: 1.0891\n",
      "Epoch [1/5], Step [6188/10336], Loss: 0.2879\n",
      "Epoch [1/5], Step [6190/10336], Loss: 0.4800\n",
      "Epoch [1/5], Step [6192/10336], Loss: 2.2115\n",
      "Epoch [1/5], Step [6194/10336], Loss: 1.3964\n",
      "Epoch [1/5], Step [6196/10336], Loss: 2.1000\n",
      "Epoch [1/5], Step [6198/10336], Loss: 3.5228\n",
      "Epoch [1/5], Step [6200/10336], Loss: 2.0369\n",
      "Epoch [1/5], Step [6202/10336], Loss: 0.4210\n",
      "Epoch [1/5], Step [6204/10336], Loss: 1.4634\n",
      "Epoch [1/5], Step [6206/10336], Loss: 1.6398\n",
      "Epoch [1/5], Step [6208/10336], Loss: 2.4833\n",
      "Epoch [1/5], Step [6210/10336], Loss: 1.7106\n",
      "Epoch [1/5], Step [6212/10336], Loss: 1.7939\n",
      "Epoch [1/5], Step [6214/10336], Loss: 1.5065\n",
      "Epoch [1/5], Step [6216/10336], Loss: 2.9820\n",
      "Epoch [1/5], Step [6218/10336], Loss: 3.2017\n",
      "Epoch [1/5], Step [6220/10336], Loss: 2.0789\n",
      "Epoch [1/5], Step [6222/10336], Loss: 0.1683\n",
      "Epoch [1/5], Step [6224/10336], Loss: 0.2156\n",
      "Epoch [1/5], Step [6226/10336], Loss: 2.6478\n",
      "Epoch [1/5], Step [6228/10336], Loss: 4.5352\n",
      "Epoch [1/5], Step [6230/10336], Loss: 2.2657\n",
      "Epoch [1/5], Step [6232/10336], Loss: 2.8254\n",
      "Epoch [1/5], Step [6234/10336], Loss: 2.9127\n",
      "Epoch [1/5], Step [6236/10336], Loss: 1.1282\n",
      "Epoch [1/5], Step [6238/10336], Loss: 1.5126\n",
      "Epoch [1/5], Step [6240/10336], Loss: 0.7969\n",
      "Epoch [1/5], Step [6242/10336], Loss: 1.8698\n",
      "Epoch [1/5], Step [6244/10336], Loss: 1.7030\n",
      "Epoch [1/5], Step [6246/10336], Loss: 1.6017\n",
      "Epoch [1/5], Step [6248/10336], Loss: 0.7015\n",
      "Epoch [1/5], Step [6250/10336], Loss: 0.3298\n",
      "Epoch [1/5], Step [6252/10336], Loss: 3.3281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [6254/10336], Loss: 0.5620\n",
      "Epoch [1/5], Step [6256/10336], Loss: 0.6904\n",
      "Epoch [1/5], Step [6258/10336], Loss: 1.9141\n",
      "Epoch [1/5], Step [6260/10336], Loss: 1.0431\n",
      "Epoch [1/5], Step [6262/10336], Loss: 3.5070\n",
      "Epoch [1/5], Step [6264/10336], Loss: 0.0762\n",
      "Epoch [1/5], Step [6266/10336], Loss: 0.0159\n",
      "Epoch [1/5], Step [6268/10336], Loss: 0.5035\n",
      "Epoch [1/5], Step [6270/10336], Loss: 2.1425\n",
      "Epoch [1/5], Step [6272/10336], Loss: 2.3221\n",
      "Epoch [1/5], Step [6274/10336], Loss: 1.9617\n",
      "Epoch [1/5], Step [6276/10336], Loss: 2.8677\n",
      "Epoch [1/5], Step [6278/10336], Loss: 2.5402\n",
      "Epoch [1/5], Step [6280/10336], Loss: 1.2616\n",
      "Epoch [1/5], Step [6282/10336], Loss: 0.9187\n",
      "Epoch [1/5], Step [6284/10336], Loss: 0.3054\n",
      "Epoch [1/5], Step [6286/10336], Loss: 1.3047\n",
      "Epoch [1/5], Step [6288/10336], Loss: 2.4503\n",
      "Epoch [1/5], Step [6290/10336], Loss: 2.9311\n",
      "Epoch [1/5], Step [6292/10336], Loss: 2.4854\n",
      "Epoch [1/5], Step [6294/10336], Loss: 0.6250\n",
      "Epoch [1/5], Step [6296/10336], Loss: 2.4346\n",
      "Epoch [1/5], Step [6298/10336], Loss: 2.6642\n",
      "Epoch [1/5], Step [6300/10336], Loss: 0.5202\n",
      "Epoch [1/5], Step [6302/10336], Loss: 0.9626\n",
      "Epoch [1/5], Step [6304/10336], Loss: 0.1366\n",
      "Epoch [1/5], Step [6306/10336], Loss: 1.8001\n",
      "Epoch [1/5], Step [6308/10336], Loss: 0.4930\n",
      "Epoch [1/5], Step [6310/10336], Loss: 0.6297\n",
      "Epoch [1/5], Step [6312/10336], Loss: 1.3745\n",
      "Epoch [1/5], Step [6314/10336], Loss: 1.7872\n",
      "Epoch [1/5], Step [6316/10336], Loss: 0.1660\n",
      "Epoch [1/5], Step [6318/10336], Loss: 0.3797\n",
      "Epoch [1/5], Step [6320/10336], Loss: 0.1476\n",
      "Epoch [1/5], Step [6322/10336], Loss: 2.2248\n",
      "Epoch [1/5], Step [6324/10336], Loss: 2.6786\n",
      "Epoch [1/5], Step [6326/10336], Loss: 0.8120\n",
      "Epoch [1/5], Step [6328/10336], Loss: 0.6745\n",
      "Epoch [1/5], Step [6330/10336], Loss: 0.9412\n",
      "Epoch [1/5], Step [6332/10336], Loss: 1.0516\n",
      "Epoch [1/5], Step [6334/10336], Loss: 3.0761\n",
      "Epoch [1/5], Step [6336/10336], Loss: 1.0921\n",
      "Epoch [1/5], Step [6338/10336], Loss: 1.4346\n",
      "Epoch [1/5], Step [6340/10336], Loss: 3.0784\n",
      "Epoch [1/5], Step [6342/10336], Loss: 2.9700\n",
      "Epoch [1/5], Step [6344/10336], Loss: 2.6707\n",
      "Epoch [1/5], Step [6346/10336], Loss: 2.2997\n",
      "Epoch [1/5], Step [6348/10336], Loss: 0.8144\n",
      "Epoch [1/5], Step [6350/10336], Loss: 0.4505\n",
      "Epoch [1/5], Step [6352/10336], Loss: 1.3546\n",
      "Epoch [1/5], Step [6354/10336], Loss: 0.4877\n",
      "Epoch [1/5], Step [6356/10336], Loss: 0.2067\n",
      "Epoch [1/5], Step [6358/10336], Loss: 1.8087\n",
      "Epoch [1/5], Step [6360/10336], Loss: 0.2016\n",
      "Epoch [1/5], Step [6362/10336], Loss: 0.5429\n",
      "Epoch [1/5], Step [6364/10336], Loss: 1.1030\n",
      "Epoch [1/5], Step [6366/10336], Loss: 0.5905\n",
      "Epoch [1/5], Step [6368/10336], Loss: 0.1269\n",
      "Epoch [1/5], Step [6370/10336], Loss: 2.9843\n",
      "Epoch [1/5], Step [6372/10336], Loss: 0.6414\n",
      "Epoch [1/5], Step [6374/10336], Loss: 1.9963\n",
      "Epoch [1/5], Step [6376/10336], Loss: 0.4553\n",
      "Epoch [1/5], Step [6378/10336], Loss: 0.9519\n",
      "Epoch [1/5], Step [6380/10336], Loss: 3.7457\n",
      "Epoch [1/5], Step [6382/10336], Loss: 0.7402\n",
      "Epoch [1/5], Step [6384/10336], Loss: 2.8606\n",
      "Epoch [1/5], Step [6386/10336], Loss: 2.2386\n",
      "Epoch [1/5], Step [6388/10336], Loss: 1.8492\n",
      "Epoch [1/5], Step [6390/10336], Loss: 0.5686\n",
      "Epoch [1/5], Step [6392/10336], Loss: 0.8173\n",
      "Epoch [1/5], Step [6394/10336], Loss: 0.0248\n",
      "Epoch [1/5], Step [6396/10336], Loss: 2.0748\n",
      "Epoch [1/5], Step [6398/10336], Loss: 1.0801\n",
      "Epoch [1/5], Step [6400/10336], Loss: 0.2373\n",
      "Epoch [1/5], Step [6402/10336], Loss: 2.0392\n",
      "Epoch [1/5], Step [6404/10336], Loss: 0.5041\n",
      "Epoch [1/5], Step [6406/10336], Loss: 2.1018\n",
      "Epoch [1/5], Step [6408/10336], Loss: 0.7405\n",
      "Epoch [1/5], Step [6410/10336], Loss: 0.1758\n",
      "Epoch [1/5], Step [6412/10336], Loss: 0.0811\n",
      "Epoch [1/5], Step [6414/10336], Loss: 0.6555\n",
      "Epoch [1/5], Step [6416/10336], Loss: 3.1639\n",
      "Epoch [1/5], Step [6418/10336], Loss: 1.3416\n",
      "Epoch [1/5], Step [6420/10336], Loss: 1.0554\n",
      "Epoch [1/5], Step [6422/10336], Loss: 1.5846\n",
      "Epoch [1/5], Step [6424/10336], Loss: 2.9094\n",
      "Epoch [1/5], Step [6426/10336], Loss: 1.0084\n",
      "Epoch [1/5], Step [6428/10336], Loss: 2.0451\n",
      "Epoch [1/5], Step [6430/10336], Loss: 0.4392\n",
      "Epoch [1/5], Step [6432/10336], Loss: 0.9728\n",
      "Epoch [1/5], Step [6434/10336], Loss: 1.8141\n",
      "Epoch [1/5], Step [6436/10336], Loss: 3.0721\n",
      "Epoch [1/5], Step [6438/10336], Loss: 0.3210\n",
      "Epoch [1/5], Step [6440/10336], Loss: 1.3983\n",
      "Epoch [1/5], Step [6442/10336], Loss: 0.6481\n",
      "Epoch [1/5], Step [6444/10336], Loss: 2.5135\n",
      "Epoch [1/5], Step [6446/10336], Loss: 3.5070\n",
      "Epoch [1/5], Step [6448/10336], Loss: 0.3436\n",
      "Epoch [1/5], Step [6450/10336], Loss: 1.6747\n",
      "Epoch [1/5], Step [6452/10336], Loss: 0.3928\n",
      "Epoch [1/5], Step [6454/10336], Loss: 1.0512\n",
      "Epoch [1/5], Step [6456/10336], Loss: 0.2274\n",
      "Epoch [1/5], Step [6458/10336], Loss: 2.0248\n",
      "Epoch [1/5], Step [6460/10336], Loss: 0.2557\n",
      "Epoch [1/5], Step [6462/10336], Loss: 0.0058\n",
      "Epoch [1/5], Step [6464/10336], Loss: 1.1549\n",
      "Epoch [1/5], Step [6466/10336], Loss: 4.5256\n",
      "Epoch [1/5], Step [6468/10336], Loss: 3.6352\n",
      "Epoch [1/5], Step [6470/10336], Loss: 1.8424\n",
      "Epoch [1/5], Step [6472/10336], Loss: 0.5324\n",
      "Epoch [1/5], Step [6474/10336], Loss: 1.3871\n",
      "Epoch [1/5], Step [6476/10336], Loss: 0.0664\n",
      "Epoch [1/5], Step [6478/10336], Loss: 2.6904\n",
      "Epoch [1/5], Step [6480/10336], Loss: 1.4159\n",
      "Epoch [1/5], Step [6482/10336], Loss: 1.3417\n",
      "Epoch [1/5], Step [6484/10336], Loss: 1.2206\n",
      "Epoch [1/5], Step [6486/10336], Loss: 2.6950\n",
      "Epoch [1/5], Step [6488/10336], Loss: 1.1241\n",
      "Epoch [1/5], Step [6490/10336], Loss: 0.1094\n",
      "Epoch [1/5], Step [6492/10336], Loss: 2.9835\n",
      "Epoch [1/5], Step [6494/10336], Loss: 1.5195\n",
      "Epoch [1/5], Step [6496/10336], Loss: 1.3642\n",
      "Epoch [1/5], Step [6498/10336], Loss: 1.0721\n",
      "Epoch [1/5], Step [6500/10336], Loss: 0.1774\n",
      "Epoch [1/5], Step [6502/10336], Loss: 0.6534\n",
      "Epoch [1/5], Step [6504/10336], Loss: 1.9665\n",
      "Epoch [1/5], Step [6506/10336], Loss: 1.2774\n",
      "Epoch [1/5], Step [6508/10336], Loss: 0.0400\n",
      "Epoch [1/5], Step [6510/10336], Loss: 0.7492\n",
      "Epoch [1/5], Step [6512/10336], Loss: 0.8925\n",
      "Epoch [1/5], Step [6514/10336], Loss: 2.8147\n",
      "Epoch [1/5], Step [6516/10336], Loss: 0.1632\n",
      "Epoch [1/5], Step [6518/10336], Loss: 1.1980\n",
      "Epoch [1/5], Step [6520/10336], Loss: 1.0582\n",
      "Epoch [1/5], Step [6522/10336], Loss: 1.7146\n",
      "Epoch [1/5], Step [6524/10336], Loss: 1.2845\n",
      "Epoch [1/5], Step [6526/10336], Loss: 0.0733\n",
      "Epoch [1/5], Step [6528/10336], Loss: 0.9596\n",
      "Epoch [1/5], Step [6530/10336], Loss: 1.2990\n",
      "Epoch [1/5], Step [6532/10336], Loss: 5.5054\n",
      "Epoch [1/5], Step [6534/10336], Loss: 1.0254\n",
      "Epoch [1/5], Step [6536/10336], Loss: 2.9950\n",
      "Epoch [1/5], Step [6538/10336], Loss: 1.0570\n",
      "Epoch [1/5], Step [6540/10336], Loss: 2.8828\n",
      "Epoch [1/5], Step [6542/10336], Loss: 0.8779\n",
      "Epoch [1/5], Step [6544/10336], Loss: 0.9636\n",
      "Epoch [1/5], Step [6546/10336], Loss: 1.3991\n",
      "Epoch [1/5], Step [6548/10336], Loss: 0.2661\n",
      "Epoch [1/5], Step [6550/10336], Loss: 4.4960\n",
      "Epoch [1/5], Step [6552/10336], Loss: 1.5113\n",
      "Epoch [1/5], Step [6554/10336], Loss: 1.8181\n",
      "Epoch [1/5], Step [6556/10336], Loss: 1.3873\n",
      "Epoch [1/5], Step [6558/10336], Loss: 2.2714\n",
      "Epoch [1/5], Step [6560/10336], Loss: 2.0770\n",
      "Epoch [1/5], Step [6562/10336], Loss: 1.7953\n",
      "Epoch [1/5], Step [6564/10336], Loss: 0.7386\n",
      "Epoch [1/5], Step [6566/10336], Loss: 0.5309\n",
      "Epoch [1/5], Step [6568/10336], Loss: 0.2947\n",
      "Epoch [1/5], Step [6570/10336], Loss: 2.4721\n",
      "Epoch [1/5], Step [6572/10336], Loss: 1.1973\n",
      "Epoch [1/5], Step [6574/10336], Loss: 1.0186\n",
      "Epoch [1/5], Step [6576/10336], Loss: 1.1954\n",
      "Epoch [1/5], Step [6578/10336], Loss: 0.7884\n",
      "Epoch [1/5], Step [6580/10336], Loss: 0.2686\n",
      "Epoch [1/5], Step [6582/10336], Loss: 1.8453\n",
      "Epoch [1/5], Step [6584/10336], Loss: 1.7154\n",
      "Epoch [1/5], Step [6586/10336], Loss: 2.2101\n",
      "Epoch [1/5], Step [6588/10336], Loss: 0.1829\n",
      "Epoch [1/5], Step [6590/10336], Loss: 1.1781\n",
      "Epoch [1/5], Step [6592/10336], Loss: 0.6551\n",
      "Epoch [1/5], Step [6594/10336], Loss: 1.6481\n",
      "Epoch [1/5], Step [6596/10336], Loss: 1.2083\n",
      "Epoch [1/5], Step [6598/10336], Loss: 0.0391\n",
      "Epoch [1/5], Step [6600/10336], Loss: 0.6445\n",
      "Epoch [1/5], Step [6602/10336], Loss: 0.4003\n",
      "Epoch [1/5], Step [6604/10336], Loss: 2.6114\n",
      "Epoch [1/5], Step [6606/10336], Loss: 2.9604\n",
      "Epoch [1/5], Step [6608/10336], Loss: 0.8526\n",
      "Epoch [1/5], Step [6610/10336], Loss: 0.2606\n",
      "Epoch [1/5], Step [6612/10336], Loss: 0.7684\n",
      "Epoch [1/5], Step [6614/10336], Loss: 1.6517\n",
      "Epoch [1/5], Step [6616/10336], Loss: 1.3100\n",
      "Epoch [1/5], Step [6618/10336], Loss: 4.8411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [6620/10336], Loss: 1.9444\n",
      "Epoch [1/5], Step [6622/10336], Loss: 1.2122\n",
      "Epoch [1/5], Step [6624/10336], Loss: 1.2151\n",
      "Epoch [1/5], Step [6626/10336], Loss: 0.2444\n",
      "Epoch [1/5], Step [6628/10336], Loss: 0.3889\n",
      "Epoch [1/5], Step [6630/10336], Loss: 1.7922\n",
      "Epoch [1/5], Step [6632/10336], Loss: 0.3412\n",
      "Epoch [1/5], Step [6634/10336], Loss: 1.6965\n",
      "Epoch [1/5], Step [6636/10336], Loss: 1.7702\n",
      "Epoch [1/5], Step [6638/10336], Loss: 0.7328\n",
      "Epoch [1/5], Step [6640/10336], Loss: 0.3027\n",
      "Epoch [1/5], Step [6642/10336], Loss: 0.3992\n",
      "Epoch [1/5], Step [6644/10336], Loss: 2.5193\n",
      "Epoch [1/5], Step [6646/10336], Loss: 3.1883\n",
      "Epoch [1/5], Step [6648/10336], Loss: 1.6934\n",
      "Epoch [1/5], Step [6650/10336], Loss: 1.5313\n",
      "Epoch [1/5], Step [6652/10336], Loss: 0.7915\n",
      "Epoch [1/5], Step [6654/10336], Loss: 2.0120\n",
      "Epoch [1/5], Step [6656/10336], Loss: 0.3044\n",
      "Epoch [1/5], Step [6658/10336], Loss: 1.5208\n",
      "Epoch [1/5], Step [6660/10336], Loss: 0.0500\n",
      "Epoch [1/5], Step [6662/10336], Loss: 0.0392\n",
      "Epoch [1/5], Step [6664/10336], Loss: 1.4319\n",
      "Epoch [1/5], Step [6666/10336], Loss: 2.0278\n",
      "Epoch [1/5], Step [6668/10336], Loss: 1.6507\n",
      "Epoch [1/5], Step [6670/10336], Loss: 1.3887\n",
      "Epoch [1/5], Step [6672/10336], Loss: 0.6864\n",
      "Epoch [1/5], Step [6674/10336], Loss: 0.7418\n",
      "Epoch [1/5], Step [6676/10336], Loss: 4.9610\n",
      "Epoch [1/5], Step [6678/10336], Loss: 0.5891\n",
      "Epoch [1/5], Step [6680/10336], Loss: 3.8465\n",
      "Epoch [1/5], Step [6682/10336], Loss: 5.2054\n",
      "Epoch [1/5], Step [6684/10336], Loss: 1.9753\n",
      "Epoch [1/5], Step [6686/10336], Loss: 1.7643\n",
      "Epoch [1/5], Step [6688/10336], Loss: 0.1259\n",
      "Epoch [1/5], Step [6690/10336], Loss: 1.9002\n",
      "Epoch [1/5], Step [6692/10336], Loss: 2.6913\n",
      "Epoch [1/5], Step [6694/10336], Loss: 0.3881\n",
      "Epoch [1/5], Step [6696/10336], Loss: 1.2200\n",
      "Epoch [1/5], Step [6698/10336], Loss: 0.7527\n",
      "Epoch [1/5], Step [6700/10336], Loss: 0.3436\n",
      "Epoch [1/5], Step [6702/10336], Loss: 0.1992\n",
      "Epoch [1/5], Step [6704/10336], Loss: 3.5639\n",
      "Epoch [1/5], Step [6706/10336], Loss: 1.7688\n",
      "Epoch [1/5], Step [6708/10336], Loss: 0.9801\n",
      "Epoch [1/5], Step [6710/10336], Loss: 1.3668\n",
      "Epoch [1/5], Step [6712/10336], Loss: 0.2989\n",
      "Epoch [1/5], Step [6714/10336], Loss: 2.1170\n",
      "Epoch [1/5], Step [6716/10336], Loss: 0.4132\n",
      "Epoch [1/5], Step [6718/10336], Loss: 1.1380\n",
      "Epoch [1/5], Step [6720/10336], Loss: 1.9611\n",
      "Epoch [1/5], Step [6722/10336], Loss: 0.7623\n",
      "Epoch [1/5], Step [6724/10336], Loss: 1.2550\n",
      "Epoch [1/5], Step [6726/10336], Loss: 0.0260\n",
      "Epoch [1/5], Step [6728/10336], Loss: 2.1229\n",
      "Epoch [1/5], Step [6730/10336], Loss: 0.9784\n",
      "Epoch [1/5], Step [6732/10336], Loss: 1.3849\n",
      "Epoch [1/5], Step [6734/10336], Loss: 0.7639\n",
      "Epoch [1/5], Step [6736/10336], Loss: 3.5854\n",
      "Epoch [1/5], Step [6738/10336], Loss: 4.1250\n",
      "Epoch [1/5], Step [6740/10336], Loss: 3.1389\n",
      "Epoch [1/5], Step [6742/10336], Loss: 1.0284\n",
      "Epoch [1/5], Step [6744/10336], Loss: 0.0860\n",
      "Epoch [1/5], Step [6746/10336], Loss: 0.8594\n",
      "Epoch [1/5], Step [6748/10336], Loss: 0.6312\n",
      "Epoch [1/5], Step [6750/10336], Loss: 2.4327\n",
      "Epoch [1/5], Step [6752/10336], Loss: 3.6155\n",
      "Epoch [1/5], Step [6754/10336], Loss: 0.5222\n",
      "Epoch [1/5], Step [6756/10336], Loss: 0.7399\n",
      "Epoch [1/5], Step [6758/10336], Loss: 1.6305\n",
      "Epoch [1/5], Step [6760/10336], Loss: 0.8063\n",
      "Epoch [1/5], Step [6762/10336], Loss: 0.3195\n",
      "Epoch [1/5], Step [6764/10336], Loss: 0.6592\n",
      "Epoch [1/5], Step [6766/10336], Loss: 1.9572\n",
      "Epoch [1/5], Step [6768/10336], Loss: 0.9770\n",
      "Epoch [1/5], Step [6770/10336], Loss: 1.8668\n",
      "Epoch [1/5], Step [6772/10336], Loss: 1.7001\n",
      "Epoch [1/5], Step [6774/10336], Loss: 1.0399\n",
      "Epoch [1/5], Step [6776/10336], Loss: 2.4510\n",
      "Epoch [1/5], Step [6778/10336], Loss: 0.8609\n",
      "Epoch [1/5], Step [6780/10336], Loss: 1.6831\n",
      "Epoch [1/5], Step [6782/10336], Loss: 2.5204\n",
      "Epoch [1/5], Step [6784/10336], Loss: 0.2661\n",
      "Epoch [1/5], Step [6786/10336], Loss: 2.9519\n",
      "Epoch [1/5], Step [6788/10336], Loss: 0.7338\n",
      "Epoch [1/5], Step [6790/10336], Loss: 0.9011\n",
      "Epoch [1/5], Step [6792/10336], Loss: 0.8500\n",
      "Epoch [1/5], Step [6794/10336], Loss: 0.9971\n",
      "Epoch [1/5], Step [6796/10336], Loss: 0.4457\n",
      "Epoch [1/5], Step [6798/10336], Loss: 1.2549\n",
      "Epoch [1/5], Step [6800/10336], Loss: 0.4807\n",
      "Epoch [1/5], Step [6802/10336], Loss: 0.6797\n",
      "Epoch [1/5], Step [6804/10336], Loss: 0.0541\n",
      "Epoch [1/5], Step [6806/10336], Loss: 3.4140\n",
      "Epoch [1/5], Step [6808/10336], Loss: 1.2463\n",
      "Epoch [1/5], Step [6810/10336], Loss: 0.0366\n",
      "Epoch [1/5], Step [6812/10336], Loss: 0.7227\n",
      "Epoch [1/5], Step [6814/10336], Loss: 1.9307\n",
      "Epoch [1/5], Step [6816/10336], Loss: 1.3577\n",
      "Epoch [1/5], Step [6818/10336], Loss: 2.1454\n",
      "Epoch [1/5], Step [6820/10336], Loss: 1.4688\n",
      "Epoch [1/5], Step [6822/10336], Loss: 1.5267\n",
      "Epoch [1/5], Step [6824/10336], Loss: 0.1964\n",
      "Epoch [1/5], Step [6826/10336], Loss: 2.8209\n",
      "Epoch [1/5], Step [6828/10336], Loss: 3.0450\n",
      "Epoch [1/5], Step [6830/10336], Loss: 0.0332\n",
      "Epoch [1/5], Step [6832/10336], Loss: 0.6889\n",
      "Epoch [1/5], Step [6834/10336], Loss: 0.3264\n",
      "Epoch [1/5], Step [6836/10336], Loss: 2.5671\n",
      "Epoch [1/5], Step [6838/10336], Loss: 0.3493\n",
      "Epoch [1/5], Step [6840/10336], Loss: 0.3780\n",
      "Epoch [1/5], Step [6842/10336], Loss: 0.8050\n",
      "Epoch [1/5], Step [6844/10336], Loss: 0.0399\n",
      "Epoch [1/5], Step [6846/10336], Loss: 1.8779\n",
      "Epoch [1/5], Step [6848/10336], Loss: 0.9428\n",
      "Epoch [1/5], Step [6850/10336], Loss: 0.8714\n",
      "Epoch [1/5], Step [6852/10336], Loss: 1.1894\n",
      "Epoch [1/5], Step [6854/10336], Loss: 3.1167\n",
      "Epoch [1/5], Step [6856/10336], Loss: 1.0199\n",
      "Epoch [1/5], Step [6858/10336], Loss: 0.1083\n",
      "Epoch [1/5], Step [6860/10336], Loss: 0.5629\n",
      "Epoch [1/5], Step [6862/10336], Loss: 1.2966\n",
      "Epoch [1/5], Step [6864/10336], Loss: 1.7362\n",
      "Epoch [1/5], Step [6866/10336], Loss: 1.3645\n",
      "Epoch [1/5], Step [6868/10336], Loss: 0.0486\n",
      "Epoch [1/5], Step [6870/10336], Loss: 0.4798\n",
      "Epoch [1/5], Step [6872/10336], Loss: 0.6572\n",
      "Epoch [1/5], Step [6874/10336], Loss: 2.4818\n",
      "Epoch [1/5], Step [6876/10336], Loss: 0.7801\n",
      "Epoch [1/5], Step [6878/10336], Loss: 0.1664\n",
      "Epoch [1/5], Step [6880/10336], Loss: 0.3834\n",
      "Epoch [1/5], Step [6882/10336], Loss: 0.6050\n",
      "Epoch [1/5], Step [6884/10336], Loss: 4.5309\n",
      "Epoch [1/5], Step [6886/10336], Loss: 1.4257\n",
      "Epoch [1/5], Step [6888/10336], Loss: 1.0343\n",
      "Epoch [1/5], Step [6890/10336], Loss: 2.8743\n",
      "Epoch [1/5], Step [6892/10336], Loss: 1.7526\n",
      "Epoch [1/5], Step [6894/10336], Loss: 2.1716\n",
      "Epoch [1/5], Step [6896/10336], Loss: 0.1825\n",
      "Epoch [1/5], Step [6898/10336], Loss: 1.0456\n",
      "Epoch [1/5], Step [6900/10336], Loss: 1.3262\n",
      "Epoch [1/5], Step [6902/10336], Loss: 1.5641\n",
      "Epoch [1/5], Step [6904/10336], Loss: 0.0624\n",
      "Epoch [1/5], Step [6906/10336], Loss: 1.3451\n",
      "Epoch [1/5], Step [6908/10336], Loss: 0.9042\n",
      "Epoch [1/5], Step [6910/10336], Loss: 0.1335\n",
      "Epoch [1/5], Step [6912/10336], Loss: 1.6679\n",
      "Epoch [1/5], Step [6914/10336], Loss: 2.0888\n",
      "Epoch [1/5], Step [6916/10336], Loss: 0.1495\n",
      "Epoch [1/5], Step [6918/10336], Loss: 1.6874\n",
      "Epoch [1/5], Step [6920/10336], Loss: 1.2942\n",
      "Epoch [1/5], Step [6922/10336], Loss: 1.7395\n",
      "Epoch [1/5], Step [6924/10336], Loss: 1.8926\n",
      "Epoch [1/5], Step [6926/10336], Loss: 2.2014\n",
      "Epoch [1/5], Step [6928/10336], Loss: 0.7465\n",
      "Epoch [1/5], Step [6930/10336], Loss: 0.9540\n",
      "Epoch [1/5], Step [6932/10336], Loss: 0.0032\n",
      "Epoch [1/5], Step [6934/10336], Loss: 2.7793\n",
      "Epoch [1/5], Step [6936/10336], Loss: 0.6209\n",
      "Epoch [1/5], Step [6938/10336], Loss: 0.6849\n",
      "Epoch [1/5], Step [6940/10336], Loss: 0.2912\n",
      "Epoch [1/5], Step [6942/10336], Loss: 1.5067\n",
      "Epoch [1/5], Step [6944/10336], Loss: 1.7368\n",
      "Epoch [1/5], Step [6946/10336], Loss: 1.2298\n",
      "Epoch [1/5], Step [6948/10336], Loss: 2.1993\n",
      "Epoch [1/5], Step [6950/10336], Loss: 0.6417\n",
      "Epoch [1/5], Step [6952/10336], Loss: 1.7355\n",
      "Epoch [1/5], Step [6954/10336], Loss: 4.8986\n",
      "Epoch [1/5], Step [6956/10336], Loss: 3.1832\n",
      "Epoch [1/5], Step [6958/10336], Loss: 3.6936\n",
      "Epoch [1/5], Step [6960/10336], Loss: 0.6897\n",
      "Epoch [1/5], Step [6962/10336], Loss: 1.7631\n",
      "Epoch [1/5], Step [6964/10336], Loss: 0.4877\n",
      "Epoch [1/5], Step [6966/10336], Loss: 0.6882\n",
      "Epoch [1/5], Step [6968/10336], Loss: 3.1575\n",
      "Epoch [1/5], Step [6970/10336], Loss: 1.1014\n",
      "Epoch [1/5], Step [6972/10336], Loss: 1.2728\n",
      "Epoch [1/5], Step [6974/10336], Loss: 0.7414\n",
      "Epoch [1/5], Step [6976/10336], Loss: 0.1887\n",
      "Epoch [1/5], Step [6978/10336], Loss: 0.5546\n",
      "Epoch [1/5], Step [6980/10336], Loss: 0.2670\n",
      "Epoch [1/5], Step [6982/10336], Loss: 1.5540\n",
      "Epoch [1/5], Step [6984/10336], Loss: 2.1053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [6986/10336], Loss: 0.9615\n",
      "Epoch [1/5], Step [6988/10336], Loss: 3.5479\n",
      "Epoch [1/5], Step [6990/10336], Loss: 1.8518\n",
      "Epoch [1/5], Step [6992/10336], Loss: 0.0220\n",
      "Epoch [1/5], Step [6994/10336], Loss: 2.0442\n",
      "Epoch [1/5], Step [6996/10336], Loss: 1.8712\n",
      "Epoch [1/5], Step [6998/10336], Loss: 1.3012\n",
      "Epoch [1/5], Step [7000/10336], Loss: 0.5279\n",
      "Epoch [1/5], Step [7002/10336], Loss: 0.4375\n",
      "Epoch [1/5], Step [7004/10336], Loss: 0.7223\n",
      "Epoch [1/5], Step [7006/10336], Loss: 1.4764\n",
      "Epoch [1/5], Step [7008/10336], Loss: 0.7854\n",
      "Epoch [1/5], Step [7010/10336], Loss: 3.8485\n",
      "Epoch [1/5], Step [7012/10336], Loss: 0.7148\n",
      "Epoch [1/5], Step [7014/10336], Loss: 0.5202\n",
      "Epoch [1/5], Step [7016/10336], Loss: 2.1559\n",
      "Epoch [1/5], Step [7018/10336], Loss: 1.7929\n",
      "Epoch [1/5], Step [7020/10336], Loss: 4.7161\n",
      "Epoch [1/5], Step [7022/10336], Loss: 0.0940\n",
      "Epoch [1/5], Step [7024/10336], Loss: 0.1592\n",
      "Epoch [1/5], Step [7026/10336], Loss: 1.6513\n",
      "Epoch [1/5], Step [7028/10336], Loss: 1.1525\n",
      "Epoch [1/5], Step [7030/10336], Loss: 1.2456\n",
      "Epoch [1/5], Step [7032/10336], Loss: 2.4493\n",
      "Epoch [1/5], Step [7034/10336], Loss: 1.8815\n",
      "Epoch [1/5], Step [7036/10336], Loss: 0.9138\n",
      "Epoch [1/5], Step [7038/10336], Loss: 2.0203\n",
      "Epoch [1/5], Step [7040/10336], Loss: 1.1400\n",
      "Epoch [1/5], Step [7042/10336], Loss: 0.3563\n",
      "Epoch [1/5], Step [7044/10336], Loss: 1.2918\n",
      "Epoch [1/5], Step [7046/10336], Loss: 0.8925\n",
      "Epoch [1/5], Step [7048/10336], Loss: 3.3779\n",
      "Epoch [1/5], Step [7050/10336], Loss: 1.1786\n",
      "Epoch [1/5], Step [7052/10336], Loss: 0.4678\n",
      "Epoch [1/5], Step [7054/10336], Loss: 1.0109\n",
      "Epoch [1/5], Step [7056/10336], Loss: 0.0401\n",
      "Epoch [1/5], Step [7058/10336], Loss: 0.1394\n",
      "Epoch [1/5], Step [7060/10336], Loss: 0.9740\n",
      "Epoch [1/5], Step [7062/10336], Loss: 2.3182\n",
      "Epoch [1/5], Step [7064/10336], Loss: 1.7482\n",
      "Epoch [1/5], Step [7066/10336], Loss: 1.3489\n",
      "Epoch [1/5], Step [7068/10336], Loss: 0.7003\n",
      "Epoch [1/5], Step [7070/10336], Loss: 1.8489\n",
      "Epoch [1/5], Step [7072/10336], Loss: 0.2383\n",
      "Epoch [1/5], Step [7074/10336], Loss: 0.0370\n",
      "Epoch [1/5], Step [7076/10336], Loss: 2.9891\n",
      "Epoch [1/5], Step [7078/10336], Loss: 1.2199\n",
      "Epoch [1/5], Step [7080/10336], Loss: 1.6327\n",
      "Epoch [1/5], Step [7082/10336], Loss: 0.1218\n",
      "Epoch [1/5], Step [7084/10336], Loss: 1.8342\n",
      "Epoch [1/5], Step [7086/10336], Loss: 1.1428\n",
      "Epoch [1/5], Step [7088/10336], Loss: 3.0322\n",
      "Epoch [1/5], Step [7090/10336], Loss: 2.4183\n",
      "Epoch [1/5], Step [7092/10336], Loss: 2.2431\n",
      "Epoch [1/5], Step [7094/10336], Loss: 0.0411\n",
      "Epoch [1/5], Step [7096/10336], Loss: 3.0471\n",
      "Epoch [1/5], Step [7098/10336], Loss: 1.0868\n",
      "Epoch [1/5], Step [7100/10336], Loss: 0.9059\n",
      "Epoch [1/5], Step [7102/10336], Loss: 0.1641\n",
      "Epoch [1/5], Step [7104/10336], Loss: 1.4982\n",
      "Epoch [1/5], Step [7106/10336], Loss: 3.1118\n",
      "Epoch [1/5], Step [7108/10336], Loss: 0.0273\n",
      "Epoch [1/5], Step [7110/10336], Loss: 1.1250\n",
      "Epoch [1/5], Step [7112/10336], Loss: 0.8622\n",
      "Epoch [1/5], Step [7114/10336], Loss: 0.4331\n",
      "Epoch [1/5], Step [7116/10336], Loss: 2.7162\n",
      "Epoch [1/5], Step [7118/10336], Loss: 2.1152\n",
      "Epoch [1/5], Step [7120/10336], Loss: 1.9754\n",
      "Epoch [1/5], Step [7122/10336], Loss: 1.3231\n",
      "Epoch [1/5], Step [7124/10336], Loss: 0.3378\n",
      "Epoch [1/5], Step [7126/10336], Loss: 0.0353\n",
      "Epoch [1/5], Step [7128/10336], Loss: 3.6117\n",
      "Epoch [1/5], Step [7130/10336], Loss: 1.6244\n",
      "Epoch [1/5], Step [7132/10336], Loss: 0.0175\n",
      "Epoch [1/5], Step [7134/10336], Loss: 0.0669\n",
      "Epoch [1/5], Step [7136/10336], Loss: 0.3475\n",
      "Epoch [1/5], Step [7138/10336], Loss: 2.2149\n",
      "Epoch [1/5], Step [7140/10336], Loss: 1.9564\n",
      "Epoch [1/5], Step [7142/10336], Loss: 1.9909\n",
      "Epoch [1/5], Step [7144/10336], Loss: 0.3339\n",
      "Epoch [1/5], Step [7146/10336], Loss: 2.9159\n",
      "Epoch [1/5], Step [7148/10336], Loss: 1.7551\n",
      "Epoch [1/5], Step [7150/10336], Loss: 2.2419\n",
      "Epoch [1/5], Step [7152/10336], Loss: 0.1229\n",
      "Epoch [1/5], Step [7154/10336], Loss: 0.9507\n",
      "Epoch [1/5], Step [7156/10336], Loss: 2.1822\n",
      "Epoch [1/5], Step [7158/10336], Loss: 2.1035\n",
      "Epoch [1/5], Step [7160/10336], Loss: 2.5433\n",
      "Epoch [1/5], Step [7162/10336], Loss: 2.8786\n",
      "Epoch [1/5], Step [7164/10336], Loss: 0.6842\n",
      "Epoch [1/5], Step [7166/10336], Loss: 1.3113\n",
      "Epoch [1/5], Step [7168/10336], Loss: 1.1520\n",
      "Epoch [1/5], Step [7170/10336], Loss: 1.7206\n",
      "Epoch [1/5], Step [7172/10336], Loss: 0.5904\n",
      "Epoch [1/5], Step [7174/10336], Loss: 2.0107\n",
      "Epoch [1/5], Step [7176/10336], Loss: 1.1863\n",
      "Epoch [1/5], Step [7178/10336], Loss: 0.5550\n",
      "Epoch [1/5], Step [7180/10336], Loss: 0.9488\n",
      "Epoch [1/5], Step [7182/10336], Loss: 2.1053\n",
      "Epoch [1/5], Step [7184/10336], Loss: 0.7035\n",
      "Epoch [1/5], Step [7186/10336], Loss: 0.4066\n",
      "Epoch [1/5], Step [7188/10336], Loss: 1.3204\n",
      "Epoch [1/5], Step [7190/10336], Loss: 2.4330\n",
      "Epoch [1/5], Step [7192/10336], Loss: 0.0465\n",
      "Epoch [1/5], Step [7194/10336], Loss: 0.8495\n",
      "Epoch [1/5], Step [7196/10336], Loss: 1.4070\n",
      "Epoch [1/5], Step [7198/10336], Loss: 0.9052\n",
      "Epoch [1/5], Step [7200/10336], Loss: 1.8649\n",
      "Epoch [1/5], Step [7202/10336], Loss: 0.7900\n",
      "Epoch [1/5], Step [7204/10336], Loss: 1.1265\n",
      "Epoch [1/5], Step [7206/10336], Loss: 2.5607\n",
      "Epoch [1/5], Step [7208/10336], Loss: 1.0556\n",
      "Epoch [1/5], Step [7210/10336], Loss: 2.2593\n",
      "Epoch [1/5], Step [7212/10336], Loss: 0.3432\n",
      "Epoch [1/5], Step [7214/10336], Loss: 0.8002\n",
      "Epoch [1/5], Step [7216/10336], Loss: 0.8039\n",
      "Epoch [1/5], Step [7218/10336], Loss: 0.4010\n",
      "Epoch [1/5], Step [7220/10336], Loss: 0.9835\n",
      "Epoch [1/5], Step [7222/10336], Loss: 0.8568\n",
      "Epoch [1/5], Step [7224/10336], Loss: 0.5787\n",
      "Epoch [1/5], Step [7226/10336], Loss: 0.0364\n",
      "Epoch [1/5], Step [7228/10336], Loss: 3.0837\n",
      "Epoch [1/5], Step [7230/10336], Loss: 0.9650\n",
      "Epoch [1/5], Step [7232/10336], Loss: 0.5364\n",
      "Epoch [1/5], Step [7234/10336], Loss: 0.1844\n",
      "Epoch [1/5], Step [7236/10336], Loss: 1.5401\n",
      "Epoch [1/5], Step [7238/10336], Loss: 0.5182\n",
      "Epoch [1/5], Step [7240/10336], Loss: 1.6031\n",
      "Epoch [1/5], Step [7242/10336], Loss: 0.7788\n",
      "Epoch [1/5], Step [7244/10336], Loss: 0.7691\n",
      "Epoch [1/5], Step [7246/10336], Loss: 4.4247\n",
      "Epoch [1/5], Step [7248/10336], Loss: 0.7602\n",
      "Epoch [1/5], Step [7250/10336], Loss: 1.4716\n",
      "Epoch [1/5], Step [7252/10336], Loss: 0.0280\n",
      "Epoch [1/5], Step [7254/10336], Loss: 1.9216\n",
      "Epoch [1/5], Step [7256/10336], Loss: 0.2747\n",
      "Epoch [1/5], Step [7258/10336], Loss: 4.1811\n",
      "Epoch [1/5], Step [7260/10336], Loss: 1.9840\n",
      "Epoch [1/5], Step [7262/10336], Loss: 1.0596\n",
      "Epoch [1/5], Step [7264/10336], Loss: 0.9653\n",
      "Epoch [1/5], Step [7266/10336], Loss: 0.8581\n",
      "Epoch [1/5], Step [7268/10336], Loss: 0.2458\n",
      "Epoch [1/5], Step [7270/10336], Loss: 0.1892\n",
      "Epoch [1/5], Step [7272/10336], Loss: 0.3638\n",
      "Epoch [1/5], Step [7274/10336], Loss: 2.1073\n",
      "Epoch [1/5], Step [7276/10336], Loss: 1.8622\n",
      "Epoch [1/5], Step [7278/10336], Loss: 0.0734\n",
      "Epoch [1/5], Step [7280/10336], Loss: 3.1986\n",
      "Epoch [1/5], Step [7282/10336], Loss: 1.3642\n",
      "Epoch [1/5], Step [7284/10336], Loss: 1.0992\n",
      "Epoch [1/5], Step [7286/10336], Loss: 0.4725\n",
      "Epoch [1/5], Step [7288/10336], Loss: 1.5722\n",
      "Epoch [1/5], Step [7290/10336], Loss: 3.3259\n",
      "Epoch [1/5], Step [7292/10336], Loss: 1.8947\n",
      "Epoch [1/5], Step [7294/10336], Loss: 1.8672\n",
      "Epoch [1/5], Step [7296/10336], Loss: 0.5017\n",
      "Epoch [1/5], Step [7298/10336], Loss: 1.0568\n",
      "Epoch [1/5], Step [7300/10336], Loss: 0.2132\n",
      "Epoch [1/5], Step [7302/10336], Loss: 1.4770\n",
      "Epoch [1/5], Step [7304/10336], Loss: 0.7916\n",
      "Epoch [1/5], Step [7306/10336], Loss: 1.2632\n",
      "Epoch [1/5], Step [7308/10336], Loss: 0.0222\n",
      "Epoch [1/5], Step [7310/10336], Loss: 5.3270\n",
      "Epoch [1/5], Step [7312/10336], Loss: 3.7581\n",
      "Epoch [1/5], Step [7314/10336], Loss: 0.1160\n",
      "Epoch [1/5], Step [7316/10336], Loss: 3.2376\n",
      "Epoch [1/5], Step [7318/10336], Loss: 1.5011\n",
      "Epoch [1/5], Step [7320/10336], Loss: 0.5791\n",
      "Epoch [1/5], Step [7322/10336], Loss: 0.5095\n",
      "Epoch [1/5], Step [7324/10336], Loss: 0.2950\n",
      "Epoch [1/5], Step [7326/10336], Loss: 2.4959\n",
      "Epoch [1/5], Step [7328/10336], Loss: 0.2685\n",
      "Epoch [1/5], Step [7330/10336], Loss: 1.8432\n",
      "Epoch [1/5], Step [7332/10336], Loss: 0.9441\n",
      "Epoch [1/5], Step [7334/10336], Loss: 0.7400\n",
      "Epoch [1/5], Step [7336/10336], Loss: 2.4164\n",
      "Epoch [1/5], Step [7338/10336], Loss: 0.0252\n",
      "Epoch [1/5], Step [7340/10336], Loss: 0.0439\n",
      "Epoch [1/5], Step [7342/10336], Loss: 0.9238\n",
      "Epoch [1/5], Step [7344/10336], Loss: 1.6298\n",
      "Epoch [1/5], Step [7346/10336], Loss: 2.2747\n",
      "Epoch [1/5], Step [7348/10336], Loss: 1.9814\n",
      "Epoch [1/5], Step [7350/10336], Loss: 1.1857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [7352/10336], Loss: 0.7307\n",
      "Epoch [1/5], Step [7354/10336], Loss: 0.0564\n",
      "Epoch [1/5], Step [7356/10336], Loss: 0.5501\n",
      "Epoch [1/5], Step [7358/10336], Loss: 0.8739\n",
      "Epoch [1/5], Step [7360/10336], Loss: 2.2217\n",
      "Epoch [1/5], Step [7362/10336], Loss: 1.0458\n",
      "Epoch [1/5], Step [7364/10336], Loss: 0.2362\n",
      "Epoch [1/5], Step [7366/10336], Loss: 0.5818\n",
      "Epoch [1/5], Step [7368/10336], Loss: 0.4144\n",
      "Epoch [1/5], Step [7370/10336], Loss: 0.1987\n",
      "Epoch [1/5], Step [7372/10336], Loss: 1.2064\n",
      "Epoch [1/5], Step [7374/10336], Loss: 0.9175\n",
      "Epoch [1/5], Step [7376/10336], Loss: 0.5200\n",
      "Epoch [1/5], Step [7378/10336], Loss: 1.3542\n",
      "Epoch [1/5], Step [7380/10336], Loss: 1.3815\n",
      "Epoch [1/5], Step [7382/10336], Loss: 0.5342\n",
      "Epoch [1/5], Step [7384/10336], Loss: 0.0341\n",
      "Epoch [1/5], Step [7386/10336], Loss: 1.6705\n",
      "Epoch [1/5], Step [7388/10336], Loss: 0.0188\n",
      "Epoch [1/5], Step [7390/10336], Loss: 0.1122\n",
      "Epoch [1/5], Step [7392/10336], Loss: 2.1476\n",
      "Epoch [1/5], Step [7394/10336], Loss: 0.0770\n",
      "Epoch [1/5], Step [7396/10336], Loss: 3.9835\n",
      "Epoch [1/5], Step [7398/10336], Loss: 0.1620\n",
      "Epoch [1/5], Step [7400/10336], Loss: 1.0167\n",
      "Epoch [1/5], Step [7402/10336], Loss: 0.0171\n",
      "Epoch [1/5], Step [7404/10336], Loss: 0.8542\n",
      "Epoch [1/5], Step [7406/10336], Loss: 0.3342\n",
      "Epoch [1/5], Step [7408/10336], Loss: 3.9654\n",
      "Epoch [1/5], Step [7410/10336], Loss: 0.1588\n",
      "Epoch [1/5], Step [7412/10336], Loss: 0.0168\n",
      "Epoch [1/5], Step [7414/10336], Loss: 0.3130\n",
      "Epoch [1/5], Step [7416/10336], Loss: 2.8434\n",
      "Epoch [1/5], Step [7418/10336], Loss: 1.5328\n",
      "Epoch [1/5], Step [7420/10336], Loss: 2.1338\n",
      "Epoch [1/5], Step [7422/10336], Loss: 0.2950\n",
      "Epoch [1/5], Step [7424/10336], Loss: 1.9515\n",
      "Epoch [1/5], Step [7426/10336], Loss: 0.6740\n",
      "Epoch [1/5], Step [7428/10336], Loss: 1.4811\n",
      "Epoch [1/5], Step [7430/10336], Loss: 0.8992\n",
      "Epoch [1/5], Step [7432/10336], Loss: 0.5345\n",
      "Epoch [1/5], Step [7434/10336], Loss: 1.9074\n",
      "Epoch [1/5], Step [7436/10336], Loss: 1.2330\n",
      "Epoch [1/5], Step [7438/10336], Loss: 0.0480\n",
      "Epoch [1/5], Step [7440/10336], Loss: 1.0978\n",
      "Epoch [1/5], Step [7442/10336], Loss: 1.6176\n",
      "Epoch [1/5], Step [7444/10336], Loss: 0.6264\n",
      "Epoch [1/5], Step [7446/10336], Loss: 0.1389\n",
      "Epoch [1/5], Step [7448/10336], Loss: 0.0446\n",
      "Epoch [1/5], Step [7450/10336], Loss: 0.1578\n",
      "Epoch [1/5], Step [7452/10336], Loss: 0.2052\n",
      "Epoch [1/5], Step [7454/10336], Loss: 2.2072\n",
      "Epoch [1/5], Step [7456/10336], Loss: 2.4193\n",
      "Epoch [1/5], Step [7458/10336], Loss: 0.0608\n",
      "Epoch [1/5], Step [7460/10336], Loss: 0.4071\n",
      "Epoch [1/5], Step [7462/10336], Loss: 0.0337\n",
      "Epoch [1/5], Step [7464/10336], Loss: 0.6211\n",
      "Epoch [1/5], Step [7466/10336], Loss: 1.0951\n",
      "Epoch [1/5], Step [7468/10336], Loss: 0.1483\n",
      "Epoch [1/5], Step [7470/10336], Loss: 0.1110\n",
      "Epoch [1/5], Step [7472/10336], Loss: 0.5357\n",
      "Epoch [1/5], Step [7474/10336], Loss: 3.3086\n",
      "Epoch [1/5], Step [7476/10336], Loss: 0.2454\n",
      "Epoch [1/5], Step [7478/10336], Loss: 4.4130\n",
      "Epoch [1/5], Step [7480/10336], Loss: 0.5153\n",
      "Epoch [1/5], Step [7482/10336], Loss: 0.2235\n",
      "Epoch [1/5], Step [7484/10336], Loss: 0.7757\n",
      "Epoch [1/5], Step [7486/10336], Loss: 1.1719\n",
      "Epoch [1/5], Step [7488/10336], Loss: 1.7167\n",
      "Epoch [1/5], Step [7490/10336], Loss: 0.4751\n",
      "Epoch [1/5], Step [7492/10336], Loss: 1.0510\n",
      "Epoch [1/5], Step [7494/10336], Loss: 1.6352\n",
      "Epoch [1/5], Step [7496/10336], Loss: 1.1191\n",
      "Epoch [1/5], Step [7498/10336], Loss: 1.0007\n",
      "Epoch [1/5], Step [7500/10336], Loss: 0.6351\n",
      "Epoch [1/5], Step [7502/10336], Loss: 0.9913\n",
      "Epoch [1/5], Step [7504/10336], Loss: 0.6709\n",
      "Epoch [1/5], Step [7506/10336], Loss: 0.0875\n",
      "Epoch [1/5], Step [7508/10336], Loss: 0.7085\n",
      "Epoch [1/5], Step [7510/10336], Loss: 3.2192\n",
      "Epoch [1/5], Step [7512/10336], Loss: 2.3605\n",
      "Epoch [1/5], Step [7514/10336], Loss: 0.0024\n",
      "Epoch [1/5], Step [7516/10336], Loss: 0.9704\n",
      "Epoch [1/5], Step [7518/10336], Loss: 3.8761\n",
      "Epoch [1/5], Step [7520/10336], Loss: 2.8311\n",
      "Epoch [1/5], Step [7522/10336], Loss: 0.6168\n",
      "Epoch [1/5], Step [7524/10336], Loss: 1.8827\n",
      "Epoch [1/5], Step [7526/10336], Loss: 0.4612\n",
      "Epoch [1/5], Step [7528/10336], Loss: 2.2211\n",
      "Epoch [1/5], Step [7530/10336], Loss: 0.3987\n",
      "Epoch [1/5], Step [7532/10336], Loss: 1.7242\n",
      "Epoch [1/5], Step [7534/10336], Loss: 0.3030\n",
      "Epoch [1/5], Step [7536/10336], Loss: 1.8212\n",
      "Epoch [1/5], Step [7538/10336], Loss: 2.9551\n",
      "Epoch [1/5], Step [7540/10336], Loss: 0.0332\n",
      "Epoch [1/5], Step [7542/10336], Loss: 0.4133\n",
      "Epoch [1/5], Step [7544/10336], Loss: 1.0297\n",
      "Epoch [1/5], Step [7546/10336], Loss: 0.9031\n",
      "Epoch [1/5], Step [7548/10336], Loss: 0.0868\n",
      "Epoch [1/5], Step [7550/10336], Loss: 0.5099\n",
      "Epoch [1/5], Step [7552/10336], Loss: 0.2975\n",
      "Epoch [1/5], Step [7554/10336], Loss: 0.0512\n",
      "Epoch [1/5], Step [7556/10336], Loss: 0.6493\n",
      "Epoch [1/5], Step [7558/10336], Loss: 0.8114\n",
      "Epoch [1/5], Step [7560/10336], Loss: 2.1526\n",
      "Epoch [1/5], Step [7562/10336], Loss: 2.9358\n",
      "Epoch [1/5], Step [7564/10336], Loss: 1.6397\n",
      "Epoch [1/5], Step [7566/10336], Loss: 1.3328\n",
      "Epoch [1/5], Step [7568/10336], Loss: 1.0821\n",
      "Epoch [1/5], Step [7570/10336], Loss: 0.6771\n",
      "Epoch [1/5], Step [7572/10336], Loss: 0.6961\n",
      "Epoch [1/5], Step [7574/10336], Loss: 2.9350\n",
      "Epoch [1/5], Step [7576/10336], Loss: 1.2474\n",
      "Epoch [1/5], Step [7578/10336], Loss: 2.2567\n",
      "Epoch [1/5], Step [7580/10336], Loss: 0.5266\n",
      "Epoch [1/5], Step [7582/10336], Loss: 3.2654\n",
      "Epoch [1/5], Step [7584/10336], Loss: 0.1629\n",
      "Epoch [1/5], Step [7586/10336], Loss: 1.0747\n",
      "Epoch [1/5], Step [7588/10336], Loss: 2.6448\n",
      "Epoch [1/5], Step [7590/10336], Loss: 1.9022\n",
      "Epoch [1/5], Step [7592/10336], Loss: 1.1400\n",
      "Epoch [1/5], Step [7594/10336], Loss: 1.5596\n",
      "Epoch [1/5], Step [7596/10336], Loss: 1.5730\n",
      "Epoch [1/5], Step [7598/10336], Loss: 0.4285\n",
      "Epoch [1/5], Step [7600/10336], Loss: 2.7878\n",
      "Epoch [1/5], Step [7602/10336], Loss: 2.7674\n",
      "Epoch [1/5], Step [7604/10336], Loss: 0.8054\n",
      "Epoch [1/5], Step [7606/10336], Loss: 0.5889\n",
      "Epoch [1/5], Step [7608/10336], Loss: 0.5354\n",
      "Epoch [1/5], Step [7610/10336], Loss: 3.4995\n",
      "Epoch [1/5], Step [7612/10336], Loss: 0.4305\n",
      "Epoch [1/5], Step [7614/10336], Loss: 0.1626\n",
      "Epoch [1/5], Step [7616/10336], Loss: 1.4034\n",
      "Epoch [1/5], Step [7618/10336], Loss: 2.3895\n",
      "Epoch [1/5], Step [7620/10336], Loss: 0.1929\n",
      "Epoch [1/5], Step [7622/10336], Loss: 2.5319\n",
      "Epoch [1/5], Step [7624/10336], Loss: 2.1256\n",
      "Epoch [1/5], Step [7626/10336], Loss: 2.6227\n",
      "Epoch [1/5], Step [7628/10336], Loss: 0.6563\n",
      "Epoch [1/5], Step [7630/10336], Loss: 0.7105\n",
      "Epoch [1/5], Step [7632/10336], Loss: 1.5245\n",
      "Epoch [1/5], Step [7634/10336], Loss: 2.0928\n",
      "Epoch [1/5], Step [7636/10336], Loss: 0.6450\n",
      "Epoch [1/5], Step [7638/10336], Loss: 0.1442\n",
      "Epoch [1/5], Step [7640/10336], Loss: 0.7944\n",
      "Epoch [1/5], Step [7642/10336], Loss: 0.7805\n",
      "Epoch [1/5], Step [7644/10336], Loss: 0.7233\n",
      "Epoch [1/5], Step [7646/10336], Loss: 0.2391\n",
      "Epoch [1/5], Step [7648/10336], Loss: 1.0705\n",
      "Epoch [1/5], Step [7650/10336], Loss: 0.2064\n",
      "Epoch [1/5], Step [7652/10336], Loss: 1.6175\n",
      "Epoch [1/5], Step [7654/10336], Loss: 0.0419\n",
      "Epoch [1/5], Step [7656/10336], Loss: 0.8523\n",
      "Epoch [1/5], Step [7658/10336], Loss: 0.4710\n",
      "Epoch [1/5], Step [7660/10336], Loss: 0.6596\n",
      "Epoch [1/5], Step [7662/10336], Loss: 0.0941\n",
      "Epoch [1/5], Step [7664/10336], Loss: 3.4940\n",
      "Epoch [1/5], Step [7666/10336], Loss: 4.3136\n",
      "Epoch [1/5], Step [7668/10336], Loss: 0.2323\n",
      "Epoch [1/5], Step [7670/10336], Loss: 1.7013\n",
      "Epoch [1/5], Step [7672/10336], Loss: 1.0695\n",
      "Epoch [1/5], Step [7674/10336], Loss: 1.2661\n",
      "Epoch [1/5], Step [7676/10336], Loss: 0.2870\n",
      "Epoch [1/5], Step [7678/10336], Loss: 1.8580\n",
      "Epoch [1/5], Step [7680/10336], Loss: 0.1285\n",
      "Epoch [1/5], Step [7682/10336], Loss: 2.1388\n",
      "Epoch [1/5], Step [7684/10336], Loss: 0.1794\n",
      "Epoch [1/5], Step [7686/10336], Loss: 0.9570\n",
      "Epoch [1/5], Step [7688/10336], Loss: 1.8000\n",
      "Epoch [1/5], Step [7690/10336], Loss: 0.1759\n",
      "Epoch [1/5], Step [7692/10336], Loss: 1.0118\n",
      "Epoch [1/5], Step [7694/10336], Loss: 1.2986\n",
      "Epoch [1/5], Step [7696/10336], Loss: 2.8149\n",
      "Epoch [1/5], Step [7698/10336], Loss: 1.1322\n",
      "Epoch [1/5], Step [7700/10336], Loss: 3.7976\n",
      "Epoch [1/5], Step [7702/10336], Loss: 1.2281\n",
      "Epoch [1/5], Step [7704/10336], Loss: 1.6900\n",
      "Epoch [1/5], Step [7706/10336], Loss: 0.6987\n",
      "Epoch [1/5], Step [7708/10336], Loss: 0.2960\n",
      "Epoch [1/5], Step [7710/10336], Loss: 0.1512\n",
      "Epoch [1/5], Step [7712/10336], Loss: 3.9904\n",
      "Epoch [1/5], Step [7714/10336], Loss: 1.3775\n",
      "Epoch [1/5], Step [7716/10336], Loss: 3.1501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [7718/10336], Loss: 0.3660\n",
      "Epoch [1/5], Step [7720/10336], Loss: 2.3997\n",
      "Epoch [1/5], Step [7722/10336], Loss: 0.4951\n",
      "Epoch [1/5], Step [7724/10336], Loss: 0.9366\n",
      "Epoch [1/5], Step [7726/10336], Loss: 0.1923\n",
      "Epoch [1/5], Step [7728/10336], Loss: 0.4929\n",
      "Epoch [1/5], Step [7730/10336], Loss: 0.6801\n",
      "Epoch [1/5], Step [7732/10336], Loss: 1.3178\n",
      "Epoch [1/5], Step [7734/10336], Loss: 0.4058\n",
      "Epoch [1/5], Step [7736/10336], Loss: 1.9630\n",
      "Epoch [1/5], Step [7738/10336], Loss: 2.8080\n",
      "Epoch [1/5], Step [7740/10336], Loss: 4.7954\n",
      "Epoch [1/5], Step [7742/10336], Loss: 4.4601\n",
      "Epoch [1/5], Step [7744/10336], Loss: 0.3666\n",
      "Epoch [1/5], Step [7746/10336], Loss: 3.9008\n",
      "Epoch [1/5], Step [7748/10336], Loss: 0.5855\n",
      "Epoch [1/5], Step [7750/10336], Loss: 0.0114\n",
      "Epoch [1/5], Step [7752/10336], Loss: 2.0681\n",
      "Epoch [1/5], Step [7754/10336], Loss: 2.2949\n",
      "Epoch [1/5], Step [7756/10336], Loss: 0.9313\n",
      "Epoch [1/5], Step [7758/10336], Loss: 1.2537\n",
      "Epoch [1/5], Step [7760/10336], Loss: 1.2563\n",
      "Epoch [1/5], Step [7762/10336], Loss: 0.9790\n",
      "Epoch [1/5], Step [7764/10336], Loss: 1.0315\n",
      "Epoch [1/5], Step [7766/10336], Loss: 1.5706\n",
      "Epoch [1/5], Step [7768/10336], Loss: 5.9050\n",
      "Epoch [1/5], Step [7770/10336], Loss: 1.4158\n",
      "Epoch [1/5], Step [7772/10336], Loss: 0.5066\n",
      "Epoch [1/5], Step [7774/10336], Loss: 2.6035\n",
      "Epoch [1/5], Step [7776/10336], Loss: 2.3152\n",
      "Epoch [1/5], Step [7778/10336], Loss: 0.8918\n",
      "Epoch [1/5], Step [7780/10336], Loss: 0.7827\n",
      "Epoch [1/5], Step [7782/10336], Loss: 1.3096\n",
      "Epoch [1/5], Step [7784/10336], Loss: 2.4415\n",
      "Epoch [1/5], Step [7786/10336], Loss: 0.7995\n",
      "Epoch [1/5], Step [7788/10336], Loss: 0.4104\n",
      "Epoch [1/5], Step [7790/10336], Loss: 1.8273\n",
      "Epoch [1/5], Step [7792/10336], Loss: 0.0065\n",
      "Epoch [1/5], Step [7794/10336], Loss: 2.1404\n",
      "Epoch [1/5], Step [7796/10336], Loss: 1.1981\n",
      "Epoch [1/5], Step [7798/10336], Loss: 0.9797\n",
      "Epoch [1/5], Step [7800/10336], Loss: 0.2232\n",
      "Epoch [1/5], Step [7802/10336], Loss: 3.6399\n",
      "Epoch [1/5], Step [7804/10336], Loss: 0.1095\n",
      "Epoch [1/5], Step [7806/10336], Loss: 0.3242\n",
      "Epoch [1/5], Step [7808/10336], Loss: 3.3494\n",
      "Epoch [1/5], Step [7810/10336], Loss: 0.2351\n",
      "Epoch [1/5], Step [7812/10336], Loss: 3.8069\n",
      "Epoch [1/5], Step [7814/10336], Loss: 1.6516\n",
      "Epoch [1/5], Step [7816/10336], Loss: 0.3998\n",
      "Epoch [1/5], Step [7818/10336], Loss: 1.0782\n",
      "Epoch [1/5], Step [7820/10336], Loss: 0.1571\n",
      "Epoch [1/5], Step [7822/10336], Loss: 0.1431\n",
      "Epoch [1/5], Step [7824/10336], Loss: 1.2173\n",
      "Epoch [1/5], Step [7826/10336], Loss: 0.9776\n",
      "Epoch [1/5], Step [7828/10336], Loss: 0.5992\n",
      "Epoch [1/5], Step [7830/10336], Loss: 0.5987\n",
      "Epoch [1/5], Step [7832/10336], Loss: 0.2157\n",
      "Epoch [1/5], Step [7834/10336], Loss: 0.5694\n",
      "Epoch [1/5], Step [7836/10336], Loss: 0.7835\n",
      "Epoch [1/5], Step [7838/10336], Loss: 0.8945\n",
      "Epoch [1/5], Step [7840/10336], Loss: 2.9773\n",
      "Epoch [1/5], Step [7842/10336], Loss: 2.9731\n",
      "Epoch [1/5], Step [7844/10336], Loss: 1.4743\n",
      "Epoch [1/5], Step [7846/10336], Loss: 1.8924\n",
      "Epoch [1/5], Step [7848/10336], Loss: 0.1875\n",
      "Epoch [1/5], Step [7850/10336], Loss: 5.0766\n",
      "Epoch [1/5], Step [7852/10336], Loss: 0.6491\n",
      "Epoch [1/5], Step [7854/10336], Loss: 0.3371\n",
      "Epoch [1/5], Step [7856/10336], Loss: 0.0239\n",
      "Epoch [1/5], Step [7858/10336], Loss: 0.6454\n",
      "Epoch [1/5], Step [7860/10336], Loss: 0.7220\n",
      "Epoch [1/5], Step [7862/10336], Loss: 1.7277\n",
      "Epoch [1/5], Step [7864/10336], Loss: 1.7180\n",
      "Epoch [1/5], Step [7866/10336], Loss: 0.3108\n",
      "Epoch [1/5], Step [7868/10336], Loss: 1.7837\n",
      "Epoch [1/5], Step [7870/10336], Loss: 1.4228\n",
      "Epoch [1/5], Step [7872/10336], Loss: 0.8345\n",
      "Epoch [1/5], Step [7874/10336], Loss: 0.0129\n",
      "Epoch [1/5], Step [7876/10336], Loss: 1.2444\n",
      "Epoch [1/5], Step [7878/10336], Loss: 1.4533\n",
      "Epoch [1/5], Step [7880/10336], Loss: 0.6974\n",
      "Epoch [1/5], Step [7882/10336], Loss: 0.2614\n",
      "Epoch [1/5], Step [7884/10336], Loss: 0.5326\n",
      "Epoch [1/5], Step [7886/10336], Loss: 0.7755\n",
      "Epoch [1/5], Step [7888/10336], Loss: 1.3480\n",
      "Epoch [1/5], Step [7890/10336], Loss: 1.0808\n",
      "Epoch [1/5], Step [7892/10336], Loss: 1.4700\n",
      "Epoch [1/5], Step [7894/10336], Loss: 1.0492\n",
      "Epoch [1/5], Step [7896/10336], Loss: 0.2157\n",
      "Epoch [1/5], Step [7898/10336], Loss: 1.2752\n",
      "Epoch [1/5], Step [7900/10336], Loss: 0.4200\n",
      "Epoch [1/5], Step [7902/10336], Loss: 1.2492\n",
      "Epoch [1/5], Step [7904/10336], Loss: 2.6194\n",
      "Epoch [1/5], Step [7906/10336], Loss: 0.5700\n",
      "Epoch [1/5], Step [7908/10336], Loss: 0.0246\n",
      "Epoch [1/5], Step [7910/10336], Loss: 0.6098\n",
      "Epoch [1/5], Step [7912/10336], Loss: 0.6779\n",
      "Epoch [1/5], Step [7914/10336], Loss: 3.1089\n",
      "Epoch [1/5], Step [7916/10336], Loss: 0.8968\n",
      "Epoch [1/5], Step [7918/10336], Loss: 1.1506\n",
      "Epoch [1/5], Step [7920/10336], Loss: 1.9883\n",
      "Epoch [1/5], Step [7922/10336], Loss: 0.1270\n",
      "Epoch [1/5], Step [7924/10336], Loss: 1.2972\n",
      "Epoch [1/5], Step [7926/10336], Loss: 2.7510\n",
      "Epoch [1/5], Step [7928/10336], Loss: 0.9749\n",
      "Epoch [1/5], Step [7930/10336], Loss: 1.8647\n",
      "Epoch [1/5], Step [7932/10336], Loss: 0.8537\n",
      "Epoch [1/5], Step [7934/10336], Loss: 0.5978\n",
      "Epoch [1/5], Step [7936/10336], Loss: 0.7056\n",
      "Epoch [1/5], Step [7938/10336], Loss: 2.4440\n",
      "Epoch [1/5], Step [7940/10336], Loss: 0.2421\n",
      "Epoch [1/5], Step [7942/10336], Loss: 5.0527\n",
      "Epoch [1/5], Step [7944/10336], Loss: 2.3485\n",
      "Epoch [1/5], Step [7946/10336], Loss: 2.7632\n",
      "Epoch [1/5], Step [7948/10336], Loss: 2.1289\n",
      "Epoch [1/5], Step [7950/10336], Loss: 0.4273\n",
      "Epoch [1/5], Step [7952/10336], Loss: 2.6692\n",
      "Epoch [1/5], Step [7954/10336], Loss: 2.6707\n",
      "Epoch [1/5], Step [7956/10336], Loss: 0.8180\n",
      "Epoch [1/5], Step [7958/10336], Loss: 1.8369\n",
      "Epoch [1/5], Step [7960/10336], Loss: 0.0773\n",
      "Epoch [1/5], Step [7962/10336], Loss: 1.0579\n",
      "Epoch [1/5], Step [7964/10336], Loss: 0.8290\n",
      "Epoch [1/5], Step [7966/10336], Loss: 2.6030\n",
      "Epoch [1/5], Step [7968/10336], Loss: 3.9297\n",
      "Epoch [1/5], Step [7970/10336], Loss: 1.2531\n",
      "Epoch [1/5], Step [7972/10336], Loss: 0.1280\n",
      "Epoch [1/5], Step [7974/10336], Loss: 1.3581\n",
      "Epoch [1/5], Step [7976/10336], Loss: 0.5278\n",
      "Epoch [1/5], Step [7978/10336], Loss: 0.9088\n",
      "Epoch [1/5], Step [7980/10336], Loss: 0.5873\n",
      "Epoch [1/5], Step [7982/10336], Loss: 0.3564\n",
      "Epoch [1/5], Step [7984/10336], Loss: 0.8635\n",
      "Epoch [1/5], Step [7986/10336], Loss: 3.6910\n",
      "Epoch [1/5], Step [7988/10336], Loss: 3.5349\n",
      "Epoch [1/5], Step [7990/10336], Loss: 0.4221\n",
      "Epoch [1/5], Step [7992/10336], Loss: 1.4076\n",
      "Epoch [1/5], Step [7994/10336], Loss: 1.7565\n",
      "Epoch [1/5], Step [7996/10336], Loss: 0.1590\n",
      "Epoch [1/5], Step [7998/10336], Loss: 1.4708\n",
      "Epoch [1/5], Step [8000/10336], Loss: 2.3629\n",
      "Epoch [1/5], Step [8002/10336], Loss: 0.7995\n",
      "Epoch [1/5], Step [8004/10336], Loss: 1.2821\n",
      "Epoch [1/5], Step [8006/10336], Loss: 3.2905\n",
      "Epoch [1/5], Step [8008/10336], Loss: 2.5419\n",
      "Epoch [1/5], Step [8010/10336], Loss: 1.2280\n",
      "Epoch [1/5], Step [8012/10336], Loss: 1.7776\n",
      "Epoch [1/5], Step [8014/10336], Loss: 0.4340\n",
      "Epoch [1/5], Step [8016/10336], Loss: 1.3782\n",
      "Epoch [1/5], Step [8018/10336], Loss: 0.0480\n",
      "Epoch [1/5], Step [8020/10336], Loss: 2.1807\n",
      "Epoch [1/5], Step [8022/10336], Loss: 2.2067\n",
      "Epoch [1/5], Step [8024/10336], Loss: 1.7464\n",
      "Epoch [1/5], Step [8026/10336], Loss: 0.0124\n",
      "Epoch [1/5], Step [8028/10336], Loss: 0.1142\n",
      "Epoch [1/5], Step [8030/10336], Loss: 3.5057\n",
      "Epoch [1/5], Step [8032/10336], Loss: 0.7230\n",
      "Epoch [1/5], Step [8034/10336], Loss: 0.2253\n",
      "Epoch [1/5], Step [8036/10336], Loss: 0.0074\n",
      "Epoch [1/5], Step [8038/10336], Loss: 2.0051\n",
      "Epoch [1/5], Step [8040/10336], Loss: 0.8250\n",
      "Epoch [1/5], Step [8042/10336], Loss: 1.0222\n",
      "Epoch [1/5], Step [8044/10336], Loss: 0.8864\n",
      "Epoch [1/5], Step [8046/10336], Loss: 1.6337\n",
      "Epoch [1/5], Step [8048/10336], Loss: 2.9194\n",
      "Epoch [1/5], Step [8050/10336], Loss: 0.2290\n",
      "Epoch [1/5], Step [8052/10336], Loss: 1.2760\n",
      "Epoch [1/5], Step [8054/10336], Loss: 0.8788\n",
      "Epoch [1/5], Step [8056/10336], Loss: 2.0008\n",
      "Epoch [1/5], Step [8058/10336], Loss: 0.0807\n",
      "Epoch [1/5], Step [8060/10336], Loss: 3.3557\n",
      "Epoch [1/5], Step [8062/10336], Loss: 1.2656\n",
      "Epoch [1/5], Step [8064/10336], Loss: 0.2842\n",
      "Epoch [1/5], Step [8066/10336], Loss: 0.5494\n",
      "Epoch [1/5], Step [8068/10336], Loss: 2.9769\n",
      "Epoch [1/5], Step [8070/10336], Loss: 0.6171\n",
      "Epoch [1/5], Step [8072/10336], Loss: 2.1811\n",
      "Epoch [1/5], Step [8074/10336], Loss: 0.7840\n",
      "Epoch [1/5], Step [8076/10336], Loss: 0.1134\n",
      "Epoch [1/5], Step [8078/10336], Loss: 0.4181\n",
      "Epoch [1/5], Step [8080/10336], Loss: 1.7274\n",
      "Epoch [1/5], Step [8082/10336], Loss: 0.6698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [8084/10336], Loss: 1.8809\n",
      "Epoch [1/5], Step [8086/10336], Loss: 5.0496\n",
      "Epoch [1/5], Step [8088/10336], Loss: 0.1645\n",
      "Epoch [1/5], Step [8090/10336], Loss: 1.5716\n",
      "Epoch [1/5], Step [8092/10336], Loss: 0.5259\n",
      "Epoch [1/5], Step [8094/10336], Loss: 0.9196\n",
      "Epoch [1/5], Step [8096/10336], Loss: 0.9304\n",
      "Epoch [1/5], Step [8098/10336], Loss: 0.0863\n",
      "Epoch [1/5], Step [8100/10336], Loss: 0.7219\n",
      "Epoch [1/5], Step [8102/10336], Loss: 0.2508\n",
      "Epoch [1/5], Step [8104/10336], Loss: 1.3177\n",
      "Epoch [1/5], Step [8106/10336], Loss: 1.3626\n",
      "Epoch [1/5], Step [8108/10336], Loss: 1.1170\n",
      "Epoch [1/5], Step [8110/10336], Loss: 0.3764\n",
      "Epoch [1/5], Step [8112/10336], Loss: 1.3472\n",
      "Epoch [1/5], Step [8114/10336], Loss: 0.2435\n",
      "Epoch [1/5], Step [8116/10336], Loss: 0.6156\n",
      "Epoch [1/5], Step [8118/10336], Loss: 1.3665\n",
      "Epoch [1/5], Step [8120/10336], Loss: 1.0320\n",
      "Epoch [1/5], Step [8122/10336], Loss: 0.2306\n",
      "Epoch [1/5], Step [8124/10336], Loss: 1.2375\n",
      "Epoch [1/5], Step [8126/10336], Loss: 1.6705\n",
      "Epoch [1/5], Step [8128/10336], Loss: 0.3870\n",
      "Epoch [1/5], Step [8130/10336], Loss: 1.2285\n",
      "Epoch [1/5], Step [8132/10336], Loss: 0.0206\n",
      "Epoch [1/5], Step [8134/10336], Loss: 0.6391\n",
      "Epoch [1/5], Step [8136/10336], Loss: 0.1250\n",
      "Epoch [1/5], Step [8138/10336], Loss: 0.7634\n",
      "Epoch [1/5], Step [8140/10336], Loss: 0.6004\n",
      "Epoch [1/5], Step [8142/10336], Loss: 0.5597\n",
      "Epoch [1/5], Step [8144/10336], Loss: 1.1486\n",
      "Epoch [1/5], Step [8146/10336], Loss: 0.4057\n",
      "Epoch [1/5], Step [8148/10336], Loss: 0.1406\n",
      "Epoch [1/5], Step [8150/10336], Loss: 1.8645\n",
      "Epoch [1/5], Step [8152/10336], Loss: 0.0278\n",
      "Epoch [1/5], Step [8154/10336], Loss: 0.6359\n",
      "Epoch [1/5], Step [8156/10336], Loss: 2.8387\n",
      "Epoch [1/5], Step [8158/10336], Loss: 3.9990\n",
      "Epoch [1/5], Step [8160/10336], Loss: 0.4958\n",
      "Epoch [1/5], Step [8162/10336], Loss: 0.5859\n",
      "Epoch [1/5], Step [8164/10336], Loss: 4.3152\n",
      "Epoch [1/5], Step [8166/10336], Loss: 0.8469\n",
      "Epoch [1/5], Step [8168/10336], Loss: 0.2330\n",
      "Epoch [1/5], Step [8170/10336], Loss: 0.3080\n",
      "Epoch [1/5], Step [8172/10336], Loss: 1.3738\n",
      "Epoch [1/5], Step [8174/10336], Loss: 0.9535\n",
      "Epoch [1/5], Step [8176/10336], Loss: 0.6167\n",
      "Epoch [1/5], Step [8178/10336], Loss: 1.9270\n",
      "Epoch [1/5], Step [8180/10336], Loss: 2.5437\n",
      "Epoch [1/5], Step [8182/10336], Loss: 1.3619\n",
      "Epoch [1/5], Step [8184/10336], Loss: 0.3679\n",
      "Epoch [1/5], Step [8186/10336], Loss: 0.2239\n",
      "Epoch [1/5], Step [8188/10336], Loss: 0.4489\n",
      "Epoch [1/5], Step [8190/10336], Loss: 0.0522\n",
      "Epoch [1/5], Step [8192/10336], Loss: 0.1050\n",
      "Epoch [1/5], Step [8194/10336], Loss: 0.8256\n",
      "Epoch [1/5], Step [8196/10336], Loss: 0.0533\n",
      "Epoch [1/5], Step [8198/10336], Loss: 0.5436\n",
      "Epoch [1/5], Step [8200/10336], Loss: 2.7052\n",
      "Epoch [1/5], Step [8202/10336], Loss: 1.0477\n",
      "Epoch [1/5], Step [8204/10336], Loss: 0.6148\n",
      "Epoch [1/5], Step [8206/10336], Loss: 0.2202\n",
      "Epoch [1/5], Step [8208/10336], Loss: 1.9809\n",
      "Epoch [1/5], Step [8210/10336], Loss: 0.7900\n",
      "Epoch [1/5], Step [8212/10336], Loss: 1.5802\n",
      "Epoch [1/5], Step [8214/10336], Loss: 1.7976\n",
      "Epoch [1/5], Step [8216/10336], Loss: 1.3571\n",
      "Epoch [1/5], Step [8218/10336], Loss: 3.2005\n",
      "Epoch [1/5], Step [8220/10336], Loss: 1.8400\n",
      "Epoch [1/5], Step [8222/10336], Loss: 0.3129\n",
      "Epoch [1/5], Step [8224/10336], Loss: 0.0200\n",
      "Epoch [1/5], Step [8226/10336], Loss: 2.4482\n",
      "Epoch [1/5], Step [8228/10336], Loss: 0.0492\n",
      "Epoch [1/5], Step [8230/10336], Loss: 0.7129\n",
      "Epoch [1/5], Step [8232/10336], Loss: 0.2685\n",
      "Epoch [1/5], Step [8234/10336], Loss: 1.0421\n",
      "Epoch [1/5], Step [8236/10336], Loss: 0.4902\n",
      "Epoch [1/5], Step [8238/10336], Loss: 2.5841\n",
      "Epoch [1/5], Step [8240/10336], Loss: 0.2059\n",
      "Epoch [1/5], Step [8242/10336], Loss: 0.3073\n",
      "Epoch [1/5], Step [8244/10336], Loss: 1.2750\n",
      "Epoch [1/5], Step [8246/10336], Loss: 2.5393\n",
      "Epoch [1/5], Step [8248/10336], Loss: 2.3234\n",
      "Epoch [1/5], Step [8250/10336], Loss: 1.5810\n",
      "Epoch [1/5], Step [8252/10336], Loss: 0.3925\n",
      "Epoch [1/5], Step [8254/10336], Loss: 1.4671\n",
      "Epoch [1/5], Step [8256/10336], Loss: 0.0990\n",
      "Epoch [1/5], Step [8258/10336], Loss: 0.6249\n",
      "Epoch [1/5], Step [8260/10336], Loss: 2.7517\n",
      "Epoch [1/5], Step [8262/10336], Loss: 0.2774\n",
      "Epoch [1/5], Step [8264/10336], Loss: 1.1198\n",
      "Epoch [1/5], Step [8266/10336], Loss: 1.9421\n",
      "Epoch [1/5], Step [8268/10336], Loss: 0.0662\n",
      "Epoch [1/5], Step [8270/10336], Loss: 0.0440\n",
      "Epoch [1/5], Step [8272/10336], Loss: 0.2855\n",
      "Epoch [1/5], Step [8274/10336], Loss: 0.1140\n",
      "Epoch [1/5], Step [8276/10336], Loss: 0.3627\n",
      "Epoch [1/5], Step [8278/10336], Loss: 2.5203\n",
      "Epoch [1/5], Step [8280/10336], Loss: 0.3580\n",
      "Epoch [1/5], Step [8282/10336], Loss: 0.0907\n",
      "Epoch [1/5], Step [8284/10336], Loss: 0.4791\n",
      "Epoch [1/5], Step [8286/10336], Loss: 0.0257\n",
      "Epoch [1/5], Step [8288/10336], Loss: 0.1484\n",
      "Epoch [1/5], Step [8290/10336], Loss: 0.0779\n",
      "Epoch [1/5], Step [8292/10336], Loss: 0.0880\n",
      "Epoch [1/5], Step [8294/10336], Loss: 0.1938\n",
      "Epoch [1/5], Step [8296/10336], Loss: 0.6266\n",
      "Epoch [1/5], Step [8298/10336], Loss: 0.7961\n",
      "Epoch [1/5], Step [8300/10336], Loss: 1.2427\n",
      "Epoch [1/5], Step [8302/10336], Loss: 2.1414\n",
      "Epoch [1/5], Step [8304/10336], Loss: 0.8829\n",
      "Epoch [1/5], Step [8306/10336], Loss: 1.5623\n",
      "Epoch [1/5], Step [8308/10336], Loss: 2.4820\n",
      "Epoch [1/5], Step [8310/10336], Loss: 2.6806\n",
      "Epoch [1/5], Step [8312/10336], Loss: 0.8003\n",
      "Epoch [1/5], Step [8314/10336], Loss: 2.3379\n",
      "Epoch [1/5], Step [8316/10336], Loss: 3.8824\n",
      "Epoch [1/5], Step [8318/10336], Loss: 0.4556\n",
      "Epoch [1/5], Step [8320/10336], Loss: 0.3647\n",
      "Epoch [1/5], Step [8322/10336], Loss: 1.4595\n",
      "Epoch [1/5], Step [8324/10336], Loss: 1.0139\n",
      "Epoch [1/5], Step [8326/10336], Loss: 0.0347\n",
      "Epoch [1/5], Step [8328/10336], Loss: 0.7803\n",
      "Epoch [1/5], Step [8330/10336], Loss: 0.0226\n",
      "Epoch [1/5], Step [8332/10336], Loss: 0.2764\n",
      "Epoch [1/5], Step [8334/10336], Loss: 1.0396\n",
      "Epoch [1/5], Step [8336/10336], Loss: 1.2667\n",
      "Epoch [1/5], Step [8338/10336], Loss: 1.8112\n",
      "Epoch [1/5], Step [8340/10336], Loss: 1.9154\n",
      "Epoch [1/5], Step [8342/10336], Loss: 0.7776\n",
      "Epoch [1/5], Step [8344/10336], Loss: 0.4499\n",
      "Epoch [1/5], Step [8346/10336], Loss: 1.2569\n",
      "Epoch [1/5], Step [8348/10336], Loss: 2.1052\n",
      "Epoch [1/5], Step [8350/10336], Loss: 0.8289\n",
      "Epoch [1/5], Step [8352/10336], Loss: 0.9023\n",
      "Epoch [1/5], Step [8354/10336], Loss: 2.9029\n",
      "Epoch [1/5], Step [8356/10336], Loss: 2.8392\n",
      "Epoch [1/5], Step [8358/10336], Loss: 0.4703\n",
      "Epoch [1/5], Step [8360/10336], Loss: 1.2332\n",
      "Epoch [1/5], Step [8362/10336], Loss: 0.7423\n",
      "Epoch [1/5], Step [8364/10336], Loss: 1.1163\n",
      "Epoch [1/5], Step [8366/10336], Loss: 0.8159\n",
      "Epoch [1/5], Step [8368/10336], Loss: 2.0697\n",
      "Epoch [1/5], Step [8370/10336], Loss: 0.0841\n",
      "Epoch [1/5], Step [8372/10336], Loss: 0.2636\n",
      "Epoch [1/5], Step [8374/10336], Loss: 0.7543\n",
      "Epoch [1/5], Step [8376/10336], Loss: 0.0665\n",
      "Epoch [1/5], Step [8378/10336], Loss: 1.9084\n",
      "Epoch [1/5], Step [8380/10336], Loss: 0.6717\n",
      "Epoch [1/5], Step [8382/10336], Loss: 1.7469\n",
      "Epoch [1/5], Step [8384/10336], Loss: 0.0427\n",
      "Epoch [1/5], Step [8386/10336], Loss: 0.2147\n",
      "Epoch [1/5], Step [8388/10336], Loss: 3.9598\n",
      "Epoch [1/5], Step [8390/10336], Loss: 0.1073\n",
      "Epoch [1/5], Step [8392/10336], Loss: 0.2213\n",
      "Epoch [1/5], Step [8394/10336], Loss: 1.1123\n",
      "Epoch [1/5], Step [8396/10336], Loss: 2.2790\n",
      "Epoch [1/5], Step [8398/10336], Loss: 2.4552\n",
      "Epoch [1/5], Step [8400/10336], Loss: 1.3668\n",
      "Epoch [1/5], Step [8402/10336], Loss: 0.0543\n",
      "Epoch [1/5], Step [8404/10336], Loss: 1.3457\n",
      "Epoch [1/5], Step [8406/10336], Loss: 0.2279\n",
      "Epoch [1/5], Step [8408/10336], Loss: 2.4155\n",
      "Epoch [1/5], Step [8410/10336], Loss: 0.3519\n",
      "Epoch [1/5], Step [8412/10336], Loss: 1.3366\n",
      "Epoch [1/5], Step [8414/10336], Loss: 0.8927\n",
      "Epoch [1/5], Step [8416/10336], Loss: 0.9200\n",
      "Epoch [1/5], Step [8418/10336], Loss: 0.3860\n",
      "Epoch [1/5], Step [8420/10336], Loss: 0.5156\n",
      "Epoch [1/5], Step [8422/10336], Loss: 3.4422\n",
      "Epoch [1/5], Step [8424/10336], Loss: 2.5672\n",
      "Epoch [1/5], Step [8426/10336], Loss: 2.6257\n",
      "Epoch [1/5], Step [8428/10336], Loss: 0.4113\n",
      "Epoch [1/5], Step [8430/10336], Loss: 1.4230\n",
      "Epoch [1/5], Step [8432/10336], Loss: 3.1237\n",
      "Epoch [1/5], Step [8434/10336], Loss: 1.7616\n",
      "Epoch [1/5], Step [8436/10336], Loss: 3.2836\n",
      "Epoch [1/5], Step [8438/10336], Loss: 1.8805\n",
      "Epoch [1/5], Step [8440/10336], Loss: 0.1779\n",
      "Epoch [1/5], Step [8442/10336], Loss: 0.5293\n",
      "Epoch [1/5], Step [8444/10336], Loss: 2.0061\n",
      "Epoch [1/5], Step [8446/10336], Loss: 0.8810\n",
      "Epoch [1/5], Step [8448/10336], Loss: 0.0777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [8450/10336], Loss: 1.5053\n",
      "Epoch [1/5], Step [8452/10336], Loss: 1.0113\n",
      "Epoch [1/5], Step [8454/10336], Loss: 2.0346\n",
      "Epoch [1/5], Step [8456/10336], Loss: 0.2221\n",
      "Epoch [1/5], Step [8458/10336], Loss: 1.9281\n",
      "Epoch [1/5], Step [8460/10336], Loss: 0.6733\n",
      "Epoch [1/5], Step [8462/10336], Loss: 0.1207\n",
      "Epoch [1/5], Step [8464/10336], Loss: 0.0876\n",
      "Epoch [1/5], Step [8466/10336], Loss: 0.6950\n",
      "Epoch [1/5], Step [8468/10336], Loss: 0.8514\n",
      "Epoch [1/5], Step [8470/10336], Loss: 1.4430\n",
      "Epoch [1/5], Step [8472/10336], Loss: 0.0551\n",
      "Epoch [1/5], Step [8474/10336], Loss: 0.1372\n",
      "Epoch [1/5], Step [8476/10336], Loss: 1.9558\n",
      "Epoch [1/5], Step [8478/10336], Loss: 3.1082\n",
      "Epoch [1/5], Step [8480/10336], Loss: 0.3591\n",
      "Epoch [1/5], Step [8482/10336], Loss: 4.4855\n",
      "Epoch [1/5], Step [8484/10336], Loss: 0.3098\n",
      "Epoch [1/5], Step [8486/10336], Loss: 0.0444\n",
      "Epoch [1/5], Step [8488/10336], Loss: 0.6338\n",
      "Epoch [1/5], Step [8490/10336], Loss: 1.6744\n",
      "Epoch [1/5], Step [8492/10336], Loss: 0.0630\n",
      "Epoch [1/5], Step [8494/10336], Loss: 1.7959\n",
      "Epoch [1/5], Step [8496/10336], Loss: 0.4962\n",
      "Epoch [1/5], Step [8498/10336], Loss: 1.4246\n",
      "Epoch [1/5], Step [8500/10336], Loss: 0.2926\n",
      "Epoch [1/5], Step [8502/10336], Loss: 0.0641\n",
      "Epoch [1/5], Step [8504/10336], Loss: 2.3076\n",
      "Epoch [1/5], Step [8506/10336], Loss: 0.1522\n",
      "Epoch [1/5], Step [8508/10336], Loss: 1.8287\n",
      "Epoch [1/5], Step [8510/10336], Loss: 0.1303\n",
      "Epoch [1/5], Step [8512/10336], Loss: 2.0415\n",
      "Epoch [1/5], Step [8514/10336], Loss: 4.7247\n",
      "Epoch [1/5], Step [8516/10336], Loss: 1.7642\n",
      "Epoch [1/5], Step [8518/10336], Loss: 2.7761\n",
      "Epoch [1/5], Step [8520/10336], Loss: 2.5656\n",
      "Epoch [1/5], Step [8522/10336], Loss: 0.9747\n",
      "Epoch [1/5], Step [8524/10336], Loss: 1.1638\n",
      "Epoch [1/5], Step [8526/10336], Loss: 0.6462\n",
      "Epoch [1/5], Step [8528/10336], Loss: 2.1712\n",
      "Epoch [1/5], Step [8530/10336], Loss: 0.3660\n",
      "Epoch [1/5], Step [8532/10336], Loss: 0.8149\n",
      "Epoch [1/5], Step [8534/10336], Loss: 0.8471\n",
      "Epoch [1/5], Step [8536/10336], Loss: 0.0979\n",
      "Epoch [1/5], Step [8538/10336], Loss: 0.3866\n",
      "Epoch [1/5], Step [8540/10336], Loss: 1.1370\n",
      "Epoch [1/5], Step [8542/10336], Loss: 0.7010\n",
      "Epoch [1/5], Step [8544/10336], Loss: 0.6063\n",
      "Epoch [1/5], Step [8546/10336], Loss: 0.8185\n",
      "Epoch [1/5], Step [8548/10336], Loss: 0.3735\n",
      "Epoch [1/5], Step [8550/10336], Loss: 3.3531\n",
      "Epoch [1/5], Step [8552/10336], Loss: 0.1499\n",
      "Epoch [1/5], Step [8554/10336], Loss: 1.0322\n",
      "Epoch [1/5], Step [8556/10336], Loss: 1.3852\n",
      "Epoch [1/5], Step [8558/10336], Loss: 0.7637\n",
      "Epoch [1/5], Step [8560/10336], Loss: 0.3753\n",
      "Epoch [1/5], Step [8562/10336], Loss: 0.1114\n",
      "Epoch [1/5], Step [8564/10336], Loss: 1.5062\n",
      "Epoch [1/5], Step [8566/10336], Loss: 1.2929\n",
      "Epoch [1/5], Step [8568/10336], Loss: 0.5115\n",
      "Epoch [1/5], Step [8570/10336], Loss: 0.1494\n",
      "Epoch [1/5], Step [8572/10336], Loss: 3.2651\n",
      "Epoch [1/5], Step [8574/10336], Loss: 0.9485\n",
      "Epoch [1/5], Step [8576/10336], Loss: 1.2621\n",
      "Epoch [1/5], Step [8578/10336], Loss: 0.3990\n",
      "Epoch [1/5], Step [8580/10336], Loss: 1.0230\n",
      "Epoch [1/5], Step [8582/10336], Loss: 1.6863\n",
      "Epoch [1/5], Step [8584/10336], Loss: 0.9995\n",
      "Epoch [1/5], Step [8586/10336], Loss: 1.8696\n",
      "Epoch [1/5], Step [8588/10336], Loss: 0.0733\n",
      "Epoch [1/5], Step [8590/10336], Loss: 1.6984\n",
      "Epoch [1/5], Step [8592/10336], Loss: 0.6091\n",
      "Epoch [1/5], Step [8594/10336], Loss: 0.4403\n",
      "Epoch [1/5], Step [8596/10336], Loss: 1.0739\n",
      "Epoch [1/5], Step [8598/10336], Loss: 0.1035\n",
      "Epoch [1/5], Step [8600/10336], Loss: 2.4278\n",
      "Epoch [1/5], Step [8602/10336], Loss: 1.2808\n",
      "Epoch [1/5], Step [8604/10336], Loss: 0.1561\n",
      "Epoch [1/5], Step [8606/10336], Loss: 0.0498\n",
      "Epoch [1/5], Step [8608/10336], Loss: 0.0320\n",
      "Epoch [1/5], Step [8610/10336], Loss: 0.1380\n",
      "Epoch [1/5], Step [8612/10336], Loss: 0.7123\n",
      "Epoch [1/5], Step [8614/10336], Loss: 0.2340\n",
      "Epoch [1/5], Step [8616/10336], Loss: 0.1390\n",
      "Epoch [1/5], Step [8618/10336], Loss: 0.2409\n",
      "Epoch [1/5], Step [8620/10336], Loss: 2.0511\n",
      "Epoch [1/5], Step [8622/10336], Loss: 0.4070\n",
      "Epoch [1/5], Step [8624/10336], Loss: 1.1869\n",
      "Epoch [1/5], Step [8626/10336], Loss: 0.1853\n",
      "Epoch [1/5], Step [8628/10336], Loss: 0.0634\n",
      "Epoch [1/5], Step [8630/10336], Loss: 0.2881\n",
      "Epoch [1/5], Step [8632/10336], Loss: 2.1114\n",
      "Epoch [1/5], Step [8634/10336], Loss: 0.3812\n",
      "Epoch [1/5], Step [8636/10336], Loss: 2.8579\n",
      "Epoch [1/5], Step [8638/10336], Loss: 0.6960\n",
      "Epoch [1/5], Step [8640/10336], Loss: 1.6529\n",
      "Epoch [1/5], Step [8642/10336], Loss: 2.0892\n",
      "Epoch [1/5], Step [8644/10336], Loss: 1.0555\n",
      "Epoch [1/5], Step [8646/10336], Loss: 0.4108\n",
      "Epoch [1/5], Step [8648/10336], Loss: 0.1059\n",
      "Epoch [1/5], Step [8650/10336], Loss: 0.4726\n",
      "Epoch [1/5], Step [8652/10336], Loss: 1.9422\n",
      "Epoch [1/5], Step [8654/10336], Loss: 1.2673\n",
      "Epoch [1/5], Step [8656/10336], Loss: 1.5837\n",
      "Epoch [1/5], Step [8658/10336], Loss: 3.2544\n",
      "Epoch [1/5], Step [8660/10336], Loss: 0.0704\n",
      "Epoch [1/5], Step [8662/10336], Loss: 0.8101\n",
      "Epoch [1/5], Step [8664/10336], Loss: 0.7068\n",
      "Epoch [1/5], Step [8666/10336], Loss: 1.1306\n",
      "Epoch [1/5], Step [8668/10336], Loss: 1.9602\n",
      "Epoch [1/5], Step [8670/10336], Loss: 0.7768\n",
      "Epoch [1/5], Step [8672/10336], Loss: 1.6685\n",
      "Epoch [1/5], Step [8674/10336], Loss: 0.7441\n",
      "Epoch [1/5], Step [8676/10336], Loss: 2.9849\n",
      "Epoch [1/5], Step [8678/10336], Loss: 0.1427\n",
      "Epoch [1/5], Step [8680/10336], Loss: 2.5607\n",
      "Epoch [1/5], Step [8682/10336], Loss: 0.8876\n",
      "Epoch [1/5], Step [8684/10336], Loss: 1.6596\n",
      "Epoch [1/5], Step [8686/10336], Loss: 1.6619\n",
      "Epoch [1/5], Step [8688/10336], Loss: 0.3004\n",
      "Epoch [1/5], Step [8690/10336], Loss: 3.5388\n",
      "Epoch [1/5], Step [8692/10336], Loss: 2.6183\n",
      "Epoch [1/5], Step [8694/10336], Loss: 0.4716\n",
      "Epoch [1/5], Step [8696/10336], Loss: 0.1374\n",
      "Epoch [1/5], Step [8698/10336], Loss: 0.1025\n",
      "Epoch [1/5], Step [8700/10336], Loss: 0.8789\n",
      "Epoch [1/5], Step [8702/10336], Loss: 0.6416\n",
      "Epoch [1/5], Step [8704/10336], Loss: 0.7267\n",
      "Epoch [1/5], Step [8706/10336], Loss: 0.9474\n",
      "Epoch [1/5], Step [8708/10336], Loss: 1.4555\n",
      "Epoch [1/5], Step [8710/10336], Loss: 0.6830\n",
      "Epoch [1/5], Step [8712/10336], Loss: 0.2565\n",
      "Epoch [1/5], Step [8714/10336], Loss: 1.3473\n",
      "Epoch [1/5], Step [8716/10336], Loss: 2.5712\n",
      "Epoch [1/5], Step [8718/10336], Loss: 0.0313\n",
      "Epoch [1/5], Step [8720/10336], Loss: 2.4427\n",
      "Epoch [1/5], Step [8722/10336], Loss: 0.6522\n",
      "Epoch [1/5], Step [8724/10336], Loss: 1.7779\n",
      "Epoch [1/5], Step [8726/10336], Loss: 0.8638\n",
      "Epoch [1/5], Step [8728/10336], Loss: 0.0613\n",
      "Epoch [1/5], Step [8730/10336], Loss: 2.1580\n",
      "Epoch [1/5], Step [8732/10336], Loss: 1.5669\n",
      "Epoch [1/5], Step [8734/10336], Loss: 0.2246\n",
      "Epoch [1/5], Step [8736/10336], Loss: 1.1662\n",
      "Epoch [1/5], Step [8738/10336], Loss: 0.1808\n",
      "Epoch [1/5], Step [8740/10336], Loss: 2.3797\n",
      "Epoch [1/5], Step [8742/10336], Loss: 0.0803\n",
      "Epoch [1/5], Step [8744/10336], Loss: 0.2500\n",
      "Epoch [1/5], Step [8746/10336], Loss: 3.2408\n",
      "Epoch [1/5], Step [8748/10336], Loss: 0.0788\n",
      "Epoch [1/5], Step [8750/10336], Loss: 0.7087\n",
      "Epoch [1/5], Step [8752/10336], Loss: 0.3299\n",
      "Epoch [1/5], Step [8754/10336], Loss: 2.1035\n",
      "Epoch [1/5], Step [8756/10336], Loss: 0.1818\n",
      "Epoch [1/5], Step [8758/10336], Loss: 0.3811\n",
      "Epoch [1/5], Step [8760/10336], Loss: 1.3105\n",
      "Epoch [1/5], Step [8762/10336], Loss: 0.5571\n",
      "Epoch [1/5], Step [8764/10336], Loss: 0.3956\n",
      "Epoch [1/5], Step [8766/10336], Loss: 2.8369\n",
      "Epoch [1/5], Step [8768/10336], Loss: 5.2169\n",
      "Epoch [1/5], Step [8770/10336], Loss: 2.7072\n",
      "Epoch [1/5], Step [8772/10336], Loss: 1.9784\n",
      "Epoch [1/5], Step [8774/10336], Loss: 3.5392\n",
      "Epoch [1/5], Step [8776/10336], Loss: 1.5705\n",
      "Epoch [1/5], Step [8778/10336], Loss: 0.7576\n",
      "Epoch [1/5], Step [8780/10336], Loss: 1.4068\n",
      "Epoch [1/5], Step [8782/10336], Loss: 2.3754\n",
      "Epoch [1/5], Step [8784/10336], Loss: 2.8897\n",
      "Epoch [1/5], Step [8786/10336], Loss: 1.4738\n",
      "Epoch [1/5], Step [8788/10336], Loss: 0.0805\n",
      "Epoch [1/5], Step [8790/10336], Loss: 0.4327\n",
      "Epoch [1/5], Step [8792/10336], Loss: 1.4665\n",
      "Epoch [1/5], Step [8794/10336], Loss: 1.3142\n",
      "Epoch [1/5], Step [8796/10336], Loss: 1.5745\n",
      "Epoch [1/5], Step [8798/10336], Loss: 0.1925\n",
      "Epoch [1/5], Step [8800/10336], Loss: 0.5729\n",
      "Epoch [1/5], Step [8802/10336], Loss: 0.2327\n",
      "Epoch [1/5], Step [8804/10336], Loss: 2.4337\n",
      "Epoch [1/5], Step [8806/10336], Loss: 0.0297\n",
      "Epoch [1/5], Step [8808/10336], Loss: 1.4230\n",
      "Epoch [1/5], Step [8810/10336], Loss: 1.5736\n",
      "Epoch [1/5], Step [8812/10336], Loss: 1.3264\n",
      "Epoch [1/5], Step [8814/10336], Loss: 0.4527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [8816/10336], Loss: 2.4046\n",
      "Epoch [1/5], Step [8818/10336], Loss: 1.2727\n",
      "Epoch [1/5], Step [8820/10336], Loss: 0.3343\n",
      "Epoch [1/5], Step [8822/10336], Loss: 1.2982\n",
      "Epoch [1/5], Step [8824/10336], Loss: 0.2837\n",
      "Epoch [1/5], Step [8826/10336], Loss: 1.7315\n",
      "Epoch [1/5], Step [8828/10336], Loss: 1.1095\n",
      "Epoch [1/5], Step [8830/10336], Loss: 0.0668\n",
      "Epoch [1/5], Step [8832/10336], Loss: 0.2918\n",
      "Epoch [1/5], Step [8834/10336], Loss: 0.0404\n",
      "Epoch [1/5], Step [8836/10336], Loss: 0.4740\n",
      "Epoch [1/5], Step [8838/10336], Loss: 0.6540\n",
      "Epoch [1/5], Step [8840/10336], Loss: 0.7268\n",
      "Epoch [1/5], Step [8842/10336], Loss: 0.6825\n",
      "Epoch [1/5], Step [8844/10336], Loss: 1.3469\n",
      "Epoch [1/5], Step [8846/10336], Loss: 2.0362\n",
      "Epoch [1/5], Step [8848/10336], Loss: 1.9741\n",
      "Epoch [1/5], Step [8850/10336], Loss: 0.0386\n",
      "Epoch [1/5], Step [8852/10336], Loss: 3.0541\n",
      "Epoch [1/5], Step [8854/10336], Loss: 1.8397\n",
      "Epoch [1/5], Step [8856/10336], Loss: 1.8190\n",
      "Epoch [1/5], Step [8858/10336], Loss: 4.0869\n",
      "Epoch [1/5], Step [8860/10336], Loss: 0.3662\n",
      "Epoch [1/5], Step [8862/10336], Loss: 0.2067\n",
      "Epoch [1/5], Step [8864/10336], Loss: 0.0632\n",
      "Epoch [1/5], Step [8866/10336], Loss: 0.6618\n",
      "Epoch [1/5], Step [8868/10336], Loss: 1.6120\n",
      "Epoch [1/5], Step [8870/10336], Loss: 0.9118\n",
      "Epoch [1/5], Step [8872/10336], Loss: 3.1040\n",
      "Epoch [1/5], Step [8874/10336], Loss: 2.5246\n",
      "Epoch [1/5], Step [8876/10336], Loss: 2.1762\n",
      "Epoch [1/5], Step [8878/10336], Loss: 0.1836\n",
      "Epoch [1/5], Step [8880/10336], Loss: 0.1341\n",
      "Epoch [1/5], Step [8882/10336], Loss: 2.8786\n",
      "Epoch [1/5], Step [8884/10336], Loss: 3.1185\n",
      "Epoch [1/5], Step [8886/10336], Loss: 0.1845\n",
      "Epoch [1/5], Step [8888/10336], Loss: 0.7837\n",
      "Epoch [1/5], Step [8890/10336], Loss: 1.5365\n",
      "Epoch [1/5], Step [8892/10336], Loss: 0.3090\n",
      "Epoch [1/5], Step [8894/10336], Loss: 0.0687\n",
      "Epoch [1/5], Step [8896/10336], Loss: 0.3742\n",
      "Epoch [1/5], Step [8898/10336], Loss: 0.0385\n",
      "Epoch [1/5], Step [8900/10336], Loss: 0.8438\n",
      "Epoch [1/5], Step [8902/10336], Loss: 0.1141\n",
      "Epoch [1/5], Step [8904/10336], Loss: 1.5065\n",
      "Epoch [1/5], Step [8906/10336], Loss: 0.1147\n",
      "Epoch [1/5], Step [8908/10336], Loss: 0.0243\n",
      "Epoch [1/5], Step [8910/10336], Loss: 1.8310\n",
      "Epoch [1/5], Step [8912/10336], Loss: 3.3957\n",
      "Epoch [1/5], Step [8914/10336], Loss: 0.1762\n",
      "Epoch [1/5], Step [8916/10336], Loss: 4.5007\n",
      "Epoch [1/5], Step [8918/10336], Loss: 0.3267\n",
      "Epoch [1/5], Step [8920/10336], Loss: 0.2389\n",
      "Epoch [1/5], Step [8922/10336], Loss: 0.4221\n",
      "Epoch [1/5], Step [8924/10336], Loss: 0.2373\n",
      "Epoch [1/5], Step [8926/10336], Loss: 0.0772\n",
      "Epoch [1/5], Step [8928/10336], Loss: 0.4320\n",
      "Epoch [1/5], Step [8930/10336], Loss: 1.7457\n",
      "Epoch [1/5], Step [8932/10336], Loss: 1.7151\n",
      "Epoch [1/5], Step [8934/10336], Loss: 3.6170\n",
      "Epoch [1/5], Step [8936/10336], Loss: 0.2369\n",
      "Epoch [1/5], Step [8938/10336], Loss: 0.9268\n",
      "Epoch [1/5], Step [8940/10336], Loss: 0.9695\n",
      "Epoch [1/5], Step [8942/10336], Loss: 1.5888\n",
      "Epoch [1/5], Step [8944/10336], Loss: 0.7653\n",
      "Epoch [1/5], Step [8946/10336], Loss: 0.1291\n",
      "Epoch [1/5], Step [8948/10336], Loss: 1.0210\n",
      "Epoch [1/5], Step [8950/10336], Loss: 0.5332\n",
      "Epoch [1/5], Step [8952/10336], Loss: 2.3209\n",
      "Epoch [1/5], Step [8954/10336], Loss: 1.4256\n",
      "Epoch [1/5], Step [8956/10336], Loss: 4.9616\n",
      "Epoch [1/5], Step [8958/10336], Loss: 1.7127\n",
      "Epoch [1/5], Step [8960/10336], Loss: 4.6783\n",
      "Epoch [1/5], Step [8962/10336], Loss: 0.2833\n",
      "Epoch [1/5], Step [8964/10336], Loss: 0.0695\n",
      "Epoch [1/5], Step [8966/10336], Loss: 0.6754\n",
      "Epoch [1/5], Step [8968/10336], Loss: 0.6313\n",
      "Epoch [1/5], Step [8970/10336], Loss: 0.9978\n",
      "Epoch [1/5], Step [8972/10336], Loss: 1.4441\n",
      "Epoch [1/5], Step [8974/10336], Loss: 0.8419\n",
      "Epoch [1/5], Step [8976/10336], Loss: 2.6967\n",
      "Epoch [1/5], Step [8978/10336], Loss: 0.0461\n",
      "Epoch [1/5], Step [8980/10336], Loss: 1.3573\n",
      "Epoch [1/5], Step [8982/10336], Loss: 0.5105\n",
      "Epoch [1/5], Step [8984/10336], Loss: 1.7664\n",
      "Epoch [1/5], Step [8986/10336], Loss: 2.4964\n",
      "Epoch [1/5], Step [8988/10336], Loss: 3.7542\n",
      "Epoch [1/5], Step [8990/10336], Loss: 0.7438\n",
      "Epoch [1/5], Step [8992/10336], Loss: 0.5987\n",
      "Epoch [1/5], Step [8994/10336], Loss: 0.6429\n",
      "Epoch [1/5], Step [8996/10336], Loss: 0.6694\n",
      "Epoch [1/5], Step [8998/10336], Loss: 0.2326\n",
      "Epoch [1/5], Step [9000/10336], Loss: 1.4063\n",
      "Epoch [1/5], Step [9002/10336], Loss: 0.5385\n",
      "Epoch [1/5], Step [9004/10336], Loss: 1.3396\n",
      "Epoch [1/5], Step [9006/10336], Loss: 0.0926\n",
      "Epoch [1/5], Step [9008/10336], Loss: 0.1668\n",
      "Epoch [1/5], Step [9010/10336], Loss: 0.2546\n",
      "Epoch [1/5], Step [9012/10336], Loss: 1.2712\n",
      "Epoch [1/5], Step [9014/10336], Loss: 2.0833\n",
      "Epoch [1/5], Step [9016/10336], Loss: 0.3940\n",
      "Epoch [1/5], Step [9018/10336], Loss: 2.1692\n",
      "Epoch [1/5], Step [9020/10336], Loss: 3.1089\n",
      "Epoch [1/5], Step [9022/10336], Loss: 0.2091\n",
      "Epoch [1/5], Step [9024/10336], Loss: 0.0524\n",
      "Epoch [1/5], Step [9026/10336], Loss: 0.1047\n",
      "Epoch [1/5], Step [9028/10336], Loss: 0.1780\n",
      "Epoch [1/5], Step [9030/10336], Loss: 1.4611\n",
      "Epoch [1/5], Step [9032/10336], Loss: 3.2133\n",
      "Epoch [1/5], Step [9034/10336], Loss: 0.3977\n",
      "Epoch [1/5], Step [9036/10336], Loss: 0.1131\n",
      "Epoch [1/5], Step [9038/10336], Loss: 2.6334\n",
      "Epoch [1/5], Step [9040/10336], Loss: 0.2543\n",
      "Epoch [1/5], Step [9042/10336], Loss: 1.7774\n",
      "Epoch [1/5], Step [9044/10336], Loss: 1.2108\n",
      "Epoch [1/5], Step [9046/10336], Loss: 2.4105\n",
      "Epoch [1/5], Step [9048/10336], Loss: 2.0200\n",
      "Epoch [1/5], Step [9050/10336], Loss: 0.7757\n",
      "Epoch [1/5], Step [9052/10336], Loss: 1.2810\n",
      "Epoch [1/5], Step [9054/10336], Loss: 1.1440\n",
      "Epoch [1/5], Step [9056/10336], Loss: 0.3532\n",
      "Epoch [1/5], Step [9058/10336], Loss: 0.4554\n",
      "Epoch [1/5], Step [9060/10336], Loss: 0.2555\n",
      "Epoch [1/5], Step [9062/10336], Loss: 1.4082\n",
      "Epoch [1/5], Step [9064/10336], Loss: 4.9692\n",
      "Epoch [1/5], Step [9066/10336], Loss: 3.6457\n",
      "Epoch [1/5], Step [9068/10336], Loss: 2.5151\n",
      "Epoch [1/5], Step [9070/10336], Loss: 2.3529\n",
      "Epoch [1/5], Step [9072/10336], Loss: 1.9108\n",
      "Epoch [1/5], Step [9074/10336], Loss: 0.0754\n",
      "Epoch [1/5], Step [9076/10336], Loss: 0.0516\n",
      "Epoch [1/5], Step [9078/10336], Loss: 0.5057\n",
      "Epoch [1/5], Step [9080/10336], Loss: 0.4625\n",
      "Epoch [1/5], Step [9082/10336], Loss: 0.7407\n",
      "Epoch [1/5], Step [9084/10336], Loss: 0.5244\n",
      "Epoch [1/5], Step [9086/10336], Loss: 0.8642\n",
      "Epoch [1/5], Step [9088/10336], Loss: 1.6114\n",
      "Epoch [1/5], Step [9090/10336], Loss: 2.9157\n",
      "Epoch [1/5], Step [9092/10336], Loss: 1.3763\n",
      "Epoch [1/5], Step [9094/10336], Loss: 1.8043\n",
      "Epoch [1/5], Step [9096/10336], Loss: 1.1094\n",
      "Epoch [1/5], Step [9098/10336], Loss: 0.5284\n",
      "Epoch [1/5], Step [9100/10336], Loss: 3.9957\n",
      "Epoch [1/5], Step [9102/10336], Loss: 1.5376\n",
      "Epoch [1/5], Step [9104/10336], Loss: 0.2867\n",
      "Epoch [1/5], Step [9106/10336], Loss: 1.2016\n",
      "Epoch [1/5], Step [9108/10336], Loss: 0.0801\n",
      "Epoch [1/5], Step [9110/10336], Loss: 0.0590\n",
      "Epoch [1/5], Step [9112/10336], Loss: 0.0970\n",
      "Epoch [1/5], Step [9114/10336], Loss: 0.0607\n",
      "Epoch [1/5], Step [9116/10336], Loss: 0.0664\n",
      "Epoch [1/5], Step [9118/10336], Loss: 1.3243\n",
      "Epoch [1/5], Step [9120/10336], Loss: 0.4697\n",
      "Epoch [1/5], Step [9122/10336], Loss: 0.7748\n",
      "Epoch [1/5], Step [9124/10336], Loss: 0.9986\n",
      "Epoch [1/5], Step [9126/10336], Loss: 1.0774\n",
      "Epoch [1/5], Step [9128/10336], Loss: 0.3491\n",
      "Epoch [1/5], Step [9130/10336], Loss: 0.7322\n",
      "Epoch [1/5], Step [9132/10336], Loss: 0.2164\n",
      "Epoch [1/5], Step [9134/10336], Loss: 0.2421\n",
      "Epoch [1/5], Step [9136/10336], Loss: 0.1719\n",
      "Epoch [1/5], Step [9138/10336], Loss: 0.1133\n",
      "Epoch [1/5], Step [9140/10336], Loss: 2.5092\n",
      "Epoch [1/5], Step [9142/10336], Loss: 0.3298\n",
      "Epoch [1/5], Step [9144/10336], Loss: 0.1084\n",
      "Epoch [1/5], Step [9146/10336], Loss: 0.8548\n",
      "Epoch [1/5], Step [9148/10336], Loss: 0.1935\n",
      "Epoch [1/5], Step [9150/10336], Loss: 0.3116\n",
      "Epoch [1/5], Step [9152/10336], Loss: 0.1248\n",
      "Epoch [1/5], Step [9154/10336], Loss: 1.3345\n",
      "Epoch [1/5], Step [9156/10336], Loss: 0.7060\n",
      "Epoch [1/5], Step [9158/10336], Loss: 1.5543\n",
      "Epoch [1/5], Step [9160/10336], Loss: 0.3800\n",
      "Epoch [1/5], Step [9162/10336], Loss: 1.7948\n",
      "Epoch [1/5], Step [9164/10336], Loss: 2.4067\n",
      "Epoch [1/5], Step [9166/10336], Loss: 1.3230\n",
      "Epoch [1/5], Step [9168/10336], Loss: 0.2187\n",
      "Epoch [1/5], Step [9170/10336], Loss: 0.1602\n",
      "Epoch [1/5], Step [9172/10336], Loss: 1.2536\n",
      "Epoch [1/5], Step [9174/10336], Loss: 1.6785\n",
      "Epoch [1/5], Step [9176/10336], Loss: 2.2253\n",
      "Epoch [1/5], Step [9178/10336], Loss: 0.5493\n",
      "Epoch [1/5], Step [9180/10336], Loss: 0.0650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [9182/10336], Loss: 0.2933\n",
      "Epoch [1/5], Step [9184/10336], Loss: 0.7778\n",
      "Epoch [1/5], Step [9186/10336], Loss: 1.0357\n",
      "Epoch [1/5], Step [9188/10336], Loss: 0.8929\n",
      "Epoch [1/5], Step [9190/10336], Loss: 0.8091\n",
      "Epoch [1/5], Step [9192/10336], Loss: 0.7708\n",
      "Epoch [1/5], Step [9194/10336], Loss: 0.1077\n",
      "Epoch [1/5], Step [9196/10336], Loss: 0.8429\n",
      "Epoch [1/5], Step [9198/10336], Loss: 0.4627\n",
      "Epoch [1/5], Step [9200/10336], Loss: 0.1483\n",
      "Epoch [1/5], Step [9202/10336], Loss: 1.4608\n",
      "Epoch [1/5], Step [9204/10336], Loss: 0.0404\n",
      "Epoch [1/5], Step [9206/10336], Loss: 1.4791\n",
      "Epoch [1/5], Step [9208/10336], Loss: 1.4130\n",
      "Epoch [1/5], Step [9210/10336], Loss: 5.5809\n",
      "Epoch [1/5], Step [9212/10336], Loss: 0.1595\n",
      "Epoch [1/5], Step [9214/10336], Loss: 0.0252\n",
      "Epoch [1/5], Step [9216/10336], Loss: 0.1457\n",
      "Epoch [1/5], Step [9218/10336], Loss: 2.0483\n",
      "Epoch [1/5], Step [9220/10336], Loss: 0.5130\n",
      "Epoch [1/5], Step [9222/10336], Loss: 1.0315\n",
      "Epoch [1/5], Step [9224/10336], Loss: 0.5315\n",
      "Epoch [1/5], Step [9226/10336], Loss: 0.5704\n",
      "Epoch [1/5], Step [9228/10336], Loss: 0.6274\n",
      "Epoch [1/5], Step [9230/10336], Loss: 1.8713\n",
      "Epoch [1/5], Step [9232/10336], Loss: 1.4514\n",
      "Epoch [1/5], Step [9234/10336], Loss: 0.6697\n",
      "Epoch [1/5], Step [9236/10336], Loss: 1.1136\n",
      "Epoch [1/5], Step [9238/10336], Loss: 1.3063\n",
      "Epoch [1/5], Step [9240/10336], Loss: 1.5213\n",
      "Epoch [1/5], Step [9242/10336], Loss: 0.5634\n",
      "Epoch [1/5], Step [9244/10336], Loss: 1.6066\n",
      "Epoch [1/5], Step [9246/10336], Loss: 0.3879\n",
      "Epoch [1/5], Step [9248/10336], Loss: 2.0308\n",
      "Epoch [1/5], Step [9250/10336], Loss: 3.3053\n",
      "Epoch [1/5], Step [9252/10336], Loss: 2.1412\n",
      "Epoch [1/5], Step [9254/10336], Loss: 0.3054\n",
      "Epoch [1/5], Step [9256/10336], Loss: 0.3490\n",
      "Epoch [1/5], Step [9258/10336], Loss: 2.2989\n",
      "Epoch [1/5], Step [9260/10336], Loss: 0.3932\n",
      "Epoch [1/5], Step [9262/10336], Loss: 3.0528\n",
      "Epoch [1/5], Step [9264/10336], Loss: 0.7008\n",
      "Epoch [1/5], Step [9266/10336], Loss: 2.4777\n",
      "Epoch [1/5], Step [9268/10336], Loss: 0.0156\n",
      "Epoch [1/5], Step [9270/10336], Loss: 0.6521\n",
      "Epoch [1/5], Step [9272/10336], Loss: 1.3757\n",
      "Epoch [1/5], Step [9274/10336], Loss: 2.0833\n",
      "Epoch [1/5], Step [9276/10336], Loss: 0.9109\n",
      "Epoch [1/5], Step [9278/10336], Loss: 0.1746\n",
      "Epoch [1/5], Step [9280/10336], Loss: 1.5088\n",
      "Epoch [1/5], Step [9282/10336], Loss: 1.1527\n",
      "Epoch [1/5], Step [9284/10336], Loss: 0.0408\n",
      "Epoch [1/5], Step [9286/10336], Loss: 2.3367\n",
      "Epoch [1/5], Step [9288/10336], Loss: 0.5386\n",
      "Epoch [1/5], Step [9290/10336], Loss: 2.9557\n",
      "Epoch [1/5], Step [9292/10336], Loss: 0.2471\n",
      "Epoch [1/5], Step [9294/10336], Loss: 0.1952\n",
      "Epoch [1/5], Step [9296/10336], Loss: 2.0432\n",
      "Epoch [1/5], Step [9298/10336], Loss: 0.0358\n",
      "Epoch [1/5], Step [9300/10336], Loss: 0.0290\n",
      "Epoch [1/5], Step [9302/10336], Loss: 0.2354\n",
      "Epoch [1/5], Step [9304/10336], Loss: 0.6959\n",
      "Epoch [1/5], Step [9306/10336], Loss: 0.0951\n",
      "Epoch [1/5], Step [9308/10336], Loss: 2.5081\n",
      "Epoch [1/5], Step [9310/10336], Loss: 1.1492\n",
      "Epoch [1/5], Step [9312/10336], Loss: 0.2260\n",
      "Epoch [1/5], Step [9314/10336], Loss: 0.8838\n",
      "Epoch [1/5], Step [9316/10336], Loss: 0.0106\n",
      "Epoch [1/5], Step [9318/10336], Loss: 4.9337\n",
      "Epoch [1/5], Step [9320/10336], Loss: 0.0950\n",
      "Epoch [1/5], Step [9322/10336], Loss: 0.2764\n",
      "Epoch [1/5], Step [9324/10336], Loss: 1.3493\n",
      "Epoch [1/5], Step [9326/10336], Loss: 2.4860\n",
      "Epoch [1/5], Step [9328/10336], Loss: 1.3111\n",
      "Epoch [1/5], Step [9330/10336], Loss: 1.8801\n",
      "Epoch [1/5], Step [9332/10336], Loss: 1.1852\n",
      "Epoch [1/5], Step [9334/10336], Loss: 0.4875\n",
      "Epoch [1/5], Step [9336/10336], Loss: 0.3405\n",
      "Epoch [1/5], Step [9338/10336], Loss: 3.0960\n",
      "Epoch [1/5], Step [9340/10336], Loss: 0.7225\n",
      "Epoch [1/5], Step [9342/10336], Loss: 1.1862\n",
      "Epoch [1/5], Step [9344/10336], Loss: 1.6784\n",
      "Epoch [1/5], Step [9346/10336], Loss: 0.1816\n",
      "Epoch [1/5], Step [9348/10336], Loss: 0.2156\n",
      "Epoch [1/5], Step [9350/10336], Loss: 0.6933\n",
      "Epoch [1/5], Step [9352/10336], Loss: 0.1324\n",
      "Epoch [1/5], Step [9354/10336], Loss: 3.6134\n",
      "Epoch [1/5], Step [9356/10336], Loss: 2.3498\n",
      "Epoch [1/5], Step [9358/10336], Loss: 0.8066\n",
      "Epoch [1/5], Step [9360/10336], Loss: 1.5679\n",
      "Epoch [1/5], Step [9362/10336], Loss: 1.0040\n",
      "Epoch [1/5], Step [9364/10336], Loss: 1.5447\n",
      "Epoch [1/5], Step [9366/10336], Loss: 1.0730\n",
      "Epoch [1/5], Step [9368/10336], Loss: 4.5022\n",
      "Epoch [1/5], Step [9370/10336], Loss: 0.3468\n",
      "Epoch [1/5], Step [9372/10336], Loss: 1.0572\n",
      "Epoch [1/5], Step [9374/10336], Loss: 1.4827\n",
      "Epoch [1/5], Step [9376/10336], Loss: 0.5872\n",
      "Epoch [1/5], Step [9378/10336], Loss: 0.5240\n",
      "Epoch [1/5], Step [9380/10336], Loss: 1.5288\n",
      "Epoch [1/5], Step [9382/10336], Loss: 0.2524\n",
      "Epoch [1/5], Step [9384/10336], Loss: 0.9465\n",
      "Epoch [1/5], Step [9386/10336], Loss: 1.8295\n",
      "Epoch [1/5], Step [9388/10336], Loss: 0.1277\n",
      "Epoch [1/5], Step [9390/10336], Loss: 0.9159\n",
      "Epoch [1/5], Step [9392/10336], Loss: 2.0631\n",
      "Epoch [1/5], Step [9394/10336], Loss: 1.3793\n",
      "Epoch [1/5], Step [9396/10336], Loss: 0.6058\n",
      "Epoch [1/5], Step [9398/10336], Loss: 1.5217\n",
      "Epoch [1/5], Step [9400/10336], Loss: 0.3958\n",
      "Epoch [1/5], Step [9402/10336], Loss: 0.1853\n",
      "Epoch [1/5], Step [9404/10336], Loss: 0.4547\n",
      "Epoch [1/5], Step [9406/10336], Loss: 1.9806\n",
      "Epoch [1/5], Step [9408/10336], Loss: 2.6793\n",
      "Epoch [1/5], Step [9410/10336], Loss: 0.1427\n",
      "Epoch [1/5], Step [9412/10336], Loss: 0.0457\n",
      "Epoch [1/5], Step [9414/10336], Loss: 0.0793\n",
      "Epoch [1/5], Step [9416/10336], Loss: 2.1762\n",
      "Epoch [1/5], Step [9418/10336], Loss: 0.7032\n",
      "Epoch [1/5], Step [9420/10336], Loss: 2.0845\n",
      "Epoch [1/5], Step [9422/10336], Loss: 0.1548\n",
      "Epoch [1/5], Step [9424/10336], Loss: 0.1195\n",
      "Epoch [1/5], Step [9426/10336], Loss: 1.6958\n",
      "Epoch [1/5], Step [9428/10336], Loss: 0.2419\n",
      "Epoch [1/5], Step [9430/10336], Loss: 0.4709\n",
      "Epoch [1/5], Step [9432/10336], Loss: 2.0382\n",
      "Epoch [1/5], Step [9434/10336], Loss: 2.3884\n",
      "Epoch [1/5], Step [9436/10336], Loss: 3.2261\n",
      "Epoch [1/5], Step [9438/10336], Loss: 1.5173\n",
      "Epoch [1/5], Step [9440/10336], Loss: 1.9360\n",
      "Epoch [1/5], Step [9442/10336], Loss: 0.2058\n",
      "Epoch [1/5], Step [9444/10336], Loss: 3.3328\n",
      "Epoch [1/5], Step [9446/10336], Loss: 0.3588\n",
      "Epoch [1/5], Step [9448/10336], Loss: 0.9826\n",
      "Epoch [1/5], Step [9450/10336], Loss: 1.4028\n",
      "Epoch [1/5], Step [9452/10336], Loss: 3.5028\n",
      "Epoch [1/5], Step [9454/10336], Loss: 1.5483\n",
      "Epoch [1/5], Step [9456/10336], Loss: 0.8747\n",
      "Epoch [1/5], Step [9458/10336], Loss: 1.3031\n",
      "Epoch [1/5], Step [9460/10336], Loss: 0.2947\n",
      "Epoch [1/5], Step [9462/10336], Loss: 0.9311\n",
      "Epoch [1/5], Step [9464/10336], Loss: 0.3438\n",
      "Epoch [1/5], Step [9466/10336], Loss: 0.2930\n",
      "Epoch [1/5], Step [9468/10336], Loss: 0.0686\n",
      "Epoch [1/5], Step [9470/10336], Loss: 1.7002\n",
      "Epoch [1/5], Step [9472/10336], Loss: 1.4206\n",
      "Epoch [1/5], Step [9474/10336], Loss: 2.5761\n",
      "Epoch [1/5], Step [9476/10336], Loss: 3.0904\n",
      "Epoch [1/5], Step [9478/10336], Loss: 0.6618\n",
      "Epoch [1/5], Step [9480/10336], Loss: 0.0369\n",
      "Epoch [1/5], Step [9482/10336], Loss: 0.8351\n",
      "Epoch [1/5], Step [9484/10336], Loss: 3.1787\n",
      "Epoch [1/5], Step [9486/10336], Loss: 1.4235\n",
      "Epoch [1/5], Step [9488/10336], Loss: 4.6376\n",
      "Epoch [1/5], Step [9490/10336], Loss: 1.6019\n",
      "Epoch [1/5], Step [9492/10336], Loss: 0.4702\n",
      "Epoch [1/5], Step [9494/10336], Loss: 5.5215\n",
      "Epoch [1/5], Step [9496/10336], Loss: 0.6056\n",
      "Epoch [1/5], Step [9498/10336], Loss: 0.0153\n",
      "Epoch [1/5], Step [9500/10336], Loss: 1.8021\n",
      "Epoch [1/5], Step [9502/10336], Loss: 0.0541\n",
      "Epoch [1/5], Step [9504/10336], Loss: 0.0494\n",
      "Epoch [1/5], Step [9506/10336], Loss: 1.8225\n",
      "Epoch [1/5], Step [9508/10336], Loss: 0.0257\n",
      "Epoch [1/5], Step [9510/10336], Loss: 0.8236\n",
      "Epoch [1/5], Step [9512/10336], Loss: 0.1330\n",
      "Epoch [1/5], Step [9514/10336], Loss: 0.0993\n",
      "Epoch [1/5], Step [9516/10336], Loss: 2.3983\n",
      "Epoch [1/5], Step [9518/10336], Loss: 0.6686\n",
      "Epoch [1/5], Step [9520/10336], Loss: 2.2087\n",
      "Epoch [1/5], Step [9522/10336], Loss: 2.0497\n",
      "Epoch [1/5], Step [9524/10336], Loss: 2.0170\n",
      "Epoch [1/5], Step [9526/10336], Loss: 0.0563\n",
      "Epoch [1/5], Step [9528/10336], Loss: 0.4513\n",
      "Epoch [1/5], Step [9530/10336], Loss: 1.2670\n",
      "Epoch [1/5], Step [9532/10336], Loss: 1.8665\n",
      "Epoch [1/5], Step [9534/10336], Loss: 0.7414\n",
      "Epoch [1/5], Step [9536/10336], Loss: 2.3219\n",
      "Epoch [1/5], Step [9538/10336], Loss: 1.2442\n",
      "Epoch [1/5], Step [9540/10336], Loss: 1.0056\n",
      "Epoch [1/5], Step [9542/10336], Loss: 2.6625\n",
      "Epoch [1/5], Step [9544/10336], Loss: 0.8938\n",
      "Epoch [1/5], Step [9546/10336], Loss: 3.1550\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [9548/10336], Loss: 0.4247\n",
      "Epoch [1/5], Step [9550/10336], Loss: 0.7985\n",
      "Epoch [1/5], Step [9552/10336], Loss: 0.4002\n",
      "Epoch [1/5], Step [9554/10336], Loss: 0.4460\n",
      "Epoch [1/5], Step [9556/10336], Loss: 1.6017\n",
      "Epoch [1/5], Step [9558/10336], Loss: 0.3418\n",
      "Epoch [1/5], Step [9560/10336], Loss: 1.2027\n",
      "Epoch [1/5], Step [9562/10336], Loss: 2.1744\n",
      "Epoch [1/5], Step [9564/10336], Loss: 1.9319\n",
      "Epoch [1/5], Step [9566/10336], Loss: 0.2244\n",
      "Epoch [1/5], Step [9568/10336], Loss: 0.6451\n",
      "Epoch [1/5], Step [9570/10336], Loss: 0.1955\n",
      "Epoch [1/5], Step [9572/10336], Loss: 0.3738\n",
      "Epoch [1/5], Step [9574/10336], Loss: 0.6745\n",
      "Epoch [1/5], Step [9576/10336], Loss: 5.2093\n",
      "Epoch [1/5], Step [9578/10336], Loss: 1.5385\n",
      "Epoch [1/5], Step [9580/10336], Loss: 3.0014\n",
      "Epoch [1/5], Step [9582/10336], Loss: 0.0574\n",
      "Epoch [1/5], Step [9584/10336], Loss: 0.7368\n",
      "Epoch [1/5], Step [9586/10336], Loss: 0.0522\n",
      "Epoch [1/5], Step [9588/10336], Loss: 0.8786\n",
      "Epoch [1/5], Step [9590/10336], Loss: 0.1837\n",
      "Epoch [1/5], Step [9592/10336], Loss: 0.5651\n",
      "Epoch [1/5], Step [9594/10336], Loss: 1.0372\n",
      "Epoch [1/5], Step [9596/10336], Loss: 0.9952\n",
      "Epoch [1/5], Step [9598/10336], Loss: 1.0933\n",
      "Epoch [1/5], Step [9600/10336], Loss: 0.9828\n",
      "Epoch [1/5], Step [9602/10336], Loss: 0.1307\n",
      "Epoch [1/5], Step [9604/10336], Loss: 0.5030\n",
      "Epoch [1/5], Step [9606/10336], Loss: 0.1057\n",
      "Epoch [1/5], Step [9608/10336], Loss: 1.5868\n",
      "Epoch [1/5], Step [9610/10336], Loss: 2.9628\n",
      "Epoch [1/5], Step [9612/10336], Loss: 1.9154\n",
      "Epoch [1/5], Step [9614/10336], Loss: 0.0808\n",
      "Epoch [1/5], Step [9616/10336], Loss: 0.9565\n",
      "Epoch [1/5], Step [9618/10336], Loss: 2.6627\n",
      "Epoch [1/5], Step [9620/10336], Loss: 0.7365\n",
      "Epoch [1/5], Step [9622/10336], Loss: 0.1362\n",
      "Epoch [1/5], Step [9624/10336], Loss: 0.1226\n",
      "Epoch [1/5], Step [9626/10336], Loss: 1.2546\n",
      "Epoch [1/5], Step [9628/10336], Loss: 1.2812\n",
      "Epoch [1/5], Step [9630/10336], Loss: 0.7834\n",
      "Epoch [1/5], Step [9632/10336], Loss: 0.4440\n",
      "Epoch [1/5], Step [9634/10336], Loss: 0.5422\n",
      "Epoch [1/5], Step [9636/10336], Loss: 4.6124\n",
      "Epoch [1/5], Step [9638/10336], Loss: 1.5586\n",
      "Epoch [1/5], Step [9640/10336], Loss: 0.1755\n",
      "Epoch [1/5], Step [9642/10336], Loss: 0.5362\n",
      "Epoch [1/5], Step [9644/10336], Loss: 0.1085\n",
      "Epoch [1/5], Step [9646/10336], Loss: 1.0241\n",
      "Epoch [1/5], Step [9648/10336], Loss: 0.3521\n",
      "Epoch [1/5], Step [9650/10336], Loss: 0.6552\n",
      "Epoch [1/5], Step [9652/10336], Loss: 4.1960\n",
      "Epoch [1/5], Step [9654/10336], Loss: 1.0810\n",
      "Epoch [1/5], Step [9656/10336], Loss: 0.0229\n",
      "Epoch [1/5], Step [9658/10336], Loss: 0.0030\n",
      "Epoch [1/5], Step [9660/10336], Loss: 2.4889\n",
      "Epoch [1/5], Step [9662/10336], Loss: 0.2209\n",
      "Epoch [1/5], Step [9664/10336], Loss: 0.7331\n",
      "Epoch [1/5], Step [9666/10336], Loss: 1.1482\n",
      "Epoch [1/5], Step [9668/10336], Loss: 1.2547\n",
      "Epoch [1/5], Step [9670/10336], Loss: 1.1643\n",
      "Epoch [1/5], Step [9672/10336], Loss: 0.3789\n",
      "Epoch [1/5], Step [9674/10336], Loss: 1.2552\n",
      "Epoch [1/5], Step [9676/10336], Loss: 1.2326\n",
      "Epoch [1/5], Step [9678/10336], Loss: 0.0270\n",
      "Epoch [1/5], Step [9680/10336], Loss: 4.1654\n",
      "Epoch [1/5], Step [9682/10336], Loss: 4.7229\n",
      "Epoch [1/5], Step [9684/10336], Loss: 0.3105\n",
      "Epoch [1/5], Step [9686/10336], Loss: 0.8250\n",
      "Epoch [1/5], Step [9688/10336], Loss: 0.5154\n",
      "Epoch [1/5], Step [9690/10336], Loss: 0.2672\n",
      "Epoch [1/5], Step [9692/10336], Loss: 0.0781\n",
      "Epoch [1/5], Step [9694/10336], Loss: 0.2416\n",
      "Epoch [1/5], Step [9696/10336], Loss: 0.0820\n",
      "Epoch [1/5], Step [9698/10336], Loss: 0.9526\n",
      "Epoch [1/5], Step [9700/10336], Loss: 1.6355\n",
      "Epoch [1/5], Step [9702/10336], Loss: 0.3247\n",
      "Epoch [1/5], Step [9704/10336], Loss: 1.8018\n",
      "Epoch [1/5], Step [9706/10336], Loss: 1.0518\n",
      "Epoch [1/5], Step [9708/10336], Loss: 2.0199\n",
      "Epoch [1/5], Step [9710/10336], Loss: 0.9111\n",
      "Epoch [1/5], Step [9712/10336], Loss: 2.3174\n",
      "Epoch [1/5], Step [9714/10336], Loss: 0.9460\n",
      "Epoch [1/5], Step [9716/10336], Loss: 1.7005\n",
      "Epoch [1/5], Step [9718/10336], Loss: 1.5986\n",
      "Epoch [1/5], Step [9720/10336], Loss: 0.0702\n",
      "Epoch [1/5], Step [9722/10336], Loss: 1.5954\n",
      "Epoch [1/5], Step [9724/10336], Loss: 0.5126\n",
      "Epoch [1/5], Step [9726/10336], Loss: 0.5262\n",
      "Epoch [1/5], Step [9728/10336], Loss: 0.2725\n",
      "Epoch [1/5], Step [9730/10336], Loss: 2.1243\n",
      "Epoch [1/5], Step [9732/10336], Loss: 0.1267\n",
      "Epoch [1/5], Step [9734/10336], Loss: 0.1618\n",
      "Epoch [1/5], Step [9736/10336], Loss: 0.6486\n",
      "Epoch [1/5], Step [9738/10336], Loss: 2.3478\n",
      "Epoch [1/5], Step [9740/10336], Loss: 2.6585\n",
      "Epoch [1/5], Step [9742/10336], Loss: 0.2692\n",
      "Epoch [1/5], Step [9744/10336], Loss: 0.1406\n",
      "Epoch [1/5], Step [9746/10336], Loss: 0.7740\n",
      "Epoch [1/5], Step [9748/10336], Loss: 0.2171\n",
      "Epoch [1/5], Step [9750/10336], Loss: 1.4320\n",
      "Epoch [1/5], Step [9752/10336], Loss: 2.3949\n",
      "Epoch [1/5], Step [9754/10336], Loss: 0.3976\n",
      "Epoch [1/5], Step [9756/10336], Loss: 0.9721\n",
      "Epoch [1/5], Step [9758/10336], Loss: 2.1090\n",
      "Epoch [1/5], Step [9760/10336], Loss: 0.0956\n",
      "Epoch [1/5], Step [9762/10336], Loss: 0.5598\n",
      "Epoch [1/5], Step [9764/10336], Loss: 1.0206\n",
      "Epoch [1/5], Step [9766/10336], Loss: 1.3744\n",
      "Epoch [1/5], Step [9768/10336], Loss: 2.3943\n",
      "Epoch [1/5], Step [9770/10336], Loss: 0.0314\n",
      "Epoch [1/5], Step [9772/10336], Loss: 0.6660\n",
      "Epoch [1/5], Step [9774/10336], Loss: 1.6885\n",
      "Epoch [1/5], Step [9776/10336], Loss: 1.4470\n",
      "Epoch [1/5], Step [9778/10336], Loss: 2.2663\n",
      "Epoch [1/5], Step [9780/10336], Loss: 0.1667\n",
      "Epoch [1/5], Step [9782/10336], Loss: 1.7653\n",
      "Epoch [1/5], Step [9784/10336], Loss: 0.1995\n",
      "Epoch [1/5], Step [9786/10336], Loss: 0.6658\n",
      "Epoch [1/5], Step [9788/10336], Loss: 1.9925\n",
      "Epoch [1/5], Step [9790/10336], Loss: 0.8459\n",
      "Epoch [1/5], Step [9792/10336], Loss: 5.1679\n",
      "Epoch [1/5], Step [9794/10336], Loss: 0.5741\n",
      "Epoch [1/5], Step [9796/10336], Loss: 1.5617\n",
      "Epoch [1/5], Step [9798/10336], Loss: 1.5588\n",
      "Epoch [1/5], Step [9800/10336], Loss: 0.4716\n",
      "Epoch [1/5], Step [9802/10336], Loss: 0.4510\n",
      "Epoch [1/5], Step [9804/10336], Loss: 0.4301\n",
      "Epoch [1/5], Step [9806/10336], Loss: 0.8408\n",
      "Epoch [1/5], Step [9808/10336], Loss: 0.0694\n",
      "Epoch [1/5], Step [9810/10336], Loss: 0.2335\n",
      "Epoch [1/5], Step [9812/10336], Loss: 0.0206\n",
      "Epoch [1/5], Step [9814/10336], Loss: 0.7268\n",
      "Epoch [1/5], Step [9816/10336], Loss: 2.3042\n",
      "Epoch [1/5], Step [9818/10336], Loss: 1.0129\n",
      "Epoch [1/5], Step [9820/10336], Loss: 1.5936\n",
      "Epoch [1/5], Step [9822/10336], Loss: 2.1911\n",
      "Epoch [1/5], Step [9824/10336], Loss: 0.0058\n",
      "Epoch [1/5], Step [9826/10336], Loss: 1.3885\n",
      "Epoch [1/5], Step [9828/10336], Loss: 0.1088\n",
      "Epoch [1/5], Step [9830/10336], Loss: 0.1155\n",
      "Epoch [1/5], Step [9832/10336], Loss: 1.3784\n",
      "Epoch [1/5], Step [9834/10336], Loss: 0.6329\n",
      "Epoch [1/5], Step [9836/10336], Loss: 0.3576\n",
      "Epoch [1/5], Step [9838/10336], Loss: 0.6776\n",
      "Epoch [1/5], Step [9840/10336], Loss: 1.4080\n",
      "Epoch [1/5], Step [9842/10336], Loss: 1.4771\n",
      "Epoch [1/5], Step [9844/10336], Loss: 0.1525\n",
      "Epoch [1/5], Step [9846/10336], Loss: 1.5881\n",
      "Epoch [1/5], Step [9848/10336], Loss: 0.0220\n",
      "Epoch [1/5], Step [9850/10336], Loss: 0.4651\n",
      "Epoch [1/5], Step [9852/10336], Loss: 0.2561\n",
      "Epoch [1/5], Step [9854/10336], Loss: 1.9249\n",
      "Epoch [1/5], Step [9856/10336], Loss: 1.0313\n",
      "Epoch [1/5], Step [9858/10336], Loss: 0.1208\n",
      "Epoch [1/5], Step [9860/10336], Loss: 0.0283\n",
      "Epoch [1/5], Step [9862/10336], Loss: 1.0126\n",
      "Epoch [1/5], Step [9864/10336], Loss: 1.2474\n",
      "Epoch [1/5], Step [9866/10336], Loss: 1.1470\n",
      "Epoch [1/5], Step [9868/10336], Loss: 0.0017\n",
      "Epoch [1/5], Step [9870/10336], Loss: 2.2846\n",
      "Epoch [1/5], Step [9872/10336], Loss: 0.0343\n",
      "Epoch [1/5], Step [9874/10336], Loss: 1.4093\n",
      "Epoch [1/5], Step [9876/10336], Loss: 0.0318\n",
      "Epoch [1/5], Step [9878/10336], Loss: 1.1493\n",
      "Epoch [1/5], Step [9880/10336], Loss: 0.4527\n",
      "Epoch [1/5], Step [9882/10336], Loss: 0.3404\n",
      "Epoch [1/5], Step [9884/10336], Loss: 2.4803\n",
      "Epoch [1/5], Step [9886/10336], Loss: 1.2407\n",
      "Epoch [1/5], Step [9888/10336], Loss: 0.7240\n",
      "Epoch [1/5], Step [9890/10336], Loss: 0.7361\n",
      "Epoch [1/5], Step [9892/10336], Loss: 0.7111\n",
      "Epoch [1/5], Step [9894/10336], Loss: 0.5760\n",
      "Epoch [1/5], Step [9896/10336], Loss: 1.3067\n",
      "Epoch [1/5], Step [9898/10336], Loss: 1.6666\n",
      "Epoch [1/5], Step [9900/10336], Loss: 0.9419\n",
      "Epoch [1/5], Step [9902/10336], Loss: 0.5727\n",
      "Epoch [1/5], Step [9904/10336], Loss: 0.2043\n",
      "Epoch [1/5], Step [9906/10336], Loss: 0.0493\n",
      "Epoch [1/5], Step [9908/10336], Loss: 1.5274\n",
      "Epoch [1/5], Step [9910/10336], Loss: 0.6624\n",
      "Epoch [1/5], Step [9912/10336], Loss: 0.0631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [9914/10336], Loss: 3.0198\n",
      "Epoch [1/5], Step [9916/10336], Loss: 1.3501\n",
      "Epoch [1/5], Step [9918/10336], Loss: 0.3226\n",
      "Epoch [1/5], Step [9920/10336], Loss: 0.0312\n",
      "Epoch [1/5], Step [9922/10336], Loss: 0.0854\n",
      "Epoch [1/5], Step [9924/10336], Loss: 0.2837\n",
      "Epoch [1/5], Step [9926/10336], Loss: 1.5329\n",
      "Epoch [1/5], Step [9928/10336], Loss: 0.3613\n",
      "Epoch [1/5], Step [9930/10336], Loss: 1.3663\n",
      "Epoch [1/5], Step [9932/10336], Loss: 1.8420\n",
      "Epoch [1/5], Step [9934/10336], Loss: 1.8023\n",
      "Epoch [1/5], Step [9936/10336], Loss: 0.6431\n",
      "Epoch [1/5], Step [9938/10336], Loss: 0.1473\n",
      "Epoch [1/5], Step [9940/10336], Loss: 0.2891\n",
      "Epoch [1/5], Step [9942/10336], Loss: 0.5682\n",
      "Epoch [1/5], Step [9944/10336], Loss: 0.6821\n",
      "Epoch [1/5], Step [9946/10336], Loss: 1.7903\n",
      "Epoch [1/5], Step [9948/10336], Loss: 0.7006\n",
      "Epoch [1/5], Step [9950/10336], Loss: 0.2354\n",
      "Epoch [1/5], Step [9952/10336], Loss: 0.1211\n",
      "Epoch [1/5], Step [9954/10336], Loss: 1.9154\n",
      "Epoch [1/5], Step [9956/10336], Loss: 1.1285\n",
      "Epoch [1/5], Step [9958/10336], Loss: 0.5413\n",
      "Epoch [1/5], Step [9960/10336], Loss: 2.7222\n",
      "Epoch [1/5], Step [9962/10336], Loss: 1.5168\n",
      "Epoch [1/5], Step [9964/10336], Loss: 0.1108\n",
      "Epoch [1/5], Step [9966/10336], Loss: 1.0363\n",
      "Epoch [1/5], Step [9968/10336], Loss: 0.2858\n",
      "Epoch [1/5], Step [9970/10336], Loss: 0.1238\n",
      "Epoch [1/5], Step [9972/10336], Loss: 0.5509\n",
      "Epoch [1/5], Step [9974/10336], Loss: 0.6874\n",
      "Epoch [1/5], Step [9976/10336], Loss: 1.8488\n",
      "Epoch [1/5], Step [9978/10336], Loss: 0.4288\n",
      "Epoch [1/5], Step [9980/10336], Loss: 0.6073\n",
      "Epoch [1/5], Step [9982/10336], Loss: 1.3149\n",
      "Epoch [1/5], Step [9984/10336], Loss: 0.5708\n",
      "Epoch [1/5], Step [9986/10336], Loss: 0.0077\n",
      "Epoch [1/5], Step [9988/10336], Loss: 0.4262\n",
      "Epoch [1/5], Step [9990/10336], Loss: 0.6500\n",
      "Epoch [1/5], Step [9992/10336], Loss: 0.2076\n",
      "Epoch [1/5], Step [9994/10336], Loss: 2.2346\n",
      "Epoch [1/5], Step [9996/10336], Loss: 0.1147\n",
      "Epoch [1/5], Step [9998/10336], Loss: 0.6147\n",
      "Epoch [1/5], Step [10000/10336], Loss: 0.3956\n",
      "Epoch [1/5], Step [10002/10336], Loss: 0.1156\n",
      "Epoch [1/5], Step [10004/10336], Loss: 0.3417\n",
      "Epoch [1/5], Step [10006/10336], Loss: 1.5224\n",
      "Epoch [1/5], Step [10008/10336], Loss: 0.4346\n",
      "Epoch [1/5], Step [10010/10336], Loss: 0.6537\n",
      "Epoch [1/5], Step [10012/10336], Loss: 1.8000\n",
      "Epoch [1/5], Step [10014/10336], Loss: 0.4794\n",
      "Epoch [1/5], Step [10016/10336], Loss: 1.7009\n",
      "Epoch [1/5], Step [10018/10336], Loss: 2.4652\n",
      "Epoch [1/5], Step [10020/10336], Loss: 0.2837\n",
      "Epoch [1/5], Step [10022/10336], Loss: 0.3457\n",
      "Epoch [1/5], Step [10024/10336], Loss: 0.3050\n",
      "Epoch [1/5], Step [10026/10336], Loss: 0.0896\n",
      "Epoch [1/5], Step [10028/10336], Loss: 1.3649\n",
      "Epoch [1/5], Step [10030/10336], Loss: 0.0779\n",
      "Epoch [1/5], Step [10032/10336], Loss: 1.6857\n",
      "Epoch [1/5], Step [10034/10336], Loss: 3.7530\n",
      "Epoch [1/5], Step [10036/10336], Loss: 0.2326\n",
      "Epoch [1/5], Step [10038/10336], Loss: 0.7120\n",
      "Epoch [1/5], Step [10040/10336], Loss: 0.9551\n",
      "Epoch [1/5], Step [10042/10336], Loss: 1.5058\n",
      "Epoch [1/5], Step [10044/10336], Loss: 2.0007\n",
      "Epoch [1/5], Step [10046/10336], Loss: 0.2940\n",
      "Epoch [1/5], Step [10048/10336], Loss: 0.1164\n",
      "Epoch [1/5], Step [10050/10336], Loss: 0.8966\n",
      "Epoch [1/5], Step [10052/10336], Loss: 0.5641\n",
      "Epoch [1/5], Step [10054/10336], Loss: 0.5857\n",
      "Epoch [1/5], Step [10056/10336], Loss: 0.1373\n",
      "Epoch [1/5], Step [10058/10336], Loss: 1.9029\n",
      "Epoch [1/5], Step [10060/10336], Loss: 2.8998\n",
      "Epoch [1/5], Step [10062/10336], Loss: 2.5876\n",
      "Epoch [1/5], Step [10064/10336], Loss: 1.9479\n",
      "Epoch [1/5], Step [10066/10336], Loss: 2.0151\n",
      "Epoch [1/5], Step [10068/10336], Loss: 0.2162\n",
      "Epoch [1/5], Step [10070/10336], Loss: 1.2134\n",
      "Epoch [1/5], Step [10072/10336], Loss: 0.0586\n",
      "Epoch [1/5], Step [10074/10336], Loss: 7.1024\n",
      "Epoch [1/5], Step [10076/10336], Loss: 0.3628\n",
      "Epoch [1/5], Step [10078/10336], Loss: 0.6705\n",
      "Epoch [1/5], Step [10080/10336], Loss: 0.8689\n",
      "Epoch [1/5], Step [10082/10336], Loss: 0.0731\n",
      "Epoch [1/5], Step [10084/10336], Loss: 0.6609\n",
      "Epoch [1/5], Step [10086/10336], Loss: 1.0827\n",
      "Epoch [1/5], Step [10088/10336], Loss: 0.6161\n",
      "Epoch [1/5], Step [10090/10336], Loss: 1.1240\n",
      "Epoch [1/5], Step [10092/10336], Loss: 0.1474\n",
      "Epoch [1/5], Step [10094/10336], Loss: 2.8906\n",
      "Epoch [1/5], Step [10096/10336], Loss: 0.8956\n",
      "Epoch [1/5], Step [10098/10336], Loss: 0.9507\n",
      "Epoch [1/5], Step [10100/10336], Loss: 1.2492\n",
      "Epoch [1/5], Step [10102/10336], Loss: 2.6000\n",
      "Epoch [1/5], Step [10104/10336], Loss: 6.1867\n",
      "Epoch [1/5], Step [10106/10336], Loss: 0.4600\n",
      "Epoch [1/5], Step [10108/10336], Loss: 0.8982\n",
      "Epoch [1/5], Step [10110/10336], Loss: 1.0254\n",
      "Epoch [1/5], Step [10112/10336], Loss: 0.7268\n",
      "Epoch [1/5], Step [10114/10336], Loss: 1.8771\n",
      "Epoch [1/5], Step [10116/10336], Loss: 2.5735\n",
      "Epoch [1/5], Step [10118/10336], Loss: 3.2618\n",
      "Epoch [1/5], Step [10120/10336], Loss: 0.9200\n",
      "Epoch [1/5], Step [10122/10336], Loss: 0.8024\n",
      "Epoch [1/5], Step [10124/10336], Loss: 0.5160\n",
      "Epoch [1/5], Step [10126/10336], Loss: 2.4303\n",
      "Epoch [1/5], Step [10128/10336], Loss: 0.4367\n",
      "Epoch [1/5], Step [10130/10336], Loss: 0.9344\n",
      "Epoch [1/5], Step [10132/10336], Loss: 0.3783\n",
      "Epoch [1/5], Step [10134/10336], Loss: 0.0373\n",
      "Epoch [1/5], Step [10136/10336], Loss: 1.3151\n",
      "Epoch [1/5], Step [10138/10336], Loss: 0.9410\n",
      "Epoch [1/5], Step [10140/10336], Loss: 0.1647\n",
      "Epoch [1/5], Step [10142/10336], Loss: 0.1736\n",
      "Epoch [1/5], Step [10144/10336], Loss: 1.1918\n",
      "Epoch [1/5], Step [10146/10336], Loss: 0.3152\n",
      "Epoch [1/5], Step [10148/10336], Loss: 0.2008\n",
      "Epoch [1/5], Step [10150/10336], Loss: 1.4679\n",
      "Epoch [1/5], Step [10152/10336], Loss: 1.9753\n",
      "Epoch [1/5], Step [10154/10336], Loss: 0.2955\n",
      "Epoch [1/5], Step [10156/10336], Loss: 0.1371\n",
      "Epoch [1/5], Step [10158/10336], Loss: 1.9086\n",
      "Epoch [1/5], Step [10160/10336], Loss: 1.0412\n",
      "Epoch [1/5], Step [10162/10336], Loss: 2.6328\n",
      "Epoch [1/5], Step [10164/10336], Loss: 0.2337\n",
      "Epoch [1/5], Step [10166/10336], Loss: 1.6887\n",
      "Epoch [1/5], Step [10168/10336], Loss: 1.2190\n",
      "Epoch [1/5], Step [10170/10336], Loss: 2.8281\n",
      "Epoch [1/5], Step [10172/10336], Loss: 1.0262\n",
      "Epoch [1/5], Step [10174/10336], Loss: 0.1702\n",
      "Epoch [1/5], Step [10176/10336], Loss: 0.1052\n",
      "Epoch [1/5], Step [10178/10336], Loss: 0.9035\n",
      "Epoch [1/5], Step [10180/10336], Loss: 0.0260\n",
      "Epoch [1/5], Step [10182/10336], Loss: 0.5714\n",
      "Epoch [1/5], Step [10184/10336], Loss: 0.1605\n",
      "Epoch [1/5], Step [10186/10336], Loss: 0.1235\n",
      "Epoch [1/5], Step [10188/10336], Loss: 1.7377\n",
      "Epoch [1/5], Step [10190/10336], Loss: 0.3805\n",
      "Epoch [1/5], Step [10192/10336], Loss: 0.2001\n",
      "Epoch [1/5], Step [10194/10336], Loss: 0.4943\n",
      "Epoch [1/5], Step [10196/10336], Loss: 1.5138\n",
      "Epoch [1/5], Step [10198/10336], Loss: 4.7072\n",
      "Epoch [1/5], Step [10200/10336], Loss: 1.9859\n",
      "Epoch [1/5], Step [10202/10336], Loss: 0.0694\n",
      "Epoch [1/5], Step [10204/10336], Loss: 1.0782\n",
      "Epoch [1/5], Step [10206/10336], Loss: 0.8169\n",
      "Epoch [1/5], Step [10208/10336], Loss: 1.4161\n",
      "Epoch [1/5], Step [10210/10336], Loss: 3.5623\n",
      "Epoch [1/5], Step [10212/10336], Loss: 1.2073\n",
      "Epoch [1/5], Step [10214/10336], Loss: 1.3669\n",
      "Epoch [1/5], Step [10216/10336], Loss: 0.9787\n",
      "Epoch [1/5], Step [10218/10336], Loss: 3.5912\n",
      "Epoch [1/5], Step [10220/10336], Loss: 0.5388\n",
      "Epoch [1/5], Step [10222/10336], Loss: 0.7195\n",
      "Epoch [1/5], Step [10224/10336], Loss: 0.0189\n",
      "Epoch [1/5], Step [10226/10336], Loss: 0.6400\n",
      "Epoch [1/5], Step [10228/10336], Loss: 1.6691\n",
      "Epoch [1/5], Step [10230/10336], Loss: 1.3778\n",
      "Epoch [1/5], Step [10232/10336], Loss: 0.5738\n",
      "Epoch [1/5], Step [10234/10336], Loss: 1.6707\n",
      "Epoch [1/5], Step [10236/10336], Loss: 1.1158\n",
      "Epoch [1/5], Step [10238/10336], Loss: 1.5692\n",
      "Epoch [1/5], Step [10240/10336], Loss: 0.8240\n",
      "Epoch [1/5], Step [10242/10336], Loss: 0.0379\n",
      "Epoch [1/5], Step [10244/10336], Loss: 0.1252\n",
      "Epoch [1/5], Step [10246/10336], Loss: 0.6538\n",
      "Epoch [1/5], Step [10248/10336], Loss: 0.0495\n",
      "Epoch [1/5], Step [10250/10336], Loss: 1.2407\n",
      "Epoch [1/5], Step [10252/10336], Loss: 0.1212\n",
      "Epoch [1/5], Step [10254/10336], Loss: 2.2064\n",
      "Epoch [1/5], Step [10256/10336], Loss: 0.2793\n",
      "Epoch [1/5], Step [10258/10336], Loss: 0.0115\n",
      "Epoch [1/5], Step [10260/10336], Loss: 0.0507\n",
      "Epoch [1/5], Step [10262/10336], Loss: 0.1512\n",
      "Epoch [1/5], Step [10264/10336], Loss: 1.6629\n",
      "Epoch [1/5], Step [10266/10336], Loss: 1.3706\n",
      "Epoch [1/5], Step [10268/10336], Loss: 0.0732\n",
      "Epoch [1/5], Step [10270/10336], Loss: 0.1569\n",
      "Epoch [1/5], Step [10272/10336], Loss: 1.3816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [10274/10336], Loss: 0.0729\n",
      "Epoch [1/5], Step [10276/10336], Loss: 0.0429\n",
      "Epoch [1/5], Step [10278/10336], Loss: 0.5733\n",
      "Epoch [1/5], Step [10280/10336], Loss: 2.0632\n",
      "Epoch [1/5], Step [10282/10336], Loss: 0.0947\n",
      "Epoch [1/5], Step [10284/10336], Loss: 2.3999\n",
      "Epoch [1/5], Step [10286/10336], Loss: 1.5187\n",
      "Epoch [1/5], Step [10288/10336], Loss: 1.9291\n",
      "Epoch [1/5], Step [10290/10336], Loss: 0.2906\n",
      "Epoch [1/5], Step [10292/10336], Loss: 1.5586\n",
      "Epoch [1/5], Step [10294/10336], Loss: 1.0159\n",
      "Epoch [1/5], Step [10296/10336], Loss: 3.1103\n",
      "Epoch [1/5], Step [10298/10336], Loss: 2.0819\n",
      "Epoch [1/5], Step [10300/10336], Loss: 0.6614\n",
      "Epoch [1/5], Step [10302/10336], Loss: 0.1195\n",
      "Epoch [1/5], Step [10304/10336], Loss: 0.0166\n",
      "Epoch [1/5], Step [10306/10336], Loss: 1.1491\n",
      "Epoch [1/5], Step [10308/10336], Loss: 0.0191\n",
      "Epoch [1/5], Step [10310/10336], Loss: 0.0654\n",
      "Epoch [1/5], Step [10312/10336], Loss: 0.6383\n",
      "Epoch [1/5], Step [10314/10336], Loss: 0.3087\n",
      "Epoch [1/5], Step [10316/10336], Loss: 0.1562\n",
      "Epoch [1/5], Step [10318/10336], Loss: 3.6324\n",
      "Epoch [1/5], Step [10320/10336], Loss: 0.4298\n",
      "Epoch [1/5], Step [10322/10336], Loss: 0.3653\n",
      "Epoch [1/5], Step [10324/10336], Loss: 0.0905\n",
      "Epoch [1/5], Step [10326/10336], Loss: 0.3740\n",
      "Epoch [1/5], Step [10328/10336], Loss: 0.0369\n",
      "Epoch [1/5], Step [10330/10336], Loss: 0.0927\n",
      "Epoch [1/5], Step [10332/10336], Loss: 0.3483\n",
      "Epoch [1/5], Step [10334/10336], Loss: 0.0067\n",
      "Epoch [1/5], Step [10336/10336], Loss: 0.0494\n",
      "Epoch [2/5], Step [2/10336], Loss: 0.5672\n",
      "Epoch [2/5], Step [4/10336], Loss: 0.0111\n",
      "Epoch [2/5], Step [6/10336], Loss: 1.1971\n",
      "Epoch [2/5], Step [8/10336], Loss: 0.3298\n",
      "Epoch [2/5], Step [10/10336], Loss: 1.4696\n",
      "Epoch [2/5], Step [12/10336], Loss: 2.2717\n",
      "Epoch [2/5], Step [14/10336], Loss: 0.7855\n",
      "Epoch [2/5], Step [16/10336], Loss: 0.2465\n",
      "Epoch [2/5], Step [18/10336], Loss: 3.7775\n",
      "Epoch [2/5], Step [20/10336], Loss: 4.0915\n",
      "Epoch [2/5], Step [22/10336], Loss: 0.1970\n",
      "Epoch [2/5], Step [24/10336], Loss: 0.0204\n",
      "Epoch [2/5], Step [26/10336], Loss: 1.8698\n",
      "Epoch [2/5], Step [28/10336], Loss: 0.6253\n",
      "Epoch [2/5], Step [30/10336], Loss: 2.9170\n",
      "Epoch [2/5], Step [32/10336], Loss: 2.7078\n",
      "Epoch [2/5], Step [34/10336], Loss: 0.8569\n",
      "Epoch [2/5], Step [36/10336], Loss: 2.1397\n",
      "Epoch [2/5], Step [38/10336], Loss: 0.9166\n",
      "Epoch [2/5], Step [40/10336], Loss: 0.8220\n",
      "Epoch [2/5], Step [42/10336], Loss: 2.3481\n",
      "Epoch [2/5], Step [44/10336], Loss: 0.5541\n",
      "Epoch [2/5], Step [46/10336], Loss: 0.1188\n",
      "Epoch [2/5], Step [48/10336], Loss: 0.1572\n",
      "Epoch [2/5], Step [50/10336], Loss: 3.9925\n",
      "Epoch [2/5], Step [52/10336], Loss: 0.0747\n",
      "Epoch [2/5], Step [54/10336], Loss: 0.0434\n",
      "Epoch [2/5], Step [56/10336], Loss: 2.7777\n",
      "Epoch [2/5], Step [58/10336], Loss: 0.7871\n",
      "Epoch [2/5], Step [60/10336], Loss: 1.7659\n",
      "Epoch [2/5], Step [62/10336], Loss: 2.1468\n",
      "Epoch [2/5], Step [64/10336], Loss: 0.4435\n",
      "Epoch [2/5], Step [66/10336], Loss: 0.1074\n",
      "Epoch [2/5], Step [68/10336], Loss: 1.1554\n",
      "Epoch [2/5], Step [70/10336], Loss: 0.9398\n",
      "Epoch [2/5], Step [72/10336], Loss: 2.8572\n",
      "Epoch [2/5], Step [74/10336], Loss: 3.7232\n",
      "Epoch [2/5], Step [76/10336], Loss: 1.1841\n",
      "Epoch [2/5], Step [78/10336], Loss: 0.2951\n",
      "Epoch [2/5], Step [80/10336], Loss: 1.0054\n",
      "Epoch [2/5], Step [82/10336], Loss: 1.3044\n",
      "Epoch [2/5], Step [84/10336], Loss: 2.8558\n",
      "Epoch [2/5], Step [86/10336], Loss: 0.7900\n",
      "Epoch [2/5], Step [88/10336], Loss: 0.0299\n",
      "Epoch [2/5], Step [90/10336], Loss: 0.9284\n",
      "Epoch [2/5], Step [92/10336], Loss: 1.1242\n",
      "Epoch [2/5], Step [94/10336], Loss: 1.3149\n",
      "Epoch [2/5], Step [96/10336], Loss: 1.3218\n",
      "Epoch [2/5], Step [98/10336], Loss: 0.0110\n",
      "Epoch [2/5], Step [100/10336], Loss: 3.1406\n",
      "Epoch [2/5], Step [102/10336], Loss: 1.7982\n",
      "Epoch [2/5], Step [104/10336], Loss: 0.8418\n",
      "Epoch [2/5], Step [106/10336], Loss: 0.5154\n",
      "Epoch [2/5], Step [108/10336], Loss: 0.2342\n",
      "Epoch [2/5], Step [110/10336], Loss: 0.0329\n",
      "Epoch [2/5], Step [112/10336], Loss: 2.1148\n",
      "Epoch [2/5], Step [114/10336], Loss: 0.4929\n",
      "Epoch [2/5], Step [116/10336], Loss: 1.2038\n",
      "Epoch [2/5], Step [118/10336], Loss: 1.1963\n",
      "Epoch [2/5], Step [120/10336], Loss: 2.2320\n",
      "Epoch [2/5], Step [122/10336], Loss: 1.3887\n",
      "Epoch [2/5], Step [124/10336], Loss: 0.7827\n",
      "Epoch [2/5], Step [126/10336], Loss: 2.8666\n",
      "Epoch [2/5], Step [128/10336], Loss: 0.9673\n",
      "Epoch [2/5], Step [130/10336], Loss: 0.2962\n",
      "Epoch [2/5], Step [132/10336], Loss: 1.0647\n",
      "Epoch [2/5], Step [134/10336], Loss: 0.5917\n",
      "Epoch [2/5], Step [136/10336], Loss: 0.4151\n",
      "Epoch [2/5], Step [138/10336], Loss: 0.0446\n",
      "Epoch [2/5], Step [140/10336], Loss: 1.4234\n",
      "Epoch [2/5], Step [142/10336], Loss: 0.5322\n",
      "Epoch [2/5], Step [144/10336], Loss: 2.1685\n",
      "Epoch [2/5], Step [146/10336], Loss: 1.1562\n",
      "Epoch [2/5], Step [148/10336], Loss: 0.1327\n",
      "Epoch [2/5], Step [150/10336], Loss: 0.2826\n",
      "Epoch [2/5], Step [152/10336], Loss: 0.1165\n",
      "Epoch [2/5], Step [154/10336], Loss: 0.9028\n",
      "Epoch [2/5], Step [156/10336], Loss: 0.2427\n",
      "Epoch [2/5], Step [158/10336], Loss: 3.3867\n",
      "Epoch [2/5], Step [160/10336], Loss: 0.2209\n",
      "Epoch [2/5], Step [162/10336], Loss: 0.0235\n",
      "Epoch [2/5], Step [164/10336], Loss: 2.6130\n",
      "Epoch [2/5], Step [166/10336], Loss: 2.4062\n",
      "Epoch [2/5], Step [168/10336], Loss: 0.5382\n",
      "Epoch [2/5], Step [170/10336], Loss: 0.5231\n",
      "Epoch [2/5], Step [172/10336], Loss: 2.0574\n",
      "Epoch [2/5], Step [174/10336], Loss: 5.2322\n",
      "Epoch [2/5], Step [176/10336], Loss: 0.0627\n",
      "Epoch [2/5], Step [178/10336], Loss: 2.9523\n",
      "Epoch [2/5], Step [180/10336], Loss: 5.9881\n",
      "Epoch [2/5], Step [182/10336], Loss: 1.6086\n",
      "Epoch [2/5], Step [184/10336], Loss: 0.1022\n",
      "Epoch [2/5], Step [186/10336], Loss: 2.2413\n",
      "Epoch [2/5], Step [188/10336], Loss: 1.3472\n",
      "Epoch [2/5], Step [190/10336], Loss: 1.6178\n",
      "Epoch [2/5], Step [192/10336], Loss: 0.2488\n",
      "Epoch [2/5], Step [194/10336], Loss: 1.2722\n",
      "Epoch [2/5], Step [196/10336], Loss: 1.1455\n",
      "Epoch [2/5], Step [198/10336], Loss: 0.0994\n",
      "Epoch [2/5], Step [200/10336], Loss: 1.4291\n",
      "Epoch [2/5], Step [202/10336], Loss: 0.0442\n",
      "Epoch [2/5], Step [204/10336], Loss: 1.2205\n",
      "Epoch [2/5], Step [206/10336], Loss: 2.0311\n",
      "Epoch [2/5], Step [208/10336], Loss: 1.0568\n",
      "Epoch [2/5], Step [210/10336], Loss: 1.6315\n",
      "Epoch [2/5], Step [212/10336], Loss: 0.5387\n",
      "Epoch [2/5], Step [214/10336], Loss: 3.2136\n",
      "Epoch [2/5], Step [216/10336], Loss: 1.7546\n",
      "Epoch [2/5], Step [218/10336], Loss: 0.1109\n",
      "Epoch [2/5], Step [220/10336], Loss: 0.3907\n",
      "Epoch [2/5], Step [222/10336], Loss: 1.5830\n",
      "Epoch [2/5], Step [224/10336], Loss: 1.5568\n",
      "Epoch [2/5], Step [226/10336], Loss: 1.2513\n",
      "Epoch [2/5], Step [228/10336], Loss: 0.2054\n",
      "Epoch [2/5], Step [230/10336], Loss: 0.2137\n",
      "Epoch [2/5], Step [232/10336], Loss: 0.8967\n",
      "Epoch [2/5], Step [234/10336], Loss: 0.6328\n",
      "Epoch [2/5], Step [236/10336], Loss: 1.2929\n",
      "Epoch [2/5], Step [238/10336], Loss: 2.6672\n",
      "Epoch [2/5], Step [240/10336], Loss: 0.0572\n",
      "Epoch [2/5], Step [242/10336], Loss: 0.1220\n",
      "Epoch [2/5], Step [244/10336], Loss: 1.0728\n",
      "Epoch [2/5], Step [246/10336], Loss: 2.6707\n",
      "Epoch [2/5], Step [248/10336], Loss: 1.4806\n",
      "Epoch [2/5], Step [250/10336], Loss: 2.7036\n",
      "Epoch [2/5], Step [252/10336], Loss: 0.7621\n",
      "Epoch [2/5], Step [254/10336], Loss: 1.0105\n",
      "Epoch [2/5], Step [256/10336], Loss: 0.5667\n",
      "Epoch [2/5], Step [258/10336], Loss: 0.4680\n",
      "Epoch [2/5], Step [260/10336], Loss: 0.5582\n",
      "Epoch [2/5], Step [262/10336], Loss: 0.3066\n",
      "Epoch [2/5], Step [264/10336], Loss: 0.6572\n",
      "Epoch [2/5], Step [266/10336], Loss: 1.3124\n",
      "Epoch [2/5], Step [268/10336], Loss: 0.0777\n",
      "Epoch [2/5], Step [270/10336], Loss: 0.1514\n",
      "Epoch [2/5], Step [272/10336], Loss: 3.5450\n",
      "Epoch [2/5], Step [274/10336], Loss: 1.6308\n",
      "Epoch [2/5], Step [276/10336], Loss: 1.5180\n",
      "Epoch [2/5], Step [278/10336], Loss: 0.3062\n",
      "Epoch [2/5], Step [280/10336], Loss: 0.5952\n",
      "Epoch [2/5], Step [282/10336], Loss: 1.7806\n",
      "Epoch [2/5], Step [284/10336], Loss: 2.5769\n",
      "Epoch [2/5], Step [286/10336], Loss: 2.0559\n",
      "Epoch [2/5], Step [288/10336], Loss: 0.3092\n",
      "Epoch [2/5], Step [290/10336], Loss: 1.2843\n",
      "Epoch [2/5], Step [292/10336], Loss: 0.9815\n",
      "Epoch [2/5], Step [294/10336], Loss: 0.1866\n",
      "Epoch [2/5], Step [296/10336], Loss: 1.3189\n",
      "Epoch [2/5], Step [298/10336], Loss: 0.1203\n",
      "Epoch [2/5], Step [300/10336], Loss: 0.6488\n",
      "Epoch [2/5], Step [302/10336], Loss: 0.0239\n",
      "Epoch [2/5], Step [304/10336], Loss: 0.8946\n",
      "Epoch [2/5], Step [306/10336], Loss: 1.7537\n",
      "Epoch [2/5], Step [308/10336], Loss: 0.0534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [310/10336], Loss: 0.6012\n",
      "Epoch [2/5], Step [312/10336], Loss: 2.5899\n",
      "Epoch [2/5], Step [314/10336], Loss: 0.0865\n",
      "Epoch [2/5], Step [316/10336], Loss: 1.6707\n",
      "Epoch [2/5], Step [318/10336], Loss: 2.8534\n",
      "Epoch [2/5], Step [320/10336], Loss: 2.1143\n",
      "Epoch [2/5], Step [322/10336], Loss: 1.1229\n",
      "Epoch [2/5], Step [324/10336], Loss: 3.0861\n",
      "Epoch [2/5], Step [326/10336], Loss: 0.6797\n",
      "Epoch [2/5], Step [328/10336], Loss: 0.7758\n",
      "Epoch [2/5], Step [330/10336], Loss: 1.4987\n",
      "Epoch [2/5], Step [332/10336], Loss: 2.0291\n",
      "Epoch [2/5], Step [334/10336], Loss: 0.6792\n",
      "Epoch [2/5], Step [336/10336], Loss: 1.3896\n",
      "Epoch [2/5], Step [338/10336], Loss: 0.0971\n",
      "Epoch [2/5], Step [340/10336], Loss: 0.3120\n",
      "Epoch [2/5], Step [342/10336], Loss: 0.3361\n",
      "Epoch [2/5], Step [344/10336], Loss: 1.6351\n",
      "Epoch [2/5], Step [346/10336], Loss: 0.5874\n",
      "Epoch [2/5], Step [348/10336], Loss: 2.7014\n",
      "Epoch [2/5], Step [350/10336], Loss: 1.6906\n",
      "Epoch [2/5], Step [352/10336], Loss: 0.7372\n",
      "Epoch [2/5], Step [354/10336], Loss: 0.0515\n",
      "Epoch [2/5], Step [356/10336], Loss: 1.6688\n",
      "Epoch [2/5], Step [358/10336], Loss: 0.5905\n",
      "Epoch [2/5], Step [360/10336], Loss: 0.5570\n",
      "Epoch [2/5], Step [362/10336], Loss: 0.8779\n",
      "Epoch [2/5], Step [364/10336], Loss: 0.1322\n",
      "Epoch [2/5], Step [366/10336], Loss: 0.9532\n",
      "Epoch [2/5], Step [368/10336], Loss: 2.2378\n",
      "Epoch [2/5], Step [370/10336], Loss: 0.1630\n",
      "Epoch [2/5], Step [372/10336], Loss: 0.0407\n",
      "Epoch [2/5], Step [374/10336], Loss: 1.1344\n",
      "Epoch [2/5], Step [376/10336], Loss: 2.0711\n",
      "Epoch [2/5], Step [378/10336], Loss: 0.3554\n",
      "Epoch [2/5], Step [380/10336], Loss: 1.4729\n",
      "Epoch [2/5], Step [382/10336], Loss: 0.0127\n",
      "Epoch [2/5], Step [384/10336], Loss: 0.0978\n",
      "Epoch [2/5], Step [386/10336], Loss: 0.1379\n",
      "Epoch [2/5], Step [388/10336], Loss: 0.0189\n",
      "Epoch [2/5], Step [390/10336], Loss: 1.0891\n",
      "Epoch [2/5], Step [392/10336], Loss: 2.5266\n",
      "Epoch [2/5], Step [394/10336], Loss: 0.0070\n",
      "Epoch [2/5], Step [396/10336], Loss: 1.1906\n",
      "Epoch [2/5], Step [398/10336], Loss: 0.1649\n",
      "Epoch [2/5], Step [400/10336], Loss: 4.2326\n",
      "Epoch [2/5], Step [402/10336], Loss: 0.1278\n",
      "Epoch [2/5], Step [404/10336], Loss: 0.1076\n",
      "Epoch [2/5], Step [406/10336], Loss: 1.3632\n",
      "Epoch [2/5], Step [408/10336], Loss: 0.6942\n",
      "Epoch [2/5], Step [410/10336], Loss: 0.9025\n",
      "Epoch [2/5], Step [412/10336], Loss: 0.3607\n",
      "Epoch [2/5], Step [414/10336], Loss: 1.0405\n",
      "Epoch [2/5], Step [416/10336], Loss: 1.1577\n",
      "Epoch [2/5], Step [418/10336], Loss: 0.0524\n",
      "Epoch [2/5], Step [420/10336], Loss: 0.2077\n",
      "Epoch [2/5], Step [422/10336], Loss: 2.8424\n",
      "Epoch [2/5], Step [424/10336], Loss: 3.5945\n",
      "Epoch [2/5], Step [426/10336], Loss: 0.3544\n",
      "Epoch [2/5], Step [428/10336], Loss: 0.3584\n",
      "Epoch [2/5], Step [430/10336], Loss: 0.9756\n",
      "Epoch [2/5], Step [432/10336], Loss: 1.2914\n",
      "Epoch [2/5], Step [434/10336], Loss: 2.1187\n",
      "Epoch [2/5], Step [436/10336], Loss: 1.2360\n",
      "Epoch [2/5], Step [438/10336], Loss: 2.2765\n",
      "Epoch [2/5], Step [440/10336], Loss: 0.6387\n",
      "Epoch [2/5], Step [442/10336], Loss: 1.9402\n",
      "Epoch [2/5], Step [444/10336], Loss: 1.7461\n",
      "Epoch [2/5], Step [446/10336], Loss: 2.0071\n",
      "Epoch [2/5], Step [448/10336], Loss: 2.1068\n",
      "Epoch [2/5], Step [450/10336], Loss: 0.0531\n",
      "Epoch [2/5], Step [452/10336], Loss: 0.4062\n",
      "Epoch [2/5], Step [454/10336], Loss: 1.3960\n",
      "Epoch [2/5], Step [456/10336], Loss: 1.5265\n",
      "Epoch [2/5], Step [458/10336], Loss: 0.7427\n",
      "Epoch [2/5], Step [460/10336], Loss: 0.0525\n",
      "Epoch [2/5], Step [462/10336], Loss: 0.5355\n",
      "Epoch [2/5], Step [464/10336], Loss: 0.3373\n",
      "Epoch [2/5], Step [466/10336], Loss: 3.4307\n",
      "Epoch [2/5], Step [468/10336], Loss: 0.1665\n",
      "Epoch [2/5], Step [470/10336], Loss: 0.0646\n",
      "Epoch [2/5], Step [472/10336], Loss: 1.6901\n",
      "Epoch [2/5], Step [474/10336], Loss: 1.3694\n",
      "Epoch [2/5], Step [476/10336], Loss: 0.3254\n",
      "Epoch [2/5], Step [478/10336], Loss: 0.2201\n",
      "Epoch [2/5], Step [480/10336], Loss: 0.0243\n",
      "Epoch [2/5], Step [482/10336], Loss: 1.3380\n",
      "Epoch [2/5], Step [484/10336], Loss: 0.0610\n",
      "Epoch [2/5], Step [486/10336], Loss: 0.0390\n",
      "Epoch [2/5], Step [488/10336], Loss: 1.1336\n",
      "Epoch [2/5], Step [490/10336], Loss: 4.6417\n",
      "Epoch [2/5], Step [492/10336], Loss: 0.2096\n",
      "Epoch [2/5], Step [494/10336], Loss: 0.7687\n",
      "Epoch [2/5], Step [496/10336], Loss: 1.3674\n",
      "Epoch [2/5], Step [498/10336], Loss: 0.8666\n",
      "Epoch [2/5], Step [500/10336], Loss: 1.1941\n",
      "Epoch [2/5], Step [502/10336], Loss: 0.9390\n",
      "Epoch [2/5], Step [504/10336], Loss: 0.8729\n",
      "Epoch [2/5], Step [506/10336], Loss: 0.4722\n",
      "Epoch [2/5], Step [508/10336], Loss: 1.6480\n",
      "Epoch [2/5], Step [510/10336], Loss: 0.9945\n",
      "Epoch [2/5], Step [512/10336], Loss: 2.3443\n",
      "Epoch [2/5], Step [514/10336], Loss: 1.4107\n",
      "Epoch [2/5], Step [516/10336], Loss: 2.4597\n",
      "Epoch [2/5], Step [518/10336], Loss: 1.1472\n",
      "Epoch [2/5], Step [520/10336], Loss: 4.5603\n",
      "Epoch [2/5], Step [522/10336], Loss: 1.3168\n",
      "Epoch [2/5], Step [524/10336], Loss: 0.7747\n",
      "Epoch [2/5], Step [526/10336], Loss: 1.1214\n",
      "Epoch [2/5], Step [528/10336], Loss: 1.6347\n",
      "Epoch [2/5], Step [530/10336], Loss: 0.0140\n",
      "Epoch [2/5], Step [532/10336], Loss: 2.6161\n",
      "Epoch [2/5], Step [534/10336], Loss: 0.1817\n",
      "Epoch [2/5], Step [536/10336], Loss: 0.1732\n",
      "Epoch [2/5], Step [538/10336], Loss: 6.9508\n",
      "Epoch [2/5], Step [540/10336], Loss: 1.6887\n",
      "Epoch [2/5], Step [542/10336], Loss: 0.0912\n",
      "Epoch [2/5], Step [544/10336], Loss: 1.1005\n",
      "Epoch [2/5], Step [546/10336], Loss: 0.1646\n",
      "Epoch [2/5], Step [548/10336], Loss: 0.2045\n",
      "Epoch [2/5], Step [550/10336], Loss: 1.6297\n",
      "Epoch [2/5], Step [552/10336], Loss: 0.0239\n",
      "Epoch [2/5], Step [554/10336], Loss: 3.0636\n",
      "Epoch [2/5], Step [556/10336], Loss: 0.1523\n",
      "Epoch [2/5], Step [558/10336], Loss: 1.0274\n",
      "Epoch [2/5], Step [560/10336], Loss: 0.1797\n",
      "Epoch [2/5], Step [562/10336], Loss: 0.4335\n",
      "Epoch [2/5], Step [564/10336], Loss: 0.2878\n",
      "Epoch [2/5], Step [566/10336], Loss: 0.0329\n",
      "Epoch [2/5], Step [568/10336], Loss: 0.3211\n",
      "Epoch [2/5], Step [570/10336], Loss: 1.7088\n",
      "Epoch [2/5], Step [572/10336], Loss: 0.4858\n",
      "Epoch [2/5], Step [574/10336], Loss: 2.2566\n",
      "Epoch [2/5], Step [576/10336], Loss: 1.7559\n",
      "Epoch [2/5], Step [578/10336], Loss: 0.5446\n",
      "Epoch [2/5], Step [580/10336], Loss: 0.5480\n",
      "Epoch [2/5], Step [582/10336], Loss: 0.0509\n",
      "Epoch [2/5], Step [584/10336], Loss: 0.6768\n",
      "Epoch [2/5], Step [586/10336], Loss: 0.1374\n",
      "Epoch [2/5], Step [588/10336], Loss: 3.4173\n",
      "Epoch [2/5], Step [590/10336], Loss: 0.5960\n",
      "Epoch [2/5], Step [592/10336], Loss: 2.7614\n",
      "Epoch [2/5], Step [594/10336], Loss: 0.2120\n",
      "Epoch [2/5], Step [596/10336], Loss: 0.4578\n",
      "Epoch [2/5], Step [598/10336], Loss: 0.4065\n",
      "Epoch [2/5], Step [600/10336], Loss: 1.1478\n",
      "Epoch [2/5], Step [602/10336], Loss: 1.3125\n",
      "Epoch [2/5], Step [604/10336], Loss: 1.1033\n",
      "Epoch [2/5], Step [606/10336], Loss: 2.6673\n",
      "Epoch [2/5], Step [608/10336], Loss: 0.5821\n",
      "Epoch [2/5], Step [610/10336], Loss: 0.0950\n",
      "Epoch [2/5], Step [612/10336], Loss: 0.6278\n",
      "Epoch [2/5], Step [614/10336], Loss: 1.5257\n",
      "Epoch [2/5], Step [616/10336], Loss: 1.5784\n",
      "Epoch [2/5], Step [618/10336], Loss: 1.1668\n",
      "Epoch [2/5], Step [620/10336], Loss: 0.2117\n",
      "Epoch [2/5], Step [622/10336], Loss: 1.8078\n",
      "Epoch [2/5], Step [624/10336], Loss: 0.8962\n",
      "Epoch [2/5], Step [626/10336], Loss: 2.7449\n",
      "Epoch [2/5], Step [628/10336], Loss: 1.9996\n",
      "Epoch [2/5], Step [630/10336], Loss: 0.9618\n",
      "Epoch [2/5], Step [632/10336], Loss: 1.3834\n",
      "Epoch [2/5], Step [634/10336], Loss: 0.2095\n",
      "Epoch [2/5], Step [636/10336], Loss: 2.3775\n",
      "Epoch [2/5], Step [638/10336], Loss: 0.0771\n",
      "Epoch [2/5], Step [640/10336], Loss: 0.5457\n",
      "Epoch [2/5], Step [642/10336], Loss: 0.5039\n",
      "Epoch [2/5], Step [644/10336], Loss: 0.1628\n",
      "Epoch [2/5], Step [646/10336], Loss: 0.7370\n",
      "Epoch [2/5], Step [648/10336], Loss: 0.6091\n",
      "Epoch [2/5], Step [650/10336], Loss: 0.0059\n",
      "Epoch [2/5], Step [652/10336], Loss: 0.0270\n",
      "Epoch [2/5], Step [654/10336], Loss: 0.0933\n",
      "Epoch [2/5], Step [656/10336], Loss: 0.5829\n",
      "Epoch [2/5], Step [658/10336], Loss: 0.2385\n",
      "Epoch [2/5], Step [660/10336], Loss: 2.2980\n",
      "Epoch [2/5], Step [662/10336], Loss: 2.4133\n",
      "Epoch [2/5], Step [664/10336], Loss: 1.3032\n",
      "Epoch [2/5], Step [666/10336], Loss: 2.1045\n",
      "Epoch [2/5], Step [668/10336], Loss: 1.4050\n",
      "Epoch [2/5], Step [670/10336], Loss: 0.4012\n",
      "Epoch [2/5], Step [672/10336], Loss: 0.1873\n",
      "Epoch [2/5], Step [674/10336], Loss: 2.3294\n",
      "Epoch [2/5], Step [676/10336], Loss: 2.9343\n",
      "Epoch [2/5], Step [678/10336], Loss: 0.0675\n",
      "Epoch [2/5], Step [680/10336], Loss: 1.3858\n",
      "Epoch [2/5], Step [682/10336], Loss: 0.6375\n",
      "Epoch [2/5], Step [684/10336], Loss: 0.6420\n",
      "Epoch [2/5], Step [686/10336], Loss: 0.5819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [688/10336], Loss: 1.0891\n",
      "Epoch [2/5], Step [690/10336], Loss: 1.7505\n",
      "Epoch [2/5], Step [692/10336], Loss: 1.3848\n",
      "Epoch [2/5], Step [694/10336], Loss: 1.7707\n",
      "Epoch [2/5], Step [696/10336], Loss: 0.0527\n",
      "Epoch [2/5], Step [698/10336], Loss: 1.8288\n",
      "Epoch [2/5], Step [700/10336], Loss: 0.6285\n",
      "Epoch [2/5], Step [702/10336], Loss: 0.2074\n",
      "Epoch [2/5], Step [704/10336], Loss: 0.6746\n",
      "Epoch [2/5], Step [706/10336], Loss: 2.0520\n",
      "Epoch [2/5], Step [708/10336], Loss: 1.4979\n",
      "Epoch [2/5], Step [710/10336], Loss: 1.3835\n",
      "Epoch [2/5], Step [712/10336], Loss: 1.8550\n",
      "Epoch [2/5], Step [714/10336], Loss: 0.0294\n",
      "Epoch [2/5], Step [716/10336], Loss: 1.2560\n",
      "Epoch [2/5], Step [718/10336], Loss: 1.5411\n",
      "Epoch [2/5], Step [720/10336], Loss: 0.8650\n",
      "Epoch [2/5], Step [722/10336], Loss: 2.7429\n",
      "Epoch [2/5], Step [724/10336], Loss: 3.1838\n",
      "Epoch [2/5], Step [726/10336], Loss: 1.6748\n",
      "Epoch [2/5], Step [728/10336], Loss: 1.2876\n",
      "Epoch [2/5], Step [730/10336], Loss: 2.0452\n",
      "Epoch [2/5], Step [732/10336], Loss: 0.8362\n",
      "Epoch [2/5], Step [734/10336], Loss: 1.5966\n",
      "Epoch [2/5], Step [736/10336], Loss: 0.8674\n",
      "Epoch [2/5], Step [738/10336], Loss: 2.6058\n",
      "Epoch [2/5], Step [740/10336], Loss: 2.5207\n",
      "Epoch [2/5], Step [742/10336], Loss: 0.4500\n",
      "Epoch [2/5], Step [744/10336], Loss: 0.0130\n",
      "Epoch [2/5], Step [746/10336], Loss: 0.5683\n",
      "Epoch [2/5], Step [748/10336], Loss: 0.1611\n",
      "Epoch [2/5], Step [750/10336], Loss: 0.5447\n",
      "Epoch [2/5], Step [752/10336], Loss: 0.3370\n",
      "Epoch [2/5], Step [754/10336], Loss: 1.1449\n",
      "Epoch [2/5], Step [756/10336], Loss: 0.6976\n",
      "Epoch [2/5], Step [758/10336], Loss: 0.0558\n",
      "Epoch [2/5], Step [760/10336], Loss: 0.0186\n",
      "Epoch [2/5], Step [762/10336], Loss: 3.1485\n",
      "Epoch [2/5], Step [764/10336], Loss: 1.0715\n",
      "Epoch [2/5], Step [766/10336], Loss: 1.6968\n",
      "Epoch [2/5], Step [768/10336], Loss: 0.0552\n",
      "Epoch [2/5], Step [770/10336], Loss: 0.8739\n",
      "Epoch [2/5], Step [772/10336], Loss: 0.0286\n",
      "Epoch [2/5], Step [774/10336], Loss: 0.6208\n",
      "Epoch [2/5], Step [776/10336], Loss: 2.1961\n",
      "Epoch [2/5], Step [778/10336], Loss: 0.3075\n",
      "Epoch [2/5], Step [780/10336], Loss: 1.5022\n",
      "Epoch [2/5], Step [782/10336], Loss: 0.0684\n",
      "Epoch [2/5], Step [784/10336], Loss: 2.5911\n",
      "Epoch [2/5], Step [786/10336], Loss: 0.0469\n",
      "Epoch [2/5], Step [788/10336], Loss: 1.1635\n",
      "Epoch [2/5], Step [790/10336], Loss: 2.3474\n",
      "Epoch [2/5], Step [792/10336], Loss: 0.8081\n",
      "Epoch [2/5], Step [794/10336], Loss: 1.4991\n",
      "Epoch [2/5], Step [796/10336], Loss: 0.1203\n",
      "Epoch [2/5], Step [798/10336], Loss: 1.8916\n",
      "Epoch [2/5], Step [800/10336], Loss: 4.8151\n",
      "Epoch [2/5], Step [802/10336], Loss: 1.0271\n",
      "Epoch [2/5], Step [804/10336], Loss: 2.6344\n",
      "Epoch [2/5], Step [806/10336], Loss: 2.6243\n",
      "Epoch [2/5], Step [808/10336], Loss: 0.0182\n",
      "Epoch [2/5], Step [810/10336], Loss: 1.8216\n",
      "Epoch [2/5], Step [812/10336], Loss: 0.6359\n",
      "Epoch [2/5], Step [814/10336], Loss: 0.1410\n",
      "Epoch [2/5], Step [816/10336], Loss: 1.5131\n",
      "Epoch [2/5], Step [818/10336], Loss: 1.4644\n",
      "Epoch [2/5], Step [820/10336], Loss: 1.6778\n",
      "Epoch [2/5], Step [822/10336], Loss: 1.6869\n",
      "Epoch [2/5], Step [824/10336], Loss: 0.2701\n",
      "Epoch [2/5], Step [826/10336], Loss: 0.9683\n",
      "Epoch [2/5], Step [828/10336], Loss: 0.7137\n",
      "Epoch [2/5], Step [830/10336], Loss: 1.4737\n",
      "Epoch [2/5], Step [832/10336], Loss: 1.7075\n",
      "Epoch [2/5], Step [834/10336], Loss: 0.2497\n",
      "Epoch [2/5], Step [836/10336], Loss: 1.1180\n",
      "Epoch [2/5], Step [838/10336], Loss: 2.1301\n",
      "Epoch [2/5], Step [840/10336], Loss: 2.4145\n",
      "Epoch [2/5], Step [842/10336], Loss: 2.5160\n",
      "Epoch [2/5], Step [844/10336], Loss: 0.4015\n",
      "Epoch [2/5], Step [846/10336], Loss: 1.5048\n",
      "Epoch [2/5], Step [848/10336], Loss: 0.5033\n",
      "Epoch [2/5], Step [850/10336], Loss: 0.0655\n",
      "Epoch [2/5], Step [852/10336], Loss: 0.1538\n",
      "Epoch [2/5], Step [854/10336], Loss: 4.9806\n",
      "Epoch [2/5], Step [856/10336], Loss: 0.1789\n",
      "Epoch [2/5], Step [858/10336], Loss: 0.2179\n",
      "Epoch [2/5], Step [860/10336], Loss: 0.5986\n",
      "Epoch [2/5], Step [862/10336], Loss: 1.0937\n",
      "Epoch [2/5], Step [864/10336], Loss: 0.0119\n",
      "Epoch [2/5], Step [866/10336], Loss: 0.2631\n",
      "Epoch [2/5], Step [868/10336], Loss: 2.2503\n",
      "Epoch [2/5], Step [870/10336], Loss: 2.0403\n",
      "Epoch [2/5], Step [872/10336], Loss: 0.0658\n",
      "Epoch [2/5], Step [874/10336], Loss: 0.0137\n",
      "Epoch [2/5], Step [876/10336], Loss: 2.8139\n",
      "Epoch [2/5], Step [878/10336], Loss: 1.3263\n",
      "Epoch [2/5], Step [880/10336], Loss: 3.2973\n",
      "Epoch [2/5], Step [882/10336], Loss: 0.1326\n",
      "Epoch [2/5], Step [884/10336], Loss: 0.2573\n",
      "Epoch [2/5], Step [886/10336], Loss: 0.4312\n",
      "Epoch [2/5], Step [888/10336], Loss: 0.0526\n",
      "Epoch [2/5], Step [890/10336], Loss: 1.0986\n",
      "Epoch [2/5], Step [892/10336], Loss: 2.0179\n",
      "Epoch [2/5], Step [894/10336], Loss: 0.1534\n",
      "Epoch [2/5], Step [896/10336], Loss: 1.2701\n",
      "Epoch [2/5], Step [898/10336], Loss: 2.9052\n",
      "Epoch [2/5], Step [900/10336], Loss: 1.0185\n",
      "Epoch [2/5], Step [902/10336], Loss: 0.2193\n",
      "Epoch [2/5], Step [904/10336], Loss: 0.0201\n",
      "Epoch [2/5], Step [906/10336], Loss: 2.5355\n",
      "Epoch [2/5], Step [908/10336], Loss: 1.3298\n",
      "Epoch [2/5], Step [910/10336], Loss: 2.1980\n",
      "Epoch [2/5], Step [912/10336], Loss: 0.0878\n",
      "Epoch [2/5], Step [914/10336], Loss: 0.0171\n",
      "Epoch [2/5], Step [916/10336], Loss: 1.8278\n",
      "Epoch [2/5], Step [918/10336], Loss: 0.1917\n",
      "Epoch [2/5], Step [920/10336], Loss: 0.6411\n",
      "Epoch [2/5], Step [922/10336], Loss: 0.8290\n",
      "Epoch [2/5], Step [924/10336], Loss: 0.1072\n",
      "Epoch [2/5], Step [926/10336], Loss: 1.7322\n",
      "Epoch [2/5], Step [928/10336], Loss: 1.9207\n",
      "Epoch [2/5], Step [930/10336], Loss: 3.3748\n",
      "Epoch [2/5], Step [932/10336], Loss: 1.1414\n",
      "Epoch [2/5], Step [934/10336], Loss: 0.5978\n",
      "Epoch [2/5], Step [936/10336], Loss: 0.6330\n",
      "Epoch [2/5], Step [938/10336], Loss: 1.0089\n",
      "Epoch [2/5], Step [940/10336], Loss: 0.8570\n",
      "Epoch [2/5], Step [942/10336], Loss: 4.6718\n",
      "Epoch [2/5], Step [944/10336], Loss: 1.7729\n",
      "Epoch [2/5], Step [946/10336], Loss: 0.2404\n",
      "Epoch [2/5], Step [948/10336], Loss: 1.0952\n",
      "Epoch [2/5], Step [950/10336], Loss: 0.0952\n",
      "Epoch [2/5], Step [952/10336], Loss: 0.1129\n",
      "Epoch [2/5], Step [954/10336], Loss: 0.2434\n",
      "Epoch [2/5], Step [956/10336], Loss: 0.0207\n",
      "Epoch [2/5], Step [958/10336], Loss: 0.1440\n",
      "Epoch [2/5], Step [960/10336], Loss: 1.0637\n",
      "Epoch [2/5], Step [962/10336], Loss: 1.7056\n",
      "Epoch [2/5], Step [964/10336], Loss: 0.4655\n",
      "Epoch [2/5], Step [966/10336], Loss: 0.2324\n",
      "Epoch [2/5], Step [968/10336], Loss: 0.8084\n",
      "Epoch [2/5], Step [970/10336], Loss: 0.2293\n",
      "Epoch [2/5], Step [972/10336], Loss: 0.0664\n",
      "Epoch [2/5], Step [974/10336], Loss: 0.7021\n",
      "Epoch [2/5], Step [976/10336], Loss: 0.2565\n",
      "Epoch [2/5], Step [978/10336], Loss: 0.4777\n",
      "Epoch [2/5], Step [980/10336], Loss: 0.3766\n",
      "Epoch [2/5], Step [982/10336], Loss: 0.0610\n",
      "Epoch [2/5], Step [984/10336], Loss: 1.7085\n",
      "Epoch [2/5], Step [986/10336], Loss: 0.3316\n",
      "Epoch [2/5], Step [988/10336], Loss: 0.2426\n",
      "Epoch [2/5], Step [990/10336], Loss: 0.0355\n",
      "Epoch [2/5], Step [992/10336], Loss: 2.7494\n",
      "Epoch [2/5], Step [994/10336], Loss: 0.2938\n",
      "Epoch [2/5], Step [996/10336], Loss: 0.2946\n",
      "Epoch [2/5], Step [998/10336], Loss: 0.1770\n",
      "Epoch [2/5], Step [1000/10336], Loss: 0.0545\n",
      "Epoch [2/5], Step [1002/10336], Loss: 1.0249\n",
      "Epoch [2/5], Step [1004/10336], Loss: 0.0234\n",
      "Epoch [2/5], Step [1006/10336], Loss: 0.1338\n",
      "Epoch [2/5], Step [1008/10336], Loss: 2.3703\n",
      "Epoch [2/5], Step [1010/10336], Loss: 0.2246\n",
      "Epoch [2/5], Step [1012/10336], Loss: 3.0219\n",
      "Epoch [2/5], Step [1014/10336], Loss: 0.0290\n",
      "Epoch [2/5], Step [1016/10336], Loss: 2.5499\n",
      "Epoch [2/5], Step [1018/10336], Loss: 1.1985\n",
      "Epoch [2/5], Step [1020/10336], Loss: 0.7700\n",
      "Epoch [2/5], Step [1022/10336], Loss: 0.1033\n",
      "Epoch [2/5], Step [1024/10336], Loss: 0.0717\n",
      "Epoch [2/5], Step [1026/10336], Loss: 0.0897\n",
      "Epoch [2/5], Step [1028/10336], Loss: 1.2616\n",
      "Epoch [2/5], Step [1030/10336], Loss: 1.0341\n",
      "Epoch [2/5], Step [1032/10336], Loss: 0.3928\n",
      "Epoch [2/5], Step [1034/10336], Loss: 0.0232\n",
      "Epoch [2/5], Step [1036/10336], Loss: 0.0264\n",
      "Epoch [2/5], Step [1038/10336], Loss: 0.7671\n",
      "Epoch [2/5], Step [1040/10336], Loss: 1.8150\n",
      "Epoch [2/5], Step [1042/10336], Loss: 0.0416\n",
      "Epoch [2/5], Step [1044/10336], Loss: 0.8795\n",
      "Epoch [2/5], Step [1046/10336], Loss: 2.9293\n",
      "Epoch [2/5], Step [1048/10336], Loss: 1.9448\n",
      "Epoch [2/5], Step [1050/10336], Loss: 0.6979\n",
      "Epoch [2/5], Step [1052/10336], Loss: 0.4391\n",
      "Epoch [2/5], Step [1054/10336], Loss: 0.0677\n",
      "Epoch [2/5], Step [1056/10336], Loss: 4.3113\n",
      "Epoch [2/5], Step [1058/10336], Loss: 0.4885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [1060/10336], Loss: 0.0274\n",
      "Epoch [2/5], Step [1062/10336], Loss: 0.2859\n",
      "Epoch [2/5], Step [1064/10336], Loss: 1.9932\n",
      "Epoch [2/5], Step [1066/10336], Loss: 3.8102\n",
      "Epoch [2/5], Step [1068/10336], Loss: 0.2591\n",
      "Epoch [2/5], Step [1070/10336], Loss: 0.9496\n",
      "Epoch [2/5], Step [1072/10336], Loss: 0.5391\n",
      "Epoch [2/5], Step [1074/10336], Loss: 0.2647\n",
      "Epoch [2/5], Step [1076/10336], Loss: 0.5960\n",
      "Epoch [2/5], Step [1078/10336], Loss: 0.4285\n",
      "Epoch [2/5], Step [1080/10336], Loss: 1.2817\n",
      "Epoch [2/5], Step [1082/10336], Loss: 1.3109\n",
      "Epoch [2/5], Step [1084/10336], Loss: 1.8483\n",
      "Epoch [2/5], Step [1086/10336], Loss: 0.0440\n",
      "Epoch [2/5], Step [1088/10336], Loss: 0.6916\n",
      "Epoch [2/5], Step [1090/10336], Loss: 1.1699\n",
      "Epoch [2/5], Step [1092/10336], Loss: 2.2810\n",
      "Epoch [2/5], Step [1094/10336], Loss: 0.8684\n",
      "Epoch [2/5], Step [1096/10336], Loss: 1.1156\n",
      "Epoch [2/5], Step [1098/10336], Loss: 2.6256\n",
      "Epoch [2/5], Step [1100/10336], Loss: 0.1115\n",
      "Epoch [2/5], Step [1102/10336], Loss: 2.3115\n",
      "Epoch [2/5], Step [1104/10336], Loss: 0.2964\n",
      "Epoch [2/5], Step [1106/10336], Loss: 1.2694\n",
      "Epoch [2/5], Step [1108/10336], Loss: 0.8698\n",
      "Epoch [2/5], Step [1110/10336], Loss: 0.6272\n",
      "Epoch [2/5], Step [1112/10336], Loss: 1.7427\n",
      "Epoch [2/5], Step [1114/10336], Loss: 0.0496\n",
      "Epoch [2/5], Step [1116/10336], Loss: 0.0533\n",
      "Epoch [2/5], Step [1118/10336], Loss: 2.0194\n",
      "Epoch [2/5], Step [1120/10336], Loss: 0.8411\n",
      "Epoch [2/5], Step [1122/10336], Loss: 1.0968\n",
      "Epoch [2/5], Step [1124/10336], Loss: 0.1453\n",
      "Epoch [2/5], Step [1126/10336], Loss: 2.0785\n",
      "Epoch [2/5], Step [1128/10336], Loss: 0.9043\n",
      "Epoch [2/5], Step [1130/10336], Loss: 1.3103\n",
      "Epoch [2/5], Step [1132/10336], Loss: 1.9553\n",
      "Epoch [2/5], Step [1134/10336], Loss: 4.8625\n",
      "Epoch [2/5], Step [1136/10336], Loss: 1.3759\n",
      "Epoch [2/5], Step [1138/10336], Loss: 1.4893\n",
      "Epoch [2/5], Step [1140/10336], Loss: 1.0916\n",
      "Epoch [2/5], Step [1142/10336], Loss: 0.6936\n",
      "Epoch [2/5], Step [1144/10336], Loss: 2.0405\n",
      "Epoch [2/5], Step [1146/10336], Loss: 1.5033\n",
      "Epoch [2/5], Step [1148/10336], Loss: 0.9729\n",
      "Epoch [2/5], Step [1150/10336], Loss: 1.6085\n",
      "Epoch [2/5], Step [1152/10336], Loss: 0.6785\n",
      "Epoch [2/5], Step [1154/10336], Loss: 0.7108\n",
      "Epoch [2/5], Step [1156/10336], Loss: 1.0970\n",
      "Epoch [2/5], Step [1158/10336], Loss: 2.0516\n",
      "Epoch [2/5], Step [1160/10336], Loss: 1.7347\n",
      "Epoch [2/5], Step [1162/10336], Loss: 0.6112\n",
      "Epoch [2/5], Step [1164/10336], Loss: 0.0501\n",
      "Epoch [2/5], Step [1166/10336], Loss: 0.5029\n",
      "Epoch [2/5], Step [1168/10336], Loss: 0.8082\n",
      "Epoch [2/5], Step [1170/10336], Loss: 1.3099\n",
      "Epoch [2/5], Step [1172/10336], Loss: 2.7740\n",
      "Epoch [2/5], Step [1174/10336], Loss: 3.2103\n",
      "Epoch [2/5], Step [1176/10336], Loss: 0.0117\n",
      "Epoch [2/5], Step [1178/10336], Loss: 2.9738\n",
      "Epoch [2/5], Step [1180/10336], Loss: 0.0462\n",
      "Epoch [2/5], Step [1182/10336], Loss: 1.5078\n",
      "Epoch [2/5], Step [1184/10336], Loss: 1.0939\n",
      "Epoch [2/5], Step [1186/10336], Loss: 1.7614\n",
      "Epoch [2/5], Step [1188/10336], Loss: 0.1692\n",
      "Epoch [2/5], Step [1190/10336], Loss: 1.4855\n",
      "Epoch [2/5], Step [1192/10336], Loss: 2.2051\n",
      "Epoch [2/5], Step [1194/10336], Loss: 0.4282\n",
      "Epoch [2/5], Step [1196/10336], Loss: 2.9920\n",
      "Epoch [2/5], Step [1198/10336], Loss: 1.1653\n",
      "Epoch [2/5], Step [1200/10336], Loss: 2.6149\n",
      "Epoch [2/5], Step [1202/10336], Loss: 0.3801\n",
      "Epoch [2/5], Step [1204/10336], Loss: 0.5049\n",
      "Epoch [2/5], Step [1206/10336], Loss: 0.4223\n",
      "Epoch [2/5], Step [1208/10336], Loss: 0.8157\n",
      "Epoch [2/5], Step [1210/10336], Loss: 0.4236\n",
      "Epoch [2/5], Step [1212/10336], Loss: 0.8408\n",
      "Epoch [2/5], Step [1214/10336], Loss: 0.6026\n",
      "Epoch [2/5], Step [1216/10336], Loss: 0.1543\n",
      "Epoch [2/5], Step [1218/10336], Loss: 0.0687\n",
      "Epoch [2/5], Step [1220/10336], Loss: 0.1150\n",
      "Epoch [2/5], Step [1222/10336], Loss: 1.2270\n",
      "Epoch [2/5], Step [1224/10336], Loss: 1.7169\n",
      "Epoch [2/5], Step [1226/10336], Loss: 0.1872\n",
      "Epoch [2/5], Step [1228/10336], Loss: 0.2224\n",
      "Epoch [2/5], Step [1230/10336], Loss: 0.5128\n",
      "Epoch [2/5], Step [1232/10336], Loss: 0.2430\n",
      "Epoch [2/5], Step [1234/10336], Loss: 1.8751\n",
      "Epoch [2/5], Step [1236/10336], Loss: 0.0779\n",
      "Epoch [2/5], Step [1238/10336], Loss: 0.1117\n",
      "Epoch [2/5], Step [1240/10336], Loss: 3.6389\n",
      "Epoch [2/5], Step [1242/10336], Loss: 0.0685\n",
      "Epoch [2/5], Step [1244/10336], Loss: 4.9225\n",
      "Epoch [2/5], Step [1246/10336], Loss: 0.1055\n",
      "Epoch [2/5], Step [1248/10336], Loss: 1.0025\n",
      "Epoch [2/5], Step [1250/10336], Loss: 0.5900\n",
      "Epoch [2/5], Step [1252/10336], Loss: 2.5269\n",
      "Epoch [2/5], Step [1254/10336], Loss: 0.3187\n",
      "Epoch [2/5], Step [1256/10336], Loss: 2.6552\n",
      "Epoch [2/5], Step [1258/10336], Loss: 0.1516\n",
      "Epoch [2/5], Step [1260/10336], Loss: 0.2034\n",
      "Epoch [2/5], Step [1262/10336], Loss: 1.1370\n",
      "Epoch [2/5], Step [1264/10336], Loss: 0.0828\n",
      "Epoch [2/5], Step [1266/10336], Loss: 1.9438\n",
      "Epoch [2/5], Step [1268/10336], Loss: 0.1789\n",
      "Epoch [2/5], Step [1270/10336], Loss: 0.3955\n",
      "Epoch [2/5], Step [1272/10336], Loss: 0.9265\n",
      "Epoch [2/5], Step [1274/10336], Loss: 0.0473\n",
      "Epoch [2/5], Step [1276/10336], Loss: 1.7347\n",
      "Epoch [2/5], Step [1278/10336], Loss: 1.0321\n",
      "Epoch [2/5], Step [1280/10336], Loss: 0.6919\n",
      "Epoch [2/5], Step [1282/10336], Loss: 0.3971\n",
      "Epoch [2/5], Step [1284/10336], Loss: 0.0092\n",
      "Epoch [2/5], Step [1286/10336], Loss: 0.7762\n",
      "Epoch [2/5], Step [1288/10336], Loss: 0.0937\n",
      "Epoch [2/5], Step [1290/10336], Loss: 0.4826\n",
      "Epoch [2/5], Step [1292/10336], Loss: 1.5857\n",
      "Epoch [2/5], Step [1294/10336], Loss: 2.1236\n",
      "Epoch [2/5], Step [1296/10336], Loss: 0.2797\n",
      "Epoch [2/5], Step [1298/10336], Loss: 1.2347\n",
      "Epoch [2/5], Step [1300/10336], Loss: 1.4289\n",
      "Epoch [2/5], Step [1302/10336], Loss: 0.8724\n",
      "Epoch [2/5], Step [1304/10336], Loss: 0.1771\n",
      "Epoch [2/5], Step [1306/10336], Loss: 0.2483\n",
      "Epoch [2/5], Step [1308/10336], Loss: 0.4630\n",
      "Epoch [2/5], Step [1310/10336], Loss: 0.5025\n",
      "Epoch [2/5], Step [1312/10336], Loss: 1.7764\n",
      "Epoch [2/5], Step [1314/10336], Loss: 2.7440\n",
      "Epoch [2/5], Step [1316/10336], Loss: 0.7927\n",
      "Epoch [2/5], Step [1318/10336], Loss: 0.0248\n",
      "Epoch [2/5], Step [1320/10336], Loss: 3.4109\n",
      "Epoch [2/5], Step [1322/10336], Loss: 3.9033\n",
      "Epoch [2/5], Step [1324/10336], Loss: 0.6817\n",
      "Epoch [2/5], Step [1326/10336], Loss: 0.3305\n",
      "Epoch [2/5], Step [1328/10336], Loss: 2.9109\n",
      "Epoch [2/5], Step [1330/10336], Loss: 0.2669\n",
      "Epoch [2/5], Step [1332/10336], Loss: 1.6960\n",
      "Epoch [2/5], Step [1334/10336], Loss: 1.8082\n",
      "Epoch [2/5], Step [1336/10336], Loss: 0.2433\n",
      "Epoch [2/5], Step [1338/10336], Loss: 2.5963\n",
      "Epoch [2/5], Step [1340/10336], Loss: 0.9277\n",
      "Epoch [2/5], Step [1342/10336], Loss: 1.3523\n",
      "Epoch [2/5], Step [1344/10336], Loss: 0.9728\n",
      "Epoch [2/5], Step [1346/10336], Loss: 0.8454\n",
      "Epoch [2/5], Step [1348/10336], Loss: 0.7069\n",
      "Epoch [2/5], Step [1350/10336], Loss: 0.2735\n",
      "Epoch [2/5], Step [1352/10336], Loss: 0.1954\n",
      "Epoch [2/5], Step [1354/10336], Loss: 0.6637\n",
      "Epoch [2/5], Step [1356/10336], Loss: 0.0876\n",
      "Epoch [2/5], Step [1358/10336], Loss: 0.1368\n",
      "Epoch [2/5], Step [1360/10336], Loss: 0.4096\n",
      "Epoch [2/5], Step [1362/10336], Loss: 3.4222\n",
      "Epoch [2/5], Step [1364/10336], Loss: 0.1993\n",
      "Epoch [2/5], Step [1366/10336], Loss: 0.0119\n",
      "Epoch [2/5], Step [1368/10336], Loss: 0.0215\n",
      "Epoch [2/5], Step [1370/10336], Loss: 3.8093\n",
      "Epoch [2/5], Step [1372/10336], Loss: 0.1115\n",
      "Epoch [2/5], Step [1374/10336], Loss: 4.3493\n",
      "Epoch [2/5], Step [1376/10336], Loss: 0.0210\n",
      "Epoch [2/5], Step [1378/10336], Loss: 3.5560\n",
      "Epoch [2/5], Step [1380/10336], Loss: 1.7787\n",
      "Epoch [2/5], Step [1382/10336], Loss: 1.1767\n",
      "Epoch [2/5], Step [1384/10336], Loss: 2.1692\n",
      "Epoch [2/5], Step [1386/10336], Loss: 2.3620\n",
      "Epoch [2/5], Step [1388/10336], Loss: 0.6716\n",
      "Epoch [2/5], Step [1390/10336], Loss: 2.1811\n",
      "Epoch [2/5], Step [1392/10336], Loss: 0.2868\n",
      "Epoch [2/5], Step [1394/10336], Loss: 2.3571\n",
      "Epoch [2/5], Step [1396/10336], Loss: 0.4528\n",
      "Epoch [2/5], Step [1398/10336], Loss: 0.1824\n",
      "Epoch [2/5], Step [1400/10336], Loss: 0.6993\n",
      "Epoch [2/5], Step [1402/10336], Loss: 1.0052\n",
      "Epoch [2/5], Step [1404/10336], Loss: 0.7944\n",
      "Epoch [2/5], Step [1406/10336], Loss: 0.8310\n",
      "Epoch [2/5], Step [1408/10336], Loss: 1.3527\n",
      "Epoch [2/5], Step [1410/10336], Loss: 0.7013\n",
      "Epoch [2/5], Step [1412/10336], Loss: 0.3294\n",
      "Epoch [2/5], Step [1414/10336], Loss: 0.9026\n",
      "Epoch [2/5], Step [1416/10336], Loss: 0.3813\n",
      "Epoch [2/5], Step [1418/10336], Loss: 1.4010\n",
      "Epoch [2/5], Step [1420/10336], Loss: 1.0533\n",
      "Epoch [2/5], Step [1422/10336], Loss: 1.7547\n",
      "Epoch [2/5], Step [1424/10336], Loss: 0.0154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [1426/10336], Loss: 1.7775\n",
      "Epoch [2/5], Step [1428/10336], Loss: 2.6202\n",
      "Epoch [2/5], Step [1430/10336], Loss: 3.8301\n",
      "Epoch [2/5], Step [1432/10336], Loss: 2.6576\n",
      "Epoch [2/5], Step [1434/10336], Loss: 2.0059\n",
      "Epoch [2/5], Step [1436/10336], Loss: 1.2773\n",
      "Epoch [2/5], Step [1438/10336], Loss: 0.8225\n",
      "Epoch [2/5], Step [1440/10336], Loss: 0.1709\n",
      "Epoch [2/5], Step [1442/10336], Loss: 2.1101\n",
      "Epoch [2/5], Step [1444/10336], Loss: 0.2009\n",
      "Epoch [2/5], Step [1446/10336], Loss: 0.4149\n",
      "Epoch [2/5], Step [1448/10336], Loss: 0.9743\n",
      "Epoch [2/5], Step [1450/10336], Loss: 1.7287\n",
      "Epoch [2/5], Step [1452/10336], Loss: 0.4870\n",
      "Epoch [2/5], Step [1454/10336], Loss: 1.0142\n",
      "Epoch [2/5], Step [1456/10336], Loss: 1.8340\n",
      "Epoch [2/5], Step [1458/10336], Loss: 1.5574\n",
      "Epoch [2/5], Step [1460/10336], Loss: 1.2167\n",
      "Epoch [2/5], Step [1462/10336], Loss: 0.0544\n",
      "Epoch [2/5], Step [1464/10336], Loss: 0.7432\n",
      "Epoch [2/5], Step [1466/10336], Loss: 0.2425\n",
      "Epoch [2/5], Step [1468/10336], Loss: 0.3104\n",
      "Epoch [2/5], Step [1470/10336], Loss: 0.2139\n",
      "Epoch [2/5], Step [1472/10336], Loss: 0.5793\n",
      "Epoch [2/5], Step [1474/10336], Loss: 2.8574\n",
      "Epoch [2/5], Step [1476/10336], Loss: 0.3420\n",
      "Epoch [2/5], Step [1478/10336], Loss: 1.1831\n",
      "Epoch [2/5], Step [1480/10336], Loss: 0.8388\n",
      "Epoch [2/5], Step [1482/10336], Loss: 0.6751\n",
      "Epoch [2/5], Step [1484/10336], Loss: 1.8313\n",
      "Epoch [2/5], Step [1486/10336], Loss: 1.0077\n",
      "Epoch [2/5], Step [1488/10336], Loss: 2.6240\n",
      "Epoch [2/5], Step [1490/10336], Loss: 1.0711\n",
      "Epoch [2/5], Step [1492/10336], Loss: 2.5548\n",
      "Epoch [2/5], Step [1494/10336], Loss: 1.0304\n",
      "Epoch [2/5], Step [1496/10336], Loss: 1.5367\n",
      "Epoch [2/5], Step [1498/10336], Loss: 0.2814\n",
      "Epoch [2/5], Step [1500/10336], Loss: 0.3938\n",
      "Epoch [2/5], Step [1502/10336], Loss: 0.6592\n",
      "Epoch [2/5], Step [1504/10336], Loss: 1.8761\n",
      "Epoch [2/5], Step [1506/10336], Loss: 0.2539\n",
      "Epoch [2/5], Step [1508/10336], Loss: 3.1944\n",
      "Epoch [2/5], Step [1510/10336], Loss: 1.4468\n",
      "Epoch [2/5], Step [1512/10336], Loss: 4.1842\n",
      "Epoch [2/5], Step [1514/10336], Loss: 3.1560\n",
      "Epoch [2/5], Step [1516/10336], Loss: 0.2309\n",
      "Epoch [2/5], Step [1518/10336], Loss: 5.0529\n",
      "Epoch [2/5], Step [1520/10336], Loss: 3.6341\n",
      "Epoch [2/5], Step [1522/10336], Loss: 0.0519\n",
      "Epoch [2/5], Step [1524/10336], Loss: 1.0114\n",
      "Epoch [2/5], Step [1526/10336], Loss: 1.6152\n",
      "Epoch [2/5], Step [1528/10336], Loss: 0.1924\n",
      "Epoch [2/5], Step [1530/10336], Loss: 0.9219\n",
      "Epoch [2/5], Step [1532/10336], Loss: 0.6736\n",
      "Epoch [2/5], Step [1534/10336], Loss: 0.6825\n",
      "Epoch [2/5], Step [1536/10336], Loss: 0.4697\n",
      "Epoch [2/5], Step [1538/10336], Loss: 1.7671\n",
      "Epoch [2/5], Step [1540/10336], Loss: 0.0212\n",
      "Epoch [2/5], Step [1542/10336], Loss: 1.5297\n",
      "Epoch [2/5], Step [1544/10336], Loss: 0.6777\n",
      "Epoch [2/5], Step [1546/10336], Loss: 0.3008\n",
      "Epoch [2/5], Step [1548/10336], Loss: 0.0161\n",
      "Epoch [2/5], Step [1550/10336], Loss: 0.3940\n",
      "Epoch [2/5], Step [1552/10336], Loss: 0.5044\n",
      "Epoch [2/5], Step [1554/10336], Loss: 3.2455\n",
      "Epoch [2/5], Step [1556/10336], Loss: 0.0900\n",
      "Epoch [2/5], Step [1558/10336], Loss: 0.4416\n",
      "Epoch [2/5], Step [1560/10336], Loss: 0.4633\n",
      "Epoch [2/5], Step [1562/10336], Loss: 0.1230\n",
      "Epoch [2/5], Step [1564/10336], Loss: 0.6497\n",
      "Epoch [2/5], Step [1566/10336], Loss: 1.7392\n",
      "Epoch [2/5], Step [1568/10336], Loss: 0.2134\n",
      "Epoch [2/5], Step [1570/10336], Loss: 1.5758\n",
      "Epoch [2/5], Step [1572/10336], Loss: 0.9169\n",
      "Epoch [2/5], Step [1574/10336], Loss: 2.0649\n",
      "Epoch [2/5], Step [1576/10336], Loss: 2.3431\n",
      "Epoch [2/5], Step [1578/10336], Loss: 0.6905\n",
      "Epoch [2/5], Step [1580/10336], Loss: 0.7219\n",
      "Epoch [2/5], Step [1582/10336], Loss: 0.1785\n",
      "Epoch [2/5], Step [1584/10336], Loss: 0.3843\n",
      "Epoch [2/5], Step [1586/10336], Loss: 0.8171\n",
      "Epoch [2/5], Step [1588/10336], Loss: 0.8846\n",
      "Epoch [2/5], Step [1590/10336], Loss: 2.0037\n",
      "Epoch [2/5], Step [1592/10336], Loss: 2.3250\n",
      "Epoch [2/5], Step [1594/10336], Loss: 1.2825\n",
      "Epoch [2/5], Step [1596/10336], Loss: 1.3856\n",
      "Epoch [2/5], Step [1598/10336], Loss: 0.6517\n",
      "Epoch [2/5], Step [1600/10336], Loss: 2.7827\n",
      "Epoch [2/5], Step [1602/10336], Loss: 0.8501\n",
      "Epoch [2/5], Step [1604/10336], Loss: 1.9650\n",
      "Epoch [2/5], Step [1606/10336], Loss: 0.3558\n",
      "Epoch [2/5], Step [1608/10336], Loss: 0.2595\n",
      "Epoch [2/5], Step [1610/10336], Loss: 0.2416\n",
      "Epoch [2/5], Step [1612/10336], Loss: 2.9799\n",
      "Epoch [2/5], Step [1614/10336], Loss: 0.1404\n",
      "Epoch [2/5], Step [1616/10336], Loss: 0.3614\n",
      "Epoch [2/5], Step [1618/10336], Loss: 0.0770\n",
      "Epoch [2/5], Step [1620/10336], Loss: 0.1029\n",
      "Epoch [2/5], Step [1622/10336], Loss: 0.5064\n",
      "Epoch [2/5], Step [1624/10336], Loss: 0.9637\n",
      "Epoch [2/5], Step [1626/10336], Loss: 0.6779\n",
      "Epoch [2/5], Step [1628/10336], Loss: 2.0734\n",
      "Epoch [2/5], Step [1630/10336], Loss: 0.0138\n",
      "Epoch [2/5], Step [1632/10336], Loss: 0.0156\n",
      "Epoch [2/5], Step [1634/10336], Loss: 1.0134\n",
      "Epoch [2/5], Step [1636/10336], Loss: 5.8755\n",
      "Epoch [2/5], Step [1638/10336], Loss: 0.3592\n",
      "Epoch [2/5], Step [1640/10336], Loss: 3.1818\n",
      "Epoch [2/5], Step [1642/10336], Loss: 0.0828\n",
      "Epoch [2/5], Step [1644/10336], Loss: 0.8301\n",
      "Epoch [2/5], Step [1646/10336], Loss: 0.0665\n",
      "Epoch [2/5], Step [1648/10336], Loss: 0.8745\n",
      "Epoch [2/5], Step [1650/10336], Loss: 0.5590\n",
      "Epoch [2/5], Step [1652/10336], Loss: 0.5559\n",
      "Epoch [2/5], Step [1654/10336], Loss: 0.6995\n",
      "Epoch [2/5], Step [1656/10336], Loss: 0.9967\n",
      "Epoch [2/5], Step [1658/10336], Loss: 1.2187\n",
      "Epoch [2/5], Step [1660/10336], Loss: 2.9015\n",
      "Epoch [2/5], Step [1662/10336], Loss: 0.7907\n",
      "Epoch [2/5], Step [1664/10336], Loss: 2.8302\n",
      "Epoch [2/5], Step [1666/10336], Loss: 4.1978\n",
      "Epoch [2/5], Step [1668/10336], Loss: 0.2045\n",
      "Epoch [2/5], Step [1670/10336], Loss: 0.4234\n",
      "Epoch [2/5], Step [1672/10336], Loss: 0.2933\n",
      "Epoch [2/5], Step [1674/10336], Loss: 0.6285\n",
      "Epoch [2/5], Step [1676/10336], Loss: 0.1202\n",
      "Epoch [2/5], Step [1678/10336], Loss: 0.0222\n",
      "Epoch [2/5], Step [1680/10336], Loss: 0.0839\n",
      "Epoch [2/5], Step [1682/10336], Loss: 0.4358\n",
      "Epoch [2/5], Step [1684/10336], Loss: 0.4034\n",
      "Epoch [2/5], Step [1686/10336], Loss: 2.2620\n",
      "Epoch [2/5], Step [1688/10336], Loss: 2.6183\n",
      "Epoch [2/5], Step [1690/10336], Loss: 0.8945\n",
      "Epoch [2/5], Step [1692/10336], Loss: 1.7007\n",
      "Epoch [2/5], Step [1694/10336], Loss: 0.3363\n",
      "Epoch [2/5], Step [1696/10336], Loss: 0.6283\n",
      "Epoch [2/5], Step [1698/10336], Loss: 0.1213\n",
      "Epoch [2/5], Step [1700/10336], Loss: 3.3532\n",
      "Epoch [2/5], Step [1702/10336], Loss: 0.8711\n",
      "Epoch [2/5], Step [1704/10336], Loss: 1.2172\n",
      "Epoch [2/5], Step [1706/10336], Loss: 1.2640\n",
      "Epoch [2/5], Step [1708/10336], Loss: 0.2849\n",
      "Epoch [2/5], Step [1710/10336], Loss: 0.9054\n",
      "Epoch [2/5], Step [1712/10336], Loss: 0.6786\n",
      "Epoch [2/5], Step [1714/10336], Loss: 0.8898\n",
      "Epoch [2/5], Step [1716/10336], Loss: 0.0808\n",
      "Epoch [2/5], Step [1718/10336], Loss: 0.6295\n",
      "Epoch [2/5], Step [1720/10336], Loss: 0.5529\n",
      "Epoch [2/5], Step [1722/10336], Loss: 1.1265\n",
      "Epoch [2/5], Step [1724/10336], Loss: 1.2890\n",
      "Epoch [2/5], Step [1726/10336], Loss: 1.3836\n",
      "Epoch [2/5], Step [1728/10336], Loss: 0.0303\n",
      "Epoch [2/5], Step [1730/10336], Loss: 0.0518\n",
      "Epoch [2/5], Step [1732/10336], Loss: 0.1916\n",
      "Epoch [2/5], Step [1734/10336], Loss: 0.2160\n",
      "Epoch [2/5], Step [1736/10336], Loss: 1.7948\n",
      "Epoch [2/5], Step [1738/10336], Loss: 0.1674\n",
      "Epoch [2/5], Step [1740/10336], Loss: 0.1399\n",
      "Epoch [2/5], Step [1742/10336], Loss: 2.0630\n",
      "Epoch [2/5], Step [1744/10336], Loss: 0.2601\n",
      "Epoch [2/5], Step [1746/10336], Loss: 1.2211\n",
      "Epoch [2/5], Step [1748/10336], Loss: 0.1723\n",
      "Epoch [2/5], Step [1750/10336], Loss: 2.1138\n",
      "Epoch [2/5], Step [1752/10336], Loss: 0.2210\n",
      "Epoch [2/5], Step [1754/10336], Loss: 0.5505\n",
      "Epoch [2/5], Step [1756/10336], Loss: 0.0526\n",
      "Epoch [2/5], Step [1758/10336], Loss: 1.9554\n",
      "Epoch [2/5], Step [1760/10336], Loss: 2.5553\n",
      "Epoch [2/5], Step [1762/10336], Loss: 1.4502\n",
      "Epoch [2/5], Step [1764/10336], Loss: 0.3558\n",
      "Epoch [2/5], Step [1766/10336], Loss: 0.9664\n",
      "Epoch [2/5], Step [1768/10336], Loss: 1.4900\n",
      "Epoch [2/5], Step [1770/10336], Loss: 3.1936\n",
      "Epoch [2/5], Step [1772/10336], Loss: 0.5417\n",
      "Epoch [2/5], Step [1774/10336], Loss: 0.1602\n",
      "Epoch [2/5], Step [1776/10336], Loss: 0.1202\n",
      "Epoch [2/5], Step [1778/10336], Loss: 2.3463\n",
      "Epoch [2/5], Step [1780/10336], Loss: 2.7073\n",
      "Epoch [2/5], Step [1782/10336], Loss: 1.1350\n",
      "Epoch [2/5], Step [1784/10336], Loss: 0.0097\n",
      "Epoch [2/5], Step [1786/10336], Loss: 1.9772\n",
      "Epoch [2/5], Step [1788/10336], Loss: 0.3653\n",
      "Epoch [2/5], Step [1790/10336], Loss: 0.6484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [1792/10336], Loss: 1.0507\n",
      "Epoch [2/5], Step [1794/10336], Loss: 0.4563\n",
      "Epoch [2/5], Step [1796/10336], Loss: 0.4206\n",
      "Epoch [2/5], Step [1798/10336], Loss: 2.4795\n",
      "Epoch [2/5], Step [1800/10336], Loss: 0.0140\n",
      "Epoch [2/5], Step [1802/10336], Loss: 0.5747\n",
      "Epoch [2/5], Step [1804/10336], Loss: 0.0803\n",
      "Epoch [2/5], Step [1806/10336], Loss: 0.8771\n",
      "Epoch [2/5], Step [1808/10336], Loss: 0.4365\n",
      "Epoch [2/5], Step [1810/10336], Loss: 0.0154\n",
      "Epoch [2/5], Step [1812/10336], Loss: 0.3165\n",
      "Epoch [2/5], Step [1814/10336], Loss: 2.1558\n",
      "Epoch [2/5], Step [1816/10336], Loss: 2.4411\n",
      "Epoch [2/5], Step [1818/10336], Loss: 0.0522\n",
      "Epoch [2/5], Step [1820/10336], Loss: 1.1477\n",
      "Epoch [2/5], Step [1822/10336], Loss: 2.4701\n",
      "Epoch [2/5], Step [1824/10336], Loss: 0.5009\n",
      "Epoch [2/5], Step [1826/10336], Loss: 0.1048\n",
      "Epoch [2/5], Step [1828/10336], Loss: 2.0167\n",
      "Epoch [2/5], Step [1830/10336], Loss: 1.7814\n",
      "Epoch [2/5], Step [1832/10336], Loss: 0.2188\n",
      "Epoch [2/5], Step [1834/10336], Loss: 0.2617\n",
      "Epoch [2/5], Step [1836/10336], Loss: 2.3943\n",
      "Epoch [2/5], Step [1838/10336], Loss: 1.4844\n",
      "Epoch [2/5], Step [1840/10336], Loss: 0.3837\n",
      "Epoch [2/5], Step [1842/10336], Loss: 0.2625\n",
      "Epoch [2/5], Step [1844/10336], Loss: 0.1420\n",
      "Epoch [2/5], Step [1846/10336], Loss: 0.9632\n",
      "Epoch [2/5], Step [1848/10336], Loss: 0.0769\n",
      "Epoch [2/5], Step [1850/10336], Loss: 0.5562\n",
      "Epoch [2/5], Step [1852/10336], Loss: 1.9307\n",
      "Epoch [2/5], Step [1854/10336], Loss: 1.0621\n",
      "Epoch [2/5], Step [1856/10336], Loss: 0.3767\n",
      "Epoch [2/5], Step [1858/10336], Loss: 0.5730\n",
      "Epoch [2/5], Step [1860/10336], Loss: 1.3408\n",
      "Epoch [2/5], Step [1862/10336], Loss: 1.8665\n",
      "Epoch [2/5], Step [1864/10336], Loss: 0.2732\n",
      "Epoch [2/5], Step [1866/10336], Loss: 1.0161\n",
      "Epoch [2/5], Step [1868/10336], Loss: 1.1948\n",
      "Epoch [2/5], Step [1870/10336], Loss: 0.4409\n",
      "Epoch [2/5], Step [1872/10336], Loss: 0.1795\n",
      "Epoch [2/5], Step [1874/10336], Loss: 0.2396\n",
      "Epoch [2/5], Step [1876/10336], Loss: 0.0616\n",
      "Epoch [2/5], Step [1878/10336], Loss: 0.4579\n",
      "Epoch [2/5], Step [1880/10336], Loss: 1.0686\n",
      "Epoch [2/5], Step [1882/10336], Loss: 1.0292\n",
      "Epoch [2/5], Step [1884/10336], Loss: 0.0187\n",
      "Epoch [2/5], Step [1886/10336], Loss: 2.3338\n",
      "Epoch [2/5], Step [1888/10336], Loss: 0.4766\n",
      "Epoch [2/5], Step [1890/10336], Loss: 0.0659\n",
      "Epoch [2/5], Step [1892/10336], Loss: 1.2385\n",
      "Epoch [2/5], Step [1894/10336], Loss: 0.0247\n",
      "Epoch [2/5], Step [1896/10336], Loss: 0.2077\n",
      "Epoch [2/5], Step [1898/10336], Loss: 0.5126\n",
      "Epoch [2/5], Step [1900/10336], Loss: 1.7832\n",
      "Epoch [2/5], Step [1902/10336], Loss: 0.4161\n",
      "Epoch [2/5], Step [1904/10336], Loss: 0.3921\n",
      "Epoch [2/5], Step [1906/10336], Loss: 0.3502\n",
      "Epoch [2/5], Step [1908/10336], Loss: 0.1569\n",
      "Epoch [2/5], Step [1910/10336], Loss: 3.1155\n",
      "Epoch [2/5], Step [1912/10336], Loss: 0.7262\n",
      "Epoch [2/5], Step [1914/10336], Loss: 0.8347\n",
      "Epoch [2/5], Step [1916/10336], Loss: 0.5941\n",
      "Epoch [2/5], Step [1918/10336], Loss: 0.9578\n",
      "Epoch [2/5], Step [1920/10336], Loss: 1.8665\n",
      "Epoch [2/5], Step [1922/10336], Loss: 0.2226\n",
      "Epoch [2/5], Step [1924/10336], Loss: 0.6782\n",
      "Epoch [2/5], Step [1926/10336], Loss: 2.2561\n",
      "Epoch [2/5], Step [1928/10336], Loss: 1.2283\n",
      "Epoch [2/5], Step [1930/10336], Loss: 1.8165\n",
      "Epoch [2/5], Step [1932/10336], Loss: 0.2761\n",
      "Epoch [2/5], Step [1934/10336], Loss: 3.1475\n",
      "Epoch [2/5], Step [1936/10336], Loss: 0.5456\n",
      "Epoch [2/5], Step [1938/10336], Loss: 2.8814\n",
      "Epoch [2/5], Step [1940/10336], Loss: 3.7228\n",
      "Epoch [2/5], Step [1942/10336], Loss: 3.1056\n",
      "Epoch [2/5], Step [1944/10336], Loss: 1.3649\n",
      "Epoch [2/5], Step [1946/10336], Loss: 0.9595\n",
      "Epoch [2/5], Step [1948/10336], Loss: 0.0836\n",
      "Epoch [2/5], Step [1950/10336], Loss: 5.0382\n",
      "Epoch [2/5], Step [1952/10336], Loss: 0.6172\n",
      "Epoch [2/5], Step [1954/10336], Loss: 2.3318\n",
      "Epoch [2/5], Step [1956/10336], Loss: 0.0247\n",
      "Epoch [2/5], Step [1958/10336], Loss: 0.4048\n",
      "Epoch [2/5], Step [1960/10336], Loss: 0.7687\n",
      "Epoch [2/5], Step [1962/10336], Loss: 0.8997\n",
      "Epoch [2/5], Step [1964/10336], Loss: 0.3938\n",
      "Epoch [2/5], Step [1966/10336], Loss: 1.0424\n",
      "Epoch [2/5], Step [1968/10336], Loss: 0.2191\n",
      "Epoch [2/5], Step [1970/10336], Loss: 0.2261\n",
      "Epoch [2/5], Step [1972/10336], Loss: 0.5128\n",
      "Epoch [2/5], Step [1974/10336], Loss: 1.0125\n",
      "Epoch [2/5], Step [1976/10336], Loss: 1.1903\n",
      "Epoch [2/5], Step [1978/10336], Loss: 0.0393\n",
      "Epoch [2/5], Step [1980/10336], Loss: 0.0059\n",
      "Epoch [2/5], Step [1982/10336], Loss: 1.2501\n",
      "Epoch [2/5], Step [1984/10336], Loss: 2.7627\n",
      "Epoch [2/5], Step [1986/10336], Loss: 0.5072\n",
      "Epoch [2/5], Step [1988/10336], Loss: 5.2960\n",
      "Epoch [2/5], Step [1990/10336], Loss: 0.0106\n",
      "Epoch [2/5], Step [1992/10336], Loss: 0.0388\n",
      "Epoch [2/5], Step [1994/10336], Loss: 1.0915\n",
      "Epoch [2/5], Step [1996/10336], Loss: 1.0724\n",
      "Epoch [2/5], Step [1998/10336], Loss: 3.5756\n",
      "Epoch [2/5], Step [2000/10336], Loss: 0.5233\n",
      "Epoch [2/5], Step [2002/10336], Loss: 0.0312\n",
      "Epoch [2/5], Step [2004/10336], Loss: 0.0892\n",
      "Epoch [2/5], Step [2006/10336], Loss: 0.4558\n",
      "Epoch [2/5], Step [2008/10336], Loss: 0.6286\n",
      "Epoch [2/5], Step [2010/10336], Loss: 0.0709\n",
      "Epoch [2/5], Step [2012/10336], Loss: 0.4465\n",
      "Epoch [2/5], Step [2014/10336], Loss: 1.4122\n",
      "Epoch [2/5], Step [2016/10336], Loss: 0.6786\n",
      "Epoch [2/5], Step [2018/10336], Loss: 0.6758\n",
      "Epoch [2/5], Step [2020/10336], Loss: 0.9817\n",
      "Epoch [2/5], Step [2022/10336], Loss: 1.5636\n",
      "Epoch [2/5], Step [2024/10336], Loss: 0.7786\n",
      "Epoch [2/5], Step [2026/10336], Loss: 1.1319\n",
      "Epoch [2/5], Step [2028/10336], Loss: 4.3287\n",
      "Epoch [2/5], Step [2030/10336], Loss: 0.5973\n",
      "Epoch [2/5], Step [2032/10336], Loss: 0.1122\n",
      "Epoch [2/5], Step [2034/10336], Loss: 0.8733\n",
      "Epoch [2/5], Step [2036/10336], Loss: 3.2639\n",
      "Epoch [2/5], Step [2038/10336], Loss: 2.4951\n",
      "Epoch [2/5], Step [2040/10336], Loss: 0.0165\n",
      "Epoch [2/5], Step [2042/10336], Loss: 0.2065\n",
      "Epoch [2/5], Step [2044/10336], Loss: 1.1787\n",
      "Epoch [2/5], Step [2046/10336], Loss: 0.8000\n",
      "Epoch [2/5], Step [2048/10336], Loss: 1.2969\n",
      "Epoch [2/5], Step [2050/10336], Loss: 1.0617\n",
      "Epoch [2/5], Step [2052/10336], Loss: 0.3485\n",
      "Epoch [2/5], Step [2054/10336], Loss: 0.0291\n",
      "Epoch [2/5], Step [2056/10336], Loss: 0.8937\n",
      "Epoch [2/5], Step [2058/10336], Loss: 0.1297\n",
      "Epoch [2/5], Step [2060/10336], Loss: 2.0721\n",
      "Epoch [2/5], Step [2062/10336], Loss: 0.1422\n",
      "Epoch [2/5], Step [2064/10336], Loss: 0.1865\n",
      "Epoch [2/5], Step [2066/10336], Loss: 0.9854\n",
      "Epoch [2/5], Step [2068/10336], Loss: 1.0080\n",
      "Epoch [2/5], Step [2070/10336], Loss: 1.1888\n",
      "Epoch [2/5], Step [2072/10336], Loss: 0.1722\n",
      "Epoch [2/5], Step [2074/10336], Loss: 4.6293\n",
      "Epoch [2/5], Step [2076/10336], Loss: 1.4418\n",
      "Epoch [2/5], Step [2078/10336], Loss: 0.2956\n",
      "Epoch [2/5], Step [2080/10336], Loss: 0.2167\n",
      "Epoch [2/5], Step [2082/10336], Loss: 0.1577\n",
      "Epoch [2/5], Step [2084/10336], Loss: 1.0487\n",
      "Epoch [2/5], Step [2086/10336], Loss: 0.6777\n",
      "Epoch [2/5], Step [2088/10336], Loss: 0.5360\n",
      "Epoch [2/5], Step [2090/10336], Loss: 0.1013\n",
      "Epoch [2/5], Step [2092/10336], Loss: 0.0485\n",
      "Epoch [2/5], Step [2094/10336], Loss: 0.1972\n",
      "Epoch [2/5], Step [2096/10336], Loss: 0.7342\n",
      "Epoch [2/5], Step [2098/10336], Loss: 0.5453\n",
      "Epoch [2/5], Step [2100/10336], Loss: 0.8138\n",
      "Epoch [2/5], Step [2102/10336], Loss: 1.6874\n",
      "Epoch [2/5], Step [2104/10336], Loss: 0.5469\n",
      "Epoch [2/5], Step [2106/10336], Loss: 0.8776\n",
      "Epoch [2/5], Step [2108/10336], Loss: 1.8452\n",
      "Epoch [2/5], Step [2110/10336], Loss: 2.4292\n",
      "Epoch [2/5], Step [2112/10336], Loss: 0.0517\n",
      "Epoch [2/5], Step [2114/10336], Loss: 0.5563\n",
      "Epoch [2/5], Step [2116/10336], Loss: 2.1937\n",
      "Epoch [2/5], Step [2118/10336], Loss: 0.1893\n",
      "Epoch [2/5], Step [2120/10336], Loss: 4.7861\n",
      "Epoch [2/5], Step [2122/10336], Loss: 0.2486\n",
      "Epoch [2/5], Step [2124/10336], Loss: 0.1936\n",
      "Epoch [2/5], Step [2126/10336], Loss: 2.4491\n",
      "Epoch [2/5], Step [2128/10336], Loss: 0.6008\n",
      "Epoch [2/5], Step [2130/10336], Loss: 1.0591\n",
      "Epoch [2/5], Step [2132/10336], Loss: 0.1191\n",
      "Epoch [2/5], Step [2134/10336], Loss: 0.1552\n",
      "Epoch [2/5], Step [2136/10336], Loss: 1.7728\n",
      "Epoch [2/5], Step [2138/10336], Loss: 1.8265\n",
      "Epoch [2/5], Step [2140/10336], Loss: 2.4359\n",
      "Epoch [2/5], Step [2142/10336], Loss: 0.1704\n",
      "Epoch [2/5], Step [2144/10336], Loss: 0.8693\n",
      "Epoch [2/5], Step [2146/10336], Loss: 1.5507\n",
      "Epoch [2/5], Step [2148/10336], Loss: 0.0117\n",
      "Epoch [2/5], Step [2150/10336], Loss: 2.9218\n",
      "Epoch [2/5], Step [2152/10336], Loss: 1.0693\n",
      "Epoch [2/5], Step [2154/10336], Loss: 0.3491\n",
      "Epoch [2/5], Step [2156/10336], Loss: 0.4487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [2158/10336], Loss: 0.4947\n",
      "Epoch [2/5], Step [2160/10336], Loss: 0.3567\n",
      "Epoch [2/5], Step [2162/10336], Loss: 2.2994\n",
      "Epoch [2/5], Step [2164/10336], Loss: 2.0821\n",
      "Epoch [2/5], Step [2166/10336], Loss: 0.7477\n",
      "Epoch [2/5], Step [2168/10336], Loss: 0.9293\n",
      "Epoch [2/5], Step [2170/10336], Loss: 0.1311\n",
      "Epoch [2/5], Step [2172/10336], Loss: 0.3563\n",
      "Epoch [2/5], Step [2174/10336], Loss: 0.3772\n",
      "Epoch [2/5], Step [2176/10336], Loss: 1.6711\n",
      "Epoch [2/5], Step [2178/10336], Loss: 0.4131\n",
      "Epoch [2/5], Step [2180/10336], Loss: 2.3311\n",
      "Epoch [2/5], Step [2182/10336], Loss: 0.8154\n",
      "Epoch [2/5], Step [2184/10336], Loss: 0.9751\n",
      "Epoch [2/5], Step [2186/10336], Loss: 0.3366\n",
      "Epoch [2/5], Step [2188/10336], Loss: 0.8628\n",
      "Epoch [2/5], Step [2190/10336], Loss: 0.9595\n",
      "Epoch [2/5], Step [2192/10336], Loss: 2.2978\n",
      "Epoch [2/5], Step [2194/10336], Loss: 1.9915\n",
      "Epoch [2/5], Step [2196/10336], Loss: 1.0070\n",
      "Epoch [2/5], Step [2198/10336], Loss: 0.6082\n",
      "Epoch [2/5], Step [2200/10336], Loss: 0.1163\n",
      "Epoch [2/5], Step [2202/10336], Loss: 0.0073\n",
      "Epoch [2/5], Step [2204/10336], Loss: 0.1917\n",
      "Epoch [2/5], Step [2206/10336], Loss: 0.3973\n",
      "Epoch [2/5], Step [2208/10336], Loss: 0.8481\n",
      "Epoch [2/5], Step [2210/10336], Loss: 0.1150\n",
      "Epoch [2/5], Step [2212/10336], Loss: 0.0335\n",
      "Epoch [2/5], Step [2214/10336], Loss: 0.0379\n",
      "Epoch [2/5], Step [2216/10336], Loss: 0.8715\n",
      "Epoch [2/5], Step [2218/10336], Loss: 0.5746\n",
      "Epoch [2/5], Step [2220/10336], Loss: 0.2918\n",
      "Epoch [2/5], Step [2222/10336], Loss: 0.1872\n",
      "Epoch [2/5], Step [2224/10336], Loss: 0.0492\n",
      "Epoch [2/5], Step [2226/10336], Loss: 2.3671\n",
      "Epoch [2/5], Step [2228/10336], Loss: 0.3058\n",
      "Epoch [2/5], Step [2230/10336], Loss: 0.0225\n",
      "Epoch [2/5], Step [2232/10336], Loss: 0.6365\n",
      "Epoch [2/5], Step [2234/10336], Loss: 0.0803\n",
      "Epoch [2/5], Step [2236/10336], Loss: 1.0987\n",
      "Epoch [2/5], Step [2238/10336], Loss: 1.3330\n",
      "Epoch [2/5], Step [2240/10336], Loss: 0.0030\n",
      "Epoch [2/5], Step [2242/10336], Loss: 3.5219\n",
      "Epoch [2/5], Step [2244/10336], Loss: 0.1694\n",
      "Epoch [2/5], Step [2246/10336], Loss: 2.0052\n",
      "Epoch [2/5], Step [2248/10336], Loss: 0.7742\n",
      "Epoch [2/5], Step [2250/10336], Loss: 1.0807\n",
      "Epoch [2/5], Step [2252/10336], Loss: 0.0271\n",
      "Epoch [2/5], Step [2254/10336], Loss: 0.2206\n",
      "Epoch [2/5], Step [2256/10336], Loss: 0.1929\n",
      "Epoch [2/5], Step [2258/10336], Loss: 3.0230\n",
      "Epoch [2/5], Step [2260/10336], Loss: 0.0284\n",
      "Epoch [2/5], Step [2262/10336], Loss: 3.0637\n",
      "Epoch [2/5], Step [2264/10336], Loss: 1.4501\n",
      "Epoch [2/5], Step [2266/10336], Loss: 4.1181\n",
      "Epoch [2/5], Step [2268/10336], Loss: 2.3447\n",
      "Epoch [2/5], Step [2270/10336], Loss: 0.7639\n",
      "Epoch [2/5], Step [2272/10336], Loss: 0.5451\n",
      "Epoch [2/5], Step [2274/10336], Loss: 0.7940\n",
      "Epoch [2/5], Step [2276/10336], Loss: 0.9230\n",
      "Epoch [2/5], Step [2278/10336], Loss: 0.5358\n",
      "Epoch [2/5], Step [2280/10336], Loss: 0.5681\n",
      "Epoch [2/5], Step [2282/10336], Loss: 0.4244\n",
      "Epoch [2/5], Step [2284/10336], Loss: 1.6263\n",
      "Epoch [2/5], Step [2286/10336], Loss: 0.3327\n",
      "Epoch [2/5], Step [2288/10336], Loss: 0.6111\n",
      "Epoch [2/5], Step [2290/10336], Loss: 2.5057\n",
      "Epoch [2/5], Step [2292/10336], Loss: 1.3063\n",
      "Epoch [2/5], Step [2294/10336], Loss: 0.1651\n",
      "Epoch [2/5], Step [2296/10336], Loss: 0.2960\n",
      "Epoch [2/5], Step [2298/10336], Loss: 0.0136\n",
      "Epoch [2/5], Step [2300/10336], Loss: 1.1193\n",
      "Epoch [2/5], Step [2302/10336], Loss: 0.4285\n",
      "Epoch [2/5], Step [2304/10336], Loss: 1.2912\n",
      "Epoch [2/5], Step [2306/10336], Loss: 0.6967\n",
      "Epoch [2/5], Step [2308/10336], Loss: 2.2748\n",
      "Epoch [2/5], Step [2310/10336], Loss: 0.0098\n",
      "Epoch [2/5], Step [2312/10336], Loss: 0.1702\n",
      "Epoch [2/5], Step [2314/10336], Loss: 1.7357\n",
      "Epoch [2/5], Step [2316/10336], Loss: 0.4761\n",
      "Epoch [2/5], Step [2318/10336], Loss: 3.0276\n",
      "Epoch [2/5], Step [2320/10336], Loss: 0.4985\n",
      "Epoch [2/5], Step [2322/10336], Loss: 0.0116\n",
      "Epoch [2/5], Step [2324/10336], Loss: 1.7511\n",
      "Epoch [2/5], Step [2326/10336], Loss: 1.0159\n",
      "Epoch [2/5], Step [2328/10336], Loss: 0.7986\n",
      "Epoch [2/5], Step [2330/10336], Loss: 0.6991\n",
      "Epoch [2/5], Step [2332/10336], Loss: 0.9588\n",
      "Epoch [2/5], Step [2334/10336], Loss: 0.0728\n",
      "Epoch [2/5], Step [2336/10336], Loss: 1.5389\n",
      "Epoch [2/5], Step [2338/10336], Loss: 0.9932\n",
      "Epoch [2/5], Step [2340/10336], Loss: 0.4805\n",
      "Epoch [2/5], Step [2342/10336], Loss: 3.1150\n",
      "Epoch [2/5], Step [2344/10336], Loss: 1.0183\n",
      "Epoch [2/5], Step [2346/10336], Loss: 1.5802\n",
      "Epoch [2/5], Step [2348/10336], Loss: 1.1538\n",
      "Epoch [2/5], Step [2350/10336], Loss: 2.4330\n",
      "Epoch [2/5], Step [2352/10336], Loss: 1.7700\n",
      "Epoch [2/5], Step [2354/10336], Loss: 0.1958\n",
      "Epoch [2/5], Step [2356/10336], Loss: 2.3657\n",
      "Epoch [2/5], Step [2358/10336], Loss: 1.2189\n",
      "Epoch [2/5], Step [2360/10336], Loss: 0.1202\n",
      "Epoch [2/5], Step [2362/10336], Loss: 1.9962\n",
      "Epoch [2/5], Step [2364/10336], Loss: 0.0504\n",
      "Epoch [2/5], Step [2366/10336], Loss: 1.7708\n",
      "Epoch [2/5], Step [2368/10336], Loss: 1.5880\n",
      "Epoch [2/5], Step [2370/10336], Loss: 0.0145\n",
      "Epoch [2/5], Step [2372/10336], Loss: 0.8831\n",
      "Epoch [2/5], Step [2374/10336], Loss: 0.0154\n",
      "Epoch [2/5], Step [2376/10336], Loss: 0.5879\n",
      "Epoch [2/5], Step [2378/10336], Loss: 1.2969\n",
      "Epoch [2/5], Step [2380/10336], Loss: 3.1935\n",
      "Epoch [2/5], Step [2382/10336], Loss: 1.1799\n",
      "Epoch [2/5], Step [2384/10336], Loss: 0.7345\n",
      "Epoch [2/5], Step [2386/10336], Loss: 0.1479\n",
      "Epoch [2/5], Step [2388/10336], Loss: 0.0561\n",
      "Epoch [2/5], Step [2390/10336], Loss: 0.0980\n",
      "Epoch [2/5], Step [2392/10336], Loss: 3.5643\n",
      "Epoch [2/5], Step [2394/10336], Loss: 1.1275\n",
      "Epoch [2/5], Step [2396/10336], Loss: 0.0023\n",
      "Epoch [2/5], Step [2398/10336], Loss: 1.2811\n",
      "Epoch [2/5], Step [2400/10336], Loss: 0.4673\n",
      "Epoch [2/5], Step [2402/10336], Loss: 0.1630\n",
      "Epoch [2/5], Step [2404/10336], Loss: 0.1360\n",
      "Epoch [2/5], Step [2406/10336], Loss: 0.0408\n",
      "Epoch [2/5], Step [2408/10336], Loss: 2.0381\n",
      "Epoch [2/5], Step [2410/10336], Loss: 0.1428\n",
      "Epoch [2/5], Step [2412/10336], Loss: 0.5900\n",
      "Epoch [2/5], Step [2414/10336], Loss: 0.0043\n",
      "Epoch [2/5], Step [2416/10336], Loss: 1.1976\n",
      "Epoch [2/5], Step [2418/10336], Loss: 0.0415\n",
      "Epoch [2/5], Step [2420/10336], Loss: 0.7735\n",
      "Epoch [2/5], Step [2422/10336], Loss: 1.1148\n",
      "Epoch [2/5], Step [2424/10336], Loss: 0.9846\n",
      "Epoch [2/5], Step [2426/10336], Loss: 2.1668\n",
      "Epoch [2/5], Step [2428/10336], Loss: 0.2895\n",
      "Epoch [2/5], Step [2430/10336], Loss: 0.3537\n",
      "Epoch [2/5], Step [2432/10336], Loss: 0.4105\n",
      "Epoch [2/5], Step [2434/10336], Loss: 2.2303\n",
      "Epoch [2/5], Step [2436/10336], Loss: 1.5036\n",
      "Epoch [2/5], Step [2438/10336], Loss: 0.4219\n",
      "Epoch [2/5], Step [2440/10336], Loss: 0.1344\n",
      "Epoch [2/5], Step [2442/10336], Loss: 0.0611\n",
      "Epoch [2/5], Step [2444/10336], Loss: 2.6288\n",
      "Epoch [2/5], Step [2446/10336], Loss: 0.4845\n",
      "Epoch [2/5], Step [2448/10336], Loss: 0.3560\n",
      "Epoch [2/5], Step [2450/10336], Loss: 0.0712\n",
      "Epoch [2/5], Step [2452/10336], Loss: 3.9589\n",
      "Epoch [2/5], Step [2454/10336], Loss: 4.7772\n",
      "Epoch [2/5], Step [2456/10336], Loss: 0.5156\n",
      "Epoch [2/5], Step [2458/10336], Loss: 0.6584\n",
      "Epoch [2/5], Step [2460/10336], Loss: 0.0336\n",
      "Epoch [2/5], Step [2462/10336], Loss: 0.2032\n",
      "Epoch [2/5], Step [2464/10336], Loss: 0.8623\n",
      "Epoch [2/5], Step [2466/10336], Loss: 0.5694\n",
      "Epoch [2/5], Step [2468/10336], Loss: 0.4076\n",
      "Epoch [2/5], Step [2470/10336], Loss: 2.4422\n",
      "Epoch [2/5], Step [2472/10336], Loss: 1.8549\n",
      "Epoch [2/5], Step [2474/10336], Loss: 2.1606\n",
      "Epoch [2/5], Step [2476/10336], Loss: 0.0962\n",
      "Epoch [2/5], Step [2478/10336], Loss: 2.3020\n",
      "Epoch [2/5], Step [2480/10336], Loss: 1.5335\n",
      "Epoch [2/5], Step [2482/10336], Loss: 1.4656\n",
      "Epoch [2/5], Step [2484/10336], Loss: 0.7534\n",
      "Epoch [2/5], Step [2486/10336], Loss: 0.3149\n",
      "Epoch [2/5], Step [2488/10336], Loss: 0.9305\n",
      "Epoch [2/5], Step [2490/10336], Loss: 1.1438\n",
      "Epoch [2/5], Step [2492/10336], Loss: 0.5475\n",
      "Epoch [2/5], Step [2494/10336], Loss: 2.8553\n",
      "Epoch [2/5], Step [2496/10336], Loss: 0.0165\n",
      "Epoch [2/5], Step [2498/10336], Loss: 2.8526\n",
      "Epoch [2/5], Step [2500/10336], Loss: 0.4515\n",
      "Epoch [2/5], Step [2502/10336], Loss: 1.1226\n",
      "Epoch [2/5], Step [2504/10336], Loss: 0.1072\n",
      "Epoch [2/5], Step [2506/10336], Loss: 0.7716\n",
      "Epoch [2/5], Step [2508/10336], Loss: 0.8613\n",
      "Epoch [2/5], Step [2510/10336], Loss: 1.0965\n",
      "Epoch [2/5], Step [2512/10336], Loss: 0.0954\n",
      "Epoch [2/5], Step [2514/10336], Loss: 0.8112\n",
      "Epoch [2/5], Step [2516/10336], Loss: 0.0917\n",
      "Epoch [2/5], Step [2518/10336], Loss: 0.0528\n",
      "Epoch [2/5], Step [2520/10336], Loss: 0.5596\n",
      "Epoch [2/5], Step [2522/10336], Loss: 0.1099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [2524/10336], Loss: 3.1033\n",
      "Epoch [2/5], Step [2526/10336], Loss: 0.8446\n",
      "Epoch [2/5], Step [2528/10336], Loss: 0.0648\n",
      "Epoch [2/5], Step [2530/10336], Loss: 0.6617\n",
      "Epoch [2/5], Step [2532/10336], Loss: 0.0687\n",
      "Epoch [2/5], Step [2534/10336], Loss: 0.4010\n",
      "Epoch [2/5], Step [2536/10336], Loss: 0.1660\n",
      "Epoch [2/5], Step [2538/10336], Loss: 1.9603\n",
      "Epoch [2/5], Step [2540/10336], Loss: 0.4056\n",
      "Epoch [2/5], Step [2542/10336], Loss: 0.1391\n",
      "Epoch [2/5], Step [2544/10336], Loss: 0.0819\n",
      "Epoch [2/5], Step [2546/10336], Loss: 2.2407\n",
      "Epoch [2/5], Step [2548/10336], Loss: 0.0538\n",
      "Epoch [2/5], Step [2550/10336], Loss: 2.0251\n",
      "Epoch [2/5], Step [2552/10336], Loss: 0.0558\n",
      "Epoch [2/5], Step [2554/10336], Loss: 0.0216\n",
      "Epoch [2/5], Step [2556/10336], Loss: 0.2430\n",
      "Epoch [2/5], Step [2558/10336], Loss: 0.1155\n",
      "Epoch [2/5], Step [2560/10336], Loss: 0.0112\n",
      "Epoch [2/5], Step [2562/10336], Loss: 0.0146\n",
      "Epoch [2/5], Step [2564/10336], Loss: 0.2007\n",
      "Epoch [2/5], Step [2566/10336], Loss: 0.3403\n",
      "Epoch [2/5], Step [2568/10336], Loss: 0.5141\n",
      "Epoch [2/5], Step [2570/10336], Loss: 1.9160\n",
      "Epoch [2/5], Step [2572/10336], Loss: 0.6356\n",
      "Epoch [2/5], Step [2574/10336], Loss: 2.2725\n",
      "Epoch [2/5], Step [2576/10336], Loss: 0.0167\n",
      "Epoch [2/5], Step [2578/10336], Loss: 0.1911\n",
      "Epoch [2/5], Step [2580/10336], Loss: 0.1484\n",
      "Epoch [2/5], Step [2582/10336], Loss: 2.8141\n",
      "Epoch [2/5], Step [2584/10336], Loss: 0.9729\n",
      "Epoch [2/5], Step [2586/10336], Loss: 2.7021\n",
      "Epoch [2/5], Step [2588/10336], Loss: 1.0297\n",
      "Epoch [2/5], Step [2590/10336], Loss: 0.1045\n",
      "Epoch [2/5], Step [2592/10336], Loss: 3.6942\n",
      "Epoch [2/5], Step [2594/10336], Loss: 0.8546\n",
      "Epoch [2/5], Step [2596/10336], Loss: 0.5029\n",
      "Epoch [2/5], Step [2598/10336], Loss: 1.4914\n",
      "Epoch [2/5], Step [2600/10336], Loss: 1.1474\n",
      "Epoch [2/5], Step [2602/10336], Loss: 0.8190\n",
      "Epoch [2/5], Step [2604/10336], Loss: 1.1693\n",
      "Epoch [2/5], Step [2606/10336], Loss: 1.6587\n",
      "Epoch [2/5], Step [2608/10336], Loss: 2.2248\n",
      "Epoch [2/5], Step [2610/10336], Loss: 2.0450\n",
      "Epoch [2/5], Step [2612/10336], Loss: 0.1127\n",
      "Epoch [2/5], Step [2614/10336], Loss: 0.2782\n",
      "Epoch [2/5], Step [2616/10336], Loss: 0.4956\n",
      "Epoch [2/5], Step [2618/10336], Loss: 2.7800\n",
      "Epoch [2/5], Step [2620/10336], Loss: 0.0509\n",
      "Epoch [2/5], Step [2622/10336], Loss: 2.3605\n",
      "Epoch [2/5], Step [2624/10336], Loss: 0.2345\n",
      "Epoch [2/5], Step [2626/10336], Loss: 1.6186\n",
      "Epoch [2/5], Step [2628/10336], Loss: 0.2806\n",
      "Epoch [2/5], Step [2630/10336], Loss: 1.5613\n",
      "Epoch [2/5], Step [2632/10336], Loss: 1.5852\n",
      "Epoch [2/5], Step [2634/10336], Loss: 1.4381\n",
      "Epoch [2/5], Step [2636/10336], Loss: 0.1231\n",
      "Epoch [2/5], Step [2638/10336], Loss: 1.0722\n",
      "Epoch [2/5], Step [2640/10336], Loss: 0.1160\n",
      "Epoch [2/5], Step [2642/10336], Loss: 0.0168\n",
      "Epoch [2/5], Step [2644/10336], Loss: 0.1112\n",
      "Epoch [2/5], Step [2646/10336], Loss: 2.1046\n",
      "Epoch [2/5], Step [2648/10336], Loss: 0.9293\n",
      "Epoch [2/5], Step [2650/10336], Loss: 0.4217\n",
      "Epoch [2/5], Step [2652/10336], Loss: 0.0137\n",
      "Epoch [2/5], Step [2654/10336], Loss: 0.8941\n",
      "Epoch [2/5], Step [2656/10336], Loss: 2.7909\n",
      "Epoch [2/5], Step [2658/10336], Loss: 0.2670\n",
      "Epoch [2/5], Step [2660/10336], Loss: 0.7344\n",
      "Epoch [2/5], Step [2662/10336], Loss: 0.5219\n",
      "Epoch [2/5], Step [2664/10336], Loss: 0.6908\n",
      "Epoch [2/5], Step [2666/10336], Loss: 0.5513\n",
      "Epoch [2/5], Step [2668/10336], Loss: 2.2915\n",
      "Epoch [2/5], Step [2670/10336], Loss: 0.0363\n",
      "Epoch [2/5], Step [2672/10336], Loss: 0.7934\n",
      "Epoch [2/5], Step [2674/10336], Loss: 4.8218\n",
      "Epoch [2/5], Step [2676/10336], Loss: 0.5938\n",
      "Epoch [2/5], Step [2678/10336], Loss: 0.1207\n",
      "Epoch [2/5], Step [2680/10336], Loss: 0.2555\n",
      "Epoch [2/5], Step [2682/10336], Loss: 0.0451\n",
      "Epoch [2/5], Step [2684/10336], Loss: 1.2223\n",
      "Epoch [2/5], Step [2686/10336], Loss: 1.6656\n",
      "Epoch [2/5], Step [2688/10336], Loss: 0.2097\n",
      "Epoch [2/5], Step [2690/10336], Loss: 0.0248\n",
      "Epoch [2/5], Step [2692/10336], Loss: 1.1474\n",
      "Epoch [2/5], Step [2694/10336], Loss: 0.2853\n",
      "Epoch [2/5], Step [2696/10336], Loss: 1.6125\n",
      "Epoch [2/5], Step [2698/10336], Loss: 0.3146\n",
      "Epoch [2/5], Step [2700/10336], Loss: 1.1000\n",
      "Epoch [2/5], Step [2702/10336], Loss: 0.0624\n",
      "Epoch [2/5], Step [2704/10336], Loss: 2.4197\n",
      "Epoch [2/5], Step [2706/10336], Loss: 1.9235\n",
      "Epoch [2/5], Step [2708/10336], Loss: 2.0549\n",
      "Epoch [2/5], Step [2710/10336], Loss: 0.2562\n",
      "Epoch [2/5], Step [2712/10336], Loss: 1.0115\n",
      "Epoch [2/5], Step [2714/10336], Loss: 0.1144\n",
      "Epoch [2/5], Step [2716/10336], Loss: 0.2952\n",
      "Epoch [2/5], Step [2718/10336], Loss: 0.6297\n",
      "Epoch [2/5], Step [2720/10336], Loss: 0.8066\n",
      "Epoch [2/5], Step [2722/10336], Loss: 0.0180\n",
      "Epoch [2/5], Step [2724/10336], Loss: 1.5397\n",
      "Epoch [2/5], Step [2726/10336], Loss: 0.6520\n",
      "Epoch [2/5], Step [2728/10336], Loss: 0.2270\n",
      "Epoch [2/5], Step [2730/10336], Loss: 1.3614\n",
      "Epoch [2/5], Step [2732/10336], Loss: 0.0461\n",
      "Epoch [2/5], Step [2734/10336], Loss: 0.3749\n",
      "Epoch [2/5], Step [2736/10336], Loss: 0.5431\n",
      "Epoch [2/5], Step [2738/10336], Loss: 0.5698\n",
      "Epoch [2/5], Step [2740/10336], Loss: 1.1010\n",
      "Epoch [2/5], Step [2742/10336], Loss: 0.0218\n",
      "Epoch [2/5], Step [2744/10336], Loss: 0.0314\n",
      "Epoch [2/5], Step [2746/10336], Loss: 1.3380\n",
      "Epoch [2/5], Step [2748/10336], Loss: 1.0078\n",
      "Epoch [2/5], Step [2750/10336], Loss: 0.0313\n",
      "Epoch [2/5], Step [2752/10336], Loss: 1.0321\n",
      "Epoch [2/5], Step [2754/10336], Loss: 0.0924\n",
      "Epoch [2/5], Step [2756/10336], Loss: 1.7099\n",
      "Epoch [2/5], Step [2758/10336], Loss: 0.2221\n",
      "Epoch [2/5], Step [2760/10336], Loss: 0.3521\n",
      "Epoch [2/5], Step [2762/10336], Loss: 0.1198\n",
      "Epoch [2/5], Step [2764/10336], Loss: 0.4162\n",
      "Epoch [2/5], Step [2766/10336], Loss: 0.0598\n",
      "Epoch [2/5], Step [2768/10336], Loss: 2.1569\n",
      "Epoch [2/5], Step [2770/10336], Loss: 0.2043\n",
      "Epoch [2/5], Step [2772/10336], Loss: 1.3076\n",
      "Epoch [2/5], Step [2774/10336], Loss: 1.4856\n",
      "Epoch [2/5], Step [2776/10336], Loss: 0.1276\n",
      "Epoch [2/5], Step [2778/10336], Loss: 1.3893\n",
      "Epoch [2/5], Step [2780/10336], Loss: 0.3959\n",
      "Epoch [2/5], Step [2782/10336], Loss: 2.6119\n",
      "Epoch [2/5], Step [2784/10336], Loss: 1.3759\n",
      "Epoch [2/5], Step [2786/10336], Loss: 0.0476\n",
      "Epoch [2/5], Step [2788/10336], Loss: 0.1855\n",
      "Epoch [2/5], Step [2790/10336], Loss: 0.2306\n",
      "Epoch [2/5], Step [2792/10336], Loss: 0.4761\n",
      "Epoch [2/5], Step [2794/10336], Loss: 0.4989\n",
      "Epoch [2/5], Step [2796/10336], Loss: 3.1349\n",
      "Epoch [2/5], Step [2798/10336], Loss: 0.0279\n",
      "Epoch [2/5], Step [2800/10336], Loss: 0.0388\n",
      "Epoch [2/5], Step [2802/10336], Loss: 0.1203\n",
      "Epoch [2/5], Step [2804/10336], Loss: 0.0677\n",
      "Epoch [2/5], Step [2806/10336], Loss: 1.7285\n",
      "Epoch [2/5], Step [2808/10336], Loss: 0.0263\n",
      "Epoch [2/5], Step [2810/10336], Loss: 0.4911\n",
      "Epoch [2/5], Step [2812/10336], Loss: 2.9811\n",
      "Epoch [2/5], Step [2814/10336], Loss: 2.2777\n",
      "Epoch [2/5], Step [2816/10336], Loss: 0.0100\n",
      "Epoch [2/5], Step [2818/10336], Loss: 0.2187\n",
      "Epoch [2/5], Step [2820/10336], Loss: 0.1649\n",
      "Epoch [2/5], Step [2822/10336], Loss: 1.5660\n",
      "Epoch [2/5], Step [2824/10336], Loss: 0.5740\n",
      "Epoch [2/5], Step [2826/10336], Loss: 0.0755\n",
      "Epoch [2/5], Step [2828/10336], Loss: 0.0087\n",
      "Epoch [2/5], Step [2830/10336], Loss: 1.7216\n",
      "Epoch [2/5], Step [2832/10336], Loss: 1.2855\n",
      "Epoch [2/5], Step [2834/10336], Loss: 1.9112\n",
      "Epoch [2/5], Step [2836/10336], Loss: 0.9683\n",
      "Epoch [2/5], Step [2838/10336], Loss: 6.8964\n",
      "Epoch [2/5], Step [2840/10336], Loss: 2.8594\n",
      "Epoch [2/5], Step [2842/10336], Loss: 0.5532\n",
      "Epoch [2/5], Step [2844/10336], Loss: 2.5594\n",
      "Epoch [2/5], Step [2846/10336], Loss: 3.1478\n",
      "Epoch [2/5], Step [2848/10336], Loss: 3.8325\n",
      "Epoch [2/5], Step [2850/10336], Loss: 0.2751\n",
      "Epoch [2/5], Step [2852/10336], Loss: 0.1439\n",
      "Epoch [2/5], Step [2854/10336], Loss: 0.7951\n",
      "Epoch [2/5], Step [2856/10336], Loss: 2.0412\n",
      "Epoch [2/5], Step [2858/10336], Loss: 1.6973\n",
      "Epoch [2/5], Step [2860/10336], Loss: 1.1491\n",
      "Epoch [2/5], Step [2862/10336], Loss: 0.2595\n",
      "Epoch [2/5], Step [2864/10336], Loss: 1.7761\n",
      "Epoch [2/5], Step [2866/10336], Loss: 0.5297\n",
      "Epoch [2/5], Step [2868/10336], Loss: 1.8608\n",
      "Epoch [2/5], Step [2870/10336], Loss: 1.3712\n",
      "Epoch [2/5], Step [2872/10336], Loss: 0.0369\n",
      "Epoch [2/5], Step [2874/10336], Loss: 0.0401\n",
      "Epoch [2/5], Step [2876/10336], Loss: 1.0453\n",
      "Epoch [2/5], Step [2878/10336], Loss: 0.9236\n",
      "Epoch [2/5], Step [2880/10336], Loss: 0.4000\n",
      "Epoch [2/5], Step [2882/10336], Loss: 0.0509\n",
      "Epoch [2/5], Step [2884/10336], Loss: 0.1140\n",
      "Epoch [2/5], Step [2886/10336], Loss: 1.1247\n",
      "Epoch [2/5], Step [2888/10336], Loss: 0.1666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [2890/10336], Loss: 1.8930\n",
      "Epoch [2/5], Step [2892/10336], Loss: 1.0473\n",
      "Epoch [2/5], Step [2894/10336], Loss: 0.9247\n",
      "Epoch [2/5], Step [2896/10336], Loss: 0.4153\n",
      "Epoch [2/5], Step [2898/10336], Loss: 0.1494\n",
      "Epoch [2/5], Step [2900/10336], Loss: 4.9701\n",
      "Epoch [2/5], Step [2902/10336], Loss: 0.3210\n",
      "Epoch [2/5], Step [2904/10336], Loss: 2.9711\n",
      "Epoch [2/5], Step [2906/10336], Loss: 1.2309\n",
      "Epoch [2/5], Step [2908/10336], Loss: 3.0146\n",
      "Epoch [2/5], Step [2910/10336], Loss: 1.2739\n",
      "Epoch [2/5], Step [2912/10336], Loss: 2.0351\n",
      "Epoch [2/5], Step [2914/10336], Loss: 1.3488\n",
      "Epoch [2/5], Step [2916/10336], Loss: 1.8950\n",
      "Epoch [2/5], Step [2918/10336], Loss: 1.0049\n",
      "Epoch [2/5], Step [2920/10336], Loss: 0.7712\n",
      "Epoch [2/5], Step [2922/10336], Loss: 0.0684\n",
      "Epoch [2/5], Step [2924/10336], Loss: 0.0515\n",
      "Epoch [2/5], Step [2926/10336], Loss: 1.2464\n",
      "Epoch [2/5], Step [2928/10336], Loss: 1.1031\n",
      "Epoch [2/5], Step [2930/10336], Loss: 1.1850\n",
      "Epoch [2/5], Step [2932/10336], Loss: 1.8731\n",
      "Epoch [2/5], Step [2934/10336], Loss: 1.6365\n",
      "Epoch [2/5], Step [2936/10336], Loss: 0.3426\n",
      "Epoch [2/5], Step [2938/10336], Loss: 1.2475\n",
      "Epoch [2/5], Step [2940/10336], Loss: 0.6928\n",
      "Epoch [2/5], Step [2942/10336], Loss: 0.5637\n",
      "Epoch [2/5], Step [2944/10336], Loss: 1.4494\n",
      "Epoch [2/5], Step [2946/10336], Loss: 1.6317\n",
      "Epoch [2/5], Step [2948/10336], Loss: 0.0075\n",
      "Epoch [2/5], Step [2950/10336], Loss: 5.0718\n",
      "Epoch [2/5], Step [2952/10336], Loss: 1.4810\n",
      "Epoch [2/5], Step [2954/10336], Loss: 2.5502\n",
      "Epoch [2/5], Step [2956/10336], Loss: 1.5367\n",
      "Epoch [2/5], Step [2958/10336], Loss: 0.1322\n",
      "Epoch [2/5], Step [2960/10336], Loss: 0.2143\n",
      "Epoch [2/5], Step [2962/10336], Loss: 2.3999\n",
      "Epoch [2/5], Step [2964/10336], Loss: 0.4130\n",
      "Epoch [2/5], Step [2966/10336], Loss: 0.8764\n",
      "Epoch [2/5], Step [2968/10336], Loss: 0.3688\n",
      "Epoch [2/5], Step [2970/10336], Loss: 0.2488\n",
      "Epoch [2/5], Step [2972/10336], Loss: 0.3466\n",
      "Epoch [2/5], Step [2974/10336], Loss: 1.1860\n",
      "Epoch [2/5], Step [2976/10336], Loss: 0.0242\n",
      "Epoch [2/5], Step [2978/10336], Loss: 0.1529\n",
      "Epoch [2/5], Step [2980/10336], Loss: 0.0951\n",
      "Epoch [2/5], Step [2982/10336], Loss: 0.5075\n",
      "Epoch [2/5], Step [2984/10336], Loss: 0.0757\n",
      "Epoch [2/5], Step [2986/10336], Loss: 3.1191\n",
      "Epoch [2/5], Step [2988/10336], Loss: 0.1714\n",
      "Epoch [2/5], Step [2990/10336], Loss: 3.5663\n",
      "Epoch [2/5], Step [2992/10336], Loss: 1.3720\n",
      "Epoch [2/5], Step [2994/10336], Loss: 1.2542\n",
      "Epoch [2/5], Step [2996/10336], Loss: 0.4142\n",
      "Epoch [2/5], Step [2998/10336], Loss: 1.1882\n",
      "Epoch [2/5], Step [3000/10336], Loss: 0.8962\n",
      "Epoch [2/5], Step [3002/10336], Loss: 0.6420\n",
      "Epoch [2/5], Step [3004/10336], Loss: 0.0506\n",
      "Epoch [2/5], Step [3006/10336], Loss: 1.2936\n",
      "Epoch [2/5], Step [3008/10336], Loss: 2.3013\n",
      "Epoch [2/5], Step [3010/10336], Loss: 0.1708\n",
      "Epoch [2/5], Step [3012/10336], Loss: 0.0701\n",
      "Epoch [2/5], Step [3014/10336], Loss: 3.2725\n",
      "Epoch [2/5], Step [3016/10336], Loss: 0.1451\n",
      "Epoch [2/5], Step [3018/10336], Loss: 0.1191\n",
      "Epoch [2/5], Step [3020/10336], Loss: 0.9081\n",
      "Epoch [2/5], Step [3022/10336], Loss: 0.2226\n",
      "Epoch [2/5], Step [3024/10336], Loss: 1.0558\n",
      "Epoch [2/5], Step [3026/10336], Loss: 1.1258\n",
      "Epoch [2/5], Step [3028/10336], Loss: 0.2722\n",
      "Epoch [2/5], Step [3030/10336], Loss: 1.2499\n",
      "Epoch [2/5], Step [3032/10336], Loss: 3.4409\n",
      "Epoch [2/5], Step [3034/10336], Loss: 0.8730\n",
      "Epoch [2/5], Step [3036/10336], Loss: 0.9023\n",
      "Epoch [2/5], Step [3038/10336], Loss: 0.1994\n",
      "Epoch [2/5], Step [3040/10336], Loss: 1.2584\n",
      "Epoch [2/5], Step [3042/10336], Loss: 0.2201\n",
      "Epoch [2/5], Step [3044/10336], Loss: 0.9265\n",
      "Epoch [2/5], Step [3046/10336], Loss: 0.0305\n",
      "Epoch [2/5], Step [3048/10336], Loss: 0.0392\n",
      "Epoch [2/5], Step [3050/10336], Loss: 1.7624\n",
      "Epoch [2/5], Step [3052/10336], Loss: 0.6450\n",
      "Epoch [2/5], Step [3054/10336], Loss: 5.2445\n",
      "Epoch [2/5], Step [3056/10336], Loss: 0.4257\n",
      "Epoch [2/5], Step [3058/10336], Loss: 0.4930\n",
      "Epoch [2/5], Step [3060/10336], Loss: 0.0607\n",
      "Epoch [2/5], Step [3062/10336], Loss: 2.4746\n",
      "Epoch [2/5], Step [3064/10336], Loss: 0.0586\n",
      "Epoch [2/5], Step [3066/10336], Loss: 3.4207\n",
      "Epoch [2/5], Step [3068/10336], Loss: 1.5569\n",
      "Epoch [2/5], Step [3070/10336], Loss: 0.3953\n",
      "Epoch [2/5], Step [3072/10336], Loss: 0.7117\n",
      "Epoch [2/5], Step [3074/10336], Loss: 0.4040\n",
      "Epoch [2/5], Step [3076/10336], Loss: 0.1837\n",
      "Epoch [2/5], Step [3078/10336], Loss: 2.8729\n",
      "Epoch [2/5], Step [3080/10336], Loss: 0.1996\n",
      "Epoch [2/5], Step [3082/10336], Loss: 0.4713\n",
      "Epoch [2/5], Step [3084/10336], Loss: 2.4130\n",
      "Epoch [2/5], Step [3086/10336], Loss: 0.0124\n",
      "Epoch [2/5], Step [3088/10336], Loss: 0.1856\n",
      "Epoch [2/5], Step [3090/10336], Loss: 0.5065\n",
      "Epoch [2/5], Step [3092/10336], Loss: 0.7214\n",
      "Epoch [2/5], Step [3094/10336], Loss: 0.2688\n",
      "Epoch [2/5], Step [3096/10336], Loss: 1.0667\n",
      "Epoch [2/5], Step [3098/10336], Loss: 2.6722\n",
      "Epoch [2/5], Step [3100/10336], Loss: 0.2927\n",
      "Epoch [2/5], Step [3102/10336], Loss: 0.6859\n",
      "Epoch [2/5], Step [3104/10336], Loss: 1.7307\n",
      "Epoch [2/5], Step [3106/10336], Loss: 3.0665\n",
      "Epoch [2/5], Step [3108/10336], Loss: 4.1665\n",
      "Epoch [2/5], Step [3110/10336], Loss: 0.0872\n",
      "Epoch [2/5], Step [3112/10336], Loss: 0.8314\n",
      "Epoch [2/5], Step [3114/10336], Loss: 0.5463\n",
      "Epoch [2/5], Step [3116/10336], Loss: 1.0356\n",
      "Epoch [2/5], Step [3118/10336], Loss: 1.8874\n",
      "Epoch [2/5], Step [3120/10336], Loss: 4.5227\n",
      "Epoch [2/5], Step [3122/10336], Loss: 0.2164\n",
      "Epoch [2/5], Step [3124/10336], Loss: 0.1081\n",
      "Epoch [2/5], Step [3126/10336], Loss: 0.7642\n",
      "Epoch [2/5], Step [3128/10336], Loss: 0.9023\n",
      "Epoch [2/5], Step [3130/10336], Loss: 1.5183\n",
      "Epoch [2/5], Step [3132/10336], Loss: 1.6674\n",
      "Epoch [2/5], Step [3134/10336], Loss: 0.7501\n",
      "Epoch [2/5], Step [3136/10336], Loss: 0.9867\n",
      "Epoch [2/5], Step [3138/10336], Loss: 2.9019\n",
      "Epoch [2/5], Step [3140/10336], Loss: 0.7848\n",
      "Epoch [2/5], Step [3142/10336], Loss: 0.5792\n",
      "Epoch [2/5], Step [3144/10336], Loss: 0.5241\n",
      "Epoch [2/5], Step [3146/10336], Loss: 1.8548\n",
      "Epoch [2/5], Step [3148/10336], Loss: 2.3061\n",
      "Epoch [2/5], Step [3150/10336], Loss: 0.1107\n",
      "Epoch [2/5], Step [3152/10336], Loss: 0.9060\n",
      "Epoch [2/5], Step [3154/10336], Loss: 0.0762\n",
      "Epoch [2/5], Step [3156/10336], Loss: 1.3548\n",
      "Epoch [2/5], Step [3158/10336], Loss: 0.5386\n",
      "Epoch [2/5], Step [3160/10336], Loss: 2.7373\n",
      "Epoch [2/5], Step [3162/10336], Loss: 0.2919\n",
      "Epoch [2/5], Step [3164/10336], Loss: 1.3469\n",
      "Epoch [2/5], Step [3166/10336], Loss: 1.3339\n",
      "Epoch [2/5], Step [3168/10336], Loss: 0.2682\n",
      "Epoch [2/5], Step [3170/10336], Loss: 0.6110\n",
      "Epoch [2/5], Step [3172/10336], Loss: 0.0914\n",
      "Epoch [2/5], Step [3174/10336], Loss: 1.0160\n",
      "Epoch [2/5], Step [3176/10336], Loss: 1.1653\n",
      "Epoch [2/5], Step [3178/10336], Loss: 0.3478\n",
      "Epoch [2/5], Step [3180/10336], Loss: 0.6797\n",
      "Epoch [2/5], Step [3182/10336], Loss: 1.2568\n",
      "Epoch [2/5], Step [3184/10336], Loss: 1.2684\n",
      "Epoch [2/5], Step [3186/10336], Loss: 0.1508\n",
      "Epoch [2/5], Step [3188/10336], Loss: 0.0221\n",
      "Epoch [2/5], Step [3190/10336], Loss: 0.7667\n",
      "Epoch [2/5], Step [3192/10336], Loss: 0.6821\n",
      "Epoch [2/5], Step [3194/10336], Loss: 0.0471\n",
      "Epoch [2/5], Step [3196/10336], Loss: 1.8574\n",
      "Epoch [2/5], Step [3198/10336], Loss: 0.4005\n",
      "Epoch [2/5], Step [3200/10336], Loss: 0.8834\n",
      "Epoch [2/5], Step [3202/10336], Loss: 0.4165\n",
      "Epoch [2/5], Step [3204/10336], Loss: 1.1563\n",
      "Epoch [2/5], Step [3206/10336], Loss: 3.0440\n",
      "Epoch [2/5], Step [3208/10336], Loss: 0.1410\n",
      "Epoch [2/5], Step [3210/10336], Loss: 4.8112\n",
      "Epoch [2/5], Step [3212/10336], Loss: 0.7125\n",
      "Epoch [2/5], Step [3214/10336], Loss: 2.3288\n",
      "Epoch [2/5], Step [3216/10336], Loss: 0.7919\n",
      "Epoch [2/5], Step [3218/10336], Loss: 1.3114\n",
      "Epoch [2/5], Step [3220/10336], Loss: 3.3073\n",
      "Epoch [2/5], Step [3222/10336], Loss: 2.3522\n",
      "Epoch [2/5], Step [3224/10336], Loss: 0.4703\n",
      "Epoch [2/5], Step [3226/10336], Loss: 2.5212\n",
      "Epoch [2/5], Step [3228/10336], Loss: 0.2710\n",
      "Epoch [2/5], Step [3230/10336], Loss: 0.4816\n",
      "Epoch [2/5], Step [3232/10336], Loss: 0.4778\n",
      "Epoch [2/5], Step [3234/10336], Loss: 0.6765\n",
      "Epoch [2/5], Step [3236/10336], Loss: 0.1836\n",
      "Epoch [2/5], Step [3238/10336], Loss: 1.2197\n",
      "Epoch [2/5], Step [3240/10336], Loss: 0.0164\n",
      "Epoch [2/5], Step [3242/10336], Loss: 0.1786\n",
      "Epoch [2/5], Step [3244/10336], Loss: 1.1763\n",
      "Epoch [2/5], Step [3246/10336], Loss: 1.8555\n",
      "Epoch [2/5], Step [3248/10336], Loss: 0.1170\n",
      "Epoch [2/5], Step [3250/10336], Loss: 0.0954\n",
      "Epoch [2/5], Step [3252/10336], Loss: 1.1330\n",
      "Epoch [2/5], Step [3254/10336], Loss: 1.6103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [3256/10336], Loss: 0.3015\n",
      "Epoch [2/5], Step [3258/10336], Loss: 0.4486\n",
      "Epoch [2/5], Step [3260/10336], Loss: 0.0126\n",
      "Epoch [2/5], Step [3262/10336], Loss: 2.7319\n",
      "Epoch [2/5], Step [3264/10336], Loss: 0.1298\n",
      "Epoch [2/5], Step [3266/10336], Loss: 0.3525\n",
      "Epoch [2/5], Step [3268/10336], Loss: 1.5438\n",
      "Epoch [2/5], Step [3270/10336], Loss: 0.0519\n",
      "Epoch [2/5], Step [3272/10336], Loss: 0.5633\n",
      "Epoch [2/5], Step [3274/10336], Loss: 0.4690\n",
      "Epoch [2/5], Step [3276/10336], Loss: 1.7695\n",
      "Epoch [2/5], Step [3278/10336], Loss: 0.5580\n",
      "Epoch [2/5], Step [3280/10336], Loss: 1.8548\n",
      "Epoch [2/5], Step [3282/10336], Loss: 1.2288\n",
      "Epoch [2/5], Step [3284/10336], Loss: 2.0191\n",
      "Epoch [2/5], Step [3286/10336], Loss: 0.0479\n",
      "Epoch [2/5], Step [3288/10336], Loss: 0.2853\n",
      "Epoch [2/5], Step [3290/10336], Loss: 0.0218\n",
      "Epoch [2/5], Step [3292/10336], Loss: 0.0014\n",
      "Epoch [2/5], Step [3294/10336], Loss: 1.1541\n",
      "Epoch [2/5], Step [3296/10336], Loss: 0.0020\n",
      "Epoch [2/5], Step [3298/10336], Loss: 0.5339\n",
      "Epoch [2/5], Step [3300/10336], Loss: 1.4501\n",
      "Epoch [2/5], Step [3302/10336], Loss: 0.2489\n",
      "Epoch [2/5], Step [3304/10336], Loss: 3.3746\n",
      "Epoch [2/5], Step [3306/10336], Loss: 0.1920\n",
      "Epoch [2/5], Step [3308/10336], Loss: 1.0563\n",
      "Epoch [2/5], Step [3310/10336], Loss: 0.1902\n",
      "Epoch [2/5], Step [3312/10336], Loss: 0.9769\n",
      "Epoch [2/5], Step [3314/10336], Loss: 0.7831\n",
      "Epoch [2/5], Step [3316/10336], Loss: 0.5978\n",
      "Epoch [2/5], Step [3318/10336], Loss: 0.2581\n",
      "Epoch [2/5], Step [3320/10336], Loss: 0.8109\n",
      "Epoch [2/5], Step [3322/10336], Loss: 2.1663\n",
      "Epoch [2/5], Step [3324/10336], Loss: 1.4033\n",
      "Epoch [2/5], Step [3326/10336], Loss: 0.1060\n",
      "Epoch [2/5], Step [3328/10336], Loss: 1.2396\n",
      "Epoch [2/5], Step [3330/10336], Loss: 0.9566\n",
      "Epoch [2/5], Step [3332/10336], Loss: 0.2694\n",
      "Epoch [2/5], Step [3334/10336], Loss: 0.2429\n",
      "Epoch [2/5], Step [3336/10336], Loss: 0.1259\n",
      "Epoch [2/5], Step [3338/10336], Loss: 0.0839\n",
      "Epoch [2/5], Step [3340/10336], Loss: 0.6032\n",
      "Epoch [2/5], Step [3342/10336], Loss: 0.9293\n",
      "Epoch [2/5], Step [3344/10336], Loss: 1.8885\n",
      "Epoch [2/5], Step [3346/10336], Loss: 0.1553\n",
      "Epoch [2/5], Step [3348/10336], Loss: 0.0247\n",
      "Epoch [2/5], Step [3350/10336], Loss: 0.2471\n",
      "Epoch [2/5], Step [3352/10336], Loss: 0.8851\n",
      "Epoch [2/5], Step [3354/10336], Loss: 0.9301\n",
      "Epoch [2/5], Step [3356/10336], Loss: 0.8322\n",
      "Epoch [2/5], Step [3358/10336], Loss: 2.9584\n",
      "Epoch [2/5], Step [3360/10336], Loss: 2.5428\n",
      "Epoch [2/5], Step [3362/10336], Loss: 0.1852\n",
      "Epoch [2/5], Step [3364/10336], Loss: 0.0416\n",
      "Epoch [2/5], Step [3366/10336], Loss: 0.0690\n",
      "Epoch [2/5], Step [3368/10336], Loss: 2.9633\n",
      "Epoch [2/5], Step [3370/10336], Loss: 0.1339\n",
      "Epoch [2/5], Step [3372/10336], Loss: 5.0831\n",
      "Epoch [2/5], Step [3374/10336], Loss: 1.2310\n",
      "Epoch [2/5], Step [3376/10336], Loss: 1.1499\n",
      "Epoch [2/5], Step [3378/10336], Loss: 0.1018\n",
      "Epoch [2/5], Step [3380/10336], Loss: 4.0360\n",
      "Epoch [2/5], Step [3382/10336], Loss: 0.6150\n",
      "Epoch [2/5], Step [3384/10336], Loss: 1.7728\n",
      "Epoch [2/5], Step [3386/10336], Loss: 0.2603\n",
      "Epoch [2/5], Step [3388/10336], Loss: 0.3547\n",
      "Epoch [2/5], Step [3390/10336], Loss: 1.6460\n",
      "Epoch [2/5], Step [3392/10336], Loss: 2.9712\n",
      "Epoch [2/5], Step [3394/10336], Loss: 0.3612\n",
      "Epoch [2/5], Step [3396/10336], Loss: 0.1163\n",
      "Epoch [2/5], Step [3398/10336], Loss: 2.9734\n",
      "Epoch [2/5], Step [3400/10336], Loss: 1.1513\n",
      "Epoch [2/5], Step [3402/10336], Loss: 0.0716\n",
      "Epoch [2/5], Step [3404/10336], Loss: 1.9226\n",
      "Epoch [2/5], Step [3406/10336], Loss: 0.0734\n",
      "Epoch [2/5], Step [3408/10336], Loss: 0.3222\n",
      "Epoch [2/5], Step [3410/10336], Loss: 0.7387\n",
      "Epoch [2/5], Step [3412/10336], Loss: 1.5474\n",
      "Epoch [2/5], Step [3414/10336], Loss: 1.8641\n",
      "Epoch [2/5], Step [3416/10336], Loss: 0.1851\n",
      "Epoch [2/5], Step [3418/10336], Loss: 1.2701\n",
      "Epoch [2/5], Step [3420/10336], Loss: 0.4561\n",
      "Epoch [2/5], Step [3422/10336], Loss: 0.2636\n",
      "Epoch [2/5], Step [3424/10336], Loss: 0.4386\n",
      "Epoch [2/5], Step [3426/10336], Loss: 2.1780\n",
      "Epoch [2/5], Step [3428/10336], Loss: 1.1381\n",
      "Epoch [2/5], Step [3430/10336], Loss: 0.0161\n",
      "Epoch [2/5], Step [3432/10336], Loss: 0.1023\n",
      "Epoch [2/5], Step [3434/10336], Loss: 0.0372\n",
      "Epoch [2/5], Step [3436/10336], Loss: 0.1839\n",
      "Epoch [2/5], Step [3438/10336], Loss: 1.4285\n",
      "Epoch [2/5], Step [3440/10336], Loss: 0.5718\n",
      "Epoch [2/5], Step [3442/10336], Loss: 0.2660\n",
      "Epoch [2/5], Step [3444/10336], Loss: 1.4716\n",
      "Epoch [2/5], Step [3446/10336], Loss: 0.0122\n",
      "Epoch [2/5], Step [3448/10336], Loss: 0.4002\n",
      "Epoch [2/5], Step [3450/10336], Loss: 1.4358\n",
      "Epoch [2/5], Step [3452/10336], Loss: 0.4109\n",
      "Epoch [2/5], Step [3454/10336], Loss: 0.2684\n",
      "Epoch [2/5], Step [3456/10336], Loss: 2.6925\n",
      "Epoch [2/5], Step [3458/10336], Loss: 0.0488\n",
      "Epoch [2/5], Step [3460/10336], Loss: 1.6784\n",
      "Epoch [2/5], Step [3462/10336], Loss: 0.0527\n",
      "Epoch [2/5], Step [3464/10336], Loss: 0.3056\n",
      "Epoch [2/5], Step [3466/10336], Loss: 0.0611\n",
      "Epoch [2/5], Step [3468/10336], Loss: 1.0465\n",
      "Epoch [2/5], Step [3470/10336], Loss: 0.7122\n",
      "Epoch [2/5], Step [3472/10336], Loss: 0.5848\n",
      "Epoch [2/5], Step [3474/10336], Loss: 1.0879\n",
      "Epoch [2/5], Step [3476/10336], Loss: 4.2692\n",
      "Epoch [2/5], Step [3478/10336], Loss: 0.4866\n",
      "Epoch [2/5], Step [3480/10336], Loss: 0.6947\n",
      "Epoch [2/5], Step [3482/10336], Loss: 0.4340\n",
      "Epoch [2/5], Step [3484/10336], Loss: 0.6762\n",
      "Epoch [2/5], Step [3486/10336], Loss: 0.0982\n",
      "Epoch [2/5], Step [3488/10336], Loss: 1.3497\n",
      "Epoch [2/5], Step [3490/10336], Loss: 3.8613\n",
      "Epoch [2/5], Step [3492/10336], Loss: 0.8967\n",
      "Epoch [2/5], Step [3494/10336], Loss: 0.5478\n",
      "Epoch [2/5], Step [3496/10336], Loss: 0.7144\n",
      "Epoch [2/5], Step [3498/10336], Loss: 0.6281\n",
      "Epoch [2/5], Step [3500/10336], Loss: 0.0418\n",
      "Epoch [2/5], Step [3502/10336], Loss: 0.7627\n",
      "Epoch [2/5], Step [3504/10336], Loss: 0.8514\n",
      "Epoch [2/5], Step [3506/10336], Loss: 1.9290\n",
      "Epoch [2/5], Step [3508/10336], Loss: 2.3053\n",
      "Epoch [2/5], Step [3510/10336], Loss: 0.1202\n",
      "Epoch [2/5], Step [3512/10336], Loss: 3.9870\n",
      "Epoch [2/5], Step [3514/10336], Loss: 0.0033\n",
      "Epoch [2/5], Step [3516/10336], Loss: 0.0798\n",
      "Epoch [2/5], Step [3518/10336], Loss: 0.0467\n",
      "Epoch [2/5], Step [3520/10336], Loss: 0.4421\n",
      "Epoch [2/5], Step [3522/10336], Loss: 0.2377\n",
      "Epoch [2/5], Step [3524/10336], Loss: 1.0254\n",
      "Epoch [2/5], Step [3526/10336], Loss: 1.4921\n",
      "Epoch [2/5], Step [3528/10336], Loss: 0.1105\n",
      "Epoch [2/5], Step [3530/10336], Loss: 0.2390\n",
      "Epoch [2/5], Step [3532/10336], Loss: 0.3538\n",
      "Epoch [2/5], Step [3534/10336], Loss: 0.0244\n",
      "Epoch [2/5], Step [3536/10336], Loss: 0.9345\n",
      "Epoch [2/5], Step [3538/10336], Loss: 0.8915\n",
      "Epoch [2/5], Step [3540/10336], Loss: 0.0394\n",
      "Epoch [2/5], Step [3542/10336], Loss: 1.8881\n",
      "Epoch [2/5], Step [3544/10336], Loss: 0.9368\n",
      "Epoch [2/5], Step [3546/10336], Loss: 0.7570\n",
      "Epoch [2/5], Step [3548/10336], Loss: 1.8607\n",
      "Epoch [2/5], Step [3550/10336], Loss: 2.4237\n",
      "Epoch [2/5], Step [3552/10336], Loss: 0.5848\n",
      "Epoch [2/5], Step [3554/10336], Loss: 0.0797\n",
      "Epoch [2/5], Step [3556/10336], Loss: 0.0326\n",
      "Epoch [2/5], Step [3558/10336], Loss: 2.3738\n",
      "Epoch [2/5], Step [3560/10336], Loss: 1.9038\n",
      "Epoch [2/5], Step [3562/10336], Loss: 1.3864\n",
      "Epoch [2/5], Step [3564/10336], Loss: 1.1773\n",
      "Epoch [2/5], Step [3566/10336], Loss: 0.7262\n",
      "Epoch [2/5], Step [3568/10336], Loss: 0.5561\n",
      "Epoch [2/5], Step [3570/10336], Loss: 0.9620\n",
      "Epoch [2/5], Step [3572/10336], Loss: 0.4414\n",
      "Epoch [2/5], Step [3574/10336], Loss: 0.1832\n",
      "Epoch [2/5], Step [3576/10336], Loss: 1.1785\n",
      "Epoch [2/5], Step [3578/10336], Loss: 0.8816\n",
      "Epoch [2/5], Step [3580/10336], Loss: 0.7739\n",
      "Epoch [2/5], Step [3582/10336], Loss: 0.0274\n",
      "Epoch [2/5], Step [3584/10336], Loss: 2.2413\n",
      "Epoch [2/5], Step [3586/10336], Loss: 0.8204\n",
      "Epoch [2/5], Step [3588/10336], Loss: 0.7471\n",
      "Epoch [2/5], Step [3590/10336], Loss: 1.2795\n",
      "Epoch [2/5], Step [3592/10336], Loss: 2.0287\n",
      "Epoch [2/5], Step [3594/10336], Loss: 0.2834\n",
      "Epoch [2/5], Step [3596/10336], Loss: 1.1966\n",
      "Epoch [2/5], Step [3598/10336], Loss: 0.0410\n",
      "Epoch [2/5], Step [3600/10336], Loss: 1.9565\n",
      "Epoch [2/5], Step [3602/10336], Loss: 1.0861\n",
      "Epoch [2/5], Step [3604/10336], Loss: 0.6944\n",
      "Epoch [2/5], Step [3606/10336], Loss: 0.5759\n",
      "Epoch [2/5], Step [3608/10336], Loss: 0.3707\n",
      "Epoch [2/5], Step [3610/10336], Loss: 1.7426\n",
      "Epoch [2/5], Step [3612/10336], Loss: 3.2932\n",
      "Epoch [2/5], Step [3614/10336], Loss: 1.1449\n",
      "Epoch [2/5], Step [3616/10336], Loss: 1.2741\n",
      "Epoch [2/5], Step [3618/10336], Loss: 0.6709\n",
      "Epoch [2/5], Step [3620/10336], Loss: 0.4126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [3622/10336], Loss: 2.2285\n",
      "Epoch [2/5], Step [3624/10336], Loss: 0.1417\n",
      "Epoch [2/5], Step [3626/10336], Loss: 1.4964\n",
      "Epoch [2/5], Step [3628/10336], Loss: 0.5322\n",
      "Epoch [2/5], Step [3630/10336], Loss: 0.8011\n",
      "Epoch [2/5], Step [3632/10336], Loss: 0.0394\n",
      "Epoch [2/5], Step [3634/10336], Loss: 0.0318\n",
      "Epoch [2/5], Step [3636/10336], Loss: 0.1877\n",
      "Epoch [2/5], Step [3638/10336], Loss: 2.3469\n",
      "Epoch [2/5], Step [3640/10336], Loss: 0.3158\n",
      "Epoch [2/5], Step [3642/10336], Loss: 0.0686\n",
      "Epoch [2/5], Step [3644/10336], Loss: 0.5689\n",
      "Epoch [2/5], Step [3646/10336], Loss: 0.6487\n",
      "Epoch [2/5], Step [3648/10336], Loss: 0.0199\n",
      "Epoch [2/5], Step [3650/10336], Loss: 0.4018\n",
      "Epoch [2/5], Step [3652/10336], Loss: 0.0496\n",
      "Epoch [2/5], Step [3654/10336], Loss: 0.4067\n",
      "Epoch [2/5], Step [3656/10336], Loss: 0.3093\n",
      "Epoch [2/5], Step [3658/10336], Loss: 0.0141\n",
      "Epoch [2/5], Step [3660/10336], Loss: 1.1894\n",
      "Epoch [2/5], Step [3662/10336], Loss: 1.8801\n",
      "Epoch [2/5], Step [3664/10336], Loss: 1.0985\n",
      "Epoch [2/5], Step [3666/10336], Loss: 0.1872\n",
      "Epoch [2/5], Step [3668/10336], Loss: 2.1785\n",
      "Epoch [2/5], Step [3670/10336], Loss: 0.0007\n",
      "Epoch [2/5], Step [3672/10336], Loss: 1.0436\n",
      "Epoch [2/5], Step [3674/10336], Loss: 6.3530\n",
      "Epoch [2/5], Step [3676/10336], Loss: 4.4361\n",
      "Epoch [2/5], Step [3678/10336], Loss: 0.2741\n",
      "Epoch [2/5], Step [3680/10336], Loss: 0.2745\n",
      "Epoch [2/5], Step [3682/10336], Loss: 0.0078\n",
      "Epoch [2/5], Step [3684/10336], Loss: 0.0714\n",
      "Epoch [2/5], Step [3686/10336], Loss: 1.3051\n",
      "Epoch [2/5], Step [3688/10336], Loss: 0.1124\n",
      "Epoch [2/5], Step [3690/10336], Loss: 3.0870\n",
      "Epoch [2/5], Step [3692/10336], Loss: 0.9410\n",
      "Epoch [2/5], Step [3694/10336], Loss: 0.5049\n",
      "Epoch [2/5], Step [3696/10336], Loss: 1.7198\n",
      "Epoch [2/5], Step [3698/10336], Loss: 3.1841\n",
      "Epoch [2/5], Step [3700/10336], Loss: 0.0287\n",
      "Epoch [2/5], Step [3702/10336], Loss: 0.1589\n",
      "Epoch [2/5], Step [3704/10336], Loss: 0.7492\n",
      "Epoch [2/5], Step [3706/10336], Loss: 1.2123\n",
      "Epoch [2/5], Step [3708/10336], Loss: 1.1923\n",
      "Epoch [2/5], Step [3710/10336], Loss: 3.2948\n",
      "Epoch [2/5], Step [3712/10336], Loss: 0.0854\n",
      "Epoch [2/5], Step [3714/10336], Loss: 0.0883\n",
      "Epoch [2/5], Step [3716/10336], Loss: 0.1597\n",
      "Epoch [2/5], Step [3718/10336], Loss: 1.4524\n",
      "Epoch [2/5], Step [3720/10336], Loss: 0.4652\n",
      "Epoch [2/5], Step [3722/10336], Loss: 0.0946\n",
      "Epoch [2/5], Step [3724/10336], Loss: 0.7171\n",
      "Epoch [2/5], Step [3726/10336], Loss: 0.0235\n",
      "Epoch [2/5], Step [3728/10336], Loss: 1.3787\n",
      "Epoch [2/5], Step [3730/10336], Loss: 1.0472\n",
      "Epoch [2/5], Step [3732/10336], Loss: 1.9184\n",
      "Epoch [2/5], Step [3734/10336], Loss: 1.9957\n",
      "Epoch [2/5], Step [3736/10336], Loss: 1.5504\n",
      "Epoch [2/5], Step [3738/10336], Loss: 0.7920\n",
      "Epoch [2/5], Step [3740/10336], Loss: 2.6895\n",
      "Epoch [2/5], Step [3742/10336], Loss: 0.2804\n",
      "Epoch [2/5], Step [3744/10336], Loss: 1.5029\n",
      "Epoch [2/5], Step [3746/10336], Loss: 1.6896\n",
      "Epoch [2/5], Step [3748/10336], Loss: 1.6258\n",
      "Epoch [2/5], Step [3750/10336], Loss: 0.0428\n",
      "Epoch [2/5], Step [3752/10336], Loss: 1.2201\n",
      "Epoch [2/5], Step [3754/10336], Loss: 0.0708\n",
      "Epoch [2/5], Step [3756/10336], Loss: 2.5871\n",
      "Epoch [2/5], Step [3758/10336], Loss: 3.4443\n",
      "Epoch [2/5], Step [3760/10336], Loss: 0.3603\n",
      "Epoch [2/5], Step [3762/10336], Loss: 0.0289\n",
      "Epoch [2/5], Step [3764/10336], Loss: 0.0295\n",
      "Epoch [2/5], Step [3766/10336], Loss: 0.4328\n",
      "Epoch [2/5], Step [3768/10336], Loss: 1.5769\n",
      "Epoch [2/5], Step [3770/10336], Loss: 1.7433\n",
      "Epoch [2/5], Step [3772/10336], Loss: 1.2283\n",
      "Epoch [2/5], Step [3774/10336], Loss: 1.0791\n",
      "Epoch [2/5], Step [3776/10336], Loss: 3.6969\n",
      "Epoch [2/5], Step [3778/10336], Loss: 0.3295\n",
      "Epoch [2/5], Step [3780/10336], Loss: 0.4628\n",
      "Epoch [2/5], Step [3782/10336], Loss: 0.8562\n",
      "Epoch [2/5], Step [3784/10336], Loss: 0.5879\n",
      "Epoch [2/5], Step [3786/10336], Loss: 1.6863\n",
      "Epoch [2/5], Step [3788/10336], Loss: 0.2424\n",
      "Epoch [2/5], Step [3790/10336], Loss: 1.2884\n",
      "Epoch [2/5], Step [3792/10336], Loss: 1.3745\n",
      "Epoch [2/5], Step [3794/10336], Loss: 0.1463\n",
      "Epoch [2/5], Step [3796/10336], Loss: 1.9754\n",
      "Epoch [2/5], Step [3798/10336], Loss: 1.8737\n",
      "Epoch [2/5], Step [3800/10336], Loss: 2.4213\n",
      "Epoch [2/5], Step [3802/10336], Loss: 0.1526\n",
      "Epoch [2/5], Step [3804/10336], Loss: 1.5642\n",
      "Epoch [2/5], Step [3806/10336], Loss: 0.1638\n",
      "Epoch [2/5], Step [3808/10336], Loss: 0.1945\n",
      "Epoch [2/5], Step [3810/10336], Loss: 2.5849\n",
      "Epoch [2/5], Step [3812/10336], Loss: 1.4214\n",
      "Epoch [2/5], Step [3814/10336], Loss: 1.2090\n",
      "Epoch [2/5], Step [3816/10336], Loss: 1.6645\n",
      "Epoch [2/5], Step [3818/10336], Loss: 0.0351\n",
      "Epoch [2/5], Step [3820/10336], Loss: 1.1719\n",
      "Epoch [2/5], Step [3822/10336], Loss: 0.0669\n",
      "Epoch [2/5], Step [3824/10336], Loss: 0.0747\n",
      "Epoch [2/5], Step [3826/10336], Loss: 0.4416\n",
      "Epoch [2/5], Step [3828/10336], Loss: 1.4634\n",
      "Epoch [2/5], Step [3830/10336], Loss: 1.1563\n",
      "Epoch [2/5], Step [3832/10336], Loss: 2.5055\n",
      "Epoch [2/5], Step [3834/10336], Loss: 0.1984\n",
      "Epoch [2/5], Step [3836/10336], Loss: 0.3318\n",
      "Epoch [2/5], Step [3838/10336], Loss: 0.0131\n",
      "Epoch [2/5], Step [3840/10336], Loss: 1.7886\n",
      "Epoch [2/5], Step [3842/10336], Loss: 0.2598\n",
      "Epoch [2/5], Step [3844/10336], Loss: 0.9340\n",
      "Epoch [2/5], Step [3846/10336], Loss: 0.0976\n",
      "Epoch [2/5], Step [3848/10336], Loss: 7.6022\n",
      "Epoch [2/5], Step [3850/10336], Loss: 1.0077\n",
      "Epoch [2/5], Step [3852/10336], Loss: 0.1235\n",
      "Epoch [2/5], Step [3854/10336], Loss: 2.2268\n",
      "Epoch [2/5], Step [3856/10336], Loss: 0.6711\n",
      "Epoch [2/5], Step [3858/10336], Loss: 1.3311\n",
      "Epoch [2/5], Step [3860/10336], Loss: 3.9961\n",
      "Epoch [2/5], Step [3862/10336], Loss: 0.4237\n",
      "Epoch [2/5], Step [3864/10336], Loss: 0.1400\n",
      "Epoch [2/5], Step [3866/10336], Loss: 0.5220\n",
      "Epoch [2/5], Step [3868/10336], Loss: 0.1808\n",
      "Epoch [2/5], Step [3870/10336], Loss: 0.0632\n",
      "Epoch [2/5], Step [3872/10336], Loss: 0.5843\n",
      "Epoch [2/5], Step [3874/10336], Loss: 0.6661\n",
      "Epoch [2/5], Step [3876/10336], Loss: 0.1751\n",
      "Epoch [2/5], Step [3878/10336], Loss: 1.3766\n",
      "Epoch [2/5], Step [3880/10336], Loss: 1.1250\n",
      "Epoch [2/5], Step [3882/10336], Loss: 0.9504\n",
      "Epoch [2/5], Step [3884/10336], Loss: 2.5212\n",
      "Epoch [2/5], Step [3886/10336], Loss: 1.4514\n",
      "Epoch [2/5], Step [3888/10336], Loss: 1.1359\n",
      "Epoch [2/5], Step [3890/10336], Loss: 0.0707\n",
      "Epoch [2/5], Step [3892/10336], Loss: 2.8010\n",
      "Epoch [2/5], Step [3894/10336], Loss: 3.3830\n",
      "Epoch [2/5], Step [3896/10336], Loss: 0.2606\n",
      "Epoch [2/5], Step [3898/10336], Loss: 0.0350\n",
      "Epoch [2/5], Step [3900/10336], Loss: 0.0751\n",
      "Epoch [2/5], Step [3902/10336], Loss: 0.4980\n",
      "Epoch [2/5], Step [3904/10336], Loss: 0.1507\n",
      "Epoch [2/5], Step [3906/10336], Loss: 0.9486\n",
      "Epoch [2/5], Step [3908/10336], Loss: 2.2191\n",
      "Epoch [2/5], Step [3910/10336], Loss: 3.4291\n",
      "Epoch [2/5], Step [3912/10336], Loss: 0.3489\n",
      "Epoch [2/5], Step [3914/10336], Loss: 2.3402\n",
      "Epoch [2/5], Step [3916/10336], Loss: 0.1197\n",
      "Epoch [2/5], Step [3918/10336], Loss: 1.0779\n",
      "Epoch [2/5], Step [3920/10336], Loss: 2.1967\n",
      "Epoch [2/5], Step [3922/10336], Loss: 0.4495\n",
      "Epoch [2/5], Step [3924/10336], Loss: 1.2993\n",
      "Epoch [2/5], Step [3926/10336], Loss: 0.4169\n",
      "Epoch [2/5], Step [3928/10336], Loss: 0.4653\n",
      "Epoch [2/5], Step [3930/10336], Loss: 0.0795\n",
      "Epoch [2/5], Step [3932/10336], Loss: 1.0688\n",
      "Epoch [2/5], Step [3934/10336], Loss: 0.7166\n",
      "Epoch [2/5], Step [3936/10336], Loss: 1.0438\n",
      "Epoch [2/5], Step [3938/10336], Loss: 0.3439\n",
      "Epoch [2/5], Step [3940/10336], Loss: 1.0694\n",
      "Epoch [2/5], Step [3942/10336], Loss: 0.3040\n",
      "Epoch [2/5], Step [3944/10336], Loss: 2.0987\n",
      "Epoch [2/5], Step [3946/10336], Loss: 0.5851\n",
      "Epoch [2/5], Step [3948/10336], Loss: 0.0177\n",
      "Epoch [2/5], Step [3950/10336], Loss: 0.0716\n",
      "Epoch [2/5], Step [3952/10336], Loss: 4.3445\n",
      "Epoch [2/5], Step [3954/10336], Loss: 0.2894\n",
      "Epoch [2/5], Step [3956/10336], Loss: 0.1355\n",
      "Epoch [2/5], Step [3958/10336], Loss: 0.0324\n",
      "Epoch [2/5], Step [3960/10336], Loss: 1.2565\n",
      "Epoch [2/5], Step [3962/10336], Loss: 0.1142\n",
      "Epoch [2/5], Step [3964/10336], Loss: 0.0029\n",
      "Epoch [2/5], Step [3966/10336], Loss: 0.4500\n",
      "Epoch [2/5], Step [3968/10336], Loss: 0.0512\n",
      "Epoch [2/5], Step [3970/10336], Loss: 0.1627\n",
      "Epoch [2/5], Step [3972/10336], Loss: 0.0681\n",
      "Epoch [2/5], Step [3974/10336], Loss: 3.0914\n",
      "Epoch [2/5], Step [3976/10336], Loss: 1.1663\n",
      "Epoch [2/5], Step [3978/10336], Loss: 0.0047\n",
      "Epoch [2/5], Step [3980/10336], Loss: 0.5863\n",
      "Epoch [2/5], Step [3982/10336], Loss: 5.4928\n",
      "Epoch [2/5], Step [3984/10336], Loss: 0.5776\n",
      "Epoch [2/5], Step [3986/10336], Loss: 0.0905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [3988/10336], Loss: 0.1313\n",
      "Epoch [2/5], Step [3990/10336], Loss: 0.5120\n",
      "Epoch [2/5], Step [3992/10336], Loss: 0.0656\n",
      "Epoch [2/5], Step [3994/10336], Loss: 0.6272\n",
      "Epoch [2/5], Step [3996/10336], Loss: 0.4917\n",
      "Epoch [2/5], Step [3998/10336], Loss: 0.1098\n",
      "Epoch [2/5], Step [4000/10336], Loss: 0.0902\n",
      "Epoch [2/5], Step [4002/10336], Loss: 1.0669\n",
      "Epoch [2/5], Step [4004/10336], Loss: 0.2633\n",
      "Epoch [2/5], Step [4006/10336], Loss: 0.1189\n",
      "Epoch [2/5], Step [4008/10336], Loss: 0.2040\n",
      "Epoch [2/5], Step [4010/10336], Loss: 0.4876\n",
      "Epoch [2/5], Step [4012/10336], Loss: 0.0256\n",
      "Epoch [2/5], Step [4014/10336], Loss: 0.2605\n",
      "Epoch [2/5], Step [4016/10336], Loss: 1.2516\n",
      "Epoch [2/5], Step [4018/10336], Loss: 1.7408\n",
      "Epoch [2/5], Step [4020/10336], Loss: 1.2687\n",
      "Epoch [2/5], Step [4022/10336], Loss: 0.7205\n",
      "Epoch [2/5], Step [4024/10336], Loss: 2.4355\n",
      "Epoch [2/5], Step [4026/10336], Loss: 0.3385\n",
      "Epoch [2/5], Step [4028/10336], Loss: 0.2997\n",
      "Epoch [2/5], Step [4030/10336], Loss: 0.5251\n",
      "Epoch [2/5], Step [4032/10336], Loss: 0.3616\n",
      "Epoch [2/5], Step [4034/10336], Loss: 0.1163\n",
      "Epoch [2/5], Step [4036/10336], Loss: 5.7366\n",
      "Epoch [2/5], Step [4038/10336], Loss: 0.4264\n",
      "Epoch [2/5], Step [4040/10336], Loss: 1.1016\n",
      "Epoch [2/5], Step [4042/10336], Loss: 0.3712\n",
      "Epoch [2/5], Step [4044/10336], Loss: 0.8448\n",
      "Epoch [2/5], Step [4046/10336], Loss: 0.0107\n",
      "Epoch [2/5], Step [4048/10336], Loss: 0.2337\n",
      "Epoch [2/5], Step [4050/10336], Loss: 0.7699\n",
      "Epoch [2/5], Step [4052/10336], Loss: 1.2726\n",
      "Epoch [2/5], Step [4054/10336], Loss: 0.0671\n",
      "Epoch [2/5], Step [4056/10336], Loss: 1.5041\n",
      "Epoch [2/5], Step [4058/10336], Loss: 2.1355\n",
      "Epoch [2/5], Step [4060/10336], Loss: 2.4281\n",
      "Epoch [2/5], Step [4062/10336], Loss: 0.0292\n",
      "Epoch [2/5], Step [4064/10336], Loss: 1.1738\n",
      "Epoch [2/5], Step [4066/10336], Loss: 2.7930\n",
      "Epoch [2/5], Step [4068/10336], Loss: 0.3045\n",
      "Epoch [2/5], Step [4070/10336], Loss: 0.0347\n",
      "Epoch [2/5], Step [4072/10336], Loss: 0.6505\n",
      "Epoch [2/5], Step [4074/10336], Loss: 0.4292\n",
      "Epoch [2/5], Step [4076/10336], Loss: 0.3168\n",
      "Epoch [2/5], Step [4078/10336], Loss: 0.6482\n",
      "Epoch [2/5], Step [4080/10336], Loss: 0.0082\n",
      "Epoch [2/5], Step [4082/10336], Loss: 2.0162\n",
      "Epoch [2/5], Step [4084/10336], Loss: 0.9025\n",
      "Epoch [2/5], Step [4086/10336], Loss: 0.4368\n",
      "Epoch [2/5], Step [4088/10336], Loss: 1.0005\n",
      "Epoch [2/5], Step [4090/10336], Loss: 0.0477\n",
      "Epoch [2/5], Step [4092/10336], Loss: 0.8512\n",
      "Epoch [2/5], Step [4094/10336], Loss: 1.0716\n",
      "Epoch [2/5], Step [4096/10336], Loss: 1.1326\n",
      "Epoch [2/5], Step [4098/10336], Loss: 3.5228\n",
      "Epoch [2/5], Step [4100/10336], Loss: 0.1445\n",
      "Epoch [2/5], Step [4102/10336], Loss: 0.9784\n",
      "Epoch [2/5], Step [4104/10336], Loss: 0.1026\n",
      "Epoch [2/5], Step [4106/10336], Loss: 4.8815\n",
      "Epoch [2/5], Step [4108/10336], Loss: 0.1757\n",
      "Epoch [2/5], Step [4110/10336], Loss: 0.2003\n",
      "Epoch [2/5], Step [4112/10336], Loss: 1.7282\n",
      "Epoch [2/5], Step [4114/10336], Loss: 2.4574\n",
      "Epoch [2/5], Step [4116/10336], Loss: 0.0422\n",
      "Epoch [2/5], Step [4118/10336], Loss: 0.9192\n",
      "Epoch [2/5], Step [4120/10336], Loss: 0.8656\n",
      "Epoch [2/5], Step [4122/10336], Loss: 0.1590\n",
      "Epoch [2/5], Step [4124/10336], Loss: 1.7140\n",
      "Epoch [2/5], Step [4126/10336], Loss: 1.6698\n",
      "Epoch [2/5], Step [4128/10336], Loss: 0.1598\n",
      "Epoch [2/5], Step [4130/10336], Loss: 0.2521\n",
      "Epoch [2/5], Step [4132/10336], Loss: 1.3078\n",
      "Epoch [2/5], Step [4134/10336], Loss: 0.1609\n",
      "Epoch [2/5], Step [4136/10336], Loss: 1.3668\n",
      "Epoch [2/5], Step [4138/10336], Loss: 0.0113\n",
      "Epoch [2/5], Step [4140/10336], Loss: 1.4706\n",
      "Epoch [2/5], Step [4142/10336], Loss: 1.2508\n",
      "Epoch [2/5], Step [4144/10336], Loss: 4.0544\n",
      "Epoch [2/5], Step [4146/10336], Loss: 0.7247\n",
      "Epoch [2/5], Step [4148/10336], Loss: 1.3896\n",
      "Epoch [2/5], Step [4150/10336], Loss: 0.0462\n",
      "Epoch [2/5], Step [4152/10336], Loss: 0.0883\n",
      "Epoch [2/5], Step [4154/10336], Loss: 3.3472\n",
      "Epoch [2/5], Step [4156/10336], Loss: 1.4572\n",
      "Epoch [2/5], Step [4158/10336], Loss: 1.0621\n",
      "Epoch [2/5], Step [4160/10336], Loss: 0.3652\n",
      "Epoch [2/5], Step [4162/10336], Loss: 0.2437\n",
      "Epoch [2/5], Step [4164/10336], Loss: 0.6003\n",
      "Epoch [2/5], Step [4166/10336], Loss: 0.8033\n",
      "Epoch [2/5], Step [4168/10336], Loss: 0.6108\n",
      "Epoch [2/5], Step [4170/10336], Loss: 0.8128\n",
      "Epoch [2/5], Step [4172/10336], Loss: 2.5680\n",
      "Epoch [2/5], Step [4174/10336], Loss: 0.3116\n",
      "Epoch [2/5], Step [4176/10336], Loss: 1.0661\n",
      "Epoch [2/5], Step [4178/10336], Loss: 4.9245\n",
      "Epoch [2/5], Step [4180/10336], Loss: 0.8492\n",
      "Epoch [2/5], Step [4182/10336], Loss: 0.7817\n",
      "Epoch [2/5], Step [4184/10336], Loss: 0.1376\n",
      "Epoch [2/5], Step [4186/10336], Loss: 0.0764\n",
      "Epoch [2/5], Step [4188/10336], Loss: 2.3638\n",
      "Epoch [2/5], Step [4190/10336], Loss: 1.6852\n",
      "Epoch [2/5], Step [4192/10336], Loss: 0.0555\n",
      "Epoch [2/5], Step [4194/10336], Loss: 0.0209\n",
      "Epoch [2/5], Step [4196/10336], Loss: 2.1949\n",
      "Epoch [2/5], Step [4198/10336], Loss: 1.2624\n",
      "Epoch [2/5], Step [4200/10336], Loss: 2.0289\n",
      "Epoch [2/5], Step [4202/10336], Loss: 0.3353\n",
      "Epoch [2/5], Step [4204/10336], Loss: 1.7510\n",
      "Epoch [2/5], Step [4206/10336], Loss: 1.1374\n",
      "Epoch [2/5], Step [4208/10336], Loss: 1.6615\n",
      "Epoch [2/5], Step [4210/10336], Loss: 0.1908\n",
      "Epoch [2/5], Step [4212/10336], Loss: 2.3230\n",
      "Epoch [2/5], Step [4214/10336], Loss: 0.0193\n",
      "Epoch [2/5], Step [4216/10336], Loss: 0.9200\n",
      "Epoch [2/5], Step [4218/10336], Loss: 0.1772\n",
      "Epoch [2/5], Step [4220/10336], Loss: 2.3062\n",
      "Epoch [2/5], Step [4222/10336], Loss: 0.6652\n",
      "Epoch [2/5], Step [4224/10336], Loss: 0.8583\n",
      "Epoch [2/5], Step [4226/10336], Loss: 0.2710\n",
      "Epoch [2/5], Step [4228/10336], Loss: 0.1139\n",
      "Epoch [2/5], Step [4230/10336], Loss: 1.6228\n",
      "Epoch [2/5], Step [4232/10336], Loss: 0.0197\n",
      "Epoch [2/5], Step [4234/10336], Loss: 1.2602\n",
      "Epoch [2/5], Step [4236/10336], Loss: 0.5550\n",
      "Epoch [2/5], Step [4238/10336], Loss: 0.9395\n",
      "Epoch [2/5], Step [4240/10336], Loss: 1.5169\n",
      "Epoch [2/5], Step [4242/10336], Loss: 1.9316\n",
      "Epoch [2/5], Step [4244/10336], Loss: 0.1005\n",
      "Epoch [2/5], Step [4246/10336], Loss: 0.0246\n",
      "Epoch [2/5], Step [4248/10336], Loss: 0.6515\n",
      "Epoch [2/5], Step [4250/10336], Loss: 1.0002\n",
      "Epoch [2/5], Step [4252/10336], Loss: 0.9228\n",
      "Epoch [2/5], Step [4254/10336], Loss: 2.8807\n",
      "Epoch [2/5], Step [4256/10336], Loss: 0.9556\n",
      "Epoch [2/5], Step [4258/10336], Loss: 0.7095\n",
      "Epoch [2/5], Step [4260/10336], Loss: 0.0030\n",
      "Epoch [2/5], Step [4262/10336], Loss: 0.0421\n",
      "Epoch [2/5], Step [4264/10336], Loss: 0.0653\n",
      "Epoch [2/5], Step [4266/10336], Loss: 0.1122\n",
      "Epoch [2/5], Step [4268/10336], Loss: 1.6736\n",
      "Epoch [2/5], Step [4270/10336], Loss: 1.4070\n",
      "Epoch [2/5], Step [4272/10336], Loss: 0.0403\n",
      "Epoch [2/5], Step [4274/10336], Loss: 0.0274\n",
      "Epoch [2/5], Step [4276/10336], Loss: 0.0732\n",
      "Epoch [2/5], Step [4278/10336], Loss: 1.1244\n",
      "Epoch [2/5], Step [4280/10336], Loss: 0.5344\n",
      "Epoch [2/5], Step [4282/10336], Loss: 0.0411\n",
      "Epoch [2/5], Step [4284/10336], Loss: 2.0888\n",
      "Epoch [2/5], Step [4286/10336], Loss: 0.9711\n",
      "Epoch [2/5], Step [4288/10336], Loss: 2.5047\n",
      "Epoch [2/5], Step [4290/10336], Loss: 0.0677\n",
      "Epoch [2/5], Step [4292/10336], Loss: 0.6006\n",
      "Epoch [2/5], Step [4294/10336], Loss: 0.5246\n",
      "Epoch [2/5], Step [4296/10336], Loss: 0.3774\n",
      "Epoch [2/5], Step [4298/10336], Loss: 0.4319\n",
      "Epoch [2/5], Step [4300/10336], Loss: 0.4525\n",
      "Epoch [2/5], Step [4302/10336], Loss: 0.8118\n",
      "Epoch [2/5], Step [4304/10336], Loss: 0.0142\n",
      "Epoch [2/5], Step [4306/10336], Loss: 1.3992\n",
      "Epoch [2/5], Step [4308/10336], Loss: 2.0761\n",
      "Epoch [2/5], Step [4310/10336], Loss: 0.0102\n",
      "Epoch [2/5], Step [4312/10336], Loss: 0.4445\n",
      "Epoch [2/5], Step [4314/10336], Loss: 0.2926\n",
      "Epoch [2/5], Step [4316/10336], Loss: 1.5607\n",
      "Epoch [2/5], Step [4318/10336], Loss: 0.9192\n",
      "Epoch [2/5], Step [4320/10336], Loss: 0.3036\n",
      "Epoch [2/5], Step [4322/10336], Loss: 0.1558\n",
      "Epoch [2/5], Step [4324/10336], Loss: 1.9769\n",
      "Epoch [2/5], Step [4326/10336], Loss: 0.9661\n",
      "Epoch [2/5], Step [4328/10336], Loss: 3.2345\n",
      "Epoch [2/5], Step [4330/10336], Loss: 0.4595\n",
      "Epoch [2/5], Step [4332/10336], Loss: 1.6467\n",
      "Epoch [2/5], Step [4334/10336], Loss: 0.0535\n",
      "Epoch [2/5], Step [4336/10336], Loss: 2.7664\n",
      "Epoch [2/5], Step [4338/10336], Loss: 0.0386\n",
      "Epoch [2/5], Step [4340/10336], Loss: 0.3956\n",
      "Epoch [2/5], Step [4342/10336], Loss: 0.6987\n",
      "Epoch [2/5], Step [4344/10336], Loss: 0.6776\n",
      "Epoch [2/5], Step [4346/10336], Loss: 0.0411\n",
      "Epoch [2/5], Step [4348/10336], Loss: 0.2086\n",
      "Epoch [2/5], Step [4350/10336], Loss: 0.7142\n",
      "Epoch [2/5], Step [4352/10336], Loss: 1.8158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [4354/10336], Loss: 0.7779\n",
      "Epoch [2/5], Step [4356/10336], Loss: 0.2744\n",
      "Epoch [2/5], Step [4358/10336], Loss: 0.7614\n",
      "Epoch [2/5], Step [4360/10336], Loss: 0.0788\n",
      "Epoch [2/5], Step [4362/10336], Loss: 1.3590\n",
      "Epoch [2/5], Step [4364/10336], Loss: 0.5205\n",
      "Epoch [2/5], Step [4366/10336], Loss: 0.5855\n",
      "Epoch [2/5], Step [4368/10336], Loss: 0.2970\n",
      "Epoch [2/5], Step [4370/10336], Loss: 0.3226\n",
      "Epoch [2/5], Step [4372/10336], Loss: 0.0055\n",
      "Epoch [2/5], Step [4374/10336], Loss: 0.4674\n",
      "Epoch [2/5], Step [4376/10336], Loss: 1.4927\n",
      "Epoch [2/5], Step [4378/10336], Loss: 0.1520\n",
      "Epoch [2/5], Step [4380/10336], Loss: 0.7353\n",
      "Epoch [2/5], Step [4382/10336], Loss: 1.3242\n",
      "Epoch [2/5], Step [4384/10336], Loss: 0.1963\n",
      "Epoch [2/5], Step [4386/10336], Loss: 1.0211\n",
      "Epoch [2/5], Step [4388/10336], Loss: 1.2908\n",
      "Epoch [2/5], Step [4390/10336], Loss: 1.5503\n",
      "Epoch [2/5], Step [4392/10336], Loss: 2.1299\n",
      "Epoch [2/5], Step [4394/10336], Loss: 0.4723\n",
      "Epoch [2/5], Step [4396/10336], Loss: 2.0220\n",
      "Epoch [2/5], Step [4398/10336], Loss: 0.2832\n",
      "Epoch [2/5], Step [4400/10336], Loss: 0.0875\n",
      "Epoch [2/5], Step [4402/10336], Loss: 0.7096\n",
      "Epoch [2/5], Step [4404/10336], Loss: 2.1866\n",
      "Epoch [2/5], Step [4406/10336], Loss: 0.1988\n",
      "Epoch [2/5], Step [4408/10336], Loss: 0.6958\n",
      "Epoch [2/5], Step [4410/10336], Loss: 1.8428\n",
      "Epoch [2/5], Step [4412/10336], Loss: 1.5757\n",
      "Epoch [2/5], Step [4414/10336], Loss: 0.0524\n",
      "Epoch [2/5], Step [4416/10336], Loss: 0.5344\n",
      "Epoch [2/5], Step [4418/10336], Loss: 1.8074\n",
      "Epoch [2/5], Step [4420/10336], Loss: 0.9323\n",
      "Epoch [2/5], Step [4422/10336], Loss: 0.9832\n",
      "Epoch [2/5], Step [4424/10336], Loss: 0.0616\n",
      "Epoch [2/5], Step [4426/10336], Loss: 0.1678\n",
      "Epoch [2/5], Step [4428/10336], Loss: 2.2191\n",
      "Epoch [2/5], Step [4430/10336], Loss: 0.0194\n",
      "Epoch [2/5], Step [4432/10336], Loss: 0.7343\n",
      "Epoch [2/5], Step [4434/10336], Loss: 0.4011\n",
      "Epoch [2/5], Step [4436/10336], Loss: 3.3582\n",
      "Epoch [2/5], Step [4438/10336], Loss: 0.0209\n",
      "Epoch [2/5], Step [4440/10336], Loss: 0.3913\n",
      "Epoch [2/5], Step [4442/10336], Loss: 0.8764\n",
      "Epoch [2/5], Step [4444/10336], Loss: 1.4728\n",
      "Epoch [2/5], Step [4446/10336], Loss: 0.2218\n",
      "Epoch [2/5], Step [4448/10336], Loss: 0.5412\n",
      "Epoch [2/5], Step [4450/10336], Loss: 5.1670\n",
      "Epoch [2/5], Step [4452/10336], Loss: 0.1343\n",
      "Epoch [2/5], Step [4454/10336], Loss: 0.0882\n",
      "Epoch [2/5], Step [4456/10336], Loss: 0.0103\n",
      "Epoch [2/5], Step [4458/10336], Loss: 0.4295\n",
      "Epoch [2/5], Step [4460/10336], Loss: 2.6173\n",
      "Epoch [2/5], Step [4462/10336], Loss: 0.7208\n",
      "Epoch [2/5], Step [4464/10336], Loss: 0.4265\n",
      "Epoch [2/5], Step [4466/10336], Loss: 2.6508\n",
      "Epoch [2/5], Step [4468/10336], Loss: 0.1981\n",
      "Epoch [2/5], Step [4470/10336], Loss: 1.1306\n",
      "Epoch [2/5], Step [4472/10336], Loss: 1.1156\n",
      "Epoch [2/5], Step [4474/10336], Loss: 0.5028\n",
      "Epoch [2/5], Step [4476/10336], Loss: 0.0200\n",
      "Epoch [2/5], Step [4478/10336], Loss: 1.8695\n",
      "Epoch [2/5], Step [4480/10336], Loss: 0.4903\n",
      "Epoch [2/5], Step [4482/10336], Loss: 0.7136\n",
      "Epoch [2/5], Step [4484/10336], Loss: 2.7756\n",
      "Epoch [2/5], Step [4486/10336], Loss: 1.7043\n",
      "Epoch [2/5], Step [4488/10336], Loss: 1.2697\n",
      "Epoch [2/5], Step [4490/10336], Loss: 1.1840\n",
      "Epoch [2/5], Step [4492/10336], Loss: 1.4884\n",
      "Epoch [2/5], Step [4494/10336], Loss: 1.5889\n",
      "Epoch [2/5], Step [4496/10336], Loss: 0.9929\n",
      "Epoch [2/5], Step [4498/10336], Loss: 0.3998\n",
      "Epoch [2/5], Step [4500/10336], Loss: 0.0091\n",
      "Epoch [2/5], Step [4502/10336], Loss: 2.6611\n",
      "Epoch [2/5], Step [4504/10336], Loss: 0.1287\n",
      "Epoch [2/5], Step [4506/10336], Loss: 0.7997\n",
      "Epoch [2/5], Step [4508/10336], Loss: 0.2343\n",
      "Epoch [2/5], Step [4510/10336], Loss: 1.0416\n",
      "Epoch [2/5], Step [4512/10336], Loss: 0.2890\n",
      "Epoch [2/5], Step [4514/10336], Loss: 2.2341\n",
      "Epoch [2/5], Step [4516/10336], Loss: 2.0803\n",
      "Epoch [2/5], Step [4518/10336], Loss: 0.7344\n",
      "Epoch [2/5], Step [4520/10336], Loss: 0.7107\n",
      "Epoch [2/5], Step [4522/10336], Loss: 0.3175\n",
      "Epoch [2/5], Step [4524/10336], Loss: 1.6313\n",
      "Epoch [2/5], Step [4526/10336], Loss: 0.5170\n",
      "Epoch [2/5], Step [4528/10336], Loss: 4.6723\n",
      "Epoch [2/5], Step [4530/10336], Loss: 0.7009\n",
      "Epoch [2/5], Step [4532/10336], Loss: 0.5787\n",
      "Epoch [2/5], Step [4534/10336], Loss: 0.3697\n",
      "Epoch [2/5], Step [4536/10336], Loss: 0.1273\n",
      "Epoch [2/5], Step [4538/10336], Loss: 2.0306\n",
      "Epoch [2/5], Step [4540/10336], Loss: 0.5259\n",
      "Epoch [2/5], Step [4542/10336], Loss: 0.0470\n",
      "Epoch [2/5], Step [4544/10336], Loss: 4.1757\n",
      "Epoch [2/5], Step [4546/10336], Loss: 2.3873\n",
      "Epoch [2/5], Step [4548/10336], Loss: 0.0363\n",
      "Epoch [2/5], Step [4550/10336], Loss: 0.8931\n",
      "Epoch [2/5], Step [4552/10336], Loss: 1.0045\n",
      "Epoch [2/5], Step [4554/10336], Loss: 1.3866\n",
      "Epoch [2/5], Step [4556/10336], Loss: 5.1679\n",
      "Epoch [2/5], Step [4558/10336], Loss: 1.2468\n",
      "Epoch [2/5], Step [4560/10336], Loss: 0.2827\n",
      "Epoch [2/5], Step [4562/10336], Loss: 1.7357\n",
      "Epoch [2/5], Step [4564/10336], Loss: 2.8654\n",
      "Epoch [2/5], Step [4566/10336], Loss: 0.9454\n",
      "Epoch [2/5], Step [4568/10336], Loss: 1.1401\n",
      "Epoch [2/5], Step [4570/10336], Loss: 1.7613\n",
      "Epoch [2/5], Step [4572/10336], Loss: 1.5814\n",
      "Epoch [2/5], Step [4574/10336], Loss: 1.0644\n",
      "Epoch [2/5], Step [4576/10336], Loss: 0.1698\n",
      "Epoch [2/5], Step [4578/10336], Loss: 2.4930\n",
      "Epoch [2/5], Step [4580/10336], Loss: 0.0965\n",
      "Epoch [2/5], Step [4582/10336], Loss: 1.0687\n",
      "Epoch [2/5], Step [4584/10336], Loss: 1.1186\n",
      "Epoch [2/5], Step [4586/10336], Loss: 1.0951\n",
      "Epoch [2/5], Step [4588/10336], Loss: 2.3740\n",
      "Epoch [2/5], Step [4590/10336], Loss: 1.2429\n",
      "Epoch [2/5], Step [4592/10336], Loss: 0.1736\n",
      "Epoch [2/5], Step [4594/10336], Loss: 0.0170\n",
      "Epoch [2/5], Step [4596/10336], Loss: 1.7448\n",
      "Epoch [2/5], Step [4598/10336], Loss: 1.6609\n",
      "Epoch [2/5], Step [4600/10336], Loss: 0.6028\n",
      "Epoch [2/5], Step [4602/10336], Loss: 0.7178\n",
      "Epoch [2/5], Step [4604/10336], Loss: 0.8521\n",
      "Epoch [2/5], Step [4606/10336], Loss: 0.0008\n",
      "Epoch [2/5], Step [4608/10336], Loss: 0.7283\n",
      "Epoch [2/5], Step [4610/10336], Loss: 0.1248\n",
      "Epoch [2/5], Step [4612/10336], Loss: 0.1286\n",
      "Epoch [2/5], Step [4614/10336], Loss: 2.1355\n",
      "Epoch [2/5], Step [4616/10336], Loss: 4.1899\n",
      "Epoch [2/5], Step [4618/10336], Loss: 1.5999\n",
      "Epoch [2/5], Step [4620/10336], Loss: 0.6972\n",
      "Epoch [2/5], Step [4622/10336], Loss: 0.1263\n",
      "Epoch [2/5], Step [4624/10336], Loss: 1.0126\n",
      "Epoch [2/5], Step [4626/10336], Loss: 0.3988\n",
      "Epoch [2/5], Step [4628/10336], Loss: 0.5152\n",
      "Epoch [2/5], Step [4630/10336], Loss: 2.4928\n",
      "Epoch [2/5], Step [4632/10336], Loss: 1.0173\n",
      "Epoch [2/5], Step [4634/10336], Loss: 3.3600\n",
      "Epoch [2/5], Step [4636/10336], Loss: 0.4900\n",
      "Epoch [2/5], Step [4638/10336], Loss: 0.0895\n",
      "Epoch [2/5], Step [4640/10336], Loss: 0.6439\n",
      "Epoch [2/5], Step [4642/10336], Loss: 0.0152\n",
      "Epoch [2/5], Step [4644/10336], Loss: 0.5396\n",
      "Epoch [2/5], Step [4646/10336], Loss: 0.1371\n",
      "Epoch [2/5], Step [4648/10336], Loss: 0.5459\n",
      "Epoch [2/5], Step [4650/10336], Loss: 0.0402\n",
      "Epoch [2/5], Step [4652/10336], Loss: 1.6377\n",
      "Epoch [2/5], Step [4654/10336], Loss: 0.0102\n",
      "Epoch [2/5], Step [4656/10336], Loss: 0.6851\n",
      "Epoch [2/5], Step [4658/10336], Loss: 3.2729\n",
      "Epoch [2/5], Step [4660/10336], Loss: 0.0558\n",
      "Epoch [2/5], Step [4662/10336], Loss: 2.7777\n",
      "Epoch [2/5], Step [4664/10336], Loss: 0.9952\n",
      "Epoch [2/5], Step [4666/10336], Loss: 0.4651\n",
      "Epoch [2/5], Step [4668/10336], Loss: 0.5180\n",
      "Epoch [2/5], Step [4670/10336], Loss: 0.0221\n",
      "Epoch [2/5], Step [4672/10336], Loss: 0.2154\n",
      "Epoch [2/5], Step [4674/10336], Loss: 0.4838\n",
      "Epoch [2/5], Step [4676/10336], Loss: 0.1412\n",
      "Epoch [2/5], Step [4678/10336], Loss: 0.7360\n",
      "Epoch [2/5], Step [4680/10336], Loss: 0.6017\n",
      "Epoch [2/5], Step [4682/10336], Loss: 0.1770\n",
      "Epoch [2/5], Step [4684/10336], Loss: 0.0073\n",
      "Epoch [2/5], Step [4686/10336], Loss: 0.1722\n",
      "Epoch [2/5], Step [4688/10336], Loss: 0.2091\n",
      "Epoch [2/5], Step [4690/10336], Loss: 0.7491\n",
      "Epoch [2/5], Step [4692/10336], Loss: 1.1030\n",
      "Epoch [2/5], Step [4694/10336], Loss: 1.7298\n",
      "Epoch [2/5], Step [4696/10336], Loss: 0.4962\n",
      "Epoch [2/5], Step [4698/10336], Loss: 0.0438\n",
      "Epoch [2/5], Step [4700/10336], Loss: 1.2278\n",
      "Epoch [2/5], Step [4702/10336], Loss: 5.8012\n",
      "Epoch [2/5], Step [4704/10336], Loss: 0.0034\n",
      "Epoch [2/5], Step [4706/10336], Loss: 0.0582\n",
      "Epoch [2/5], Step [4708/10336], Loss: 0.4157\n",
      "Epoch [2/5], Step [4710/10336], Loss: 1.4667\n",
      "Epoch [2/5], Step [4712/10336], Loss: 1.0793\n",
      "Epoch [2/5], Step [4714/10336], Loss: 3.6726\n",
      "Epoch [2/5], Step [4716/10336], Loss: 0.2007\n",
      "Epoch [2/5], Step [4718/10336], Loss: 0.0113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [4720/10336], Loss: 0.7596\n",
      "Epoch [2/5], Step [4722/10336], Loss: 0.5812\n",
      "Epoch [2/5], Step [4724/10336], Loss: 1.6862\n",
      "Epoch [2/5], Step [4726/10336], Loss: 1.4824\n",
      "Epoch [2/5], Step [4728/10336], Loss: 1.8722\n",
      "Epoch [2/5], Step [4730/10336], Loss: 0.3665\n",
      "Epoch [2/5], Step [4732/10336], Loss: 1.1164\n",
      "Epoch [2/5], Step [4734/10336], Loss: 0.1988\n",
      "Epoch [2/5], Step [4736/10336], Loss: 4.8377\n",
      "Epoch [2/5], Step [4738/10336], Loss: 0.1466\n",
      "Epoch [2/5], Step [4740/10336], Loss: 0.1875\n",
      "Epoch [2/5], Step [4742/10336], Loss: 0.7588\n",
      "Epoch [2/5], Step [4744/10336], Loss: 0.2914\n",
      "Epoch [2/5], Step [4746/10336], Loss: 2.1353\n",
      "Epoch [2/5], Step [4748/10336], Loss: 0.7082\n",
      "Epoch [2/5], Step [4750/10336], Loss: 1.0989\n",
      "Epoch [2/5], Step [4752/10336], Loss: 0.0064\n",
      "Epoch [2/5], Step [4754/10336], Loss: 0.1412\n",
      "Epoch [2/5], Step [4756/10336], Loss: 1.4446\n",
      "Epoch [2/5], Step [4758/10336], Loss: 0.0991\n",
      "Epoch [2/5], Step [4760/10336], Loss: 2.4611\n",
      "Epoch [2/5], Step [4762/10336], Loss: 0.0579\n",
      "Epoch [2/5], Step [4764/10336], Loss: 0.4938\n",
      "Epoch [2/5], Step [4766/10336], Loss: 0.3140\n",
      "Epoch [2/5], Step [4768/10336], Loss: 2.2824\n",
      "Epoch [2/5], Step [4770/10336], Loss: 0.8542\n",
      "Epoch [2/5], Step [4772/10336], Loss: 1.2972\n",
      "Epoch [2/5], Step [4774/10336], Loss: 0.0301\n",
      "Epoch [2/5], Step [4776/10336], Loss: 0.1689\n",
      "Epoch [2/5], Step [4778/10336], Loss: 1.2919\n",
      "Epoch [2/5], Step [4780/10336], Loss: 1.1934\n",
      "Epoch [2/5], Step [4782/10336], Loss: 1.5689\n",
      "Epoch [2/5], Step [4784/10336], Loss: 1.2868\n",
      "Epoch [2/5], Step [4786/10336], Loss: 0.3034\n",
      "Epoch [2/5], Step [4788/10336], Loss: 0.1950\n",
      "Epoch [2/5], Step [4790/10336], Loss: 2.8737\n",
      "Epoch [2/5], Step [4792/10336], Loss: 1.7403\n",
      "Epoch [2/5], Step [4794/10336], Loss: 1.5383\n",
      "Epoch [2/5], Step [4796/10336], Loss: 0.1168\n",
      "Epoch [2/5], Step [4798/10336], Loss: 3.7159\n",
      "Epoch [2/5], Step [4800/10336], Loss: 0.1936\n",
      "Epoch [2/5], Step [4802/10336], Loss: 0.1427\n",
      "Epoch [2/5], Step [4804/10336], Loss: 1.6180\n",
      "Epoch [2/5], Step [4806/10336], Loss: 0.6496\n",
      "Epoch [2/5], Step [4808/10336], Loss: 0.0251\n",
      "Epoch [2/5], Step [4810/10336], Loss: 1.0097\n",
      "Epoch [2/5], Step [4812/10336], Loss: 0.0066\n",
      "Epoch [2/5], Step [4814/10336], Loss: 0.2095\n",
      "Epoch [2/5], Step [4816/10336], Loss: 0.0114\n",
      "Epoch [2/5], Step [4818/10336], Loss: 3.1073\n",
      "Epoch [2/5], Step [4820/10336], Loss: 0.1762\n",
      "Epoch [2/5], Step [4822/10336], Loss: 0.1465\n",
      "Epoch [2/5], Step [4824/10336], Loss: 2.7645\n",
      "Epoch [2/5], Step [4826/10336], Loss: 0.8676\n",
      "Epoch [2/5], Step [4828/10336], Loss: 0.0726\n",
      "Epoch [2/5], Step [4830/10336], Loss: 0.1006\n",
      "Epoch [2/5], Step [4832/10336], Loss: 0.2071\n",
      "Epoch [2/5], Step [4834/10336], Loss: 0.3831\n",
      "Epoch [2/5], Step [4836/10336], Loss: 1.8357\n",
      "Epoch [2/5], Step [4838/10336], Loss: 1.0779\n",
      "Epoch [2/5], Step [4840/10336], Loss: 0.0746\n",
      "Epoch [2/5], Step [4842/10336], Loss: 0.0891\n",
      "Epoch [2/5], Step [4844/10336], Loss: 1.9347\n",
      "Epoch [2/5], Step [4846/10336], Loss: 0.8950\n",
      "Epoch [2/5], Step [4848/10336], Loss: 0.0194\n",
      "Epoch [2/5], Step [4850/10336], Loss: 0.4623\n",
      "Epoch [2/5], Step [4852/10336], Loss: 0.0088\n",
      "Epoch [2/5], Step [4854/10336], Loss: 0.1941\n",
      "Epoch [2/5], Step [4856/10336], Loss: 0.1766\n",
      "Epoch [2/5], Step [4858/10336], Loss: 0.3919\n",
      "Epoch [2/5], Step [4860/10336], Loss: 0.0541\n",
      "Epoch [2/5], Step [4862/10336], Loss: 0.4110\n",
      "Epoch [2/5], Step [4864/10336], Loss: 1.0319\n",
      "Epoch [2/5], Step [4866/10336], Loss: 0.0324\n",
      "Epoch [2/5], Step [4868/10336], Loss: 0.8168\n",
      "Epoch [2/5], Step [4870/10336], Loss: 0.0724\n",
      "Epoch [2/5], Step [4872/10336], Loss: 0.0606\n",
      "Epoch [2/5], Step [4874/10336], Loss: 0.6058\n",
      "Epoch [2/5], Step [4876/10336], Loss: 0.0435\n",
      "Epoch [2/5], Step [4878/10336], Loss: 0.0951\n",
      "Epoch [2/5], Step [4880/10336], Loss: 0.4558\n",
      "Epoch [2/5], Step [4882/10336], Loss: 4.2020\n",
      "Epoch [2/5], Step [4884/10336], Loss: 0.0139\n",
      "Epoch [2/5], Step [4886/10336], Loss: 1.7906\n",
      "Epoch [2/5], Step [4888/10336], Loss: 0.0532\n",
      "Epoch [2/5], Step [4890/10336], Loss: 0.0508\n",
      "Epoch [2/5], Step [4892/10336], Loss: 1.3815\n",
      "Epoch [2/5], Step [4894/10336], Loss: 0.7917\n",
      "Epoch [2/5], Step [4896/10336], Loss: 0.0557\n",
      "Epoch [2/5], Step [4898/10336], Loss: 0.1051\n",
      "Epoch [2/5], Step [4900/10336], Loss: 0.6421\n",
      "Epoch [2/5], Step [4902/10336], Loss: 0.7159\n",
      "Epoch [2/5], Step [4904/10336], Loss: 0.5645\n",
      "Epoch [2/5], Step [4906/10336], Loss: 0.2338\n",
      "Epoch [2/5], Step [4908/10336], Loss: 0.2167\n",
      "Epoch [2/5], Step [4910/10336], Loss: 1.8165\n",
      "Epoch [2/5], Step [4912/10336], Loss: 3.1194\n",
      "Epoch [2/5], Step [4914/10336], Loss: 0.8132\n",
      "Epoch [2/5], Step [4916/10336], Loss: 1.3083\n",
      "Epoch [2/5], Step [4918/10336], Loss: 0.1233\n",
      "Epoch [2/5], Step [4920/10336], Loss: 1.3913\n",
      "Epoch [2/5], Step [4922/10336], Loss: 1.3759\n",
      "Epoch [2/5], Step [4924/10336], Loss: 0.6671\n",
      "Epoch [2/5], Step [4926/10336], Loss: 0.1578\n",
      "Epoch [2/5], Step [4928/10336], Loss: 1.0743\n",
      "Epoch [2/5], Step [4930/10336], Loss: 0.2048\n",
      "Epoch [2/5], Step [4932/10336], Loss: 0.7035\n",
      "Epoch [2/5], Step [4934/10336], Loss: 0.3663\n",
      "Epoch [2/5], Step [4936/10336], Loss: 1.0058\n",
      "Epoch [2/5], Step [4938/10336], Loss: 0.4417\n",
      "Epoch [2/5], Step [4940/10336], Loss: 1.3345\n",
      "Epoch [2/5], Step [4942/10336], Loss: 0.6482\n",
      "Epoch [2/5], Step [4944/10336], Loss: 4.9190\n",
      "Epoch [2/5], Step [4946/10336], Loss: 0.0122\n",
      "Epoch [2/5], Step [4948/10336], Loss: 0.0466\n",
      "Epoch [2/5], Step [4950/10336], Loss: 1.1839\n",
      "Epoch [2/5], Step [4952/10336], Loss: 4.0419\n",
      "Epoch [2/5], Step [4954/10336], Loss: 0.3833\n",
      "Epoch [2/5], Step [4956/10336], Loss: 0.0078\n",
      "Epoch [2/5], Step [4958/10336], Loss: 1.9280\n",
      "Epoch [2/5], Step [4960/10336], Loss: 2.8459\n",
      "Epoch [2/5], Step [4962/10336], Loss: 3.6812\n",
      "Epoch [2/5], Step [4964/10336], Loss: 0.5240\n",
      "Epoch [2/5], Step [4966/10336], Loss: 0.2830\n",
      "Epoch [2/5], Step [4968/10336], Loss: 0.2964\n",
      "Epoch [2/5], Step [4970/10336], Loss: 1.0254\n",
      "Epoch [2/5], Step [4972/10336], Loss: 0.0147\n",
      "Epoch [2/5], Step [4974/10336], Loss: 0.0394\n",
      "Epoch [2/5], Step [4976/10336], Loss: 0.1276\n",
      "Epoch [2/5], Step [4978/10336], Loss: 0.1062\n",
      "Epoch [2/5], Step [4980/10336], Loss: 0.7488\n",
      "Epoch [2/5], Step [4982/10336], Loss: 1.3298\n",
      "Epoch [2/5], Step [4984/10336], Loss: 0.8754\n",
      "Epoch [2/5], Step [4986/10336], Loss: 0.3556\n",
      "Epoch [2/5], Step [4988/10336], Loss: 1.0202\n",
      "Epoch [2/5], Step [4990/10336], Loss: 1.3137\n",
      "Epoch [2/5], Step [4992/10336], Loss: 0.1841\n",
      "Epoch [2/5], Step [4994/10336], Loss: 2.3145\n",
      "Epoch [2/5], Step [4996/10336], Loss: 2.9309\n",
      "Epoch [2/5], Step [4998/10336], Loss: 1.4261\n",
      "Epoch [2/5], Step [5000/10336], Loss: 0.7303\n",
      "Epoch [2/5], Step [5002/10336], Loss: 2.3479\n",
      "Epoch [2/5], Step [5004/10336], Loss: 0.0080\n",
      "Epoch [2/5], Step [5006/10336], Loss: 1.5082\n",
      "Epoch [2/5], Step [5008/10336], Loss: 0.6783\n",
      "Epoch [2/5], Step [5010/10336], Loss: 0.3503\n",
      "Epoch [2/5], Step [5012/10336], Loss: 2.5307\n",
      "Epoch [2/5], Step [5014/10336], Loss: 0.7708\n",
      "Epoch [2/5], Step [5016/10336], Loss: 0.0311\n",
      "Epoch [2/5], Step [5018/10336], Loss: 0.3184\n",
      "Epoch [2/5], Step [5020/10336], Loss: 0.0530\n",
      "Epoch [2/5], Step [5022/10336], Loss: 0.1574\n",
      "Epoch [2/5], Step [5024/10336], Loss: 1.2915\n",
      "Epoch [2/5], Step [5026/10336], Loss: 0.1627\n",
      "Epoch [2/5], Step [5028/10336], Loss: 0.2078\n",
      "Epoch [2/5], Step [5030/10336], Loss: 0.0575\n",
      "Epoch [2/5], Step [5032/10336], Loss: 0.5869\n",
      "Epoch [2/5], Step [5034/10336], Loss: 3.5056\n",
      "Epoch [2/5], Step [5036/10336], Loss: 0.0232\n",
      "Epoch [2/5], Step [5038/10336], Loss: 0.0330\n",
      "Epoch [2/5], Step [5040/10336], Loss: 0.7784\n",
      "Epoch [2/5], Step [5042/10336], Loss: 0.2723\n",
      "Epoch [2/5], Step [5044/10336], Loss: 0.4573\n",
      "Epoch [2/5], Step [5046/10336], Loss: 3.3537\n",
      "Epoch [2/5], Step [5048/10336], Loss: 0.6105\n",
      "Epoch [2/5], Step [5050/10336], Loss: 3.2309\n",
      "Epoch [2/5], Step [5052/10336], Loss: 2.1624\n",
      "Epoch [2/5], Step [5054/10336], Loss: 0.0069\n",
      "Epoch [2/5], Step [5056/10336], Loss: 3.2571\n",
      "Epoch [2/5], Step [5058/10336], Loss: 0.1050\n",
      "Epoch [2/5], Step [5060/10336], Loss: 0.8443\n",
      "Epoch [2/5], Step [5062/10336], Loss: 0.7517\n",
      "Epoch [2/5], Step [5064/10336], Loss: 2.3751\n",
      "Epoch [2/5], Step [5066/10336], Loss: 0.3323\n",
      "Epoch [2/5], Step [5068/10336], Loss: 1.0286\n",
      "Epoch [2/5], Step [5070/10336], Loss: 0.1209\n",
      "Epoch [2/5], Step [5072/10336], Loss: 0.5460\n",
      "Epoch [2/5], Step [5074/10336], Loss: 3.6268\n",
      "Epoch [2/5], Step [5076/10336], Loss: 0.0671\n",
      "Epoch [2/5], Step [5078/10336], Loss: 0.7642\n",
      "Epoch [2/5], Step [5080/10336], Loss: 0.3044\n",
      "Epoch [2/5], Step [5082/10336], Loss: 0.3573\n",
      "Epoch [2/5], Step [5084/10336], Loss: 0.0114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [5086/10336], Loss: 0.3964\n",
      "Epoch [2/5], Step [5088/10336], Loss: 3.3379\n",
      "Epoch [2/5], Step [5090/10336], Loss: 1.2550\n",
      "Epoch [2/5], Step [5092/10336], Loss: 0.8733\n",
      "Epoch [2/5], Step [5094/10336], Loss: 1.3016\n",
      "Epoch [2/5], Step [5096/10336], Loss: 0.2755\n",
      "Epoch [2/5], Step [5098/10336], Loss: 1.2422\n",
      "Epoch [2/5], Step [5100/10336], Loss: 1.2540\n",
      "Epoch [2/5], Step [5102/10336], Loss: 0.7563\n",
      "Epoch [2/5], Step [5104/10336], Loss: 0.7727\n",
      "Epoch [2/5], Step [5106/10336], Loss: 0.4748\n",
      "Epoch [2/5], Step [5108/10336], Loss: 0.2344\n",
      "Epoch [2/5], Step [5110/10336], Loss: 0.0743\n",
      "Epoch [2/5], Step [5112/10336], Loss: 1.3708\n",
      "Epoch [2/5], Step [5114/10336], Loss: 1.2874\n",
      "Epoch [2/5], Step [5116/10336], Loss: 0.3656\n",
      "Epoch [2/5], Step [5118/10336], Loss: 0.9052\n",
      "Epoch [2/5], Step [5120/10336], Loss: 0.1820\n",
      "Epoch [2/5], Step [5122/10336], Loss: 0.0227\n",
      "Epoch [2/5], Step [5124/10336], Loss: 0.0626\n",
      "Epoch [2/5], Step [5126/10336], Loss: 0.2759\n",
      "Epoch [2/5], Step [5128/10336], Loss: 0.0737\n",
      "Epoch [2/5], Step [5130/10336], Loss: 2.9797\n",
      "Epoch [2/5], Step [5132/10336], Loss: 3.7938\n",
      "Epoch [2/5], Step [5134/10336], Loss: 1.7423\n",
      "Epoch [2/5], Step [5136/10336], Loss: 1.0405\n",
      "Epoch [2/5], Step [5138/10336], Loss: 0.0039\n",
      "Epoch [2/5], Step [5140/10336], Loss: 0.2467\n",
      "Epoch [2/5], Step [5142/10336], Loss: 0.0696\n",
      "Epoch [2/5], Step [5144/10336], Loss: 0.5939\n",
      "Epoch [2/5], Step [5146/10336], Loss: 0.7425\n",
      "Epoch [2/5], Step [5148/10336], Loss: 1.7539\n",
      "Epoch [2/5], Step [5150/10336], Loss: 0.0650\n",
      "Epoch [2/5], Step [5152/10336], Loss: 0.2198\n",
      "Epoch [2/5], Step [5154/10336], Loss: 1.0523\n",
      "Epoch [2/5], Step [5156/10336], Loss: 1.1882\n",
      "Epoch [2/5], Step [5158/10336], Loss: 1.7891\n",
      "Epoch [2/5], Step [5160/10336], Loss: 0.0255\n",
      "Epoch [2/5], Step [5162/10336], Loss: 0.5686\n",
      "Epoch [2/5], Step [5164/10336], Loss: 0.0997\n",
      "Epoch [2/5], Step [5166/10336], Loss: 0.2009\n",
      "Epoch [2/5], Step [5168/10336], Loss: 0.0586\n",
      "Epoch [2/5], Step [5170/10336], Loss: 2.5588\n",
      "Epoch [2/5], Step [5172/10336], Loss: 3.5871\n",
      "Epoch [2/5], Step [5174/10336], Loss: 1.3483\n",
      "Epoch [2/5], Step [5176/10336], Loss: 0.5995\n",
      "Epoch [2/5], Step [5178/10336], Loss: 0.0846\n",
      "Epoch [2/5], Step [5180/10336], Loss: 0.2086\n",
      "Epoch [2/5], Step [5182/10336], Loss: 0.0291\n",
      "Epoch [2/5], Step [5184/10336], Loss: 1.6215\n",
      "Epoch [2/5], Step [5186/10336], Loss: 0.0334\n",
      "Epoch [2/5], Step [5188/10336], Loss: 0.3276\n",
      "Epoch [2/5], Step [5190/10336], Loss: 0.3260\n",
      "Epoch [2/5], Step [5192/10336], Loss: 3.0423\n",
      "Epoch [2/5], Step [5194/10336], Loss: 0.4915\n",
      "Epoch [2/5], Step [5196/10336], Loss: 4.4848\n",
      "Epoch [2/5], Step [5198/10336], Loss: 0.6725\n",
      "Epoch [2/5], Step [5200/10336], Loss: 0.1226\n",
      "Epoch [2/5], Step [5202/10336], Loss: 2.3678\n",
      "Epoch [2/5], Step [5204/10336], Loss: 0.1278\n",
      "Epoch [2/5], Step [5206/10336], Loss: 1.6475\n",
      "Epoch [2/5], Step [5208/10336], Loss: 0.1567\n",
      "Epoch [2/5], Step [5210/10336], Loss: 0.0529\n",
      "Epoch [2/5], Step [5212/10336], Loss: 0.5109\n",
      "Epoch [2/5], Step [5214/10336], Loss: 0.0296\n",
      "Epoch [2/5], Step [5216/10336], Loss: 0.7024\n",
      "Epoch [2/5], Step [5218/10336], Loss: 0.0188\n",
      "Epoch [2/5], Step [5220/10336], Loss: 0.8032\n",
      "Epoch [2/5], Step [5222/10336], Loss: 1.2523\n",
      "Epoch [2/5], Step [5224/10336], Loss: 0.0541\n",
      "Epoch [2/5], Step [5226/10336], Loss: 1.3078\n",
      "Epoch [2/5], Step [5228/10336], Loss: 0.4822\n",
      "Epoch [2/5], Step [5230/10336], Loss: 2.1611\n",
      "Epoch [2/5], Step [5232/10336], Loss: 0.0049\n",
      "Epoch [2/5], Step [5234/10336], Loss: 0.8333\n",
      "Epoch [2/5], Step [5236/10336], Loss: 0.6846\n",
      "Epoch [2/5], Step [5238/10336], Loss: 0.1373\n",
      "Epoch [2/5], Step [5240/10336], Loss: 0.7449\n",
      "Epoch [2/5], Step [5242/10336], Loss: 0.0745\n",
      "Epoch [2/5], Step [5244/10336], Loss: 0.6232\n",
      "Epoch [2/5], Step [5246/10336], Loss: 0.6133\n",
      "Epoch [2/5], Step [5248/10336], Loss: 0.0631\n",
      "Epoch [2/5], Step [5250/10336], Loss: 2.3730\n",
      "Epoch [2/5], Step [5252/10336], Loss: 0.3125\n",
      "Epoch [2/5], Step [5254/10336], Loss: 1.9051\n",
      "Epoch [2/5], Step [5256/10336], Loss: 0.0714\n",
      "Epoch [2/5], Step [5258/10336], Loss: 0.1510\n",
      "Epoch [2/5], Step [5260/10336], Loss: 0.2730\n",
      "Epoch [2/5], Step [5262/10336], Loss: 6.0737\n",
      "Epoch [2/5], Step [5264/10336], Loss: 2.1565\n",
      "Epoch [2/5], Step [5266/10336], Loss: 1.3832\n",
      "Epoch [2/5], Step [5268/10336], Loss: 1.2750\n",
      "Epoch [2/5], Step [5270/10336], Loss: 0.1403\n",
      "Epoch [2/5], Step [5272/10336], Loss: 2.5758\n",
      "Epoch [2/5], Step [5274/10336], Loss: 1.2893\n",
      "Epoch [2/5], Step [5276/10336], Loss: 0.4440\n",
      "Epoch [2/5], Step [5278/10336], Loss: 0.7272\n",
      "Epoch [2/5], Step [5280/10336], Loss: 0.0172\n",
      "Epoch [2/5], Step [5282/10336], Loss: 0.0524\n",
      "Epoch [2/5], Step [5284/10336], Loss: 0.2490\n",
      "Epoch [2/5], Step [5286/10336], Loss: 3.2422\n",
      "Epoch [2/5], Step [5288/10336], Loss: 2.8233\n",
      "Epoch [2/5], Step [5290/10336], Loss: 2.6527\n",
      "Epoch [2/5], Step [5292/10336], Loss: 0.7016\n",
      "Epoch [2/5], Step [5294/10336], Loss: 0.7772\n",
      "Epoch [2/5], Step [5296/10336], Loss: 2.4696\n",
      "Epoch [2/5], Step [5298/10336], Loss: 2.3527\n",
      "Epoch [2/5], Step [5300/10336], Loss: 1.2100\n",
      "Epoch [2/5], Step [5302/10336], Loss: 1.3906\n",
      "Epoch [2/5], Step [5304/10336], Loss: 0.1259\n",
      "Epoch [2/5], Step [5306/10336], Loss: 1.7220\n",
      "Epoch [2/5], Step [5308/10336], Loss: 4.2005\n",
      "Epoch [2/5], Step [5310/10336], Loss: 0.7864\n",
      "Epoch [2/5], Step [5312/10336], Loss: 1.4600\n",
      "Epoch [2/5], Step [5314/10336], Loss: 1.2383\n",
      "Epoch [2/5], Step [5316/10336], Loss: 1.0064\n",
      "Epoch [2/5], Step [5318/10336], Loss: 0.5111\n",
      "Epoch [2/5], Step [5320/10336], Loss: 0.0328\n",
      "Epoch [2/5], Step [5322/10336], Loss: 2.4446\n",
      "Epoch [2/5], Step [5324/10336], Loss: 1.0827\n",
      "Epoch [2/5], Step [5326/10336], Loss: 0.1218\n",
      "Epoch [2/5], Step [5328/10336], Loss: 0.4260\n",
      "Epoch [2/5], Step [5330/10336], Loss: 1.7758\n",
      "Epoch [2/5], Step [5332/10336], Loss: 3.7697\n",
      "Epoch [2/5], Step [5334/10336], Loss: 0.6641\n",
      "Epoch [2/5], Step [5336/10336], Loss: 3.4836\n",
      "Epoch [2/5], Step [5338/10336], Loss: 0.1784\n",
      "Epoch [2/5], Step [5340/10336], Loss: 0.5791\n",
      "Epoch [2/5], Step [5342/10336], Loss: 0.2012\n",
      "Epoch [2/5], Step [5344/10336], Loss: 0.1626\n",
      "Epoch [2/5], Step [5346/10336], Loss: 0.9291\n",
      "Epoch [2/5], Step [5348/10336], Loss: 0.8044\n",
      "Epoch [2/5], Step [5350/10336], Loss: 0.1695\n",
      "Epoch [2/5], Step [5352/10336], Loss: 2.5156\n",
      "Epoch [2/5], Step [5354/10336], Loss: 0.2213\n",
      "Epoch [2/5], Step [5356/10336], Loss: 0.7254\n",
      "Epoch [2/5], Step [5358/10336], Loss: 0.0872\n",
      "Epoch [2/5], Step [5360/10336], Loss: 0.0103\n",
      "Epoch [2/5], Step [5362/10336], Loss: 2.8226\n",
      "Epoch [2/5], Step [5364/10336], Loss: 1.0315\n",
      "Epoch [2/5], Step [5366/10336], Loss: 0.8181\n",
      "Epoch [2/5], Step [5368/10336], Loss: 1.4918\n",
      "Epoch [2/5], Step [5370/10336], Loss: 1.7352\n",
      "Epoch [2/5], Step [5372/10336], Loss: 0.1013\n",
      "Epoch [2/5], Step [5374/10336], Loss: 0.6611\n",
      "Epoch [2/5], Step [5376/10336], Loss: 2.3215\n",
      "Epoch [2/5], Step [5378/10336], Loss: 0.1559\n",
      "Epoch [2/5], Step [5380/10336], Loss: 0.0186\n",
      "Epoch [2/5], Step [5382/10336], Loss: 0.0493\n",
      "Epoch [2/5], Step [5384/10336], Loss: 0.3182\n",
      "Epoch [2/5], Step [5386/10336], Loss: 0.2724\n",
      "Epoch [2/5], Step [5388/10336], Loss: 0.0247\n",
      "Epoch [2/5], Step [5390/10336], Loss: 0.7002\n",
      "Epoch [2/5], Step [5392/10336], Loss: 0.0871\n",
      "Epoch [2/5], Step [5394/10336], Loss: 1.1215\n",
      "Epoch [2/5], Step [5396/10336], Loss: 0.1481\n",
      "Epoch [2/5], Step [5398/10336], Loss: 0.0196\n",
      "Epoch [2/5], Step [5400/10336], Loss: 0.0951\n",
      "Epoch [2/5], Step [5402/10336], Loss: 1.0797\n",
      "Epoch [2/5], Step [5404/10336], Loss: 2.1005\n",
      "Epoch [2/5], Step [5406/10336], Loss: 0.0091\n",
      "Epoch [2/5], Step [5408/10336], Loss: 0.0004\n",
      "Epoch [2/5], Step [5410/10336], Loss: 0.4436\n",
      "Epoch [2/5], Step [5412/10336], Loss: 0.0204\n",
      "Epoch [2/5], Step [5414/10336], Loss: 0.0454\n",
      "Epoch [2/5], Step [5416/10336], Loss: 0.0088\n",
      "Epoch [2/5], Step [5418/10336], Loss: 0.2100\n",
      "Epoch [2/5], Step [5420/10336], Loss: 0.5730\n",
      "Epoch [2/5], Step [5422/10336], Loss: 1.2205\n",
      "Epoch [2/5], Step [5424/10336], Loss: 0.4252\n",
      "Epoch [2/5], Step [5426/10336], Loss: 0.1054\n",
      "Epoch [2/5], Step [5428/10336], Loss: 0.0337\n",
      "Epoch [2/5], Step [5430/10336], Loss: 0.0093\n",
      "Epoch [2/5], Step [5432/10336], Loss: 0.0022\n",
      "Epoch [2/5], Step [5434/10336], Loss: 1.0835\n",
      "Epoch [2/5], Step [5436/10336], Loss: 5.0551\n",
      "Epoch [2/5], Step [5438/10336], Loss: 4.2570\n",
      "Epoch [2/5], Step [5440/10336], Loss: 0.2815\n",
      "Epoch [2/5], Step [5442/10336], Loss: 0.0641\n",
      "Epoch [2/5], Step [5444/10336], Loss: 0.4939\n",
      "Epoch [2/5], Step [5446/10336], Loss: 1.1647\n",
      "Epoch [2/5], Step [5448/10336], Loss: 0.4098\n",
      "Epoch [2/5], Step [5450/10336], Loss: 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [5452/10336], Loss: 0.5544\n",
      "Epoch [2/5], Step [5454/10336], Loss: 1.7929\n",
      "Epoch [2/5], Step [5456/10336], Loss: 1.6241\n",
      "Epoch [2/5], Step [5458/10336], Loss: 0.1146\n",
      "Epoch [2/5], Step [5460/10336], Loss: 0.1567\n",
      "Epoch [2/5], Step [5462/10336], Loss: 0.1562\n",
      "Epoch [2/5], Step [5464/10336], Loss: 1.6495\n",
      "Epoch [2/5], Step [5466/10336], Loss: 0.0408\n",
      "Epoch [2/5], Step [5468/10336], Loss: 0.0025\n",
      "Epoch [2/5], Step [5470/10336], Loss: 0.0110\n",
      "Epoch [2/5], Step [5472/10336], Loss: 1.4783\n",
      "Epoch [2/5], Step [5474/10336], Loss: 0.6672\n",
      "Epoch [2/5], Step [5476/10336], Loss: 0.7245\n",
      "Epoch [2/5], Step [5478/10336], Loss: 0.0603\n",
      "Epoch [2/5], Step [5480/10336], Loss: 1.5298\n",
      "Epoch [2/5], Step [5482/10336], Loss: 1.9083\n",
      "Epoch [2/5], Step [5484/10336], Loss: 0.0832\n",
      "Epoch [2/5], Step [5486/10336], Loss: 0.2444\n",
      "Epoch [2/5], Step [5488/10336], Loss: 0.0679\n",
      "Epoch [2/5], Step [5490/10336], Loss: 0.0055\n",
      "Epoch [2/5], Step [5492/10336], Loss: 0.0223\n",
      "Epoch [2/5], Step [5494/10336], Loss: 0.1333\n",
      "Epoch [2/5], Step [5496/10336], Loss: 0.7216\n",
      "Epoch [2/5], Step [5498/10336], Loss: 3.1539\n",
      "Epoch [2/5], Step [5500/10336], Loss: 2.4460\n",
      "Epoch [2/5], Step [5502/10336], Loss: 2.2304\n",
      "Epoch [2/5], Step [5504/10336], Loss: 0.4336\n",
      "Epoch [2/5], Step [5506/10336], Loss: 2.7566\n",
      "Epoch [2/5], Step [5508/10336], Loss: 0.1739\n",
      "Epoch [2/5], Step [5510/10336], Loss: 1.3480\n",
      "Epoch [2/5], Step [5512/10336], Loss: 2.2763\n",
      "Epoch [2/5], Step [5514/10336], Loss: 0.4324\n",
      "Epoch [2/5], Step [5516/10336], Loss: 1.8783\n",
      "Epoch [2/5], Step [5518/10336], Loss: 0.1917\n",
      "Epoch [2/5], Step [5520/10336], Loss: 1.0258\n",
      "Epoch [2/5], Step [5522/10336], Loss: 0.0372\n",
      "Epoch [2/5], Step [5524/10336], Loss: 0.0720\n",
      "Epoch [2/5], Step [5526/10336], Loss: 0.3729\n",
      "Epoch [2/5], Step [5528/10336], Loss: 2.9140\n",
      "Epoch [2/5], Step [5530/10336], Loss: 2.0602\n",
      "Epoch [2/5], Step [5532/10336], Loss: 1.3693\n",
      "Epoch [2/5], Step [5534/10336], Loss: 2.0921\n",
      "Epoch [2/5], Step [5536/10336], Loss: 0.3745\n",
      "Epoch [2/5], Step [5538/10336], Loss: 1.6235\n",
      "Epoch [2/5], Step [5540/10336], Loss: 0.8106\n",
      "Epoch [2/5], Step [5542/10336], Loss: 0.1948\n",
      "Epoch [2/5], Step [5544/10336], Loss: 0.0486\n",
      "Epoch [2/5], Step [5546/10336], Loss: 0.0725\n",
      "Epoch [2/5], Step [5548/10336], Loss: 2.3129\n",
      "Epoch [2/5], Step [5550/10336], Loss: 2.6925\n",
      "Epoch [2/5], Step [5552/10336], Loss: 2.8093\n",
      "Epoch [2/5], Step [5554/10336], Loss: 0.7581\n",
      "Epoch [2/5], Step [5556/10336], Loss: 0.1073\n",
      "Epoch [2/5], Step [5558/10336], Loss: 2.0737\n",
      "Epoch [2/5], Step [5560/10336], Loss: 2.3926\n",
      "Epoch [2/5], Step [5562/10336], Loss: 1.8441\n",
      "Epoch [2/5], Step [5564/10336], Loss: 0.7188\n",
      "Epoch [2/5], Step [5566/10336], Loss: 0.3792\n",
      "Epoch [2/5], Step [5568/10336], Loss: 1.3188\n",
      "Epoch [2/5], Step [5570/10336], Loss: 0.2457\n",
      "Epoch [2/5], Step [5572/10336], Loss: 1.8072\n",
      "Epoch [2/5], Step [5574/10336], Loss: 1.4427\n",
      "Epoch [2/5], Step [5576/10336], Loss: 0.4591\n",
      "Epoch [2/5], Step [5578/10336], Loss: 0.4437\n",
      "Epoch [2/5], Step [5580/10336], Loss: 0.1573\n",
      "Epoch [2/5], Step [5582/10336], Loss: 1.8643\n",
      "Epoch [2/5], Step [5584/10336], Loss: 0.0487\n",
      "Epoch [2/5], Step [5586/10336], Loss: 0.1978\n",
      "Epoch [2/5], Step [5588/10336], Loss: 0.5931\n",
      "Epoch [2/5], Step [5590/10336], Loss: 0.8175\n",
      "Epoch [2/5], Step [5592/10336], Loss: 0.4559\n",
      "Epoch [2/5], Step [5594/10336], Loss: 0.6552\n",
      "Epoch [2/5], Step [5596/10336], Loss: 0.9471\n",
      "Epoch [2/5], Step [5598/10336], Loss: 0.4599\n",
      "Epoch [2/5], Step [5600/10336], Loss: 2.5460\n",
      "Epoch [2/5], Step [5602/10336], Loss: 1.0224\n",
      "Epoch [2/5], Step [5604/10336], Loss: 2.7009\n",
      "Epoch [2/5], Step [5606/10336], Loss: 0.0054\n",
      "Epoch [2/5], Step [5608/10336], Loss: 2.7608\n",
      "Epoch [2/5], Step [5610/10336], Loss: 2.5347\n",
      "Epoch [2/5], Step [5612/10336], Loss: 0.6331\n",
      "Epoch [2/5], Step [5614/10336], Loss: 1.8220\n",
      "Epoch [2/5], Step [5616/10336], Loss: 2.2638\n",
      "Epoch [2/5], Step [5618/10336], Loss: 0.9453\n",
      "Epoch [2/5], Step [5620/10336], Loss: 0.0686\n",
      "Epoch [2/5], Step [5622/10336], Loss: 1.3737\n",
      "Epoch [2/5], Step [5624/10336], Loss: 0.9483\n",
      "Epoch [2/5], Step [5626/10336], Loss: 0.1284\n",
      "Epoch [2/5], Step [5628/10336], Loss: 0.2057\n",
      "Epoch [2/5], Step [5630/10336], Loss: 1.5933\n",
      "Epoch [2/5], Step [5632/10336], Loss: 1.2359\n",
      "Epoch [2/5], Step [5634/10336], Loss: 2.6855\n",
      "Epoch [2/5], Step [5636/10336], Loss: 0.2390\n",
      "Epoch [2/5], Step [5638/10336], Loss: 0.2164\n",
      "Epoch [2/5], Step [5640/10336], Loss: 0.4786\n",
      "Epoch [2/5], Step [5642/10336], Loss: 0.1605\n",
      "Epoch [2/5], Step [5644/10336], Loss: 1.6512\n",
      "Epoch [2/5], Step [5646/10336], Loss: 0.6173\n",
      "Epoch [2/5], Step [5648/10336], Loss: 2.5984\n",
      "Epoch [2/5], Step [5650/10336], Loss: 0.3259\n",
      "Epoch [2/5], Step [5652/10336], Loss: 3.9253\n",
      "Epoch [2/5], Step [5654/10336], Loss: 0.8438\n",
      "Epoch [2/5], Step [5656/10336], Loss: 2.3382\n",
      "Epoch [2/5], Step [5658/10336], Loss: 0.0776\n",
      "Epoch [2/5], Step [5660/10336], Loss: 3.3555\n",
      "Epoch [2/5], Step [5662/10336], Loss: 1.4987\n",
      "Epoch [2/5], Step [5664/10336], Loss: 0.2965\n",
      "Epoch [2/5], Step [5666/10336], Loss: 0.3334\n",
      "Epoch [2/5], Step [5668/10336], Loss: 0.0717\n",
      "Epoch [2/5], Step [5670/10336], Loss: 0.0121\n",
      "Epoch [2/5], Step [5672/10336], Loss: 1.4364\n",
      "Epoch [2/5], Step [5674/10336], Loss: 4.1894\n",
      "Epoch [2/5], Step [5676/10336], Loss: 0.4479\n",
      "Epoch [2/5], Step [5678/10336], Loss: 2.3769\n",
      "Epoch [2/5], Step [5680/10336], Loss: 0.0242\n",
      "Epoch [2/5], Step [5682/10336], Loss: 0.4645\n",
      "Epoch [2/5], Step [5684/10336], Loss: 0.0584\n",
      "Epoch [2/5], Step [5686/10336], Loss: 0.0475\n",
      "Epoch [2/5], Step [5688/10336], Loss: 0.0406\n",
      "Epoch [2/5], Step [5690/10336], Loss: 0.5699\n",
      "Epoch [2/5], Step [5692/10336], Loss: 3.1849\n",
      "Epoch [2/5], Step [5694/10336], Loss: 0.3099\n",
      "Epoch [2/5], Step [5696/10336], Loss: 1.1793\n",
      "Epoch [2/5], Step [5698/10336], Loss: 0.2022\n",
      "Epoch [2/5], Step [5700/10336], Loss: 0.6844\n",
      "Epoch [2/5], Step [5702/10336], Loss: 0.3260\n",
      "Epoch [2/5], Step [5704/10336], Loss: 0.1398\n",
      "Epoch [2/5], Step [5706/10336], Loss: 1.0823\n",
      "Epoch [2/5], Step [5708/10336], Loss: 1.8577\n",
      "Epoch [2/5], Step [5710/10336], Loss: 0.1478\n",
      "Epoch [2/5], Step [5712/10336], Loss: 0.2032\n",
      "Epoch [2/5], Step [5714/10336], Loss: 0.0740\n",
      "Epoch [2/5], Step [5716/10336], Loss: 0.0018\n",
      "Epoch [2/5], Step [5718/10336], Loss: 0.2897\n",
      "Epoch [2/5], Step [5720/10336], Loss: 2.9138\n",
      "Epoch [2/5], Step [5722/10336], Loss: 0.6119\n",
      "Epoch [2/5], Step [5724/10336], Loss: 0.4519\n",
      "Epoch [2/5], Step [5726/10336], Loss: 0.0577\n",
      "Epoch [2/5], Step [5728/10336], Loss: 1.1420\n",
      "Epoch [2/5], Step [5730/10336], Loss: 2.2048\n",
      "Epoch [2/5], Step [5732/10336], Loss: 0.0245\n",
      "Epoch [2/5], Step [5734/10336], Loss: 1.1396\n",
      "Epoch [2/5], Step [5736/10336], Loss: 0.4133\n",
      "Epoch [2/5], Step [5738/10336], Loss: 0.1676\n",
      "Epoch [2/5], Step [5740/10336], Loss: 0.0140\n",
      "Epoch [2/5], Step [5742/10336], Loss: 0.7946\n",
      "Epoch [2/5], Step [5744/10336], Loss: 1.1837\n",
      "Epoch [2/5], Step [5746/10336], Loss: 0.2009\n",
      "Epoch [2/5], Step [5748/10336], Loss: 0.9142\n",
      "Epoch [2/5], Step [5750/10336], Loss: 0.8567\n",
      "Epoch [2/5], Step [5752/10336], Loss: 0.9482\n",
      "Epoch [2/5], Step [5754/10336], Loss: 0.0408\n",
      "Epoch [2/5], Step [5756/10336], Loss: 2.4017\n",
      "Epoch [2/5], Step [5758/10336], Loss: 0.1212\n",
      "Epoch [2/5], Step [5760/10336], Loss: 0.0148\n",
      "Epoch [2/5], Step [5762/10336], Loss: 1.6933\n",
      "Epoch [2/5], Step [5764/10336], Loss: 0.0821\n",
      "Epoch [2/5], Step [5766/10336], Loss: 2.2189\n",
      "Epoch [2/5], Step [5768/10336], Loss: 0.3208\n",
      "Epoch [2/5], Step [5770/10336], Loss: 0.2873\n",
      "Epoch [2/5], Step [5772/10336], Loss: 0.5420\n",
      "Epoch [2/5], Step [5774/10336], Loss: 0.0649\n",
      "Epoch [2/5], Step [5776/10336], Loss: 0.0979\n",
      "Epoch [2/5], Step [5778/10336], Loss: 3.0044\n",
      "Epoch [2/5], Step [5780/10336], Loss: 2.3104\n",
      "Epoch [2/5], Step [5782/10336], Loss: 0.2477\n",
      "Epoch [2/5], Step [5784/10336], Loss: 0.1122\n",
      "Epoch [2/5], Step [5786/10336], Loss: 0.8608\n",
      "Epoch [2/5], Step [5788/10336], Loss: 1.4632\n",
      "Epoch [2/5], Step [5790/10336], Loss: 1.6228\n",
      "Epoch [2/5], Step [5792/10336], Loss: 0.4996\n",
      "Epoch [2/5], Step [5794/10336], Loss: 0.2672\n",
      "Epoch [2/5], Step [5796/10336], Loss: 0.0142\n",
      "Epoch [2/5], Step [5798/10336], Loss: 0.9386\n",
      "Epoch [2/5], Step [5800/10336], Loss: 2.7805\n",
      "Epoch [2/5], Step [5802/10336], Loss: 0.8698\n",
      "Epoch [2/5], Step [5804/10336], Loss: 0.1556\n",
      "Epoch [2/5], Step [5806/10336], Loss: 0.2004\n",
      "Epoch [2/5], Step [5808/10336], Loss: 0.0572\n",
      "Epoch [2/5], Step [5810/10336], Loss: 2.8273\n",
      "Epoch [2/5], Step [5812/10336], Loss: 0.0563\n",
      "Epoch [2/5], Step [5814/10336], Loss: 1.7559\n",
      "Epoch [2/5], Step [5816/10336], Loss: 2.0694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [5818/10336], Loss: 2.0581\n",
      "Epoch [2/5], Step [5820/10336], Loss: 1.6499\n",
      "Epoch [2/5], Step [5822/10336], Loss: 0.3553\n",
      "Epoch [2/5], Step [5824/10336], Loss: 0.0324\n",
      "Epoch [2/5], Step [5826/10336], Loss: 0.1767\n",
      "Epoch [2/5], Step [5828/10336], Loss: 1.3789\n",
      "Epoch [2/5], Step [5830/10336], Loss: 1.8968\n",
      "Epoch [2/5], Step [5832/10336], Loss: 0.8656\n",
      "Epoch [2/5], Step [5834/10336], Loss: 0.6474\n",
      "Epoch [2/5], Step [5836/10336], Loss: 2.9636\n",
      "Epoch [2/5], Step [5838/10336], Loss: 0.1578\n",
      "Epoch [2/5], Step [5840/10336], Loss: 2.8302\n",
      "Epoch [2/5], Step [5842/10336], Loss: 0.4613\n",
      "Epoch [2/5], Step [5844/10336], Loss: 1.0719\n",
      "Epoch [2/5], Step [5846/10336], Loss: 0.0112\n",
      "Epoch [2/5], Step [5848/10336], Loss: 1.0219\n",
      "Epoch [2/5], Step [5850/10336], Loss: 1.0688\n",
      "Epoch [2/5], Step [5852/10336], Loss: 0.0127\n",
      "Epoch [2/5], Step [5854/10336], Loss: 0.1450\n",
      "Epoch [2/5], Step [5856/10336], Loss: 0.0561\n",
      "Epoch [2/5], Step [5858/10336], Loss: 0.0671\n",
      "Epoch [2/5], Step [5860/10336], Loss: 3.3544\n",
      "Epoch [2/5], Step [5862/10336], Loss: 2.1691\n",
      "Epoch [2/5], Step [5864/10336], Loss: 0.8927\n",
      "Epoch [2/5], Step [5866/10336], Loss: 0.0779\n",
      "Epoch [2/5], Step [5868/10336], Loss: 1.0468\n",
      "Epoch [2/5], Step [5870/10336], Loss: 2.1351\n",
      "Epoch [2/5], Step [5872/10336], Loss: 1.2822\n",
      "Epoch [2/5], Step [5874/10336], Loss: 1.6877\n",
      "Epoch [2/5], Step [5876/10336], Loss: 0.1596\n",
      "Epoch [2/5], Step [5878/10336], Loss: 0.0815\n",
      "Epoch [2/5], Step [5880/10336], Loss: 1.1548\n",
      "Epoch [2/5], Step [5882/10336], Loss: 0.8680\n",
      "Epoch [2/5], Step [5884/10336], Loss: 0.9708\n",
      "Epoch [2/5], Step [5886/10336], Loss: 0.5528\n",
      "Epoch [2/5], Step [5888/10336], Loss: 1.2635\n",
      "Epoch [2/5], Step [5890/10336], Loss: 0.2080\n",
      "Epoch [2/5], Step [5892/10336], Loss: 0.1688\n",
      "Epoch [2/5], Step [5894/10336], Loss: 0.5386\n",
      "Epoch [2/5], Step [5896/10336], Loss: 0.0853\n",
      "Epoch [2/5], Step [5898/10336], Loss: 2.6516\n",
      "Epoch [2/5], Step [5900/10336], Loss: 0.0148\n",
      "Epoch [2/5], Step [5902/10336], Loss: 0.4174\n",
      "Epoch [2/5], Step [5904/10336], Loss: 0.7079\n",
      "Epoch [2/5], Step [5906/10336], Loss: 0.2106\n",
      "Epoch [2/5], Step [5908/10336], Loss: 0.6839\n",
      "Epoch [2/5], Step [5910/10336], Loss: 1.3978\n",
      "Epoch [2/5], Step [5912/10336], Loss: 1.7039\n",
      "Epoch [2/5], Step [5914/10336], Loss: 0.8175\n",
      "Epoch [2/5], Step [5916/10336], Loss: 3.0783\n",
      "Epoch [2/5], Step [5918/10336], Loss: 2.5016\n",
      "Epoch [2/5], Step [5920/10336], Loss: 3.2444\n",
      "Epoch [2/5], Step [5922/10336], Loss: 1.9031\n",
      "Epoch [2/5], Step [5924/10336], Loss: 0.4526\n",
      "Epoch [2/5], Step [5926/10336], Loss: 2.3719\n",
      "Epoch [2/5], Step [5928/10336], Loss: 0.4563\n",
      "Epoch [2/5], Step [5930/10336], Loss: 0.4506\n",
      "Epoch [2/5], Step [5932/10336], Loss: 1.2933\n",
      "Epoch [2/5], Step [5934/10336], Loss: 1.2905\n",
      "Epoch [2/5], Step [5936/10336], Loss: 0.6281\n",
      "Epoch [2/5], Step [5938/10336], Loss: 0.1298\n",
      "Epoch [2/5], Step [5940/10336], Loss: 0.8589\n",
      "Epoch [2/5], Step [5942/10336], Loss: 0.2301\n",
      "Epoch [2/5], Step [5944/10336], Loss: 1.4514\n",
      "Epoch [2/5], Step [5946/10336], Loss: 0.0433\n",
      "Epoch [2/5], Step [5948/10336], Loss: 0.9877\n",
      "Epoch [2/5], Step [5950/10336], Loss: 0.8691\n",
      "Epoch [2/5], Step [5952/10336], Loss: 1.5214\n",
      "Epoch [2/5], Step [5954/10336], Loss: 0.2264\n",
      "Epoch [2/5], Step [5956/10336], Loss: 2.0876\n",
      "Epoch [2/5], Step [5958/10336], Loss: 0.0916\n",
      "Epoch [2/5], Step [5960/10336], Loss: 0.7283\n",
      "Epoch [2/5], Step [5962/10336], Loss: 0.0265\n",
      "Epoch [2/5], Step [5964/10336], Loss: 0.7593\n",
      "Epoch [2/5], Step [5966/10336], Loss: 1.4699\n",
      "Epoch [2/5], Step [5968/10336], Loss: 0.7152\n",
      "Epoch [2/5], Step [5970/10336], Loss: 0.2803\n",
      "Epoch [2/5], Step [5972/10336], Loss: 0.0045\n",
      "Epoch [2/5], Step [5974/10336], Loss: 0.5505\n",
      "Epoch [2/5], Step [5976/10336], Loss: 0.0955\n",
      "Epoch [2/5], Step [5978/10336], Loss: 0.9143\n",
      "Epoch [2/5], Step [5980/10336], Loss: 1.7435\n",
      "Epoch [2/5], Step [5982/10336], Loss: 0.1119\n",
      "Epoch [2/5], Step [5984/10336], Loss: 0.0842\n",
      "Epoch [2/5], Step [5986/10336], Loss: 0.4176\n",
      "Epoch [2/5], Step [5988/10336], Loss: 0.0919\n",
      "Epoch [2/5], Step [5990/10336], Loss: 0.9867\n",
      "Epoch [2/5], Step [5992/10336], Loss: 0.0580\n",
      "Epoch [2/5], Step [5994/10336], Loss: 0.6694\n",
      "Epoch [2/5], Step [5996/10336], Loss: 1.3670\n",
      "Epoch [2/5], Step [5998/10336], Loss: 0.8932\n",
      "Epoch [2/5], Step [6000/10336], Loss: 1.1942\n",
      "Epoch [2/5], Step [6002/10336], Loss: 1.4860\n",
      "Epoch [2/5], Step [6004/10336], Loss: 0.5177\n",
      "Epoch [2/5], Step [6006/10336], Loss: 0.6589\n",
      "Epoch [2/5], Step [6008/10336], Loss: 0.6341\n",
      "Epoch [2/5], Step [6010/10336], Loss: 1.3669\n",
      "Epoch [2/5], Step [6012/10336], Loss: 1.2226\n",
      "Epoch [2/5], Step [6014/10336], Loss: 0.1446\n",
      "Epoch [2/5], Step [6016/10336], Loss: 0.3101\n",
      "Epoch [2/5], Step [6018/10336], Loss: 0.0156\n",
      "Epoch [2/5], Step [6020/10336], Loss: 0.3959\n",
      "Epoch [2/5], Step [6022/10336], Loss: 1.9431\n",
      "Epoch [2/5], Step [6024/10336], Loss: 0.4752\n",
      "Epoch [2/5], Step [6026/10336], Loss: 1.4070\n",
      "Epoch [2/5], Step [6028/10336], Loss: 0.3308\n",
      "Epoch [2/5], Step [6030/10336], Loss: 4.8722\n",
      "Epoch [2/5], Step [6032/10336], Loss: 0.7074\n",
      "Epoch [2/5], Step [6034/10336], Loss: 0.3406\n",
      "Epoch [2/5], Step [6036/10336], Loss: 0.5503\n",
      "Epoch [2/5], Step [6038/10336], Loss: 3.1869\n",
      "Epoch [2/5], Step [6040/10336], Loss: 2.4384\n",
      "Epoch [2/5], Step [6042/10336], Loss: 0.9831\n",
      "Epoch [2/5], Step [6044/10336], Loss: 0.2159\n",
      "Epoch [2/5], Step [6046/10336], Loss: 0.8338\n",
      "Epoch [2/5], Step [6048/10336], Loss: 0.1986\n",
      "Epoch [2/5], Step [6050/10336], Loss: 0.2007\n",
      "Epoch [2/5], Step [6052/10336], Loss: 0.4275\n",
      "Epoch [2/5], Step [6054/10336], Loss: 1.3572\n",
      "Epoch [2/5], Step [6056/10336], Loss: 5.7808\n",
      "Epoch [2/5], Step [6058/10336], Loss: 0.0393\n",
      "Epoch [2/5], Step [6060/10336], Loss: 0.9976\n",
      "Epoch [2/5], Step [6062/10336], Loss: 2.2709\n",
      "Epoch [2/5], Step [6064/10336], Loss: 1.5399\n",
      "Epoch [2/5], Step [6066/10336], Loss: 2.7506\n",
      "Epoch [2/5], Step [6068/10336], Loss: 0.1824\n",
      "Epoch [2/5], Step [6070/10336], Loss: 0.0687\n",
      "Epoch [2/5], Step [6072/10336], Loss: 0.1687\n",
      "Epoch [2/5], Step [6074/10336], Loss: 0.1091\n",
      "Epoch [2/5], Step [6076/10336], Loss: 1.2541\n",
      "Epoch [2/5], Step [6078/10336], Loss: 0.0652\n",
      "Epoch [2/5], Step [6080/10336], Loss: 0.9742\n",
      "Epoch [2/5], Step [6082/10336], Loss: 2.9098\n",
      "Epoch [2/5], Step [6084/10336], Loss: 0.1156\n",
      "Epoch [2/5], Step [6086/10336], Loss: 0.0055\n",
      "Epoch [2/5], Step [6088/10336], Loss: 0.0283\n",
      "Epoch [2/5], Step [6090/10336], Loss: 0.9333\n",
      "Epoch [2/5], Step [6092/10336], Loss: 0.2572\n",
      "Epoch [2/5], Step [6094/10336], Loss: 0.7904\n",
      "Epoch [2/5], Step [6096/10336], Loss: 0.4551\n",
      "Epoch [2/5], Step [6098/10336], Loss: 1.6610\n",
      "Epoch [2/5], Step [6100/10336], Loss: 1.7098\n",
      "Epoch [2/5], Step [6102/10336], Loss: 0.8139\n",
      "Epoch [2/5], Step [6104/10336], Loss: 0.0776\n",
      "Epoch [2/5], Step [6106/10336], Loss: 2.3177\n",
      "Epoch [2/5], Step [6108/10336], Loss: 1.5283\n",
      "Epoch [2/5], Step [6110/10336], Loss: 4.3915\n",
      "Epoch [2/5], Step [6112/10336], Loss: 0.5103\n",
      "Epoch [2/5], Step [6114/10336], Loss: 2.4013\n",
      "Epoch [2/5], Step [6116/10336], Loss: 0.2535\n",
      "Epoch [2/5], Step [6118/10336], Loss: 3.0629\n",
      "Epoch [2/5], Step [6120/10336], Loss: 0.9616\n",
      "Epoch [2/5], Step [6122/10336], Loss: 0.0669\n",
      "Epoch [2/5], Step [6124/10336], Loss: 0.7745\n",
      "Epoch [2/5], Step [6126/10336], Loss: 0.1328\n",
      "Epoch [2/5], Step [6128/10336], Loss: 0.0466\n",
      "Epoch [2/5], Step [6130/10336], Loss: 1.1744\n",
      "Epoch [2/5], Step [6132/10336], Loss: 0.4706\n",
      "Epoch [2/5], Step [6134/10336], Loss: 2.0389\n",
      "Epoch [2/5], Step [6136/10336], Loss: 0.0199\n",
      "Epoch [2/5], Step [6138/10336], Loss: 0.2550\n",
      "Epoch [2/5], Step [6140/10336], Loss: 1.0768\n",
      "Epoch [2/5], Step [6142/10336], Loss: 0.5026\n",
      "Epoch [2/5], Step [6144/10336], Loss: 0.0365\n",
      "Epoch [2/5], Step [6146/10336], Loss: 0.8654\n",
      "Epoch [2/5], Step [6148/10336], Loss: 2.1653\n",
      "Epoch [2/5], Step [6150/10336], Loss: 0.9812\n",
      "Epoch [2/5], Step [6152/10336], Loss: 0.2790\n",
      "Epoch [2/5], Step [6154/10336], Loss: 2.2097\n",
      "Epoch [2/5], Step [6156/10336], Loss: 1.1751\n",
      "Epoch [2/5], Step [6158/10336], Loss: 4.1471\n",
      "Epoch [2/5], Step [6160/10336], Loss: 0.4560\n",
      "Epoch [2/5], Step [6162/10336], Loss: 0.3315\n",
      "Epoch [2/5], Step [6164/10336], Loss: 0.4661\n",
      "Epoch [2/5], Step [6166/10336], Loss: 2.1690\n",
      "Epoch [2/5], Step [6168/10336], Loss: 0.0085\n",
      "Epoch [2/5], Step [6170/10336], Loss: 1.2267\n",
      "Epoch [2/5], Step [6172/10336], Loss: 0.0213\n",
      "Epoch [2/5], Step [6174/10336], Loss: 1.1927\n",
      "Epoch [2/5], Step [6176/10336], Loss: 3.3314\n",
      "Epoch [2/5], Step [6178/10336], Loss: 0.3663\n",
      "Epoch [2/5], Step [6180/10336], Loss: 0.3804\n",
      "Epoch [2/5], Step [6182/10336], Loss: 0.0269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [6184/10336], Loss: 0.3119\n",
      "Epoch [2/5], Step [6186/10336], Loss: 0.0347\n",
      "Epoch [2/5], Step [6188/10336], Loss: 2.1716\n",
      "Epoch [2/5], Step [6190/10336], Loss: 2.0095\n",
      "Epoch [2/5], Step [6192/10336], Loss: 2.4425\n",
      "Epoch [2/5], Step [6194/10336], Loss: 0.8413\n",
      "Epoch [2/5], Step [6196/10336], Loss: 2.2557\n",
      "Epoch [2/5], Step [6198/10336], Loss: 1.5453\n",
      "Epoch [2/5], Step [6200/10336], Loss: 1.8344\n",
      "Epoch [2/5], Step [6202/10336], Loss: 0.1899\n",
      "Epoch [2/5], Step [6204/10336], Loss: 0.7083\n",
      "Epoch [2/5], Step [6206/10336], Loss: 0.1788\n",
      "Epoch [2/5], Step [6208/10336], Loss: 0.4811\n",
      "Epoch [2/5], Step [6210/10336], Loss: 0.2871\n",
      "Epoch [2/5], Step [6212/10336], Loss: 0.4366\n",
      "Epoch [2/5], Step [6214/10336], Loss: 0.0406\n",
      "Epoch [2/5], Step [6216/10336], Loss: 1.1414\n",
      "Epoch [2/5], Step [6218/10336], Loss: 0.6242\n",
      "Epoch [2/5], Step [6220/10336], Loss: 0.1595\n",
      "Epoch [2/5], Step [6222/10336], Loss: 1.4861\n",
      "Epoch [2/5], Step [6224/10336], Loss: 0.0334\n",
      "Epoch [2/5], Step [6226/10336], Loss: 1.9218\n",
      "Epoch [2/5], Step [6228/10336], Loss: 0.1229\n",
      "Epoch [2/5], Step [6230/10336], Loss: 1.1797\n",
      "Epoch [2/5], Step [6232/10336], Loss: 0.1981\n",
      "Epoch [2/5], Step [6234/10336], Loss: 5.3159\n",
      "Epoch [2/5], Step [6236/10336], Loss: 1.5018\n",
      "Epoch [2/5], Step [6238/10336], Loss: 1.1519\n",
      "Epoch [2/5], Step [6240/10336], Loss: 1.5662\n",
      "Epoch [2/5], Step [6242/10336], Loss: 1.3517\n",
      "Epoch [2/5], Step [6244/10336], Loss: 0.1701\n",
      "Epoch [2/5], Step [6246/10336], Loss: 0.6062\n",
      "Epoch [2/5], Step [6248/10336], Loss: 0.0965\n",
      "Epoch [2/5], Step [6250/10336], Loss: 2.1489\n",
      "Epoch [2/5], Step [6252/10336], Loss: 0.0458\n",
      "Epoch [2/5], Step [6254/10336], Loss: 0.6447\n",
      "Epoch [2/5], Step [6256/10336], Loss: 0.0105\n",
      "Epoch [2/5], Step [6258/10336], Loss: 1.3461\n",
      "Epoch [2/5], Step [6260/10336], Loss: 1.6867\n",
      "Epoch [2/5], Step [6262/10336], Loss: 0.0113\n",
      "Epoch [2/5], Step [6264/10336], Loss: 0.8318\n",
      "Epoch [2/5], Step [6266/10336], Loss: 1.8351\n",
      "Epoch [2/5], Step [6268/10336], Loss: 0.0597\n",
      "Epoch [2/5], Step [6270/10336], Loss: 0.5245\n",
      "Epoch [2/5], Step [6272/10336], Loss: 0.5395\n",
      "Epoch [2/5], Step [6274/10336], Loss: 0.0604\n",
      "Epoch [2/5], Step [6276/10336], Loss: 0.2124\n",
      "Epoch [2/5], Step [6278/10336], Loss: 0.3108\n",
      "Epoch [2/5], Step [6280/10336], Loss: 0.4753\n",
      "Epoch [2/5], Step [6282/10336], Loss: 1.2944\n",
      "Epoch [2/5], Step [6284/10336], Loss: 0.0104\n",
      "Epoch [2/5], Step [6286/10336], Loss: 0.1066\n",
      "Epoch [2/5], Step [6288/10336], Loss: 0.5202\n",
      "Epoch [2/5], Step [6290/10336], Loss: 0.2074\n",
      "Epoch [2/5], Step [6292/10336], Loss: 0.8269\n",
      "Epoch [2/5], Step [6294/10336], Loss: 1.4235\n",
      "Epoch [2/5], Step [6296/10336], Loss: 0.0377\n",
      "Epoch [2/5], Step [6298/10336], Loss: 1.3066\n",
      "Epoch [2/5], Step [6300/10336], Loss: 1.1768\n",
      "Epoch [2/5], Step [6302/10336], Loss: 0.2935\n",
      "Epoch [2/5], Step [6304/10336], Loss: 0.0352\n",
      "Epoch [2/5], Step [6306/10336], Loss: 0.0305\n",
      "Epoch [2/5], Step [6308/10336], Loss: 0.3305\n",
      "Epoch [2/5], Step [6310/10336], Loss: 0.0334\n",
      "Epoch [2/5], Step [6312/10336], Loss: 4.4582\n",
      "Epoch [2/5], Step [6314/10336], Loss: 0.2084\n",
      "Epoch [2/5], Step [6316/10336], Loss: 0.1719\n",
      "Epoch [2/5], Step [6318/10336], Loss: 0.1505\n",
      "Epoch [2/5], Step [6320/10336], Loss: 0.3408\n",
      "Epoch [2/5], Step [6322/10336], Loss: 1.6994\n",
      "Epoch [2/5], Step [6324/10336], Loss: 0.0825\n",
      "Epoch [2/5], Step [6326/10336], Loss: 2.6833\n",
      "Epoch [2/5], Step [6328/10336], Loss: 1.1650\n",
      "Epoch [2/5], Step [6330/10336], Loss: 0.6774\n",
      "Epoch [2/5], Step [6332/10336], Loss: 0.3241\n",
      "Epoch [2/5], Step [6334/10336], Loss: 0.7927\n",
      "Epoch [2/5], Step [6336/10336], Loss: 1.8282\n",
      "Epoch [2/5], Step [6338/10336], Loss: 1.7331\n",
      "Epoch [2/5], Step [6340/10336], Loss: 0.1991\n",
      "Epoch [2/5], Step [6342/10336], Loss: 1.6528\n",
      "Epoch [2/5], Step [6344/10336], Loss: 0.1779\n",
      "Epoch [2/5], Step [6346/10336], Loss: 1.9137\n",
      "Epoch [2/5], Step [6348/10336], Loss: 0.5246\n",
      "Epoch [2/5], Step [6350/10336], Loss: 0.6638\n",
      "Epoch [2/5], Step [6352/10336], Loss: 2.3318\n",
      "Epoch [2/5], Step [6354/10336], Loss: 0.0039\n",
      "Epoch [2/5], Step [6356/10336], Loss: 1.1003\n",
      "Epoch [2/5], Step [6358/10336], Loss: 0.7431\n",
      "Epoch [2/5], Step [6360/10336], Loss: 0.0451\n",
      "Epoch [2/5], Step [6362/10336], Loss: 1.2402\n",
      "Epoch [2/5], Step [6364/10336], Loss: 0.1079\n",
      "Epoch [2/5], Step [6366/10336], Loss: 1.7337\n",
      "Epoch [2/5], Step [6368/10336], Loss: 2.7620\n",
      "Epoch [2/5], Step [6370/10336], Loss: 2.4056\n",
      "Epoch [2/5], Step [6372/10336], Loss: 0.5258\n",
      "Epoch [2/5], Step [6374/10336], Loss: 0.0433\n",
      "Epoch [2/5], Step [6376/10336], Loss: 1.9449\n",
      "Epoch [2/5], Step [6378/10336], Loss: 0.1695\n",
      "Epoch [2/5], Step [6380/10336], Loss: 0.1012\n",
      "Epoch [2/5], Step [6382/10336], Loss: 0.0181\n",
      "Epoch [2/5], Step [6384/10336], Loss: 2.1480\n",
      "Epoch [2/5], Step [6386/10336], Loss: 3.4004\n",
      "Epoch [2/5], Step [6388/10336], Loss: 2.8221\n",
      "Epoch [2/5], Step [6390/10336], Loss: 0.5254\n",
      "Epoch [2/5], Step [6392/10336], Loss: 0.0040\n",
      "Epoch [2/5], Step [6394/10336], Loss: 0.3463\n",
      "Epoch [2/5], Step [6396/10336], Loss: 0.3262\n",
      "Epoch [2/5], Step [6398/10336], Loss: 0.0989\n",
      "Epoch [2/5], Step [6400/10336], Loss: 0.1903\n",
      "Epoch [2/5], Step [6402/10336], Loss: 0.3969\n",
      "Epoch [2/5], Step [6404/10336], Loss: 0.0216\n",
      "Epoch [2/5], Step [6406/10336], Loss: 0.0494\n",
      "Epoch [2/5], Step [6408/10336], Loss: 0.9645\n",
      "Epoch [2/5], Step [6410/10336], Loss: 0.0638\n",
      "Epoch [2/5], Step [6412/10336], Loss: 0.6447\n",
      "Epoch [2/5], Step [6414/10336], Loss: 0.8283\n",
      "Epoch [2/5], Step [6416/10336], Loss: 0.4671\n",
      "Epoch [2/5], Step [6418/10336], Loss: 3.5936\n",
      "Epoch [2/5], Step [6420/10336], Loss: 0.1245\n",
      "Epoch [2/5], Step [6422/10336], Loss: 0.0439\n",
      "Epoch [2/5], Step [6424/10336], Loss: 3.9687\n",
      "Epoch [2/5], Step [6426/10336], Loss: 1.0905\n",
      "Epoch [2/5], Step [6428/10336], Loss: 1.1568\n",
      "Epoch [2/5], Step [6430/10336], Loss: 0.5014\n",
      "Epoch [2/5], Step [6432/10336], Loss: 1.8312\n",
      "Epoch [2/5], Step [6434/10336], Loss: 1.4843\n",
      "Epoch [2/5], Step [6436/10336], Loss: 1.8558\n",
      "Epoch [2/5], Step [6438/10336], Loss: 0.1243\n",
      "Epoch [2/5], Step [6440/10336], Loss: 0.6733\n",
      "Epoch [2/5], Step [6442/10336], Loss: 0.8233\n",
      "Epoch [2/5], Step [6444/10336], Loss: 1.3664\n",
      "Epoch [2/5], Step [6446/10336], Loss: 0.6091\n",
      "Epoch [2/5], Step [6448/10336], Loss: 0.0491\n",
      "Epoch [2/5], Step [6450/10336], Loss: 0.4165\n",
      "Epoch [2/5], Step [6452/10336], Loss: 0.0351\n",
      "Epoch [2/5], Step [6454/10336], Loss: 0.0339\n",
      "Epoch [2/5], Step [6456/10336], Loss: 0.0356\n",
      "Epoch [2/5], Step [6458/10336], Loss: 2.7775\n",
      "Epoch [2/5], Step [6460/10336], Loss: 0.2180\n",
      "Epoch [2/5], Step [6462/10336], Loss: 0.2351\n",
      "Epoch [2/5], Step [6464/10336], Loss: 0.0023\n",
      "Epoch [2/5], Step [6466/10336], Loss: 0.0528\n",
      "Epoch [2/5], Step [6468/10336], Loss: 0.1574\n",
      "Epoch [2/5], Step [6470/10336], Loss: 0.4241\n",
      "Epoch [2/5], Step [6472/10336], Loss: 0.0099\n",
      "Epoch [2/5], Step [6474/10336], Loss: 0.2526\n",
      "Epoch [2/5], Step [6476/10336], Loss: 1.7418\n",
      "Epoch [2/5], Step [6478/10336], Loss: 3.6946\n",
      "Epoch [2/5], Step [6480/10336], Loss: 0.0015\n",
      "Epoch [2/5], Step [6482/10336], Loss: 0.1393\n",
      "Epoch [2/5], Step [6484/10336], Loss: 2.3447\n",
      "Epoch [2/5], Step [6486/10336], Loss: 0.3620\n",
      "Epoch [2/5], Step [6488/10336], Loss: 0.1341\n",
      "Epoch [2/5], Step [6490/10336], Loss: 3.6722\n",
      "Epoch [2/5], Step [6492/10336], Loss: 0.8272\n",
      "Epoch [2/5], Step [6494/10336], Loss: 1.0335\n",
      "Epoch [2/5], Step [6496/10336], Loss: 0.0700\n",
      "Epoch [2/5], Step [6498/10336], Loss: 1.7879\n",
      "Epoch [2/5], Step [6500/10336], Loss: 4.2213\n",
      "Epoch [2/5], Step [6502/10336], Loss: 0.2362\n",
      "Epoch [2/5], Step [6504/10336], Loss: 0.4300\n",
      "Epoch [2/5], Step [6506/10336], Loss: 2.3850\n",
      "Epoch [2/5], Step [6508/10336], Loss: 0.4655\n",
      "Epoch [2/5], Step [6510/10336], Loss: 4.1376\n",
      "Epoch [2/5], Step [6512/10336], Loss: 0.8939\n",
      "Epoch [2/5], Step [6514/10336], Loss: 0.9073\n",
      "Epoch [2/5], Step [6516/10336], Loss: 2.3906\n",
      "Epoch [2/5], Step [6518/10336], Loss: 0.0976\n",
      "Epoch [2/5], Step [6520/10336], Loss: 0.6278\n",
      "Epoch [2/5], Step [6522/10336], Loss: 2.0761\n",
      "Epoch [2/5], Step [6524/10336], Loss: 1.3975\n",
      "Epoch [2/5], Step [6526/10336], Loss: 1.6765\n",
      "Epoch [2/5], Step [6528/10336], Loss: 0.8426\n",
      "Epoch [2/5], Step [6530/10336], Loss: 0.0584\n",
      "Epoch [2/5], Step [6532/10336], Loss: 0.3216\n",
      "Epoch [2/5], Step [6534/10336], Loss: 1.1795\n",
      "Epoch [2/5], Step [6536/10336], Loss: 2.0127\n",
      "Epoch [2/5], Step [6538/10336], Loss: 1.0022\n",
      "Epoch [2/5], Step [6540/10336], Loss: 0.2738\n",
      "Epoch [2/5], Step [6542/10336], Loss: 2.6005\n",
      "Epoch [2/5], Step [6544/10336], Loss: 2.1107\n",
      "Epoch [2/5], Step [6546/10336], Loss: 1.4288\n",
      "Epoch [2/5], Step [6548/10336], Loss: 4.6483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [6550/10336], Loss: 0.2628\n",
      "Epoch [2/5], Step [6552/10336], Loss: 1.2803\n",
      "Epoch [2/5], Step [6554/10336], Loss: 0.6010\n",
      "Epoch [2/5], Step [6556/10336], Loss: 1.0234\n",
      "Epoch [2/5], Step [6558/10336], Loss: 0.2543\n",
      "Epoch [2/5], Step [6560/10336], Loss: 1.3393\n",
      "Epoch [2/5], Step [6562/10336], Loss: 1.5419\n",
      "Epoch [2/5], Step [6564/10336], Loss: 1.7810\n",
      "Epoch [2/5], Step [6566/10336], Loss: 0.8590\n",
      "Epoch [2/5], Step [6568/10336], Loss: 0.0090\n",
      "Epoch [2/5], Step [6570/10336], Loss: 0.7383\n",
      "Epoch [2/5], Step [6572/10336], Loss: 2.1209\n",
      "Epoch [2/5], Step [6574/10336], Loss: 0.1131\n",
      "Epoch [2/5], Step [6576/10336], Loss: 0.0176\n",
      "Epoch [2/5], Step [6578/10336], Loss: 0.0480\n",
      "Epoch [2/5], Step [6580/10336], Loss: 0.2936\n",
      "Epoch [2/5], Step [6582/10336], Loss: 0.6140\n",
      "Epoch [2/5], Step [6584/10336], Loss: 0.1274\n",
      "Epoch [2/5], Step [6586/10336], Loss: 1.2837\n",
      "Epoch [2/5], Step [6588/10336], Loss: 3.3790\n",
      "Epoch [2/5], Step [6590/10336], Loss: 0.4669\n",
      "Epoch [2/5], Step [6592/10336], Loss: 0.3873\n",
      "Epoch [2/5], Step [6594/10336], Loss: 0.0185\n",
      "Epoch [2/5], Step [6596/10336], Loss: 4.4670\n",
      "Epoch [2/5], Step [6598/10336], Loss: 2.7469\n",
      "Epoch [2/5], Step [6600/10336], Loss: 1.5426\n",
      "Epoch [2/5], Step [6602/10336], Loss: 1.0975\n",
      "Epoch [2/5], Step [6604/10336], Loss: 0.8210\n",
      "Epoch [2/5], Step [6606/10336], Loss: 0.9521\n",
      "Epoch [2/5], Step [6608/10336], Loss: 2.8996\n",
      "Epoch [2/5], Step [6610/10336], Loss: 0.0017\n",
      "Epoch [2/5], Step [6612/10336], Loss: 2.6364\n",
      "Epoch [2/5], Step [6614/10336], Loss: 0.9064\n",
      "Epoch [2/5], Step [6616/10336], Loss: 0.4514\n",
      "Epoch [2/5], Step [6618/10336], Loss: 0.0757\n",
      "Epoch [2/5], Step [6620/10336], Loss: 0.4773\n",
      "Epoch [2/5], Step [6622/10336], Loss: 2.3001\n",
      "Epoch [2/5], Step [6624/10336], Loss: 0.9062\n",
      "Epoch [2/5], Step [6626/10336], Loss: 1.6605\n",
      "Epoch [2/5], Step [6628/10336], Loss: 0.0764\n",
      "Epoch [2/5], Step [6630/10336], Loss: 0.9279\n",
      "Epoch [2/5], Step [6632/10336], Loss: 4.5539\n",
      "Epoch [2/5], Step [6634/10336], Loss: 2.2783\n",
      "Epoch [2/5], Step [6636/10336], Loss: 2.7565\n",
      "Epoch [2/5], Step [6638/10336], Loss: 0.0123\n",
      "Epoch [2/5], Step [6640/10336], Loss: 0.0379\n",
      "Epoch [2/5], Step [6642/10336], Loss: 0.1278\n",
      "Epoch [2/5], Step [6644/10336], Loss: 2.1426\n",
      "Epoch [2/5], Step [6646/10336], Loss: 4.2445\n",
      "Epoch [2/5], Step [6648/10336], Loss: 0.4899\n",
      "Epoch [2/5], Step [6650/10336], Loss: 1.2142\n",
      "Epoch [2/5], Step [6652/10336], Loss: 3.2268\n",
      "Epoch [2/5], Step [6654/10336], Loss: 1.4053\n",
      "Epoch [2/5], Step [6656/10336], Loss: 0.0158\n",
      "Epoch [2/5], Step [6658/10336], Loss: 2.5952\n",
      "Epoch [2/5], Step [6660/10336], Loss: 2.5661\n",
      "Epoch [2/5], Step [6662/10336], Loss: 0.1468\n",
      "Epoch [2/5], Step [6664/10336], Loss: 0.3002\n",
      "Epoch [2/5], Step [6666/10336], Loss: 0.0666\n",
      "Epoch [2/5], Step [6668/10336], Loss: 0.8127\n",
      "Epoch [2/5], Step [6670/10336], Loss: 2.3696\n",
      "Epoch [2/5], Step [6672/10336], Loss: 0.8504\n",
      "Epoch [2/5], Step [6674/10336], Loss: 0.2064\n",
      "Epoch [2/5], Step [6676/10336], Loss: 0.2925\n",
      "Epoch [2/5], Step [6678/10336], Loss: 0.2121\n",
      "Epoch [2/5], Step [6680/10336], Loss: 0.8374\n",
      "Epoch [2/5], Step [6682/10336], Loss: 0.7780\n",
      "Epoch [2/5], Step [6684/10336], Loss: 0.1889\n",
      "Epoch [2/5], Step [6686/10336], Loss: 0.0148\n",
      "Epoch [2/5], Step [6688/10336], Loss: 0.0625\n",
      "Epoch [2/5], Step [6690/10336], Loss: 0.7457\n",
      "Epoch [2/5], Step [6692/10336], Loss: 0.3167\n",
      "Epoch [2/5], Step [6694/10336], Loss: 1.3727\n",
      "Epoch [2/5], Step [6696/10336], Loss: 1.0604\n",
      "Epoch [2/5], Step [6698/10336], Loss: 0.3227\n",
      "Epoch [2/5], Step [6700/10336], Loss: 0.0846\n",
      "Epoch [2/5], Step [6702/10336], Loss: 1.5028\n",
      "Epoch [2/5], Step [6704/10336], Loss: 0.6440\n",
      "Epoch [2/5], Step [6706/10336], Loss: 0.0784\n",
      "Epoch [2/5], Step [6708/10336], Loss: 0.2730\n",
      "Epoch [2/5], Step [6710/10336], Loss: 0.0600\n",
      "Epoch [2/5], Step [6712/10336], Loss: 0.0190\n",
      "Epoch [2/5], Step [6714/10336], Loss: 0.5556\n",
      "Epoch [2/5], Step [6716/10336], Loss: 3.1218\n",
      "Epoch [2/5], Step [6718/10336], Loss: 1.7446\n",
      "Epoch [2/5], Step [6720/10336], Loss: 0.0317\n",
      "Epoch [2/5], Step [6722/10336], Loss: 0.3133\n",
      "Epoch [2/5], Step [6724/10336], Loss: 2.0875\n",
      "Epoch [2/5], Step [6726/10336], Loss: 0.1224\n",
      "Epoch [2/5], Step [6728/10336], Loss: 0.6534\n",
      "Epoch [2/5], Step [6730/10336], Loss: 1.0547\n",
      "Epoch [2/5], Step [6732/10336], Loss: 2.3860\n",
      "Epoch [2/5], Step [6734/10336], Loss: 0.0113\n",
      "Epoch [2/5], Step [6736/10336], Loss: 1.0712\n",
      "Epoch [2/5], Step [6738/10336], Loss: 2.0137\n",
      "Epoch [2/5], Step [6740/10336], Loss: 0.9658\n",
      "Epoch [2/5], Step [6742/10336], Loss: 0.3261\n",
      "Epoch [2/5], Step [6744/10336], Loss: 1.2485\n",
      "Epoch [2/5], Step [6746/10336], Loss: 0.8643\n",
      "Epoch [2/5], Step [6748/10336], Loss: 0.8256\n",
      "Epoch [2/5], Step [6750/10336], Loss: 0.4709\n",
      "Epoch [2/5], Step [6752/10336], Loss: 0.6574\n",
      "Epoch [2/5], Step [6754/10336], Loss: 1.8402\n",
      "Epoch [2/5], Step [6756/10336], Loss: 0.4604\n",
      "Epoch [2/5], Step [6758/10336], Loss: 0.0722\n",
      "Epoch [2/5], Step [6760/10336], Loss: 0.0450\n",
      "Epoch [2/5], Step [6762/10336], Loss: 0.9523\n",
      "Epoch [2/5], Step [6764/10336], Loss: 0.2036\n",
      "Epoch [2/5], Step [6766/10336], Loss: 0.4943\n",
      "Epoch [2/5], Step [6768/10336], Loss: 0.0522\n",
      "Epoch [2/5], Step [6770/10336], Loss: 0.0050\n",
      "Epoch [2/5], Step [6772/10336], Loss: 0.0075\n",
      "Epoch [2/5], Step [6774/10336], Loss: 0.0163\n",
      "Epoch [2/5], Step [6776/10336], Loss: 0.1388\n",
      "Epoch [2/5], Step [6778/10336], Loss: 0.0030\n",
      "Epoch [2/5], Step [6780/10336], Loss: 1.9317\n",
      "Epoch [2/5], Step [6782/10336], Loss: 0.3400\n",
      "Epoch [2/5], Step [6784/10336], Loss: 0.6951\n",
      "Epoch [2/5], Step [6786/10336], Loss: 0.6320\n",
      "Epoch [2/5], Step [6788/10336], Loss: 0.1080\n",
      "Epoch [2/5], Step [6790/10336], Loss: 0.1937\n",
      "Epoch [2/5], Step [6792/10336], Loss: 1.8177\n",
      "Epoch [2/5], Step [6794/10336], Loss: 0.6595\n",
      "Epoch [2/5], Step [6796/10336], Loss: 1.4868\n",
      "Epoch [2/5], Step [6798/10336], Loss: 0.1582\n",
      "Epoch [2/5], Step [6800/10336], Loss: 0.0180\n",
      "Epoch [2/5], Step [6802/10336], Loss: 0.8656\n",
      "Epoch [2/5], Step [6804/10336], Loss: 0.5711\n",
      "Epoch [2/5], Step [6806/10336], Loss: 2.4287\n",
      "Epoch [2/5], Step [6808/10336], Loss: 0.4473\n",
      "Epoch [2/5], Step [6810/10336], Loss: 3.3057\n",
      "Epoch [2/5], Step [6812/10336], Loss: 0.6740\n",
      "Epoch [2/5], Step [6814/10336], Loss: 0.2297\n",
      "Epoch [2/5], Step [6816/10336], Loss: 2.6880\n",
      "Epoch [2/5], Step [6818/10336], Loss: 0.0701\n",
      "Epoch [2/5], Step [6820/10336], Loss: 1.1148\n",
      "Epoch [2/5], Step [6822/10336], Loss: 0.4107\n",
      "Epoch [2/5], Step [6824/10336], Loss: 0.0227\n",
      "Epoch [2/5], Step [6826/10336], Loss: 0.4207\n",
      "Epoch [2/5], Step [6828/10336], Loss: 0.8809\n",
      "Epoch [2/5], Step [6830/10336], Loss: 1.4823\n",
      "Epoch [2/5], Step [6832/10336], Loss: 0.9527\n",
      "Epoch [2/5], Step [6834/10336], Loss: 0.5346\n",
      "Epoch [2/5], Step [6836/10336], Loss: 2.9211\n",
      "Epoch [2/5], Step [6838/10336], Loss: 1.4645\n",
      "Epoch [2/5], Step [6840/10336], Loss: 1.2604\n",
      "Epoch [2/5], Step [6842/10336], Loss: 0.8890\n",
      "Epoch [2/5], Step [6844/10336], Loss: 0.6314\n",
      "Epoch [2/5], Step [6846/10336], Loss: 0.1427\n",
      "Epoch [2/5], Step [6848/10336], Loss: 0.6613\n",
      "Epoch [2/5], Step [6850/10336], Loss: 0.0085\n",
      "Epoch [2/5], Step [6852/10336], Loss: 0.3845\n",
      "Epoch [2/5], Step [6854/10336], Loss: 4.1447\n",
      "Epoch [2/5], Step [6856/10336], Loss: 0.0996\n",
      "Epoch [2/5], Step [6858/10336], Loss: 0.3804\n",
      "Epoch [2/5], Step [6860/10336], Loss: 0.0767\n",
      "Epoch [2/5], Step [6862/10336], Loss: 2.2591\n",
      "Epoch [2/5], Step [6864/10336], Loss: 1.5298\n",
      "Epoch [2/5], Step [6866/10336], Loss: 1.0413\n",
      "Epoch [2/5], Step [6868/10336], Loss: 1.2773\n",
      "Epoch [2/5], Step [6870/10336], Loss: 0.0736\n",
      "Epoch [2/5], Step [6872/10336], Loss: 0.2218\n",
      "Epoch [2/5], Step [6874/10336], Loss: 0.0010\n",
      "Epoch [2/5], Step [6876/10336], Loss: 0.7484\n",
      "Epoch [2/5], Step [6878/10336], Loss: 2.6623\n",
      "Epoch [2/5], Step [6880/10336], Loss: 1.9829\n",
      "Epoch [2/5], Step [6882/10336], Loss: 2.3676\n",
      "Epoch [2/5], Step [6884/10336], Loss: 2.2643\n",
      "Epoch [2/5], Step [6886/10336], Loss: 1.9778\n",
      "Epoch [2/5], Step [6888/10336], Loss: 0.8522\n",
      "Epoch [2/5], Step [6890/10336], Loss: 0.0826\n",
      "Epoch [2/5], Step [6892/10336], Loss: 0.4713\n",
      "Epoch [2/5], Step [6894/10336], Loss: 1.7066\n",
      "Epoch [2/5], Step [6896/10336], Loss: 2.0582\n",
      "Epoch [2/5], Step [6898/10336], Loss: 1.1602\n",
      "Epoch [2/5], Step [6900/10336], Loss: 1.3883\n",
      "Epoch [2/5], Step [6902/10336], Loss: 0.8436\n",
      "Epoch [2/5], Step [6904/10336], Loss: 2.7251\n",
      "Epoch [2/5], Step [6906/10336], Loss: 0.0619\n",
      "Epoch [2/5], Step [6908/10336], Loss: 2.8318\n",
      "Epoch [2/5], Step [6910/10336], Loss: 0.7161\n",
      "Epoch [2/5], Step [6912/10336], Loss: 0.5182\n",
      "Epoch [2/5], Step [6914/10336], Loss: 0.0537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [6916/10336], Loss: 1.1814\n",
      "Epoch [2/5], Step [6918/10336], Loss: 0.1852\n",
      "Epoch [2/5], Step [6920/10336], Loss: 0.9725\n",
      "Epoch [2/5], Step [6922/10336], Loss: 0.0192\n",
      "Epoch [2/5], Step [6924/10336], Loss: 1.0799\n",
      "Epoch [2/5], Step [6926/10336], Loss: 0.1333\n",
      "Epoch [2/5], Step [6928/10336], Loss: 0.0626\n",
      "Epoch [2/5], Step [6930/10336], Loss: 0.0107\n",
      "Epoch [2/5], Step [6932/10336], Loss: 0.9741\n",
      "Epoch [2/5], Step [6934/10336], Loss: 0.0531\n",
      "Epoch [2/5], Step [6936/10336], Loss: 4.5302\n",
      "Epoch [2/5], Step [6938/10336], Loss: 2.4400\n",
      "Epoch [2/5], Step [6940/10336], Loss: 0.4853\n",
      "Epoch [2/5], Step [6942/10336], Loss: 0.3185\n",
      "Epoch [2/5], Step [6944/10336], Loss: 0.8353\n",
      "Epoch [2/5], Step [6946/10336], Loss: 0.1471\n",
      "Epoch [2/5], Step [6948/10336], Loss: 1.8973\n",
      "Epoch [2/5], Step [6950/10336], Loss: 0.0095\n",
      "Epoch [2/5], Step [6952/10336], Loss: 2.8019\n",
      "Epoch [2/5], Step [6954/10336], Loss: 0.6358\n",
      "Epoch [2/5], Step [6956/10336], Loss: 0.7309\n",
      "Epoch [2/5], Step [6958/10336], Loss: 0.0255\n",
      "Epoch [2/5], Step [6960/10336], Loss: 0.2430\n",
      "Epoch [2/5], Step [6962/10336], Loss: 0.0104\n",
      "Epoch [2/5], Step [6964/10336], Loss: 0.1494\n",
      "Epoch [2/5], Step [6966/10336], Loss: 1.3451\n",
      "Epoch [2/5], Step [6968/10336], Loss: 3.8920\n",
      "Epoch [2/5], Step [6970/10336], Loss: 0.0991\n",
      "Epoch [2/5], Step [6972/10336], Loss: 1.8700\n",
      "Epoch [2/5], Step [6974/10336], Loss: 1.4982\n",
      "Epoch [2/5], Step [6976/10336], Loss: 2.7062\n",
      "Epoch [2/5], Step [6978/10336], Loss: 0.5303\n",
      "Epoch [2/5], Step [6980/10336], Loss: 0.9479\n",
      "Epoch [2/5], Step [6982/10336], Loss: 1.7087\n",
      "Epoch [2/5], Step [6984/10336], Loss: 1.5411\n",
      "Epoch [2/5], Step [6986/10336], Loss: 0.0940\n",
      "Epoch [2/5], Step [6988/10336], Loss: 3.7790\n",
      "Epoch [2/5], Step [6990/10336], Loss: 0.1610\n",
      "Epoch [2/5], Step [6992/10336], Loss: 0.0665\n",
      "Epoch [2/5], Step [6994/10336], Loss: 3.0113\n",
      "Epoch [2/5], Step [6996/10336], Loss: 2.9518\n",
      "Epoch [2/5], Step [6998/10336], Loss: 1.9632\n",
      "Epoch [2/5], Step [7000/10336], Loss: 2.9010\n",
      "Epoch [2/5], Step [7002/10336], Loss: 0.1289\n",
      "Epoch [2/5], Step [7004/10336], Loss: 0.0221\n",
      "Epoch [2/5], Step [7006/10336], Loss: 0.6716\n",
      "Epoch [2/5], Step [7008/10336], Loss: 0.0987\n",
      "Epoch [2/5], Step [7010/10336], Loss: 1.0972\n",
      "Epoch [2/5], Step [7012/10336], Loss: 0.0778\n",
      "Epoch [2/5], Step [7014/10336], Loss: 0.2450\n",
      "Epoch [2/5], Step [7016/10336], Loss: 3.5778\n",
      "Epoch [2/5], Step [7018/10336], Loss: 0.5730\n",
      "Epoch [2/5], Step [7020/10336], Loss: 1.2732\n",
      "Epoch [2/5], Step [7022/10336], Loss: 1.2471\n",
      "Epoch [2/5], Step [7024/10336], Loss: 1.0571\n",
      "Epoch [2/5], Step [7026/10336], Loss: 0.6278\n",
      "Epoch [2/5], Step [7028/10336], Loss: 0.2031\n",
      "Epoch [2/5], Step [7030/10336], Loss: 0.8174\n",
      "Epoch [2/5], Step [7032/10336], Loss: 2.2646\n",
      "Epoch [2/5], Step [7034/10336], Loss: 0.3223\n",
      "Epoch [2/5], Step [7036/10336], Loss: 0.0029\n",
      "Epoch [2/5], Step [7038/10336], Loss: 0.7869\n",
      "Epoch [2/5], Step [7040/10336], Loss: 0.3859\n",
      "Epoch [2/5], Step [7042/10336], Loss: 0.2102\n",
      "Epoch [2/5], Step [7044/10336], Loss: 0.1059\n",
      "Epoch [2/5], Step [7046/10336], Loss: 0.4545\n",
      "Epoch [2/5], Step [7048/10336], Loss: 1.4143\n",
      "Epoch [2/5], Step [7050/10336], Loss: 0.3754\n",
      "Epoch [2/5], Step [7052/10336], Loss: 4.5919\n",
      "Epoch [2/5], Step [7054/10336], Loss: 0.1489\n",
      "Epoch [2/5], Step [7056/10336], Loss: 0.7370\n",
      "Epoch [2/5], Step [7058/10336], Loss: 0.3925\n",
      "Epoch [2/5], Step [7060/10336], Loss: 0.0889\n",
      "Epoch [2/5], Step [7062/10336], Loss: 1.8652\n",
      "Epoch [2/5], Step [7064/10336], Loss: 0.6464\n",
      "Epoch [2/5], Step [7066/10336], Loss: 1.7568\n",
      "Epoch [2/5], Step [7068/10336], Loss: 0.4972\n",
      "Epoch [2/5], Step [7070/10336], Loss: 0.0236\n",
      "Epoch [2/5], Step [7072/10336], Loss: 0.0429\n",
      "Epoch [2/5], Step [7074/10336], Loss: 1.0752\n",
      "Epoch [2/5], Step [7076/10336], Loss: 2.4940\n",
      "Epoch [2/5], Step [7078/10336], Loss: 2.1251\n",
      "Epoch [2/5], Step [7080/10336], Loss: 0.0019\n",
      "Epoch [2/5], Step [7082/10336], Loss: 0.6843\n",
      "Epoch [2/5], Step [7084/10336], Loss: 1.4905\n",
      "Epoch [2/5], Step [7086/10336], Loss: 0.9065\n",
      "Epoch [2/5], Step [7088/10336], Loss: 2.6514\n",
      "Epoch [2/5], Step [7090/10336], Loss: 0.0396\n",
      "Epoch [2/5], Step [7092/10336], Loss: 0.0097\n",
      "Epoch [2/5], Step [7094/10336], Loss: 1.2035\n",
      "Epoch [2/5], Step [7096/10336], Loss: 2.3398\n",
      "Epoch [2/5], Step [7098/10336], Loss: 0.2989\n",
      "Epoch [2/5], Step [7100/10336], Loss: 0.1881\n",
      "Epoch [2/5], Step [7102/10336], Loss: 0.4439\n",
      "Epoch [2/5], Step [7104/10336], Loss: 0.0492\n",
      "Epoch [2/5], Step [7106/10336], Loss: 1.6084\n",
      "Epoch [2/5], Step [7108/10336], Loss: 2.4266\n",
      "Epoch [2/5], Step [7110/10336], Loss: 0.6989\n",
      "Epoch [2/5], Step [7112/10336], Loss: 0.0697\n",
      "Epoch [2/5], Step [7114/10336], Loss: 0.0024\n",
      "Epoch [2/5], Step [7116/10336], Loss: 0.9595\n",
      "Epoch [2/5], Step [7118/10336], Loss: 0.0262\n",
      "Epoch [2/5], Step [7120/10336], Loss: 0.1716\n",
      "Epoch [2/5], Step [7122/10336], Loss: 1.0774\n",
      "Epoch [2/5], Step [7124/10336], Loss: 0.9315\n",
      "Epoch [2/5], Step [7126/10336], Loss: 1.4143\n",
      "Epoch [2/5], Step [7128/10336], Loss: 1.4013\n",
      "Epoch [2/5], Step [7130/10336], Loss: 1.6143\n",
      "Epoch [2/5], Step [7132/10336], Loss: 0.6821\n",
      "Epoch [2/5], Step [7134/10336], Loss: 0.2981\n",
      "Epoch [2/5], Step [7136/10336], Loss: 0.1539\n",
      "Epoch [2/5], Step [7138/10336], Loss: 1.5815\n",
      "Epoch [2/5], Step [7140/10336], Loss: 0.3486\n",
      "Epoch [2/5], Step [7142/10336], Loss: 0.4224\n",
      "Epoch [2/5], Step [7144/10336], Loss: 1.4417\n",
      "Epoch [2/5], Step [7146/10336], Loss: 0.0394\n",
      "Epoch [2/5], Step [7148/10336], Loss: 0.0341\n",
      "Epoch [2/5], Step [7150/10336], Loss: 2.6503\n",
      "Epoch [2/5], Step [7152/10336], Loss: 2.7709\n",
      "Epoch [2/5], Step [7154/10336], Loss: 0.1229\n",
      "Epoch [2/5], Step [7156/10336], Loss: 0.0746\n",
      "Epoch [2/5], Step [7158/10336], Loss: 1.7795\n",
      "Epoch [2/5], Step [7160/10336], Loss: 0.2064\n",
      "Epoch [2/5], Step [7162/10336], Loss: 0.0411\n",
      "Epoch [2/5], Step [7164/10336], Loss: 0.1294\n",
      "Epoch [2/5], Step [7166/10336], Loss: 1.3399\n",
      "Epoch [2/5], Step [7168/10336], Loss: 0.0481\n",
      "Epoch [2/5], Step [7170/10336], Loss: 2.5525\n",
      "Epoch [2/5], Step [7172/10336], Loss: 0.2135\n",
      "Epoch [2/5], Step [7174/10336], Loss: 0.3743\n",
      "Epoch [2/5], Step [7176/10336], Loss: 0.2578\n",
      "Epoch [2/5], Step [7178/10336], Loss: 1.4140\n",
      "Epoch [2/5], Step [7180/10336], Loss: 2.9189\n",
      "Epoch [2/5], Step [7182/10336], Loss: 1.4366\n",
      "Epoch [2/5], Step [7184/10336], Loss: 1.2623\n",
      "Epoch [2/5], Step [7186/10336], Loss: 0.0339\n",
      "Epoch [2/5], Step [7188/10336], Loss: 2.3126\n",
      "Epoch [2/5], Step [7190/10336], Loss: 1.2905\n",
      "Epoch [2/5], Step [7192/10336], Loss: 0.1356\n",
      "Epoch [2/5], Step [7194/10336], Loss: 0.0192\n",
      "Epoch [2/5], Step [7196/10336], Loss: 3.5582\n",
      "Epoch [2/5], Step [7198/10336], Loss: 2.6664\n",
      "Epoch [2/5], Step [7200/10336], Loss: 0.0792\n",
      "Epoch [2/5], Step [7202/10336], Loss: 0.1402\n",
      "Epoch [2/5], Step [7204/10336], Loss: 0.4274\n",
      "Epoch [2/5], Step [7206/10336], Loss: 1.1237\n",
      "Epoch [2/5], Step [7208/10336], Loss: 0.2913\n",
      "Epoch [2/5], Step [7210/10336], Loss: 0.3166\n",
      "Epoch [2/5], Step [7212/10336], Loss: 2.3585\n",
      "Epoch [2/5], Step [7214/10336], Loss: 1.0953\n",
      "Epoch [2/5], Step [7216/10336], Loss: 1.9976\n",
      "Epoch [2/5], Step [7218/10336], Loss: 1.8397\n",
      "Epoch [2/5], Step [7220/10336], Loss: 0.6640\n",
      "Epoch [2/5], Step [7222/10336], Loss: 1.5544\n",
      "Epoch [2/5], Step [7224/10336], Loss: 0.0499\n",
      "Epoch [2/5], Step [7226/10336], Loss: 0.9559\n",
      "Epoch [2/5], Step [7228/10336], Loss: 0.0066\n",
      "Epoch [2/5], Step [7230/10336], Loss: 0.0335\n",
      "Epoch [2/5], Step [7232/10336], Loss: 0.0504\n",
      "Epoch [2/5], Step [7234/10336], Loss: 0.8064\n",
      "Epoch [2/5], Step [7236/10336], Loss: 0.7685\n",
      "Epoch [2/5], Step [7238/10336], Loss: 0.7823\n",
      "Epoch [2/5], Step [7240/10336], Loss: 2.2919\n",
      "Epoch [2/5], Step [7242/10336], Loss: 1.5281\n",
      "Epoch [2/5], Step [7244/10336], Loss: 0.1484\n",
      "Epoch [2/5], Step [7246/10336], Loss: 0.4920\n",
      "Epoch [2/5], Step [7248/10336], Loss: 0.0221\n",
      "Epoch [2/5], Step [7250/10336], Loss: 1.7324\n",
      "Epoch [2/5], Step [7252/10336], Loss: 0.7677\n",
      "Epoch [2/5], Step [7254/10336], Loss: 0.4787\n",
      "Epoch [2/5], Step [7256/10336], Loss: 3.8857\n",
      "Epoch [2/5], Step [7258/10336], Loss: 0.0560\n",
      "Epoch [2/5], Step [7260/10336], Loss: 1.9027\n",
      "Epoch [2/5], Step [7262/10336], Loss: 0.2113\n",
      "Epoch [2/5], Step [7264/10336], Loss: 0.4705\n",
      "Epoch [2/5], Step [7266/10336], Loss: 1.7309\n",
      "Epoch [2/5], Step [7268/10336], Loss: 0.0366\n",
      "Epoch [2/5], Step [7270/10336], Loss: 0.0893\n",
      "Epoch [2/5], Step [7272/10336], Loss: 0.0614\n",
      "Epoch [2/5], Step [7274/10336], Loss: 1.4995\n",
      "Epoch [2/5], Step [7276/10336], Loss: 0.7263\n",
      "Epoch [2/5], Step [7278/10336], Loss: 1.2105\n",
      "Epoch [2/5], Step [7280/10336], Loss: 0.5216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [7282/10336], Loss: 0.2854\n",
      "Epoch [2/5], Step [7284/10336], Loss: 0.0370\n",
      "Epoch [2/5], Step [7286/10336], Loss: 0.0548\n",
      "Epoch [2/5], Step [7288/10336], Loss: 0.0085\n",
      "Epoch [2/5], Step [7290/10336], Loss: 1.3891\n",
      "Epoch [2/5], Step [7292/10336], Loss: 6.3643\n",
      "Epoch [2/5], Step [7294/10336], Loss: 1.8176\n",
      "Epoch [2/5], Step [7296/10336], Loss: 0.1975\n",
      "Epoch [2/5], Step [7298/10336], Loss: 1.8824\n",
      "Epoch [2/5], Step [7300/10336], Loss: 3.5703\n",
      "Epoch [2/5], Step [7302/10336], Loss: 0.4894\n",
      "Epoch [2/5], Step [7304/10336], Loss: 0.0891\n",
      "Epoch [2/5], Step [7306/10336], Loss: 1.3564\n",
      "Epoch [2/5], Step [7308/10336], Loss: 0.0343\n",
      "Epoch [2/5], Step [7310/10336], Loss: 0.0204\n",
      "Epoch [2/5], Step [7312/10336], Loss: 2.1796\n",
      "Epoch [2/5], Step [7314/10336], Loss: 0.0150\n",
      "Epoch [2/5], Step [7316/10336], Loss: 1.3290\n",
      "Epoch [2/5], Step [7318/10336], Loss: 0.9148\n",
      "Epoch [2/5], Step [7320/10336], Loss: 0.5825\n",
      "Epoch [2/5], Step [7322/10336], Loss: 0.7207\n",
      "Epoch [2/5], Step [7324/10336], Loss: 0.6640\n",
      "Epoch [2/5], Step [7326/10336], Loss: 0.9227\n",
      "Epoch [2/5], Step [7328/10336], Loss: 0.0140\n",
      "Epoch [2/5], Step [7330/10336], Loss: 1.5325\n",
      "Epoch [2/5], Step [7332/10336], Loss: 1.9208\n",
      "Epoch [2/5], Step [7334/10336], Loss: 4.0958\n",
      "Epoch [2/5], Step [7336/10336], Loss: 0.4821\n",
      "Epoch [2/5], Step [7338/10336], Loss: 0.2699\n",
      "Epoch [2/5], Step [7340/10336], Loss: 0.0386\n",
      "Epoch [2/5], Step [7342/10336], Loss: 2.2127\n",
      "Epoch [2/5], Step [7344/10336], Loss: 0.2380\n",
      "Epoch [2/5], Step [7346/10336], Loss: 1.5702\n",
      "Epoch [2/5], Step [7348/10336], Loss: 0.0674\n",
      "Epoch [2/5], Step [7350/10336], Loss: 0.7411\n",
      "Epoch [2/5], Step [7352/10336], Loss: 0.0605\n",
      "Epoch [2/5], Step [7354/10336], Loss: 0.1776\n",
      "Epoch [2/5], Step [7356/10336], Loss: 0.4830\n",
      "Epoch [2/5], Step [7358/10336], Loss: 1.0845\n",
      "Epoch [2/5], Step [7360/10336], Loss: 0.0124\n",
      "Epoch [2/5], Step [7362/10336], Loss: 1.1028\n",
      "Epoch [2/5], Step [7364/10336], Loss: 0.0435\n",
      "Epoch [2/5], Step [7366/10336], Loss: 0.0390\n",
      "Epoch [2/5], Step [7368/10336], Loss: 3.1654\n",
      "Epoch [2/5], Step [7370/10336], Loss: 0.2391\n",
      "Epoch [2/5], Step [7372/10336], Loss: 0.2774\n",
      "Epoch [2/5], Step [7374/10336], Loss: 0.1841\n",
      "Epoch [2/5], Step [7376/10336], Loss: 0.0475\n",
      "Epoch [2/5], Step [7378/10336], Loss: 0.0912\n",
      "Epoch [2/5], Step [7380/10336], Loss: 0.8217\n",
      "Epoch [2/5], Step [7382/10336], Loss: 3.6767\n",
      "Epoch [2/5], Step [7384/10336], Loss: 0.2120\n",
      "Epoch [2/5], Step [7386/10336], Loss: 1.4810\n",
      "Epoch [2/5], Step [7388/10336], Loss: 0.2606\n",
      "Epoch [2/5], Step [7390/10336], Loss: 0.0607\n",
      "Epoch [2/5], Step [7392/10336], Loss: 4.8292\n",
      "Epoch [2/5], Step [7394/10336], Loss: 0.7397\n",
      "Epoch [2/5], Step [7396/10336], Loss: 3.1594\n",
      "Epoch [2/5], Step [7398/10336], Loss: 0.2736\n",
      "Epoch [2/5], Step [7400/10336], Loss: 0.4556\n",
      "Epoch [2/5], Step [7402/10336], Loss: 1.3842\n",
      "Epoch [2/5], Step [7404/10336], Loss: 0.0165\n",
      "Epoch [2/5], Step [7406/10336], Loss: 0.0398\n",
      "Epoch [2/5], Step [7408/10336], Loss: 2.4936\n",
      "Epoch [2/5], Step [7410/10336], Loss: 3.5929\n",
      "Epoch [2/5], Step [7412/10336], Loss: 0.9461\n",
      "Epoch [2/5], Step [7414/10336], Loss: 0.0454\n",
      "Epoch [2/5], Step [7416/10336], Loss: 0.0691\n",
      "Epoch [2/5], Step [7418/10336], Loss: 0.5736\n",
      "Epoch [2/5], Step [7420/10336], Loss: 0.8561\n",
      "Epoch [2/5], Step [7422/10336], Loss: 0.1858\n",
      "Epoch [2/5], Step [7424/10336], Loss: 0.0579\n",
      "Epoch [2/5], Step [7426/10336], Loss: 1.2935\n",
      "Epoch [2/5], Step [7428/10336], Loss: 0.0891\n",
      "Epoch [2/5], Step [7430/10336], Loss: 1.2045\n",
      "Epoch [2/5], Step [7432/10336], Loss: 0.0629\n",
      "Epoch [2/5], Step [7434/10336], Loss: 0.7885\n",
      "Epoch [2/5], Step [7436/10336], Loss: 3.4914\n",
      "Epoch [2/5], Step [7438/10336], Loss: 0.0610\n",
      "Epoch [2/5], Step [7440/10336], Loss: 1.3114\n",
      "Epoch [2/5], Step [7442/10336], Loss: 0.1727\n",
      "Epoch [2/5], Step [7444/10336], Loss: 0.0618\n",
      "Epoch [2/5], Step [7446/10336], Loss: 0.4687\n",
      "Epoch [2/5], Step [7448/10336], Loss: 0.6401\n",
      "Epoch [2/5], Step [7450/10336], Loss: 4.1460\n",
      "Epoch [2/5], Step [7452/10336], Loss: 1.9738\n",
      "Epoch [2/5], Step [7454/10336], Loss: 0.0143\n",
      "Epoch [2/5], Step [7456/10336], Loss: 1.4591\n",
      "Epoch [2/5], Step [7458/10336], Loss: 0.7076\n",
      "Epoch [2/5], Step [7460/10336], Loss: 0.9763\n",
      "Epoch [2/5], Step [7462/10336], Loss: 0.0882\n",
      "Epoch [2/5], Step [7464/10336], Loss: 0.5963\n",
      "Epoch [2/5], Step [7466/10336], Loss: 0.8671\n",
      "Epoch [2/5], Step [7468/10336], Loss: 0.1893\n",
      "Epoch [2/5], Step [7470/10336], Loss: 0.5200\n",
      "Epoch [2/5], Step [7472/10336], Loss: 1.6207\n",
      "Epoch [2/5], Step [7474/10336], Loss: 2.6691\n",
      "Epoch [2/5], Step [7476/10336], Loss: 2.3886\n",
      "Epoch [2/5], Step [7478/10336], Loss: 0.0048\n",
      "Epoch [2/5], Step [7480/10336], Loss: 2.6104\n",
      "Epoch [2/5], Step [7482/10336], Loss: 0.0223\n",
      "Epoch [2/5], Step [7484/10336], Loss: 0.9281\n",
      "Epoch [2/5], Step [7486/10336], Loss: 0.7819\n",
      "Epoch [2/5], Step [7488/10336], Loss: 0.0733\n",
      "Epoch [2/5], Step [7490/10336], Loss: 0.1808\n",
      "Epoch [2/5], Step [7492/10336], Loss: 0.5545\n",
      "Epoch [2/5], Step [7494/10336], Loss: 1.3636\n",
      "Epoch [2/5], Step [7496/10336], Loss: 0.9401\n",
      "Epoch [2/5], Step [7498/10336], Loss: 0.1612\n",
      "Epoch [2/5], Step [7500/10336], Loss: 0.4225\n",
      "Epoch [2/5], Step [7502/10336], Loss: 0.0717\n",
      "Epoch [2/5], Step [7504/10336], Loss: 2.0425\n",
      "Epoch [2/5], Step [7506/10336], Loss: 0.0540\n",
      "Epoch [2/5], Step [7508/10336], Loss: 0.1907\n",
      "Epoch [2/5], Step [7510/10336], Loss: 0.3469\n",
      "Epoch [2/5], Step [7512/10336], Loss: 0.5715\n",
      "Epoch [2/5], Step [7514/10336], Loss: 3.1576\n",
      "Epoch [2/5], Step [7516/10336], Loss: 0.9026\n",
      "Epoch [2/5], Step [7518/10336], Loss: 1.0144\n",
      "Epoch [2/5], Step [7520/10336], Loss: 0.9508\n",
      "Epoch [2/5], Step [7522/10336], Loss: 0.0601\n",
      "Epoch [2/5], Step [7524/10336], Loss: 0.3935\n",
      "Epoch [2/5], Step [7526/10336], Loss: 4.8039\n",
      "Epoch [2/5], Step [7528/10336], Loss: 2.8751\n",
      "Epoch [2/5], Step [7530/10336], Loss: 0.0255\n",
      "Epoch [2/5], Step [7532/10336], Loss: 0.0147\n",
      "Epoch [2/5], Step [7534/10336], Loss: 4.2789\n",
      "Epoch [2/5], Step [7536/10336], Loss: 0.3717\n",
      "Epoch [2/5], Step [7538/10336], Loss: 0.0025\n",
      "Epoch [2/5], Step [7540/10336], Loss: 0.6556\n",
      "Epoch [2/5], Step [7542/10336], Loss: 0.0534\n",
      "Epoch [2/5], Step [7544/10336], Loss: 0.0026\n",
      "Epoch [2/5], Step [7546/10336], Loss: 1.0370\n",
      "Epoch [2/5], Step [7548/10336], Loss: 0.1275\n",
      "Epoch [2/5], Step [7550/10336], Loss: 3.0453\n",
      "Epoch [2/5], Step [7552/10336], Loss: 1.2504\n",
      "Epoch [2/5], Step [7554/10336], Loss: 0.3186\n",
      "Epoch [2/5], Step [7556/10336], Loss: 0.4774\n",
      "Epoch [2/5], Step [7558/10336], Loss: 1.6405\n",
      "Epoch [2/5], Step [7560/10336], Loss: 0.1049\n",
      "Epoch [2/5], Step [7562/10336], Loss: 1.1161\n",
      "Epoch [2/5], Step [7564/10336], Loss: 1.8391\n",
      "Epoch [2/5], Step [7566/10336], Loss: 2.4113\n",
      "Epoch [2/5], Step [7568/10336], Loss: 0.3026\n",
      "Epoch [2/5], Step [7570/10336], Loss: 2.5106\n",
      "Epoch [2/5], Step [7572/10336], Loss: 0.9472\n",
      "Epoch [2/5], Step [7574/10336], Loss: 0.1877\n",
      "Epoch [2/5], Step [7576/10336], Loss: 0.8622\n",
      "Epoch [2/5], Step [7578/10336], Loss: 0.0064\n",
      "Epoch [2/5], Step [7580/10336], Loss: 2.6648\n",
      "Epoch [2/5], Step [7582/10336], Loss: 4.5968\n",
      "Epoch [2/5], Step [7584/10336], Loss: 0.0436\n",
      "Epoch [2/5], Step [7586/10336], Loss: 0.3061\n",
      "Epoch [2/5], Step [7588/10336], Loss: 0.5475\n",
      "Epoch [2/5], Step [7590/10336], Loss: 0.9477\n",
      "Epoch [2/5], Step [7592/10336], Loss: 0.8799\n",
      "Epoch [2/5], Step [7594/10336], Loss: 1.0077\n",
      "Epoch [2/5], Step [7596/10336], Loss: 0.0688\n",
      "Epoch [2/5], Step [7598/10336], Loss: 0.2190\n",
      "Epoch [2/5], Step [7600/10336], Loss: 1.0199\n",
      "Epoch [2/5], Step [7602/10336], Loss: 2.0360\n",
      "Epoch [2/5], Step [7604/10336], Loss: 0.0162\n",
      "Epoch [2/5], Step [7606/10336], Loss: 0.6486\n",
      "Epoch [2/5], Step [7608/10336], Loss: 1.3281\n",
      "Epoch [2/5], Step [7610/10336], Loss: 1.0653\n",
      "Epoch [2/5], Step [7612/10336], Loss: 4.0834\n",
      "Epoch [2/5], Step [7614/10336], Loss: 1.4704\n",
      "Epoch [2/5], Step [7616/10336], Loss: 0.0538\n",
      "Epoch [2/5], Step [7618/10336], Loss: 0.9241\n",
      "Epoch [2/5], Step [7620/10336], Loss: 0.6483\n",
      "Epoch [2/5], Step [7622/10336], Loss: 0.1928\n",
      "Epoch [2/5], Step [7624/10336], Loss: 0.1754\n",
      "Epoch [2/5], Step [7626/10336], Loss: 0.2421\n",
      "Epoch [2/5], Step [7628/10336], Loss: 0.2522\n",
      "Epoch [2/5], Step [7630/10336], Loss: 2.9320\n",
      "Epoch [2/5], Step [7632/10336], Loss: 2.4058\n",
      "Epoch [2/5], Step [7634/10336], Loss: 1.1994\n",
      "Epoch [2/5], Step [7636/10336], Loss: 0.0722\n",
      "Epoch [2/5], Step [7638/10336], Loss: 0.9372\n",
      "Epoch [2/5], Step [7640/10336], Loss: 0.0157\n",
      "Epoch [2/5], Step [7642/10336], Loss: 0.0051\n",
      "Epoch [2/5], Step [7644/10336], Loss: 1.6392\n",
      "Epoch [2/5], Step [7646/10336], Loss: 2.7194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [7648/10336], Loss: 1.1917\n",
      "Epoch [2/5], Step [7650/10336], Loss: 0.2468\n",
      "Epoch [2/5], Step [7652/10336], Loss: 1.2198\n",
      "Epoch [2/5], Step [7654/10336], Loss: 0.1460\n",
      "Epoch [2/5], Step [7656/10336], Loss: 2.2090\n",
      "Epoch [2/5], Step [7658/10336], Loss: 0.0678\n",
      "Epoch [2/5], Step [7660/10336], Loss: 0.1750\n",
      "Epoch [2/5], Step [7662/10336], Loss: 1.6011\n",
      "Epoch [2/5], Step [7664/10336], Loss: 0.5744\n",
      "Epoch [2/5], Step [7666/10336], Loss: 2.5529\n",
      "Epoch [2/5], Step [7668/10336], Loss: 0.7867\n",
      "Epoch [2/5], Step [7670/10336], Loss: 2.3481\n",
      "Epoch [2/5], Step [7672/10336], Loss: 0.1088\n",
      "Epoch [2/5], Step [7674/10336], Loss: 0.9476\n",
      "Epoch [2/5], Step [7676/10336], Loss: 2.1360\n",
      "Epoch [2/5], Step [7678/10336], Loss: 0.4817\n",
      "Epoch [2/5], Step [7680/10336], Loss: 0.3256\n",
      "Epoch [2/5], Step [7682/10336], Loss: 1.5413\n",
      "Epoch [2/5], Step [7684/10336], Loss: 1.2044\n",
      "Epoch [2/5], Step [7686/10336], Loss: 1.1657\n",
      "Epoch [2/5], Step [7688/10336], Loss: 0.0078\n",
      "Epoch [2/5], Step [7690/10336], Loss: 1.7591\n",
      "Epoch [2/5], Step [7692/10336], Loss: 0.0025\n",
      "Epoch [2/5], Step [7694/10336], Loss: 1.4671\n",
      "Epoch [2/5], Step [7696/10336], Loss: 2.3900\n",
      "Epoch [2/5], Step [7698/10336], Loss: 4.0395\n",
      "Epoch [2/5], Step [7700/10336], Loss: 0.0881\n",
      "Epoch [2/5], Step [7702/10336], Loss: 1.3283\n",
      "Epoch [2/5], Step [7704/10336], Loss: 0.5996\n",
      "Epoch [2/5], Step [7706/10336], Loss: 0.2239\n",
      "Epoch [2/5], Step [7708/10336], Loss: 2.6066\n",
      "Epoch [2/5], Step [7710/10336], Loss: 1.0194\n",
      "Epoch [2/5], Step [7712/10336], Loss: 0.0280\n",
      "Epoch [2/5], Step [7714/10336], Loss: 2.1525\n",
      "Epoch [2/5], Step [7716/10336], Loss: 1.9200\n",
      "Epoch [2/5], Step [7718/10336], Loss: 1.6329\n",
      "Epoch [2/5], Step [7720/10336], Loss: 0.2567\n",
      "Epoch [2/5], Step [7722/10336], Loss: 0.7489\n",
      "Epoch [2/5], Step [7724/10336], Loss: 2.6665\n",
      "Epoch [2/5], Step [7726/10336], Loss: 0.8307\n",
      "Epoch [2/5], Step [7728/10336], Loss: 1.1692\n",
      "Epoch [2/5], Step [7730/10336], Loss: 0.5268\n",
      "Epoch [2/5], Step [7732/10336], Loss: 1.7193\n",
      "Epoch [2/5], Step [7734/10336], Loss: 0.3423\n",
      "Epoch [2/5], Step [7736/10336], Loss: 1.0442\n",
      "Epoch [2/5], Step [7738/10336], Loss: 0.1276\n",
      "Epoch [2/5], Step [7740/10336], Loss: 0.2223\n",
      "Epoch [2/5], Step [7742/10336], Loss: 0.3580\n",
      "Epoch [2/5], Step [7744/10336], Loss: 1.7465\n",
      "Epoch [2/5], Step [7746/10336], Loss: 0.0979\n",
      "Epoch [2/5], Step [7748/10336], Loss: 2.2564\n",
      "Epoch [2/5], Step [7750/10336], Loss: 0.3198\n",
      "Epoch [2/5], Step [7752/10336], Loss: 0.8965\n",
      "Epoch [2/5], Step [7754/10336], Loss: 0.3826\n",
      "Epoch [2/5], Step [7756/10336], Loss: 2.4769\n",
      "Epoch [2/5], Step [7758/10336], Loss: 0.1357\n",
      "Epoch [2/5], Step [7760/10336], Loss: 0.5154\n",
      "Epoch [2/5], Step [7762/10336], Loss: 1.2382\n",
      "Epoch [2/5], Step [7764/10336], Loss: 0.0959\n",
      "Epoch [2/5], Step [7766/10336], Loss: 1.2351\n",
      "Epoch [2/5], Step [7768/10336], Loss: 3.0985\n",
      "Epoch [2/5], Step [7770/10336], Loss: 1.7551\n",
      "Epoch [2/5], Step [7772/10336], Loss: 0.0263\n",
      "Epoch [2/5], Step [7774/10336], Loss: 0.0175\n",
      "Epoch [2/5], Step [7776/10336], Loss: 1.3163\n",
      "Epoch [2/5], Step [7778/10336], Loss: 0.7517\n",
      "Epoch [2/5], Step [7780/10336], Loss: 0.1058\n",
      "Epoch [2/5], Step [7782/10336], Loss: 0.8817\n",
      "Epoch [2/5], Step [7784/10336], Loss: 0.0079\n",
      "Epoch [2/5], Step [7786/10336], Loss: 0.4125\n",
      "Epoch [2/5], Step [7788/10336], Loss: 1.4938\n",
      "Epoch [2/5], Step [7790/10336], Loss: 1.4193\n",
      "Epoch [2/5], Step [7792/10336], Loss: 0.4529\n",
      "Epoch [2/5], Step [7794/10336], Loss: 0.0313\n",
      "Epoch [2/5], Step [7796/10336], Loss: 0.5412\n",
      "Epoch [2/5], Step [7798/10336], Loss: 0.5277\n",
      "Epoch [2/5], Step [7800/10336], Loss: 0.9352\n",
      "Epoch [2/5], Step [7802/10336], Loss: 0.6182\n",
      "Epoch [2/5], Step [7804/10336], Loss: 1.8710\n",
      "Epoch [2/5], Step [7806/10336], Loss: 1.2691\n",
      "Epoch [2/5], Step [7808/10336], Loss: 1.7005\n",
      "Epoch [2/5], Step [7810/10336], Loss: 1.6656\n",
      "Epoch [2/5], Step [7812/10336], Loss: 1.2508\n",
      "Epoch [2/5], Step [7814/10336], Loss: 0.0421\n",
      "Epoch [2/5], Step [7816/10336], Loss: 1.5935\n",
      "Epoch [2/5], Step [7818/10336], Loss: 1.8548\n",
      "Epoch [2/5], Step [7820/10336], Loss: 0.6126\n",
      "Epoch [2/5], Step [7822/10336], Loss: 1.1140\n",
      "Epoch [2/5], Step [7824/10336], Loss: 0.3388\n",
      "Epoch [2/5], Step [7826/10336], Loss: 0.0924\n",
      "Epoch [2/5], Step [7828/10336], Loss: 1.4945\n",
      "Epoch [2/5], Step [7830/10336], Loss: 0.2497\n",
      "Epoch [2/5], Step [7832/10336], Loss: 4.2962\n",
      "Epoch [2/5], Step [7834/10336], Loss: 0.7169\n",
      "Epoch [2/5], Step [7836/10336], Loss: 0.1197\n",
      "Epoch [2/5], Step [7838/10336], Loss: 0.0815\n",
      "Epoch [2/5], Step [7840/10336], Loss: 0.1043\n",
      "Epoch [2/5], Step [7842/10336], Loss: 1.6629\n",
      "Epoch [2/5], Step [7844/10336], Loss: 1.3942\n",
      "Epoch [2/5], Step [7846/10336], Loss: 2.4264\n",
      "Epoch [2/5], Step [7848/10336], Loss: 1.2838\n",
      "Epoch [2/5], Step [7850/10336], Loss: 1.9255\n",
      "Epoch [2/5], Step [7852/10336], Loss: 0.0839\n",
      "Epoch [2/5], Step [7854/10336], Loss: 1.6615\n",
      "Epoch [2/5], Step [7856/10336], Loss: 0.0185\n",
      "Epoch [2/5], Step [7858/10336], Loss: 1.8310\n",
      "Epoch [2/5], Step [7860/10336], Loss: 1.0034\n",
      "Epoch [2/5], Step [7862/10336], Loss: 1.6988\n",
      "Epoch [2/5], Step [7864/10336], Loss: 3.1885\n",
      "Epoch [2/5], Step [7866/10336], Loss: 0.0891\n",
      "Epoch [2/5], Step [7868/10336], Loss: 1.3907\n",
      "Epoch [2/5], Step [7870/10336], Loss: 0.6487\n",
      "Epoch [2/5], Step [7872/10336], Loss: 0.0668\n",
      "Epoch [2/5], Step [7874/10336], Loss: 0.7826\n",
      "Epoch [2/5], Step [7876/10336], Loss: 0.0504\n",
      "Epoch [2/5], Step [7878/10336], Loss: 0.0170\n",
      "Epoch [2/5], Step [7880/10336], Loss: 0.5033\n",
      "Epoch [2/5], Step [7882/10336], Loss: 6.2055\n",
      "Epoch [2/5], Step [7884/10336], Loss: 1.2884\n",
      "Epoch [2/5], Step [7886/10336], Loss: 1.5117\n",
      "Epoch [2/5], Step [7888/10336], Loss: 0.0641\n",
      "Epoch [2/5], Step [7890/10336], Loss: 0.0151\n",
      "Epoch [2/5], Step [7892/10336], Loss: 0.1976\n",
      "Epoch [2/5], Step [7894/10336], Loss: 0.4355\n",
      "Epoch [2/5], Step [7896/10336], Loss: 1.2729\n",
      "Epoch [2/5], Step [7898/10336], Loss: 0.0155\n",
      "Epoch [2/5], Step [7900/10336], Loss: 0.1725\n",
      "Epoch [2/5], Step [7902/10336], Loss: 2.5638\n",
      "Epoch [2/5], Step [7904/10336], Loss: 1.4628\n",
      "Epoch [2/5], Step [7906/10336], Loss: 1.9114\n",
      "Epoch [2/5], Step [7908/10336], Loss: 0.3122\n",
      "Epoch [2/5], Step [7910/10336], Loss: 1.4271\n",
      "Epoch [2/5], Step [7912/10336], Loss: 0.7320\n",
      "Epoch [2/5], Step [7914/10336], Loss: 0.8567\n",
      "Epoch [2/5], Step [7916/10336], Loss: 0.0800\n",
      "Epoch [2/5], Step [7918/10336], Loss: 0.1459\n",
      "Epoch [2/5], Step [7920/10336], Loss: 3.6470\n",
      "Epoch [2/5], Step [7922/10336], Loss: 2.4042\n",
      "Epoch [2/5], Step [7924/10336], Loss: 0.0207\n",
      "Epoch [2/5], Step [7926/10336], Loss: 0.9359\n",
      "Epoch [2/5], Step [7928/10336], Loss: 2.3256\n",
      "Epoch [2/5], Step [7930/10336], Loss: 0.2403\n",
      "Epoch [2/5], Step [7932/10336], Loss: 0.2251\n",
      "Epoch [2/5], Step [7934/10336], Loss: 0.0143\n",
      "Epoch [2/5], Step [7936/10336], Loss: 0.5288\n",
      "Epoch [2/5], Step [7938/10336], Loss: 2.0796\n",
      "Epoch [2/5], Step [7940/10336], Loss: 0.6986\n",
      "Epoch [2/5], Step [7942/10336], Loss: 0.6254\n",
      "Epoch [2/5], Step [7944/10336], Loss: 0.0103\n",
      "Epoch [2/5], Step [7946/10336], Loss: 2.2307\n",
      "Epoch [2/5], Step [7948/10336], Loss: 1.1371\n",
      "Epoch [2/5], Step [7950/10336], Loss: 0.1308\n",
      "Epoch [2/5], Step [7952/10336], Loss: 2.0695\n",
      "Epoch [2/5], Step [7954/10336], Loss: 0.1152\n",
      "Epoch [2/5], Step [7956/10336], Loss: 0.0023\n",
      "Epoch [2/5], Step [7958/10336], Loss: 1.4435\n",
      "Epoch [2/5], Step [7960/10336], Loss: 0.6060\n",
      "Epoch [2/5], Step [7962/10336], Loss: 2.7211\n",
      "Epoch [2/5], Step [7964/10336], Loss: 4.3214\n",
      "Epoch [2/5], Step [7966/10336], Loss: 1.5375\n",
      "Epoch [2/5], Step [7968/10336], Loss: 0.8841\n",
      "Epoch [2/5], Step [7970/10336], Loss: 0.4400\n",
      "Epoch [2/5], Step [7972/10336], Loss: 0.5215\n",
      "Epoch [2/5], Step [7974/10336], Loss: 2.1849\n",
      "Epoch [2/5], Step [7976/10336], Loss: 0.8132\n",
      "Epoch [2/5], Step [7978/10336], Loss: 0.0876\n",
      "Epoch [2/5], Step [7980/10336], Loss: 2.5298\n",
      "Epoch [2/5], Step [7982/10336], Loss: 0.2457\n",
      "Epoch [2/5], Step [7984/10336], Loss: 0.3178\n",
      "Epoch [2/5], Step [7986/10336], Loss: 0.0520\n",
      "Epoch [2/5], Step [7988/10336], Loss: 1.2976\n",
      "Epoch [2/5], Step [7990/10336], Loss: 0.1296\n",
      "Epoch [2/5], Step [7992/10336], Loss: 0.3697\n",
      "Epoch [2/5], Step [7994/10336], Loss: 1.3333\n",
      "Epoch [2/5], Step [7996/10336], Loss: 0.1614\n",
      "Epoch [2/5], Step [7998/10336], Loss: 0.4859\n",
      "Epoch [2/5], Step [8000/10336], Loss: 3.0281\n",
      "Epoch [2/5], Step [8002/10336], Loss: 0.0377\n",
      "Epoch [2/5], Step [8004/10336], Loss: 1.0254\n",
      "Epoch [2/5], Step [8006/10336], Loss: 1.6683\n",
      "Epoch [2/5], Step [8008/10336], Loss: 0.0011\n",
      "Epoch [2/5], Step [8010/10336], Loss: 0.2909\n",
      "Epoch [2/5], Step [8012/10336], Loss: 1.1452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [8014/10336], Loss: 1.1085\n",
      "Epoch [2/5], Step [8016/10336], Loss: 1.7540\n",
      "Epoch [2/5], Step [8018/10336], Loss: 0.1212\n",
      "Epoch [2/5], Step [8020/10336], Loss: 0.1537\n",
      "Epoch [2/5], Step [8022/10336], Loss: 0.4342\n",
      "Epoch [2/5], Step [8024/10336], Loss: 0.5516\n",
      "Epoch [2/5], Step [8026/10336], Loss: 1.3738\n",
      "Epoch [2/5], Step [8028/10336], Loss: 0.1293\n",
      "Epoch [2/5], Step [8030/10336], Loss: 0.2090\n",
      "Epoch [2/5], Step [8032/10336], Loss: 0.7152\n",
      "Epoch [2/5], Step [8034/10336], Loss: 0.0677\n",
      "Epoch [2/5], Step [8036/10336], Loss: 1.1335\n",
      "Epoch [2/5], Step [8038/10336], Loss: 0.3406\n",
      "Epoch [2/5], Step [8040/10336], Loss: 0.0331\n",
      "Epoch [2/5], Step [8042/10336], Loss: 1.7515\n",
      "Epoch [2/5], Step [8044/10336], Loss: 0.9764\n",
      "Epoch [2/5], Step [8046/10336], Loss: 0.0925\n",
      "Epoch [2/5], Step [8048/10336], Loss: 0.0052\n",
      "Epoch [2/5], Step [8050/10336], Loss: 0.0578\n",
      "Epoch [2/5], Step [8052/10336], Loss: 0.2665\n",
      "Epoch [2/5], Step [8054/10336], Loss: 0.7132\n",
      "Epoch [2/5], Step [8056/10336], Loss: 0.6340\n",
      "Epoch [2/5], Step [8058/10336], Loss: 0.0247\n",
      "Epoch [2/5], Step [8060/10336], Loss: 0.3226\n",
      "Epoch [2/5], Step [8062/10336], Loss: 0.0023\n",
      "Epoch [2/5], Step [8064/10336], Loss: 0.1989\n",
      "Epoch [2/5], Step [8066/10336], Loss: 2.2896\n",
      "Epoch [2/5], Step [8068/10336], Loss: 1.0085\n",
      "Epoch [2/5], Step [8070/10336], Loss: 0.6821\n",
      "Epoch [2/5], Step [8072/10336], Loss: 1.2887\n",
      "Epoch [2/5], Step [8074/10336], Loss: 5.0679\n",
      "Epoch [2/5], Step [8076/10336], Loss: 1.5089\n",
      "Epoch [2/5], Step [8078/10336], Loss: 1.2455\n",
      "Epoch [2/5], Step [8080/10336], Loss: 1.1089\n",
      "Epoch [2/5], Step [8082/10336], Loss: 0.0356\n",
      "Epoch [2/5], Step [8084/10336], Loss: 0.5032\n",
      "Epoch [2/5], Step [8086/10336], Loss: 2.6112\n",
      "Epoch [2/5], Step [8088/10336], Loss: 0.9816\n",
      "Epoch [2/5], Step [8090/10336], Loss: 0.3231\n",
      "Epoch [2/5], Step [8092/10336], Loss: 0.0198\n",
      "Epoch [2/5], Step [8094/10336], Loss: 0.0159\n",
      "Epoch [2/5], Step [8096/10336], Loss: 0.4055\n",
      "Epoch [2/5], Step [8098/10336], Loss: 0.0048\n",
      "Epoch [2/5], Step [8100/10336], Loss: 0.8744\n",
      "Epoch [2/5], Step [8102/10336], Loss: 1.4469\n",
      "Epoch [2/5], Step [8104/10336], Loss: 0.1508\n",
      "Epoch [2/5], Step [8106/10336], Loss: 0.6441\n",
      "Epoch [2/5], Step [8108/10336], Loss: 0.2119\n",
      "Epoch [2/5], Step [8110/10336], Loss: 0.7004\n",
      "Epoch [2/5], Step [8112/10336], Loss: 0.3451\n",
      "Epoch [2/5], Step [8114/10336], Loss: 0.5543\n",
      "Epoch [2/5], Step [8116/10336], Loss: 0.2872\n",
      "Epoch [2/5], Step [8118/10336], Loss: 2.1382\n",
      "Epoch [2/5], Step [8120/10336], Loss: 2.5902\n",
      "Epoch [2/5], Step [8122/10336], Loss: 0.3951\n",
      "Epoch [2/5], Step [8124/10336], Loss: 0.6132\n",
      "Epoch [2/5], Step [8126/10336], Loss: 0.5373\n",
      "Epoch [2/5], Step [8128/10336], Loss: 1.8327\n",
      "Epoch [2/5], Step [8130/10336], Loss: 1.9850\n",
      "Epoch [2/5], Step [8132/10336], Loss: 0.6625\n",
      "Epoch [2/5], Step [8134/10336], Loss: 0.8579\n",
      "Epoch [2/5], Step [8136/10336], Loss: 0.1262\n",
      "Epoch [2/5], Step [8138/10336], Loss: 1.9561\n",
      "Epoch [2/5], Step [8140/10336], Loss: 0.4545\n",
      "Epoch [2/5], Step [8142/10336], Loss: 0.3924\n",
      "Epoch [2/5], Step [8144/10336], Loss: 0.9841\n",
      "Epoch [2/5], Step [8146/10336], Loss: 2.7323\n",
      "Epoch [2/5], Step [8148/10336], Loss: 0.7627\n",
      "Epoch [2/5], Step [8150/10336], Loss: 0.3132\n",
      "Epoch [2/5], Step [8152/10336], Loss: 0.4520\n",
      "Epoch [2/5], Step [8154/10336], Loss: 0.9700\n",
      "Epoch [2/5], Step [8156/10336], Loss: 0.5950\n",
      "Epoch [2/5], Step [8158/10336], Loss: 1.8695\n",
      "Epoch [2/5], Step [8160/10336], Loss: 0.8578\n",
      "Epoch [2/5], Step [8162/10336], Loss: 0.2341\n",
      "Epoch [2/5], Step [8164/10336], Loss: 0.5411\n",
      "Epoch [2/5], Step [8166/10336], Loss: 0.4077\n",
      "Epoch [2/5], Step [8168/10336], Loss: 2.0991\n",
      "Epoch [2/5], Step [8170/10336], Loss: 1.2403\n",
      "Epoch [2/5], Step [8172/10336], Loss: 4.0584\n",
      "Epoch [2/5], Step [8174/10336], Loss: 2.4352\n",
      "Epoch [2/5], Step [8176/10336], Loss: 1.2243\n",
      "Epoch [2/5], Step [8178/10336], Loss: 0.3505\n",
      "Epoch [2/5], Step [8180/10336], Loss: 2.3057\n",
      "Epoch [2/5], Step [8182/10336], Loss: 1.3693\n",
      "Epoch [2/5], Step [8184/10336], Loss: 2.4576\n",
      "Epoch [2/5], Step [8186/10336], Loss: 1.1472\n",
      "Epoch [2/5], Step [8188/10336], Loss: 0.3487\n",
      "Epoch [2/5], Step [8190/10336], Loss: 2.0723\n",
      "Epoch [2/5], Step [8192/10336], Loss: 0.6104\n",
      "Epoch [2/5], Step [8194/10336], Loss: 1.2686\n",
      "Epoch [2/5], Step [8196/10336], Loss: 2.6109\n",
      "Epoch [2/5], Step [8198/10336], Loss: 0.1181\n",
      "Epoch [2/5], Step [8200/10336], Loss: 0.2872\n",
      "Epoch [2/5], Step [8202/10336], Loss: 0.0088\n",
      "Epoch [2/5], Step [8204/10336], Loss: 1.1627\n",
      "Epoch [2/5], Step [8206/10336], Loss: 1.3687\n",
      "Epoch [2/5], Step [8208/10336], Loss: 1.3305\n",
      "Epoch [2/5], Step [8210/10336], Loss: 1.0829\n",
      "Epoch [2/5], Step [8212/10336], Loss: 2.0936\n",
      "Epoch [2/5], Step [8214/10336], Loss: 2.1731\n",
      "Epoch [2/5], Step [8216/10336], Loss: 0.2746\n",
      "Epoch [2/5], Step [8218/10336], Loss: 0.0719\n",
      "Epoch [2/5], Step [8220/10336], Loss: 0.3620\n",
      "Epoch [2/5], Step [8222/10336], Loss: 0.0177\n",
      "Epoch [2/5], Step [8224/10336], Loss: 1.3864\n",
      "Epoch [2/5], Step [8226/10336], Loss: 0.0798\n",
      "Epoch [2/5], Step [8228/10336], Loss: 0.0670\n",
      "Epoch [2/5], Step [8230/10336], Loss: 1.0290\n",
      "Epoch [2/5], Step [8232/10336], Loss: 0.9322\n",
      "Epoch [2/5], Step [8234/10336], Loss: 0.2102\n",
      "Epoch [2/5], Step [8236/10336], Loss: 0.0698\n",
      "Epoch [2/5], Step [8238/10336], Loss: 1.3768\n",
      "Epoch [2/5], Step [8240/10336], Loss: 0.3936\n",
      "Epoch [2/5], Step [8242/10336], Loss: 1.0565\n",
      "Epoch [2/5], Step [8244/10336], Loss: 0.1198\n",
      "Epoch [2/5], Step [8246/10336], Loss: 0.0338\n",
      "Epoch [2/5], Step [8248/10336], Loss: 1.0215\n",
      "Epoch [2/5], Step [8250/10336], Loss: 2.7288\n",
      "Epoch [2/5], Step [8252/10336], Loss: 0.0455\n",
      "Epoch [2/5], Step [8254/10336], Loss: 0.6553\n",
      "Epoch [2/5], Step [8256/10336], Loss: 1.0193\n",
      "Epoch [2/5], Step [8258/10336], Loss: 0.4235\n",
      "Epoch [2/5], Step [8260/10336], Loss: 0.0153\n",
      "Epoch [2/5], Step [8262/10336], Loss: 0.7284\n",
      "Epoch [2/5], Step [8264/10336], Loss: 2.6910\n",
      "Epoch [2/5], Step [8266/10336], Loss: 0.0502\n",
      "Epoch [2/5], Step [8268/10336], Loss: 0.1623\n",
      "Epoch [2/5], Step [8270/10336], Loss: 0.4040\n",
      "Epoch [2/5], Step [8272/10336], Loss: 0.1966\n",
      "Epoch [2/5], Step [8274/10336], Loss: 0.6421\n",
      "Epoch [2/5], Step [8276/10336], Loss: 0.5161\n",
      "Epoch [2/5], Step [8278/10336], Loss: 0.4406\n",
      "Epoch [2/5], Step [8280/10336], Loss: 2.9823\n",
      "Epoch [2/5], Step [8282/10336], Loss: 7.0069\n",
      "Epoch [2/5], Step [8284/10336], Loss: 0.0153\n",
      "Epoch [2/5], Step [8286/10336], Loss: 0.1702\n",
      "Epoch [2/5], Step [8288/10336], Loss: 0.4954\n",
      "Epoch [2/5], Step [8290/10336], Loss: 0.0207\n",
      "Epoch [2/5], Step [8292/10336], Loss: 0.4480\n",
      "Epoch [2/5], Step [8294/10336], Loss: 0.0404\n",
      "Epoch [2/5], Step [8296/10336], Loss: 0.0217\n",
      "Epoch [2/5], Step [8298/10336], Loss: 0.3392\n",
      "Epoch [2/5], Step [8300/10336], Loss: 2.5830\n",
      "Epoch [2/5], Step [8302/10336], Loss: 0.0042\n",
      "Epoch [2/5], Step [8304/10336], Loss: 1.2839\n",
      "Epoch [2/5], Step [8306/10336], Loss: 0.6292\n",
      "Epoch [2/5], Step [8308/10336], Loss: 1.2729\n",
      "Epoch [2/5], Step [8310/10336], Loss: 1.8772\n",
      "Epoch [2/5], Step [8312/10336], Loss: 0.3350\n",
      "Epoch [2/5], Step [8314/10336], Loss: 0.4703\n",
      "Epoch [2/5], Step [8316/10336], Loss: 0.9980\n",
      "Epoch [2/5], Step [8318/10336], Loss: 0.0431\n",
      "Epoch [2/5], Step [8320/10336], Loss: 1.5989\n",
      "Epoch [2/5], Step [8322/10336], Loss: 0.0071\n",
      "Epoch [2/5], Step [8324/10336], Loss: 0.0288\n",
      "Epoch [2/5], Step [8326/10336], Loss: 0.0296\n",
      "Epoch [2/5], Step [8328/10336], Loss: 1.2781\n",
      "Epoch [2/5], Step [8330/10336], Loss: 0.2452\n",
      "Epoch [2/5], Step [8332/10336], Loss: 0.0711\n",
      "Epoch [2/5], Step [8334/10336], Loss: 2.0776\n",
      "Epoch [2/5], Step [8336/10336], Loss: 0.1092\n",
      "Epoch [2/5], Step [8338/10336], Loss: 1.1132\n",
      "Epoch [2/5], Step [8340/10336], Loss: 0.0015\n",
      "Epoch [2/5], Step [8342/10336], Loss: 3.1064\n",
      "Epoch [2/5], Step [8344/10336], Loss: 0.0391\n",
      "Epoch [2/5], Step [8346/10336], Loss: 1.6099\n",
      "Epoch [2/5], Step [8348/10336], Loss: 1.8960\n",
      "Epoch [2/5], Step [8350/10336], Loss: 0.1955\n",
      "Epoch [2/5], Step [8352/10336], Loss: 0.0371\n",
      "Epoch [2/5], Step [8354/10336], Loss: 1.6545\n",
      "Epoch [2/5], Step [8356/10336], Loss: 0.1705\n",
      "Epoch [2/5], Step [8358/10336], Loss: 0.0667\n",
      "Epoch [2/5], Step [8360/10336], Loss: 2.1473\n",
      "Epoch [2/5], Step [8362/10336], Loss: 0.1265\n",
      "Epoch [2/5], Step [8364/10336], Loss: 1.1228\n",
      "Epoch [2/5], Step [8366/10336], Loss: 2.1773\n",
      "Epoch [2/5], Step [8368/10336], Loss: 0.0411\n",
      "Epoch [2/5], Step [8370/10336], Loss: 0.3398\n",
      "Epoch [2/5], Step [8372/10336], Loss: 0.0521\n",
      "Epoch [2/5], Step [8374/10336], Loss: 0.0708\n",
      "Epoch [2/5], Step [8376/10336], Loss: 1.5471\n",
      "Epoch [2/5], Step [8378/10336], Loss: 0.1897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [8380/10336], Loss: 0.5576\n",
      "Epoch [2/5], Step [8382/10336], Loss: 0.0043\n",
      "Epoch [2/5], Step [8384/10336], Loss: 0.2798\n",
      "Epoch [2/5], Step [8386/10336], Loss: 3.4412\n",
      "Epoch [2/5], Step [8388/10336], Loss: 2.3536\n",
      "Epoch [2/5], Step [8390/10336], Loss: 0.5628\n",
      "Epoch [2/5], Step [8392/10336], Loss: 0.7683\n",
      "Epoch [2/5], Step [8394/10336], Loss: 0.6884\n",
      "Epoch [2/5], Step [8396/10336], Loss: 1.7902\n",
      "Epoch [2/5], Step [8398/10336], Loss: 0.3550\n",
      "Epoch [2/5], Step [8400/10336], Loss: 2.0741\n",
      "Epoch [2/5], Step [8402/10336], Loss: 1.3314\n",
      "Epoch [2/5], Step [8404/10336], Loss: 0.2678\n",
      "Epoch [2/5], Step [8406/10336], Loss: 0.6730\n",
      "Epoch [2/5], Step [8408/10336], Loss: 3.6793\n",
      "Epoch [2/5], Step [8410/10336], Loss: 0.5913\n",
      "Epoch [2/5], Step [8412/10336], Loss: 0.1331\n",
      "Epoch [2/5], Step [8414/10336], Loss: 1.7677\n",
      "Epoch [2/5], Step [8416/10336], Loss: 2.5797\n",
      "Epoch [2/5], Step [8418/10336], Loss: 1.9053\n",
      "Epoch [2/5], Step [8420/10336], Loss: 0.6224\n",
      "Epoch [2/5], Step [8422/10336], Loss: 0.0084\n",
      "Epoch [2/5], Step [8424/10336], Loss: 2.9640\n",
      "Epoch [2/5], Step [8426/10336], Loss: 0.8691\n",
      "Epoch [2/5], Step [8428/10336], Loss: 3.7405\n",
      "Epoch [2/5], Step [8430/10336], Loss: 0.0114\n",
      "Epoch [2/5], Step [8432/10336], Loss: 0.0155\n",
      "Epoch [2/5], Step [8434/10336], Loss: 2.0054\n",
      "Epoch [2/5], Step [8436/10336], Loss: 0.2352\n",
      "Epoch [2/5], Step [8438/10336], Loss: 2.6701\n",
      "Epoch [2/5], Step [8440/10336], Loss: 0.6250\n",
      "Epoch [2/5], Step [8442/10336], Loss: 0.3523\n",
      "Epoch [2/5], Step [8444/10336], Loss: 1.1977\n",
      "Epoch [2/5], Step [8446/10336], Loss: 0.1454\n",
      "Epoch [2/5], Step [8448/10336], Loss: 1.6307\n",
      "Epoch [2/5], Step [8450/10336], Loss: 0.0222\n",
      "Epoch [2/5], Step [8452/10336], Loss: 0.9926\n",
      "Epoch [2/5], Step [8454/10336], Loss: 0.1612\n",
      "Epoch [2/5], Step [8456/10336], Loss: 0.5496\n",
      "Epoch [2/5], Step [8458/10336], Loss: 4.2187\n",
      "Epoch [2/5], Step [8460/10336], Loss: 1.4192\n",
      "Epoch [2/5], Step [8462/10336], Loss: 0.3413\n",
      "Epoch [2/5], Step [8464/10336], Loss: 0.4374\n",
      "Epoch [2/5], Step [8466/10336], Loss: 0.0564\n",
      "Epoch [2/5], Step [8468/10336], Loss: 0.0594\n",
      "Epoch [2/5], Step [8470/10336], Loss: 0.1989\n",
      "Epoch [2/5], Step [8472/10336], Loss: 0.4142\n",
      "Epoch [2/5], Step [8474/10336], Loss: 1.3925\n",
      "Epoch [2/5], Step [8476/10336], Loss: 1.8833\n",
      "Epoch [2/5], Step [8478/10336], Loss: 1.1481\n",
      "Epoch [2/5], Step [8480/10336], Loss: 0.2688\n",
      "Epoch [2/5], Step [8482/10336], Loss: 2.9491\n",
      "Epoch [2/5], Step [8484/10336], Loss: 0.1784\n",
      "Epoch [2/5], Step [8486/10336], Loss: 0.0939\n",
      "Epoch [2/5], Step [8488/10336], Loss: 0.1130\n",
      "Epoch [2/5], Step [8490/10336], Loss: 3.2266\n",
      "Epoch [2/5], Step [8492/10336], Loss: 0.8239\n",
      "Epoch [2/5], Step [8494/10336], Loss: 0.1720\n",
      "Epoch [2/5], Step [8496/10336], Loss: 0.1065\n",
      "Epoch [2/5], Step [8498/10336], Loss: 1.6502\n",
      "Epoch [2/5], Step [8500/10336], Loss: 0.0983\n",
      "Epoch [2/5], Step [8502/10336], Loss: 0.4851\n",
      "Epoch [2/5], Step [8504/10336], Loss: 0.0164\n",
      "Epoch [2/5], Step [8506/10336], Loss: 0.4617\n",
      "Epoch [2/5], Step [8508/10336], Loss: 0.2723\n",
      "Epoch [2/5], Step [8510/10336], Loss: 0.6594\n",
      "Epoch [2/5], Step [8512/10336], Loss: 1.9562\n",
      "Epoch [2/5], Step [8514/10336], Loss: 0.0148\n",
      "Epoch [2/5], Step [8516/10336], Loss: 1.4949\n",
      "Epoch [2/5], Step [8518/10336], Loss: 1.6584\n",
      "Epoch [2/5], Step [8520/10336], Loss: 0.3346\n",
      "Epoch [2/5], Step [8522/10336], Loss: 0.3663\n",
      "Epoch [2/5], Step [8524/10336], Loss: 0.9224\n",
      "Epoch [2/5], Step [8526/10336], Loss: 0.7783\n",
      "Epoch [2/5], Step [8528/10336], Loss: 0.3822\n",
      "Epoch [2/5], Step [8530/10336], Loss: 0.5761\n",
      "Epoch [2/5], Step [8532/10336], Loss: 0.5957\n",
      "Epoch [2/5], Step [8534/10336], Loss: 1.8693\n",
      "Epoch [2/5], Step [8536/10336], Loss: 0.0711\n",
      "Epoch [2/5], Step [8538/10336], Loss: 2.2373\n",
      "Epoch [2/5], Step [8540/10336], Loss: 0.5459\n",
      "Epoch [2/5], Step [8542/10336], Loss: 0.0794\n",
      "Epoch [2/5], Step [8544/10336], Loss: 0.4478\n",
      "Epoch [2/5], Step [8546/10336], Loss: 0.0036\n",
      "Epoch [2/5], Step [8548/10336], Loss: 0.8286\n",
      "Epoch [2/5], Step [8550/10336], Loss: 1.2186\n",
      "Epoch [2/5], Step [8552/10336], Loss: 0.2739\n",
      "Epoch [2/5], Step [8554/10336], Loss: 0.0160\n",
      "Epoch [2/5], Step [8556/10336], Loss: 0.1374\n",
      "Epoch [2/5], Step [8558/10336], Loss: 1.9447\n",
      "Epoch [2/5], Step [8560/10336], Loss: 0.0218\n",
      "Epoch [2/5], Step [8562/10336], Loss: 6.7432\n",
      "Epoch [2/5], Step [8564/10336], Loss: 2.8451\n",
      "Epoch [2/5], Step [8566/10336], Loss: 0.2946\n",
      "Epoch [2/5], Step [8568/10336], Loss: 0.4507\n",
      "Epoch [2/5], Step [8570/10336], Loss: 0.0119\n",
      "Epoch [2/5], Step [8572/10336], Loss: 0.3607\n",
      "Epoch [2/5], Step [8574/10336], Loss: 0.9417\n",
      "Epoch [2/5], Step [8576/10336], Loss: 0.1866\n",
      "Epoch [2/5], Step [8578/10336], Loss: 1.7464\n",
      "Epoch [2/5], Step [8580/10336], Loss: 1.4382\n",
      "Epoch [2/5], Step [8582/10336], Loss: 2.3215\n",
      "Epoch [2/5], Step [8584/10336], Loss: 2.1391\n",
      "Epoch [2/5], Step [8586/10336], Loss: 0.5247\n",
      "Epoch [2/5], Step [8588/10336], Loss: 1.8593\n",
      "Epoch [2/5], Step [8590/10336], Loss: 2.5048\n",
      "Epoch [2/5], Step [8592/10336], Loss: 0.3400\n",
      "Epoch [2/5], Step [8594/10336], Loss: 0.6491\n",
      "Epoch [2/5], Step [8596/10336], Loss: 2.1139\n",
      "Epoch [2/5], Step [8598/10336], Loss: 0.1837\n",
      "Epoch [2/5], Step [8600/10336], Loss: 0.0077\n",
      "Epoch [2/5], Step [8602/10336], Loss: 0.9371\n",
      "Epoch [2/5], Step [8604/10336], Loss: 2.6323\n",
      "Epoch [2/5], Step [8606/10336], Loss: 0.5090\n",
      "Epoch [2/5], Step [8608/10336], Loss: 2.3881\n",
      "Epoch [2/5], Step [8610/10336], Loss: 1.0221\n",
      "Epoch [2/5], Step [8612/10336], Loss: 0.0570\n",
      "Epoch [2/5], Step [8614/10336], Loss: 0.5764\n",
      "Epoch [2/5], Step [8616/10336], Loss: 1.3205\n",
      "Epoch [2/5], Step [8618/10336], Loss: 0.0202\n",
      "Epoch [2/5], Step [8620/10336], Loss: 0.9995\n",
      "Epoch [2/5], Step [8622/10336], Loss: 0.4156\n",
      "Epoch [2/5], Step [8624/10336], Loss: 1.2673\n",
      "Epoch [2/5], Step [8626/10336], Loss: 0.0167\n",
      "Epoch [2/5], Step [8628/10336], Loss: 0.2811\n",
      "Epoch [2/5], Step [8630/10336], Loss: 1.8376\n",
      "Epoch [2/5], Step [8632/10336], Loss: 0.8687\n",
      "Epoch [2/5], Step [8634/10336], Loss: 0.0304\n",
      "Epoch [2/5], Step [8636/10336], Loss: 4.3264\n",
      "Epoch [2/5], Step [8638/10336], Loss: 0.0199\n",
      "Epoch [2/5], Step [8640/10336], Loss: 0.1640\n",
      "Epoch [2/5], Step [8642/10336], Loss: 1.6575\n",
      "Epoch [2/5], Step [8644/10336], Loss: 0.2118\n",
      "Epoch [2/5], Step [8646/10336], Loss: 0.9666\n",
      "Epoch [2/5], Step [8648/10336], Loss: 0.0389\n",
      "Epoch [2/5], Step [8650/10336], Loss: 2.5322\n",
      "Epoch [2/5], Step [8652/10336], Loss: 2.0477\n",
      "Epoch [2/5], Step [8654/10336], Loss: 0.3751\n",
      "Epoch [2/5], Step [8656/10336], Loss: 1.5882\n",
      "Epoch [2/5], Step [8658/10336], Loss: 0.1858\n",
      "Epoch [2/5], Step [8660/10336], Loss: 0.5472\n",
      "Epoch [2/5], Step [8662/10336], Loss: 0.0026\n",
      "Epoch [2/5], Step [8664/10336], Loss: 0.0579\n",
      "Epoch [2/5], Step [8666/10336], Loss: 0.1970\n",
      "Epoch [2/5], Step [8668/10336], Loss: 2.0627\n",
      "Epoch [2/5], Step [8670/10336], Loss: 2.8568\n",
      "Epoch [2/5], Step [8672/10336], Loss: 0.9318\n",
      "Epoch [2/5], Step [8674/10336], Loss: 0.5155\n",
      "Epoch [2/5], Step [8676/10336], Loss: 0.7372\n",
      "Epoch [2/5], Step [8678/10336], Loss: 0.3631\n",
      "Epoch [2/5], Step [8680/10336], Loss: 0.4048\n",
      "Epoch [2/5], Step [8682/10336], Loss: 0.5791\n",
      "Epoch [2/5], Step [8684/10336], Loss: 2.6768\n",
      "Epoch [2/5], Step [8686/10336], Loss: 0.6034\n",
      "Epoch [2/5], Step [8688/10336], Loss: 2.0800\n",
      "Epoch [2/5], Step [8690/10336], Loss: 0.0072\n",
      "Epoch [2/5], Step [8692/10336], Loss: 1.0370\n",
      "Epoch [2/5], Step [8694/10336], Loss: 0.9714\n",
      "Epoch [2/5], Step [8696/10336], Loss: 0.0223\n",
      "Epoch [2/5], Step [8698/10336], Loss: 1.7632\n",
      "Epoch [2/5], Step [8700/10336], Loss: 1.6764\n",
      "Epoch [2/5], Step [8702/10336], Loss: 0.0277\n",
      "Epoch [2/5], Step [8704/10336], Loss: 0.0219\n",
      "Epoch [2/5], Step [8706/10336], Loss: 0.3559\n",
      "Epoch [2/5], Step [8708/10336], Loss: 0.2128\n",
      "Epoch [2/5], Step [8710/10336], Loss: 1.0977\n",
      "Epoch [2/5], Step [8712/10336], Loss: 2.5260\n",
      "Epoch [2/5], Step [8714/10336], Loss: 0.8213\n",
      "Epoch [2/5], Step [8716/10336], Loss: 0.4076\n",
      "Epoch [2/5], Step [8718/10336], Loss: 0.0233\n",
      "Epoch [2/5], Step [8720/10336], Loss: 0.9748\n",
      "Epoch [2/5], Step [8722/10336], Loss: 0.7832\n",
      "Epoch [2/5], Step [8724/10336], Loss: 2.1463\n",
      "Epoch [2/5], Step [8726/10336], Loss: 0.7788\n",
      "Epoch [2/5], Step [8728/10336], Loss: 0.2011\n",
      "Epoch [2/5], Step [8730/10336], Loss: 5.5045\n",
      "Epoch [2/5], Step [8732/10336], Loss: 0.0023\n",
      "Epoch [2/5], Step [8734/10336], Loss: 0.9066\n",
      "Epoch [2/5], Step [8736/10336], Loss: 0.1943\n",
      "Epoch [2/5], Step [8738/10336], Loss: 1.7295\n",
      "Epoch [2/5], Step [8740/10336], Loss: 0.2416\n",
      "Epoch [2/5], Step [8742/10336], Loss: 1.2746\n",
      "Epoch [2/5], Step [8744/10336], Loss: 0.8358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [8746/10336], Loss: 0.1161\n",
      "Epoch [2/5], Step [8748/10336], Loss: 1.2447\n",
      "Epoch [2/5], Step [8750/10336], Loss: 0.0084\n",
      "Epoch [2/5], Step [8752/10336], Loss: 0.0792\n",
      "Epoch [2/5], Step [8754/10336], Loss: 0.0250\n",
      "Epoch [2/5], Step [8756/10336], Loss: 0.0557\n",
      "Epoch [2/5], Step [8758/10336], Loss: 3.1065\n",
      "Epoch [2/5], Step [8760/10336], Loss: 0.0649\n",
      "Epoch [2/5], Step [8762/10336], Loss: 0.0326\n",
      "Epoch [2/5], Step [8764/10336], Loss: 1.7379\n",
      "Epoch [2/5], Step [8766/10336], Loss: 0.0082\n",
      "Epoch [2/5], Step [8768/10336], Loss: 1.9322\n",
      "Epoch [2/5], Step [8770/10336], Loss: 0.2896\n",
      "Epoch [2/5], Step [8772/10336], Loss: 0.0521\n",
      "Epoch [2/5], Step [8774/10336], Loss: 1.0462\n",
      "Epoch [2/5], Step [8776/10336], Loss: 0.1159\n",
      "Epoch [2/5], Step [8778/10336], Loss: 1.0402\n",
      "Epoch [2/5], Step [8780/10336], Loss: 1.7740\n",
      "Epoch [2/5], Step [8782/10336], Loss: 0.3893\n",
      "Epoch [2/5], Step [8784/10336], Loss: 0.2378\n",
      "Epoch [2/5], Step [8786/10336], Loss: 2.3087\n",
      "Epoch [2/5], Step [8788/10336], Loss: 2.9273\n",
      "Epoch [2/5], Step [8790/10336], Loss: 0.0766\n",
      "Epoch [2/5], Step [8792/10336], Loss: 1.4189\n",
      "Epoch [2/5], Step [8794/10336], Loss: 0.2845\n",
      "Epoch [2/5], Step [8796/10336], Loss: 0.4920\n",
      "Epoch [2/5], Step [8798/10336], Loss: 0.0164\n",
      "Epoch [2/5], Step [8800/10336], Loss: 0.3010\n",
      "Epoch [2/5], Step [8802/10336], Loss: 1.8792\n",
      "Epoch [2/5], Step [8804/10336], Loss: 0.0022\n",
      "Epoch [2/5], Step [8806/10336], Loss: 0.6108\n",
      "Epoch [2/5], Step [8808/10336], Loss: 1.8898\n",
      "Epoch [2/5], Step [8810/10336], Loss: 1.6661\n",
      "Epoch [2/5], Step [8812/10336], Loss: 0.1630\n",
      "Epoch [2/5], Step [8814/10336], Loss: 1.8713\n",
      "Epoch [2/5], Step [8816/10336], Loss: 0.8786\n",
      "Epoch [2/5], Step [8818/10336], Loss: 1.5816\n",
      "Epoch [2/5], Step [8820/10336], Loss: 0.0431\n",
      "Epoch [2/5], Step [8822/10336], Loss: 0.1280\n",
      "Epoch [2/5], Step [8824/10336], Loss: 0.5159\n",
      "Epoch [2/5], Step [8826/10336], Loss: 3.6282\n",
      "Epoch [2/5], Step [8828/10336], Loss: 1.2409\n",
      "Epoch [2/5], Step [8830/10336], Loss: 0.0408\n",
      "Epoch [2/5], Step [8832/10336], Loss: 0.0391\n",
      "Epoch [2/5], Step [8834/10336], Loss: 0.0891\n",
      "Epoch [2/5], Step [8836/10336], Loss: 0.7456\n",
      "Epoch [2/5], Step [8838/10336], Loss: 1.9111\n",
      "Epoch [2/5], Step [8840/10336], Loss: 2.8051\n",
      "Epoch [2/5], Step [8842/10336], Loss: 0.0078\n",
      "Epoch [2/5], Step [8844/10336], Loss: 1.2690\n",
      "Epoch [2/5], Step [8846/10336], Loss: 0.1356\n",
      "Epoch [2/5], Step [8848/10336], Loss: 0.6846\n",
      "Epoch [2/5], Step [8850/10336], Loss: 0.7184\n",
      "Epoch [2/5], Step [8852/10336], Loss: 0.8879\n",
      "Epoch [2/5], Step [8854/10336], Loss: 0.7156\n",
      "Epoch [2/5], Step [8856/10336], Loss: 0.5092\n",
      "Epoch [2/5], Step [8858/10336], Loss: 0.1914\n",
      "Epoch [2/5], Step [8860/10336], Loss: 0.7042\n",
      "Epoch [2/5], Step [8862/10336], Loss: 0.1017\n",
      "Epoch [2/5], Step [8864/10336], Loss: 0.2023\n",
      "Epoch [2/5], Step [8866/10336], Loss: 0.0217\n",
      "Epoch [2/5], Step [8868/10336], Loss: 2.8447\n",
      "Epoch [2/5], Step [8870/10336], Loss: 0.0145\n",
      "Epoch [2/5], Step [8872/10336], Loss: 0.0992\n",
      "Epoch [2/5], Step [8874/10336], Loss: 2.6987\n",
      "Epoch [2/5], Step [8876/10336], Loss: 1.1780\n",
      "Epoch [2/5], Step [8878/10336], Loss: 0.3520\n",
      "Epoch [2/5], Step [8880/10336], Loss: 1.1562\n",
      "Epoch [2/5], Step [8882/10336], Loss: 0.0894\n",
      "Epoch [2/5], Step [8884/10336], Loss: 0.8964\n",
      "Epoch [2/5], Step [8886/10336], Loss: 1.9097\n",
      "Epoch [2/5], Step [8888/10336], Loss: 2.0935\n",
      "Epoch [2/5], Step [8890/10336], Loss: 0.7728\n",
      "Epoch [2/5], Step [8892/10336], Loss: 0.0080\n",
      "Epoch [2/5], Step [8894/10336], Loss: 0.0024\n",
      "Epoch [2/5], Step [8896/10336], Loss: 0.1843\n",
      "Epoch [2/5], Step [8898/10336], Loss: 1.3684\n",
      "Epoch [2/5], Step [8900/10336], Loss: 0.0060\n",
      "Epoch [2/5], Step [8902/10336], Loss: 1.5692\n",
      "Epoch [2/5], Step [8904/10336], Loss: 0.1443\n",
      "Epoch [2/5], Step [8906/10336], Loss: 0.1537\n",
      "Epoch [2/5], Step [8908/10336], Loss: 0.9631\n",
      "Epoch [2/5], Step [8910/10336], Loss: 0.4901\n",
      "Epoch [2/5], Step [8912/10336], Loss: 0.9749\n",
      "Epoch [2/5], Step [8914/10336], Loss: 0.1549\n",
      "Epoch [2/5], Step [8916/10336], Loss: 1.1289\n",
      "Epoch [2/5], Step [8918/10336], Loss: 0.7896\n",
      "Epoch [2/5], Step [8920/10336], Loss: 0.7131\n",
      "Epoch [2/5], Step [8922/10336], Loss: 0.5537\n",
      "Epoch [2/5], Step [8924/10336], Loss: 0.2046\n",
      "Epoch [2/5], Step [8926/10336], Loss: 0.8276\n",
      "Epoch [2/5], Step [8928/10336], Loss: 0.4955\n",
      "Epoch [2/5], Step [8930/10336], Loss: 1.3780\n",
      "Epoch [2/5], Step [8932/10336], Loss: 0.9215\n",
      "Epoch [2/5], Step [8934/10336], Loss: 0.0419\n",
      "Epoch [2/5], Step [8936/10336], Loss: 2.1100\n",
      "Epoch [2/5], Step [8938/10336], Loss: 0.5808\n",
      "Epoch [2/5], Step [8940/10336], Loss: 0.1221\n",
      "Epoch [2/5], Step [8942/10336], Loss: 0.1482\n",
      "Epoch [2/5], Step [8944/10336], Loss: 0.0378\n",
      "Epoch [2/5], Step [8946/10336], Loss: 0.8179\n",
      "Epoch [2/5], Step [8948/10336], Loss: 2.8363\n",
      "Epoch [2/5], Step [8950/10336], Loss: 0.7174\n",
      "Epoch [2/5], Step [8952/10336], Loss: 1.0513\n",
      "Epoch [2/5], Step [8954/10336], Loss: 0.2013\n",
      "Epoch [2/5], Step [8956/10336], Loss: 2.5232\n",
      "Epoch [2/5], Step [8958/10336], Loss: 0.5925\n",
      "Epoch [2/5], Step [8960/10336], Loss: 1.1220\n",
      "Epoch [2/5], Step [8962/10336], Loss: 0.4904\n",
      "Epoch [2/5], Step [8964/10336], Loss: 0.0624\n",
      "Epoch [2/5], Step [8966/10336], Loss: 0.1468\n",
      "Epoch [2/5], Step [8968/10336], Loss: 1.7532\n",
      "Epoch [2/5], Step [8970/10336], Loss: 1.1704\n",
      "Epoch [2/5], Step [8972/10336], Loss: 0.3152\n",
      "Epoch [2/5], Step [8974/10336], Loss: 1.3643\n",
      "Epoch [2/5], Step [8976/10336], Loss: 0.0052\n",
      "Epoch [2/5], Step [8978/10336], Loss: 0.0185\n",
      "Epoch [2/5], Step [8980/10336], Loss: 0.4618\n",
      "Epoch [2/5], Step [8982/10336], Loss: 0.1534\n",
      "Epoch [2/5], Step [8984/10336], Loss: 2.3875\n",
      "Epoch [2/5], Step [8986/10336], Loss: 1.7875\n",
      "Epoch [2/5], Step [8988/10336], Loss: 1.9896\n",
      "Epoch [2/5], Step [8990/10336], Loss: 0.6507\n",
      "Epoch [2/5], Step [8992/10336], Loss: 0.3141\n",
      "Epoch [2/5], Step [8994/10336], Loss: 0.8459\n",
      "Epoch [2/5], Step [8996/10336], Loss: 0.0050\n",
      "Epoch [2/5], Step [8998/10336], Loss: 0.0353\n",
      "Epoch [2/5], Step [9000/10336], Loss: 0.2834\n",
      "Epoch [2/5], Step [9002/10336], Loss: 1.7906\n",
      "Epoch [2/5], Step [9004/10336], Loss: 0.0088\n",
      "Epoch [2/5], Step [9006/10336], Loss: 0.5613\n",
      "Epoch [2/5], Step [9008/10336], Loss: 0.3476\n",
      "Epoch [2/5], Step [9010/10336], Loss: 0.1795\n",
      "Epoch [2/5], Step [9012/10336], Loss: 1.4951\n",
      "Epoch [2/5], Step [9014/10336], Loss: 0.8524\n",
      "Epoch [2/5], Step [9016/10336], Loss: 1.0821\n",
      "Epoch [2/5], Step [9018/10336], Loss: 0.0866\n",
      "Epoch [2/5], Step [9020/10336], Loss: 0.4735\n",
      "Epoch [2/5], Step [9022/10336], Loss: 0.6988\n",
      "Epoch [2/5], Step [9024/10336], Loss: 0.8876\n",
      "Epoch [2/5], Step [9026/10336], Loss: 0.0745\n",
      "Epoch [2/5], Step [9028/10336], Loss: 1.8218\n",
      "Epoch [2/5], Step [9030/10336], Loss: 3.0081\n",
      "Epoch [2/5], Step [9032/10336], Loss: 0.0918\n",
      "Epoch [2/5], Step [9034/10336], Loss: 0.1237\n",
      "Epoch [2/5], Step [9036/10336], Loss: 1.4007\n",
      "Epoch [2/5], Step [9038/10336], Loss: 0.1131\n",
      "Epoch [2/5], Step [9040/10336], Loss: 0.0935\n",
      "Epoch [2/5], Step [9042/10336], Loss: 0.0067\n",
      "Epoch [2/5], Step [9044/10336], Loss: 1.2511\n",
      "Epoch [2/5], Step [9046/10336], Loss: 1.4799\n",
      "Epoch [2/5], Step [9048/10336], Loss: 0.9068\n",
      "Epoch [2/5], Step [9050/10336], Loss: 0.0259\n",
      "Epoch [2/5], Step [9052/10336], Loss: 0.5123\n",
      "Epoch [2/5], Step [9054/10336], Loss: 0.5483\n",
      "Epoch [2/5], Step [9056/10336], Loss: 0.1447\n",
      "Epoch [2/5], Step [9058/10336], Loss: 0.7691\n",
      "Epoch [2/5], Step [9060/10336], Loss: 0.7743\n",
      "Epoch [2/5], Step [9062/10336], Loss: 0.0075\n",
      "Epoch [2/5], Step [9064/10336], Loss: 0.0092\n",
      "Epoch [2/5], Step [9066/10336], Loss: 0.3943\n",
      "Epoch [2/5], Step [9068/10336], Loss: 0.0128\n",
      "Epoch [2/5], Step [9070/10336], Loss: 0.7672\n",
      "Epoch [2/5], Step [9072/10336], Loss: 0.4559\n",
      "Epoch [2/5], Step [9074/10336], Loss: 0.0694\n",
      "Epoch [2/5], Step [9076/10336], Loss: 0.3717\n",
      "Epoch [2/5], Step [9078/10336], Loss: 0.2087\n",
      "Epoch [2/5], Step [9080/10336], Loss: 1.4656\n",
      "Epoch [2/5], Step [9082/10336], Loss: 0.0500\n",
      "Epoch [2/5], Step [9084/10336], Loss: 0.2978\n",
      "Epoch [2/5], Step [9086/10336], Loss: 0.2880\n",
      "Epoch [2/5], Step [9088/10336], Loss: 2.2336\n",
      "Epoch [2/5], Step [9090/10336], Loss: 0.0346\n",
      "Epoch [2/5], Step [9092/10336], Loss: 0.2797\n",
      "Epoch [2/5], Step [9094/10336], Loss: 1.0898\n",
      "Epoch [2/5], Step [9096/10336], Loss: 0.0744\n",
      "Epoch [2/5], Step [9098/10336], Loss: 0.2557\n",
      "Epoch [2/5], Step [9100/10336], Loss: 0.3890\n",
      "Epoch [2/5], Step [9102/10336], Loss: 0.0800\n",
      "Epoch [2/5], Step [9104/10336], Loss: 0.2022\n",
      "Epoch [2/5], Step [9106/10336], Loss: 0.9114\n",
      "Epoch [2/5], Step [9108/10336], Loss: 4.7712\n",
      "Epoch [2/5], Step [9110/10336], Loss: 1.7325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [9112/10336], Loss: 2.2354\n",
      "Epoch [2/5], Step [9114/10336], Loss: 3.7698\n",
      "Epoch [2/5], Step [9116/10336], Loss: 0.0228\n",
      "Epoch [2/5], Step [9118/10336], Loss: 2.1509\n",
      "Epoch [2/5], Step [9120/10336], Loss: 3.2682\n",
      "Epoch [2/5], Step [9122/10336], Loss: 2.8074\n",
      "Epoch [2/5], Step [9124/10336], Loss: 0.5993\n",
      "Epoch [2/5], Step [9126/10336], Loss: 0.0958\n",
      "Epoch [2/5], Step [9128/10336], Loss: 0.1490\n",
      "Epoch [2/5], Step [9130/10336], Loss: 1.2755\n",
      "Epoch [2/5], Step [9132/10336], Loss: 0.6584\n",
      "Epoch [2/5], Step [9134/10336], Loss: 3.0383\n",
      "Epoch [2/5], Step [9136/10336], Loss: 0.8273\n",
      "Epoch [2/5], Step [9138/10336], Loss: 1.6497\n",
      "Epoch [2/5], Step [9140/10336], Loss: 0.2809\n",
      "Epoch [2/5], Step [9142/10336], Loss: 1.7167\n",
      "Epoch [2/5], Step [9144/10336], Loss: 0.4377\n",
      "Epoch [2/5], Step [9146/10336], Loss: 0.4833\n",
      "Epoch [2/5], Step [9148/10336], Loss: 5.3043\n",
      "Epoch [2/5], Step [9150/10336], Loss: 0.5848\n",
      "Epoch [2/5], Step [9152/10336], Loss: 0.9643\n",
      "Epoch [2/5], Step [9154/10336], Loss: 0.0599\n",
      "Epoch [2/5], Step [9156/10336], Loss: 0.1359\n",
      "Epoch [2/5], Step [9158/10336], Loss: 1.0548\n",
      "Epoch [2/5], Step [9160/10336], Loss: 3.1623\n",
      "Epoch [2/5], Step [9162/10336], Loss: 2.9920\n",
      "Epoch [2/5], Step [9164/10336], Loss: 1.0445\n",
      "Epoch [2/5], Step [9166/10336], Loss: 5.3191\n",
      "Epoch [2/5], Step [9168/10336], Loss: 2.8491\n",
      "Epoch [2/5], Step [9170/10336], Loss: 0.5207\n",
      "Epoch [2/5], Step [9172/10336], Loss: 0.3286\n",
      "Epoch [2/5], Step [9174/10336], Loss: 0.0153\n",
      "Epoch [2/5], Step [9176/10336], Loss: 0.1584\n",
      "Epoch [2/5], Step [9178/10336], Loss: 1.5409\n",
      "Epoch [2/5], Step [9180/10336], Loss: 1.9078\n",
      "Epoch [2/5], Step [9182/10336], Loss: 0.0110\n",
      "Epoch [2/5], Step [9184/10336], Loss: 1.5895\n",
      "Epoch [2/5], Step [9186/10336], Loss: 1.4903\n",
      "Epoch [2/5], Step [9188/10336], Loss: 0.7424\n",
      "Epoch [2/5], Step [9190/10336], Loss: 0.0582\n",
      "Epoch [2/5], Step [9192/10336], Loss: 2.4822\n",
      "Epoch [2/5], Step [9194/10336], Loss: 1.6758\n",
      "Epoch [2/5], Step [9196/10336], Loss: 1.9940\n",
      "Epoch [2/5], Step [9198/10336], Loss: 1.0136\n",
      "Epoch [2/5], Step [9200/10336], Loss: 0.2566\n",
      "Epoch [2/5], Step [9202/10336], Loss: 0.1572\n",
      "Epoch [2/5], Step [9204/10336], Loss: 1.0131\n",
      "Epoch [2/5], Step [9206/10336], Loss: 0.1030\n",
      "Epoch [2/5], Step [9208/10336], Loss: 1.7202\n",
      "Epoch [2/5], Step [9210/10336], Loss: 2.2861\n",
      "Epoch [2/5], Step [9212/10336], Loss: 0.9396\n",
      "Epoch [2/5], Step [9214/10336], Loss: 0.5884\n",
      "Epoch [2/5], Step [9216/10336], Loss: 0.0363\n",
      "Epoch [2/5], Step [9218/10336], Loss: 0.1416\n",
      "Epoch [2/5], Step [9220/10336], Loss: 0.0448\n",
      "Epoch [2/5], Step [9222/10336], Loss: 1.8217\n",
      "Epoch [2/5], Step [9224/10336], Loss: 0.0214\n",
      "Epoch [2/5], Step [9226/10336], Loss: 0.0261\n",
      "Epoch [2/5], Step [9228/10336], Loss: 0.8568\n",
      "Epoch [2/5], Step [9230/10336], Loss: 2.2546\n",
      "Epoch [2/5], Step [9232/10336], Loss: 0.1748\n",
      "Epoch [2/5], Step [9234/10336], Loss: 3.2737\n",
      "Epoch [2/5], Step [9236/10336], Loss: 0.2924\n",
      "Epoch [2/5], Step [9238/10336], Loss: 1.1910\n",
      "Epoch [2/5], Step [9240/10336], Loss: 0.3250\n",
      "Epoch [2/5], Step [9242/10336], Loss: 0.2125\n",
      "Epoch [2/5], Step [9244/10336], Loss: 0.1945\n",
      "Epoch [2/5], Step [9246/10336], Loss: 0.9984\n",
      "Epoch [2/5], Step [9248/10336], Loss: 0.8178\n",
      "Epoch [2/5], Step [9250/10336], Loss: 1.8159\n",
      "Epoch [2/5], Step [9252/10336], Loss: 1.8578\n",
      "Epoch [2/5], Step [9254/10336], Loss: 0.2909\n",
      "Epoch [2/5], Step [9256/10336], Loss: 0.3609\n",
      "Epoch [2/5], Step [9258/10336], Loss: 0.9844\n",
      "Epoch [2/5], Step [9260/10336], Loss: 0.0185\n",
      "Epoch [2/5], Step [9262/10336], Loss: 0.0356\n",
      "Epoch [2/5], Step [9264/10336], Loss: 3.2079\n",
      "Epoch [2/5], Step [9266/10336], Loss: 1.2230\n",
      "Epoch [2/5], Step [9268/10336], Loss: 0.3280\n",
      "Epoch [2/5], Step [9270/10336], Loss: 0.3799\n",
      "Epoch [2/5], Step [9272/10336], Loss: 0.3693\n",
      "Epoch [2/5], Step [9274/10336], Loss: 1.8773\n",
      "Epoch [2/5], Step [9276/10336], Loss: 0.6424\n",
      "Epoch [2/5], Step [9278/10336], Loss: 0.8007\n",
      "Epoch [2/5], Step [9280/10336], Loss: 0.8674\n",
      "Epoch [2/5], Step [9282/10336], Loss: 4.0232\n",
      "Epoch [2/5], Step [9284/10336], Loss: 2.1711\n",
      "Epoch [2/5], Step [9286/10336], Loss: 0.0007\n",
      "Epoch [2/5], Step [9288/10336], Loss: 1.2728\n",
      "Epoch [2/5], Step [9290/10336], Loss: 0.1896\n",
      "Epoch [2/5], Step [9292/10336], Loss: 0.0520\n",
      "Epoch [2/5], Step [9294/10336], Loss: 0.1281\n",
      "Epoch [2/5], Step [9296/10336], Loss: 0.0263\n",
      "Epoch [2/5], Step [9298/10336], Loss: 1.0599\n",
      "Epoch [2/5], Step [9300/10336], Loss: 0.1543\n",
      "Epoch [2/5], Step [9302/10336], Loss: 1.1208\n",
      "Epoch [2/5], Step [9304/10336], Loss: 1.4835\n",
      "Epoch [2/5], Step [9306/10336], Loss: 3.5845\n",
      "Epoch [2/5], Step [9308/10336], Loss: 0.2701\n",
      "Epoch [2/5], Step [9310/10336], Loss: 0.4805\n",
      "Epoch [2/5], Step [9312/10336], Loss: 1.7359\n",
      "Epoch [2/5], Step [9314/10336], Loss: 1.2728\n",
      "Epoch [2/5], Step [9316/10336], Loss: 0.0505\n",
      "Epoch [2/5], Step [9318/10336], Loss: 0.0689\n",
      "Epoch [2/5], Step [9320/10336], Loss: 0.2322\n",
      "Epoch [2/5], Step [9322/10336], Loss: 0.4326\n",
      "Epoch [2/5], Step [9324/10336], Loss: 0.0069\n",
      "Epoch [2/5], Step [9326/10336], Loss: 1.1832\n",
      "Epoch [2/5], Step [9328/10336], Loss: 2.5267\n",
      "Epoch [2/5], Step [9330/10336], Loss: 0.8465\n",
      "Epoch [2/5], Step [9332/10336], Loss: 4.4937\n",
      "Epoch [2/5], Step [9334/10336], Loss: 0.6790\n",
      "Epoch [2/5], Step [9336/10336], Loss: 2.6681\n",
      "Epoch [2/5], Step [9338/10336], Loss: 0.5135\n",
      "Epoch [2/5], Step [9340/10336], Loss: 2.4208\n",
      "Epoch [2/5], Step [9342/10336], Loss: 0.0165\n",
      "Epoch [2/5], Step [9344/10336], Loss: 0.4286\n",
      "Epoch [2/5], Step [9346/10336], Loss: 0.9437\n",
      "Epoch [2/5], Step [9348/10336], Loss: 2.4175\n",
      "Epoch [2/5], Step [9350/10336], Loss: 0.1343\n",
      "Epoch [2/5], Step [9352/10336], Loss: 0.4790\n",
      "Epoch [2/5], Step [9354/10336], Loss: 1.2724\n",
      "Epoch [2/5], Step [9356/10336], Loss: 0.6647\n",
      "Epoch [2/5], Step [9358/10336], Loss: 0.1992\n",
      "Epoch [2/5], Step [9360/10336], Loss: 1.9280\n",
      "Epoch [2/5], Step [9362/10336], Loss: 0.5304\n",
      "Epoch [2/5], Step [9364/10336], Loss: 0.9454\n",
      "Epoch [2/5], Step [9366/10336], Loss: 1.3187\n",
      "Epoch [2/5], Step [9368/10336], Loss: 1.5022\n",
      "Epoch [2/5], Step [9370/10336], Loss: 2.0370\n",
      "Epoch [2/5], Step [9372/10336], Loss: 1.0766\n",
      "Epoch [2/5], Step [9374/10336], Loss: 0.3585\n",
      "Epoch [2/5], Step [9376/10336], Loss: 0.0303\n",
      "Epoch [2/5], Step [9378/10336], Loss: 0.0211\n",
      "Epoch [2/5], Step [9380/10336], Loss: 0.0154\n",
      "Epoch [2/5], Step [9382/10336], Loss: 0.0953\n",
      "Epoch [2/5], Step [9384/10336], Loss: 1.3964\n",
      "Epoch [2/5], Step [9386/10336], Loss: 0.4013\n",
      "Epoch [2/5], Step [9388/10336], Loss: 1.5956\n",
      "Epoch [2/5], Step [9390/10336], Loss: 1.0696\n",
      "Epoch [2/5], Step [9392/10336], Loss: 0.3880\n",
      "Epoch [2/5], Step [9394/10336], Loss: 0.3333\n",
      "Epoch [2/5], Step [9396/10336], Loss: 1.1502\n",
      "Epoch [2/5], Step [9398/10336], Loss: 2.0509\n",
      "Epoch [2/5], Step [9400/10336], Loss: 2.5525\n",
      "Epoch [2/5], Step [9402/10336], Loss: 0.0141\n",
      "Epoch [2/5], Step [9404/10336], Loss: 2.1760\n",
      "Epoch [2/5], Step [9406/10336], Loss: 0.1828\n",
      "Epoch [2/5], Step [9408/10336], Loss: 0.5476\n",
      "Epoch [2/5], Step [9410/10336], Loss: 0.8646\n",
      "Epoch [2/5], Step [9412/10336], Loss: 0.2634\n",
      "Epoch [2/5], Step [9414/10336], Loss: 0.3220\n",
      "Epoch [2/5], Step [9416/10336], Loss: 1.2859\n",
      "Epoch [2/5], Step [9418/10336], Loss: 2.7157\n",
      "Epoch [2/5], Step [9420/10336], Loss: 0.4689\n",
      "Epoch [2/5], Step [9422/10336], Loss: 2.2944\n",
      "Epoch [2/5], Step [9424/10336], Loss: 0.0985\n",
      "Epoch [2/5], Step [9426/10336], Loss: 0.0117\n",
      "Epoch [2/5], Step [9428/10336], Loss: 2.0489\n",
      "Epoch [2/5], Step [9430/10336], Loss: 3.9129\n",
      "Epoch [2/5], Step [9432/10336], Loss: 2.0153\n",
      "Epoch [2/5], Step [9434/10336], Loss: 0.8697\n",
      "Epoch [2/5], Step [9436/10336], Loss: 0.1163\n",
      "Epoch [2/5], Step [9438/10336], Loss: 1.1921\n",
      "Epoch [2/5], Step [9440/10336], Loss: 0.6688\n",
      "Epoch [2/5], Step [9442/10336], Loss: 0.0447\n",
      "Epoch [2/5], Step [9444/10336], Loss: 0.6599\n",
      "Epoch [2/5], Step [9446/10336], Loss: 0.0471\n",
      "Epoch [2/5], Step [9448/10336], Loss: 0.0147\n",
      "Epoch [2/5], Step [9450/10336], Loss: 2.9283\n",
      "Epoch [2/5], Step [9452/10336], Loss: 0.1539\n",
      "Epoch [2/5], Step [9454/10336], Loss: 0.2856\n",
      "Epoch [2/5], Step [9456/10336], Loss: 0.0861\n",
      "Epoch [2/5], Step [9458/10336], Loss: 0.5819\n",
      "Epoch [2/5], Step [9460/10336], Loss: 0.4157\n",
      "Epoch [2/5], Step [9462/10336], Loss: 0.2909\n",
      "Epoch [2/5], Step [9464/10336], Loss: 0.0406\n",
      "Epoch [2/5], Step [9466/10336], Loss: 0.7891\n",
      "Epoch [2/5], Step [9468/10336], Loss: 0.7219\n",
      "Epoch [2/5], Step [9470/10336], Loss: 0.2238\n",
      "Epoch [2/5], Step [9472/10336], Loss: 0.4959\n",
      "Epoch [2/5], Step [9474/10336], Loss: 1.6317\n",
      "Epoch [2/5], Step [9476/10336], Loss: 0.0397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [9478/10336], Loss: 0.4484\n",
      "Epoch [2/5], Step [9480/10336], Loss: 0.7983\n",
      "Epoch [2/5], Step [9482/10336], Loss: 0.0250\n",
      "Epoch [2/5], Step [9484/10336], Loss: 0.1271\n",
      "Epoch [2/5], Step [9486/10336], Loss: 0.9342\n",
      "Epoch [2/5], Step [9488/10336], Loss: 1.7263\n",
      "Epoch [2/5], Step [9490/10336], Loss: 0.0208\n",
      "Epoch [2/5], Step [9492/10336], Loss: 0.9305\n",
      "Epoch [2/5], Step [9494/10336], Loss: 2.1677\n",
      "Epoch [2/5], Step [9496/10336], Loss: 0.0121\n",
      "Epoch [2/5], Step [9498/10336], Loss: 0.2665\n",
      "Epoch [2/5], Step [9500/10336], Loss: 0.3207\n",
      "Epoch [2/5], Step [9502/10336], Loss: 0.4656\n",
      "Epoch [2/5], Step [9504/10336], Loss: 0.2931\n",
      "Epoch [2/5], Step [9506/10336], Loss: 1.6610\n",
      "Epoch [2/5], Step [9508/10336], Loss: 1.1664\n",
      "Epoch [2/5], Step [9510/10336], Loss: 0.5871\n",
      "Epoch [2/5], Step [9512/10336], Loss: 0.0046\n",
      "Epoch [2/5], Step [9514/10336], Loss: 0.7807\n",
      "Epoch [2/5], Step [9516/10336], Loss: 0.2300\n",
      "Epoch [2/5], Step [9518/10336], Loss: 0.0084\n",
      "Epoch [2/5], Step [9520/10336], Loss: 1.2798\n",
      "Epoch [2/5], Step [9522/10336], Loss: 2.8934\n",
      "Epoch [2/5], Step [9524/10336], Loss: 2.0726\n",
      "Epoch [2/5], Step [9526/10336], Loss: 1.3819\n",
      "Epoch [2/5], Step [9528/10336], Loss: 0.1395\n",
      "Epoch [2/5], Step [9530/10336], Loss: 0.5303\n",
      "Epoch [2/5], Step [9532/10336], Loss: 1.0690\n",
      "Epoch [2/5], Step [9534/10336], Loss: 0.3928\n",
      "Epoch [2/5], Step [9536/10336], Loss: 0.9560\n",
      "Epoch [2/5], Step [9538/10336], Loss: 0.2422\n",
      "Epoch [2/5], Step [9540/10336], Loss: 0.5161\n",
      "Epoch [2/5], Step [9542/10336], Loss: 1.7487\n",
      "Epoch [2/5], Step [9544/10336], Loss: 0.0960\n",
      "Epoch [2/5], Step [9546/10336], Loss: 0.1366\n",
      "Epoch [2/5], Step [9548/10336], Loss: 0.0588\n",
      "Epoch [2/5], Step [9550/10336], Loss: 0.1212\n",
      "Epoch [2/5], Step [9552/10336], Loss: 3.4113\n",
      "Epoch [2/5], Step [9554/10336], Loss: 1.6303\n",
      "Epoch [2/5], Step [9556/10336], Loss: 2.2133\n",
      "Epoch [2/5], Step [9558/10336], Loss: 0.6952\n",
      "Epoch [2/5], Step [9560/10336], Loss: 0.0743\n",
      "Epoch [2/5], Step [9562/10336], Loss: 2.9658\n",
      "Epoch [2/5], Step [9564/10336], Loss: 0.3072\n",
      "Epoch [2/5], Step [9566/10336], Loss: 0.8637\n",
      "Epoch [2/5], Step [9568/10336], Loss: 0.4707\n",
      "Epoch [2/5], Step [9570/10336], Loss: 0.0266\n",
      "Epoch [2/5], Step [9572/10336], Loss: 3.7848\n",
      "Epoch [2/5], Step [9574/10336], Loss: 0.7943\n",
      "Epoch [2/5], Step [9576/10336], Loss: 2.2515\n",
      "Epoch [2/5], Step [9578/10336], Loss: 0.4620\n",
      "Epoch [2/5], Step [9580/10336], Loss: 1.0608\n",
      "Epoch [2/5], Step [9582/10336], Loss: 0.5987\n",
      "Epoch [2/5], Step [9584/10336], Loss: 0.1401\n",
      "Epoch [2/5], Step [9586/10336], Loss: 0.3608\n",
      "Epoch [2/5], Step [9588/10336], Loss: 2.0684\n",
      "Epoch [2/5], Step [9590/10336], Loss: 1.0506\n",
      "Epoch [2/5], Step [9592/10336], Loss: 1.0063\n",
      "Epoch [2/5], Step [9594/10336], Loss: 0.3407\n",
      "Epoch [2/5], Step [9596/10336], Loss: 0.1003\n",
      "Epoch [2/5], Step [9598/10336], Loss: 1.1661\n",
      "Epoch [2/5], Step [9600/10336], Loss: 1.6689\n",
      "Epoch [2/5], Step [9602/10336], Loss: 0.3387\n",
      "Epoch [2/5], Step [9604/10336], Loss: 0.4466\n",
      "Epoch [2/5], Step [9606/10336], Loss: 1.0423\n",
      "Epoch [2/5], Step [9608/10336], Loss: 1.1846\n",
      "Epoch [2/5], Step [9610/10336], Loss: 0.8703\n",
      "Epoch [2/5], Step [9612/10336], Loss: 0.1878\n",
      "Epoch [2/5], Step [9614/10336], Loss: 1.3797\n",
      "Epoch [2/5], Step [9616/10336], Loss: 0.4267\n",
      "Epoch [2/5], Step [9618/10336], Loss: 3.4413\n",
      "Epoch [2/5], Step [9620/10336], Loss: 1.1321\n",
      "Epoch [2/5], Step [9622/10336], Loss: 0.0789\n",
      "Epoch [2/5], Step [9624/10336], Loss: 0.2496\n",
      "Epoch [2/5], Step [9626/10336], Loss: 0.0309\n",
      "Epoch [2/5], Step [9628/10336], Loss: 0.0370\n",
      "Epoch [2/5], Step [9630/10336], Loss: 5.9123\n",
      "Epoch [2/5], Step [9632/10336], Loss: 0.1766\n",
      "Epoch [2/5], Step [9634/10336], Loss: 0.2082\n",
      "Epoch [2/5], Step [9636/10336], Loss: 0.5658\n",
      "Epoch [2/5], Step [9638/10336], Loss: 1.3115\n",
      "Epoch [2/5], Step [9640/10336], Loss: 0.0050\n",
      "Epoch [2/5], Step [9642/10336], Loss: 1.6758\n",
      "Epoch [2/5], Step [9644/10336], Loss: 5.9184\n",
      "Epoch [2/5], Step [9646/10336], Loss: 0.7707\n",
      "Epoch [2/5], Step [9648/10336], Loss: 0.6275\n",
      "Epoch [2/5], Step [9650/10336], Loss: 0.8507\n",
      "Epoch [2/5], Step [9652/10336], Loss: 0.0258\n",
      "Epoch [2/5], Step [9654/10336], Loss: 1.5828\n",
      "Epoch [2/5], Step [9656/10336], Loss: 1.3817\n",
      "Epoch [2/5], Step [9658/10336], Loss: 0.0904\n",
      "Epoch [2/5], Step [9660/10336], Loss: 0.1876\n",
      "Epoch [2/5], Step [9662/10336], Loss: 1.1579\n",
      "Epoch [2/5], Step [9664/10336], Loss: 2.2338\n",
      "Epoch [2/5], Step [9666/10336], Loss: 0.2906\n",
      "Epoch [2/5], Step [9668/10336], Loss: 0.0263\n",
      "Epoch [2/5], Step [9670/10336], Loss: 0.3164\n",
      "Epoch [2/5], Step [9672/10336], Loss: 0.0103\n",
      "Epoch [2/5], Step [9674/10336], Loss: 1.4848\n",
      "Epoch [2/5], Step [9676/10336], Loss: 0.0235\n",
      "Epoch [2/5], Step [9678/10336], Loss: 0.1185\n",
      "Epoch [2/5], Step [9680/10336], Loss: 1.2964\n",
      "Epoch [2/5], Step [9682/10336], Loss: 2.2290\n",
      "Epoch [2/5], Step [9684/10336], Loss: 0.8351\n",
      "Epoch [2/5], Step [9686/10336], Loss: 0.0194\n",
      "Epoch [2/5], Step [9688/10336], Loss: 0.6994\n",
      "Epoch [2/5], Step [9690/10336], Loss: 0.6385\n",
      "Epoch [2/5], Step [9692/10336], Loss: 0.3617\n",
      "Epoch [2/5], Step [9694/10336], Loss: 0.1076\n",
      "Epoch [2/5], Step [9696/10336], Loss: 1.8840\n",
      "Epoch [2/5], Step [9698/10336], Loss: 0.0294\n",
      "Epoch [2/5], Step [9700/10336], Loss: 0.0294\n",
      "Epoch [2/5], Step [9702/10336], Loss: 0.3290\n",
      "Epoch [2/5], Step [9704/10336], Loss: 2.9247\n",
      "Epoch [2/5], Step [9706/10336], Loss: 0.2656\n",
      "Epoch [2/5], Step [9708/10336], Loss: 0.5090\n",
      "Epoch [2/5], Step [9710/10336], Loss: 1.6044\n",
      "Epoch [2/5], Step [9712/10336], Loss: 0.2060\n",
      "Epoch [2/5], Step [9714/10336], Loss: 1.0169\n",
      "Epoch [2/5], Step [9716/10336], Loss: 0.1254\n",
      "Epoch [2/5], Step [9718/10336], Loss: 0.5898\n",
      "Epoch [2/5], Step [9720/10336], Loss: 0.7117\n",
      "Epoch [2/5], Step [9722/10336], Loss: 0.0985\n",
      "Epoch [2/5], Step [9724/10336], Loss: 0.1019\n",
      "Epoch [2/5], Step [9726/10336], Loss: 1.3369\n",
      "Epoch [2/5], Step [9728/10336], Loss: 3.1761\n",
      "Epoch [2/5], Step [9730/10336], Loss: 0.7015\n",
      "Epoch [2/5], Step [9732/10336], Loss: 1.3271\n",
      "Epoch [2/5], Step [9734/10336], Loss: 2.6448\n",
      "Epoch [2/5], Step [9736/10336], Loss: 0.0196\n",
      "Epoch [2/5], Step [9738/10336], Loss: 1.7885\n",
      "Epoch [2/5], Step [9740/10336], Loss: 0.1498\n",
      "Epoch [2/5], Step [9742/10336], Loss: 0.0257\n",
      "Epoch [2/5], Step [9744/10336], Loss: 0.3440\n",
      "Epoch [2/5], Step [9746/10336], Loss: 0.3692\n",
      "Epoch [2/5], Step [9748/10336], Loss: 2.6956\n",
      "Epoch [2/5], Step [9750/10336], Loss: 0.0150\n",
      "Epoch [2/5], Step [9752/10336], Loss: 1.0056\n",
      "Epoch [2/5], Step [9754/10336], Loss: 1.8965\n",
      "Epoch [2/5], Step [9756/10336], Loss: 0.5391\n",
      "Epoch [2/5], Step [9758/10336], Loss: 3.7060\n",
      "Epoch [2/5], Step [9760/10336], Loss: 0.0719\n",
      "Epoch [2/5], Step [9762/10336], Loss: 0.1688\n",
      "Epoch [2/5], Step [9764/10336], Loss: 1.9319\n",
      "Epoch [2/5], Step [9766/10336], Loss: 0.4871\n",
      "Epoch [2/5], Step [9768/10336], Loss: 0.0499\n",
      "Epoch [2/5], Step [9770/10336], Loss: 0.0060\n",
      "Epoch [2/5], Step [9772/10336], Loss: 0.6014\n",
      "Epoch [2/5], Step [9774/10336], Loss: 0.0929\n",
      "Epoch [2/5], Step [9776/10336], Loss: 1.5553\n",
      "Epoch [2/5], Step [9778/10336], Loss: 0.2466\n",
      "Epoch [2/5], Step [9780/10336], Loss: 1.3024\n",
      "Epoch [2/5], Step [9782/10336], Loss: 0.0518\n",
      "Epoch [2/5], Step [9784/10336], Loss: 0.3664\n",
      "Epoch [2/5], Step [9786/10336], Loss: 1.6173\n",
      "Epoch [2/5], Step [9788/10336], Loss: 0.1685\n",
      "Epoch [2/5], Step [9790/10336], Loss: 0.4813\n",
      "Epoch [2/5], Step [9792/10336], Loss: 1.6355\n",
      "Epoch [2/5], Step [9794/10336], Loss: 0.0509\n",
      "Epoch [2/5], Step [9796/10336], Loss: 1.4534\n",
      "Epoch [2/5], Step [9798/10336], Loss: 0.9232\n",
      "Epoch [2/5], Step [9800/10336], Loss: 2.3501\n",
      "Epoch [2/5], Step [9802/10336], Loss: 0.6273\n",
      "Epoch [2/5], Step [9804/10336], Loss: 0.0168\n",
      "Epoch [2/5], Step [9806/10336], Loss: 4.8758\n",
      "Epoch [2/5], Step [9808/10336], Loss: 0.2000\n",
      "Epoch [2/5], Step [9810/10336], Loss: 0.0056\n",
      "Epoch [2/5], Step [9812/10336], Loss: 0.2621\n",
      "Epoch [2/5], Step [9814/10336], Loss: 0.2230\n",
      "Epoch [2/5], Step [9816/10336], Loss: 1.3542\n",
      "Epoch [2/5], Step [9818/10336], Loss: 2.2861\n",
      "Epoch [2/5], Step [9820/10336], Loss: 1.9250\n",
      "Epoch [2/5], Step [9822/10336], Loss: 0.3787\n",
      "Epoch [2/5], Step [9824/10336], Loss: 0.0583\n",
      "Epoch [2/5], Step [9826/10336], Loss: 0.7399\n",
      "Epoch [2/5], Step [9828/10336], Loss: 0.1188\n",
      "Epoch [2/5], Step [9830/10336], Loss: 0.7960\n",
      "Epoch [2/5], Step [9832/10336], Loss: 0.1365\n",
      "Epoch [2/5], Step [9834/10336], Loss: 0.0754\n",
      "Epoch [2/5], Step [9836/10336], Loss: 1.3825\n",
      "Epoch [2/5], Step [9838/10336], Loss: 0.8488\n",
      "Epoch [2/5], Step [9840/10336], Loss: 0.1366\n",
      "Epoch [2/5], Step [9842/10336], Loss: 1.7392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [9844/10336], Loss: 1.0803\n",
      "Epoch [2/5], Step [9846/10336], Loss: 0.1258\n",
      "Epoch [2/5], Step [9848/10336], Loss: 0.6127\n",
      "Epoch [2/5], Step [9850/10336], Loss: 0.2107\n",
      "Epoch [2/5], Step [9852/10336], Loss: 0.0709\n",
      "Epoch [2/5], Step [9854/10336], Loss: 0.0880\n",
      "Epoch [2/5], Step [9856/10336], Loss: 0.3844\n",
      "Epoch [2/5], Step [9858/10336], Loss: 0.1362\n",
      "Epoch [2/5], Step [9860/10336], Loss: 0.0224\n",
      "Epoch [2/5], Step [9862/10336], Loss: 0.1417\n",
      "Epoch [2/5], Step [9864/10336], Loss: 0.3703\n",
      "Epoch [2/5], Step [9866/10336], Loss: 1.8330\n",
      "Epoch [2/5], Step [9868/10336], Loss: 1.3379\n",
      "Epoch [2/5], Step [9870/10336], Loss: 1.1800\n",
      "Epoch [2/5], Step [9872/10336], Loss: 0.0313\n",
      "Epoch [2/5], Step [9874/10336], Loss: 1.5599\n",
      "Epoch [2/5], Step [9876/10336], Loss: 0.1554\n",
      "Epoch [2/5], Step [9878/10336], Loss: 0.2854\n",
      "Epoch [2/5], Step [9880/10336], Loss: 1.0201\n",
      "Epoch [2/5], Step [9882/10336], Loss: 0.8188\n",
      "Epoch [2/5], Step [9884/10336], Loss: 0.4618\n",
      "Epoch [2/5], Step [9886/10336], Loss: 1.1732\n",
      "Epoch [2/5], Step [9888/10336], Loss: 0.0278\n",
      "Epoch [2/5], Step [9890/10336], Loss: 0.2940\n",
      "Epoch [2/5], Step [9892/10336], Loss: 3.1210\n",
      "Epoch [2/5], Step [9894/10336], Loss: 0.7414\n",
      "Epoch [2/5], Step [9896/10336], Loss: 1.8542\n",
      "Epoch [2/5], Step [9898/10336], Loss: 0.1850\n",
      "Epoch [2/5], Step [9900/10336], Loss: 0.0376\n",
      "Epoch [2/5], Step [9902/10336], Loss: 2.0761\n",
      "Epoch [2/5], Step [9904/10336], Loss: 0.1170\n",
      "Epoch [2/5], Step [9906/10336], Loss: 0.6570\n",
      "Epoch [2/5], Step [9908/10336], Loss: 0.4741\n",
      "Epoch [2/5], Step [9910/10336], Loss: 1.1471\n",
      "Epoch [2/5], Step [9912/10336], Loss: 0.4652\n",
      "Epoch [2/5], Step [9914/10336], Loss: 0.6660\n",
      "Epoch [2/5], Step [9916/10336], Loss: 0.3240\n",
      "Epoch [2/5], Step [9918/10336], Loss: 0.3123\n",
      "Epoch [2/5], Step [9920/10336], Loss: 0.0164\n",
      "Epoch [2/5], Step [9922/10336], Loss: 0.1262\n",
      "Epoch [2/5], Step [9924/10336], Loss: 2.3769\n",
      "Epoch [2/5], Step [9926/10336], Loss: 1.2567\n",
      "Epoch [2/5], Step [9928/10336], Loss: 0.4916\n",
      "Epoch [2/5], Step [9930/10336], Loss: 2.2752\n",
      "Epoch [2/5], Step [9932/10336], Loss: 0.0751\n",
      "Epoch [2/5], Step [9934/10336], Loss: 0.0213\n",
      "Epoch [2/5], Step [9936/10336], Loss: 0.3766\n",
      "Epoch [2/5], Step [9938/10336], Loss: 1.4605\n",
      "Epoch [2/5], Step [9940/10336], Loss: 0.0029\n",
      "Epoch [2/5], Step [9942/10336], Loss: 0.1068\n",
      "Epoch [2/5], Step [9944/10336], Loss: 0.8887\n",
      "Epoch [2/5], Step [9946/10336], Loss: 1.9258\n",
      "Epoch [2/5], Step [9948/10336], Loss: 3.9663\n",
      "Epoch [2/5], Step [9950/10336], Loss: 2.2969\n",
      "Epoch [2/5], Step [9952/10336], Loss: 1.4336\n",
      "Epoch [2/5], Step [9954/10336], Loss: 0.1361\n",
      "Epoch [2/5], Step [9956/10336], Loss: 0.0426\n",
      "Epoch [2/5], Step [9958/10336], Loss: 0.1915\n",
      "Epoch [2/5], Step [9960/10336], Loss: 0.7525\n",
      "Epoch [2/5], Step [9962/10336], Loss: 0.9954\n",
      "Epoch [2/5], Step [9964/10336], Loss: 0.6004\n",
      "Epoch [2/5], Step [9966/10336], Loss: 0.9660\n",
      "Epoch [2/5], Step [9968/10336], Loss: 0.3159\n",
      "Epoch [2/5], Step [9970/10336], Loss: 0.0191\n",
      "Epoch [2/5], Step [9972/10336], Loss: 0.2507\n",
      "Epoch [2/5], Step [9974/10336], Loss: 1.3885\n",
      "Epoch [2/5], Step [9976/10336], Loss: 0.7559\n",
      "Epoch [2/5], Step [9978/10336], Loss: 0.1580\n",
      "Epoch [2/5], Step [9980/10336], Loss: 0.0105\n",
      "Epoch [2/5], Step [9982/10336], Loss: 0.3642\n",
      "Epoch [2/5], Step [9984/10336], Loss: 0.8020\n",
      "Epoch [2/5], Step [9986/10336], Loss: 0.1366\n",
      "Epoch [2/5], Step [9988/10336], Loss: 0.6753\n",
      "Epoch [2/5], Step [9990/10336], Loss: 1.4804\n",
      "Epoch [2/5], Step [9992/10336], Loss: 0.4335\n",
      "Epoch [2/5], Step [9994/10336], Loss: 4.5181\n",
      "Epoch [2/5], Step [9996/10336], Loss: 2.1161\n",
      "Epoch [2/5], Step [9998/10336], Loss: 0.1545\n",
      "Epoch [2/5], Step [10000/10336], Loss: 0.7412\n",
      "Epoch [2/5], Step [10002/10336], Loss: 0.0170\n",
      "Epoch [2/5], Step [10004/10336], Loss: 0.0184\n",
      "Epoch [2/5], Step [10006/10336], Loss: 0.0877\n",
      "Epoch [2/5], Step [10008/10336], Loss: 0.0252\n",
      "Epoch [2/5], Step [10010/10336], Loss: 0.2414\n",
      "Epoch [2/5], Step [10012/10336], Loss: 0.2075\n",
      "Epoch [2/5], Step [10014/10336], Loss: 0.0575\n",
      "Epoch [2/5], Step [10016/10336], Loss: 0.5510\n",
      "Epoch [2/5], Step [10018/10336], Loss: 1.9691\n",
      "Epoch [2/5], Step [10020/10336], Loss: 0.4528\n",
      "Epoch [2/5], Step [10022/10336], Loss: 0.0219\n",
      "Epoch [2/5], Step [10024/10336], Loss: 0.0050\n",
      "Epoch [2/5], Step [10026/10336], Loss: 0.6318\n",
      "Epoch [2/5], Step [10028/10336], Loss: 0.0776\n",
      "Epoch [2/5], Step [10030/10336], Loss: 2.4795\n",
      "Epoch [2/5], Step [10032/10336], Loss: 0.1253\n",
      "Epoch [2/5], Step [10034/10336], Loss: 0.2295\n",
      "Epoch [2/5], Step [10036/10336], Loss: 0.0990\n",
      "Epoch [2/5], Step [10038/10336], Loss: 0.6226\n",
      "Epoch [2/5], Step [10040/10336], Loss: 0.0051\n",
      "Epoch [2/5], Step [10042/10336], Loss: 3.3804\n",
      "Epoch [2/5], Step [10044/10336], Loss: 0.3823\n",
      "Epoch [2/5], Step [10046/10336], Loss: 1.3285\n",
      "Epoch [2/5], Step [10048/10336], Loss: 0.0174\n",
      "Epoch [2/5], Step [10050/10336], Loss: 0.1467\n",
      "Epoch [2/5], Step [10052/10336], Loss: 0.6343\n",
      "Epoch [2/5], Step [10054/10336], Loss: 1.2148\n",
      "Epoch [2/5], Step [10056/10336], Loss: 1.0482\n",
      "Epoch [2/5], Step [10058/10336], Loss: 0.0478\n",
      "Epoch [2/5], Step [10060/10336], Loss: 0.2137\n",
      "Epoch [2/5], Step [10062/10336], Loss: 2.4399\n",
      "Epoch [2/5], Step [10064/10336], Loss: 0.5254\n",
      "Epoch [2/5], Step [10066/10336], Loss: 0.0043\n",
      "Epoch [2/5], Step [10068/10336], Loss: 0.5284\n",
      "Epoch [2/5], Step [10070/10336], Loss: 1.6209\n",
      "Epoch [2/5], Step [10072/10336], Loss: 1.9182\n",
      "Epoch [2/5], Step [10074/10336], Loss: 0.1331\n",
      "Epoch [2/5], Step [10076/10336], Loss: 0.5093\n",
      "Epoch [2/5], Step [10078/10336], Loss: 0.2349\n",
      "Epoch [2/5], Step [10080/10336], Loss: 0.1782\n",
      "Epoch [2/5], Step [10082/10336], Loss: 2.5058\n",
      "Epoch [2/5], Step [10084/10336], Loss: 1.0174\n",
      "Epoch [2/5], Step [10086/10336], Loss: 1.9221\n",
      "Epoch [2/5], Step [10088/10336], Loss: 1.5687\n",
      "Epoch [2/5], Step [10090/10336], Loss: 0.0614\n",
      "Epoch [2/5], Step [10092/10336], Loss: 1.5044\n",
      "Epoch [2/5], Step [10094/10336], Loss: 1.9572\n",
      "Epoch [2/5], Step [10096/10336], Loss: 0.0046\n",
      "Epoch [2/5], Step [10098/10336], Loss: 0.0080\n",
      "Epoch [2/5], Step [10100/10336], Loss: 0.4341\n",
      "Epoch [2/5], Step [10102/10336], Loss: 1.8828\n",
      "Epoch [2/5], Step [10104/10336], Loss: 1.6602\n",
      "Epoch [2/5], Step [10106/10336], Loss: 0.2330\n",
      "Epoch [2/5], Step [10108/10336], Loss: 0.2106\n",
      "Epoch [2/5], Step [10110/10336], Loss: 1.6452\n",
      "Epoch [2/5], Step [10112/10336], Loss: 0.0077\n",
      "Epoch [2/5], Step [10114/10336], Loss: 3.5698\n",
      "Epoch [2/5], Step [10116/10336], Loss: 1.2572\n",
      "Epoch [2/5], Step [10118/10336], Loss: 0.0374\n",
      "Epoch [2/5], Step [10120/10336], Loss: 1.9698\n",
      "Epoch [2/5], Step [10122/10336], Loss: 0.7669\n",
      "Epoch [2/5], Step [10124/10336], Loss: 0.5227\n",
      "Epoch [2/5], Step [10126/10336], Loss: 2.3524\n",
      "Epoch [2/5], Step [10128/10336], Loss: 0.0765\n",
      "Epoch [2/5], Step [10130/10336], Loss: 0.0607\n",
      "Epoch [2/5], Step [10132/10336], Loss: 2.3199\n",
      "Epoch [2/5], Step [10134/10336], Loss: 1.1330\n",
      "Epoch [2/5], Step [10136/10336], Loss: 0.7665\n",
      "Epoch [2/5], Step [10138/10336], Loss: 0.3365\n",
      "Epoch [2/5], Step [10140/10336], Loss: 1.3845\n",
      "Epoch [2/5], Step [10142/10336], Loss: 1.4303\n",
      "Epoch [2/5], Step [10144/10336], Loss: 0.8131\n",
      "Epoch [2/5], Step [10146/10336], Loss: 0.0646\n",
      "Epoch [2/5], Step [10148/10336], Loss: 0.0258\n",
      "Epoch [2/5], Step [10150/10336], Loss: 0.2279\n",
      "Epoch [2/5], Step [10152/10336], Loss: 0.4517\n",
      "Epoch [2/5], Step [10154/10336], Loss: 2.2426\n",
      "Epoch [2/5], Step [10156/10336], Loss: 2.2722\n",
      "Epoch [2/5], Step [10158/10336], Loss: 0.7584\n",
      "Epoch [2/5], Step [10160/10336], Loss: 0.8933\n",
      "Epoch [2/5], Step [10162/10336], Loss: 0.1176\n",
      "Epoch [2/5], Step [10164/10336], Loss: 0.7078\n",
      "Epoch [2/5], Step [10166/10336], Loss: 0.1681\n",
      "Epoch [2/5], Step [10168/10336], Loss: 2.2079\n",
      "Epoch [2/5], Step [10170/10336], Loss: 0.7858\n",
      "Epoch [2/5], Step [10172/10336], Loss: 0.4815\n",
      "Epoch [2/5], Step [10174/10336], Loss: 1.9684\n",
      "Epoch [2/5], Step [10176/10336], Loss: 0.6713\n",
      "Epoch [2/5], Step [10178/10336], Loss: 0.0757\n",
      "Epoch [2/5], Step [10180/10336], Loss: 1.7079\n",
      "Epoch [2/5], Step [10182/10336], Loss: 4.4696\n",
      "Epoch [2/5], Step [10184/10336], Loss: 0.3153\n",
      "Epoch [2/5], Step [10186/10336], Loss: 0.0347\n",
      "Epoch [2/5], Step [10188/10336], Loss: 0.5276\n",
      "Epoch [2/5], Step [10190/10336], Loss: 4.9777\n",
      "Epoch [2/5], Step [10192/10336], Loss: 0.5750\n",
      "Epoch [2/5], Step [10194/10336], Loss: 1.3465\n",
      "Epoch [2/5], Step [10196/10336], Loss: 0.5511\n",
      "Epoch [2/5], Step [10198/10336], Loss: 0.3868\n",
      "Epoch [2/5], Step [10200/10336], Loss: 4.0523\n",
      "Epoch [2/5], Step [10202/10336], Loss: 1.4321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [10204/10336], Loss: 3.7639\n",
      "Epoch [2/5], Step [10206/10336], Loss: 0.0091\n",
      "Epoch [2/5], Step [10208/10336], Loss: 1.3269\n",
      "Epoch [2/5], Step [10210/10336], Loss: 0.8255\n",
      "Epoch [2/5], Step [10212/10336], Loss: 0.2485\n",
      "Epoch [2/5], Step [10214/10336], Loss: 0.4415\n",
      "Epoch [2/5], Step [10216/10336], Loss: 0.0195\n",
      "Epoch [2/5], Step [10218/10336], Loss: 0.0378\n",
      "Epoch [2/5], Step [10220/10336], Loss: 0.1908\n",
      "Epoch [2/5], Step [10222/10336], Loss: 0.0027\n",
      "Epoch [2/5], Step [10224/10336], Loss: 0.0900\n",
      "Epoch [2/5], Step [10226/10336], Loss: 0.2040\n",
      "Epoch [2/5], Step [10228/10336], Loss: 0.0218\n",
      "Epoch [2/5], Step [10230/10336], Loss: 0.0322\n",
      "Epoch [2/5], Step [10232/10336], Loss: 1.3791\n",
      "Epoch [2/5], Step [10234/10336], Loss: 1.8670\n",
      "Epoch [2/5], Step [10236/10336], Loss: 0.9180\n",
      "Epoch [2/5], Step [10238/10336], Loss: 0.0033\n",
      "Epoch [2/5], Step [10240/10336], Loss: 0.0874\n",
      "Epoch [2/5], Step [10242/10336], Loss: 0.2678\n",
      "Epoch [2/5], Step [10244/10336], Loss: 0.0130\n",
      "Epoch [2/5], Step [10246/10336], Loss: 0.3887\n",
      "Epoch [2/5], Step [10248/10336], Loss: 3.5463\n",
      "Epoch [2/5], Step [10250/10336], Loss: 1.4788\n",
      "Epoch [2/5], Step [10252/10336], Loss: 0.6707\n",
      "Epoch [2/5], Step [10254/10336], Loss: 2.2117\n",
      "Epoch [2/5], Step [10256/10336], Loss: 1.4530\n",
      "Epoch [2/5], Step [10258/10336], Loss: 1.5046\n",
      "Epoch [2/5], Step [10260/10336], Loss: 0.0307\n",
      "Epoch [2/5], Step [10262/10336], Loss: 0.0670\n",
      "Epoch [2/5], Step [10264/10336], Loss: 0.2267\n",
      "Epoch [2/5], Step [10266/10336], Loss: 0.0439\n",
      "Epoch [2/5], Step [10268/10336], Loss: 0.1338\n",
      "Epoch [2/5], Step [10270/10336], Loss: 1.0978\n",
      "Epoch [2/5], Step [10272/10336], Loss: 0.0096\n",
      "Epoch [2/5], Step [10274/10336], Loss: 3.1844\n",
      "Epoch [2/5], Step [10276/10336], Loss: 2.3435\n",
      "Epoch [2/5], Step [10278/10336], Loss: 0.5501\n",
      "Epoch [2/5], Step [10280/10336], Loss: 0.3694\n",
      "Epoch [2/5], Step [10282/10336], Loss: 0.6671\n",
      "Epoch [2/5], Step [10284/10336], Loss: 2.1788\n",
      "Epoch [2/5], Step [10286/10336], Loss: 0.1594\n",
      "Epoch [2/5], Step [10288/10336], Loss: 0.1336\n",
      "Epoch [2/5], Step [10290/10336], Loss: 0.0116\n",
      "Epoch [2/5], Step [10292/10336], Loss: 0.0062\n",
      "Epoch [2/5], Step [10294/10336], Loss: 0.1289\n",
      "Epoch [2/5], Step [10296/10336], Loss: 0.1567\n",
      "Epoch [2/5], Step [10298/10336], Loss: 0.1355\n",
      "Epoch [2/5], Step [10300/10336], Loss: 0.5126\n",
      "Epoch [2/5], Step [10302/10336], Loss: 1.6053\n",
      "Epoch [2/5], Step [10304/10336], Loss: 0.3367\n",
      "Epoch [2/5], Step [10306/10336], Loss: 0.4796\n",
      "Epoch [2/5], Step [10308/10336], Loss: 0.1774\n",
      "Epoch [2/5], Step [10310/10336], Loss: 0.6868\n",
      "Epoch [2/5], Step [10312/10336], Loss: 0.6988\n",
      "Epoch [2/5], Step [10314/10336], Loss: 0.0479\n",
      "Epoch [2/5], Step [10316/10336], Loss: 0.6695\n",
      "Epoch [2/5], Step [10318/10336], Loss: 0.1588\n",
      "Epoch [2/5], Step [10320/10336], Loss: 0.7309\n",
      "Epoch [2/5], Step [10322/10336], Loss: 0.2478\n",
      "Epoch [2/5], Step [10324/10336], Loss: 0.5559\n",
      "Epoch [2/5], Step [10326/10336], Loss: 0.5320\n",
      "Epoch [2/5], Step [10328/10336], Loss: 2.5980\n",
      "Epoch [2/5], Step [10330/10336], Loss: 1.9633\n",
      "Epoch [2/5], Step [10332/10336], Loss: 0.0143\n",
      "Epoch [2/5], Step [10334/10336], Loss: 0.2108\n",
      "Epoch [2/5], Step [10336/10336], Loss: 3.0843\n",
      "Epoch [3/5], Step [2/10336], Loss: 1.6472\n",
      "Epoch [3/5], Step [4/10336], Loss: 0.1228\n",
      "Epoch [3/5], Step [6/10336], Loss: 0.2790\n",
      "Epoch [3/5], Step [8/10336], Loss: 0.4143\n",
      "Epoch [3/5], Step [10/10336], Loss: 0.1088\n",
      "Epoch [3/5], Step [12/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [14/10336], Loss: 0.2482\n",
      "Epoch [3/5], Step [16/10336], Loss: 0.2973\n",
      "Epoch [3/5], Step [18/10336], Loss: 0.2996\n",
      "Epoch [3/5], Step [20/10336], Loss: 0.4338\n",
      "Epoch [3/5], Step [22/10336], Loss: 0.0422\n",
      "Epoch [3/5], Step [24/10336], Loss: 0.7915\n",
      "Epoch [3/5], Step [26/10336], Loss: 1.3557\n",
      "Epoch [3/5], Step [28/10336], Loss: 0.8105\n",
      "Epoch [3/5], Step [30/10336], Loss: 2.3787\n",
      "Epoch [3/5], Step [32/10336], Loss: 1.1222\n",
      "Epoch [3/5], Step [34/10336], Loss: 1.4027\n",
      "Epoch [3/5], Step [36/10336], Loss: 0.5437\n",
      "Epoch [3/5], Step [38/10336], Loss: 2.2279\n",
      "Epoch [3/5], Step [40/10336], Loss: 0.1416\n",
      "Epoch [3/5], Step [42/10336], Loss: 0.0356\n",
      "Epoch [3/5], Step [44/10336], Loss: 0.6006\n",
      "Epoch [3/5], Step [46/10336], Loss: 0.2059\n",
      "Epoch [3/5], Step [48/10336], Loss: 0.4994\n",
      "Epoch [3/5], Step [50/10336], Loss: 2.1914\n",
      "Epoch [3/5], Step [52/10336], Loss: 0.0435\n",
      "Epoch [3/5], Step [54/10336], Loss: 3.4125\n",
      "Epoch [3/5], Step [56/10336], Loss: 1.2468\n",
      "Epoch [3/5], Step [58/10336], Loss: 1.1031\n",
      "Epoch [3/5], Step [60/10336], Loss: 0.8473\n",
      "Epoch [3/5], Step [62/10336], Loss: 0.0283\n",
      "Epoch [3/5], Step [64/10336], Loss: 1.1682\n",
      "Epoch [3/5], Step [66/10336], Loss: 0.3108\n",
      "Epoch [3/5], Step [68/10336], Loss: 0.9600\n",
      "Epoch [3/5], Step [70/10336], Loss: 1.1363\n",
      "Epoch [3/5], Step [72/10336], Loss: 0.0393\n",
      "Epoch [3/5], Step [74/10336], Loss: 0.0074\n",
      "Epoch [3/5], Step [76/10336], Loss: 0.3601\n",
      "Epoch [3/5], Step [78/10336], Loss: 4.1454\n",
      "Epoch [3/5], Step [80/10336], Loss: 0.0295\n",
      "Epoch [3/5], Step [82/10336], Loss: 0.0008\n",
      "Epoch [3/5], Step [84/10336], Loss: 1.3809\n",
      "Epoch [3/5], Step [86/10336], Loss: 1.1928\n",
      "Epoch [3/5], Step [88/10336], Loss: 0.6522\n",
      "Epoch [3/5], Step [90/10336], Loss: 0.0094\n",
      "Epoch [3/5], Step [92/10336], Loss: 0.4460\n",
      "Epoch [3/5], Step [94/10336], Loss: 0.9464\n",
      "Epoch [3/5], Step [96/10336], Loss: 0.4067\n",
      "Epoch [3/5], Step [98/10336], Loss: 1.1280\n",
      "Epoch [3/5], Step [100/10336], Loss: 0.7391\n",
      "Epoch [3/5], Step [102/10336], Loss: 0.0072\n",
      "Epoch [3/5], Step [104/10336], Loss: 1.4060\n",
      "Epoch [3/5], Step [106/10336], Loss: 0.2636\n",
      "Epoch [3/5], Step [108/10336], Loss: 0.5426\n",
      "Epoch [3/5], Step [110/10336], Loss: 0.0260\n",
      "Epoch [3/5], Step [112/10336], Loss: 0.4878\n",
      "Epoch [3/5], Step [114/10336], Loss: 1.1429\n",
      "Epoch [3/5], Step [116/10336], Loss: 2.5502\n",
      "Epoch [3/5], Step [118/10336], Loss: 0.5665\n",
      "Epoch [3/5], Step [120/10336], Loss: 1.6652\n",
      "Epoch [3/5], Step [122/10336], Loss: 0.0105\n",
      "Epoch [3/5], Step [124/10336], Loss: 0.1098\n",
      "Epoch [3/5], Step [126/10336], Loss: 0.3716\n",
      "Epoch [3/5], Step [128/10336], Loss: 0.3891\n",
      "Epoch [3/5], Step [130/10336], Loss: 0.8105\n",
      "Epoch [3/5], Step [132/10336], Loss: 0.1950\n",
      "Epoch [3/5], Step [134/10336], Loss: 1.3644\n",
      "Epoch [3/5], Step [136/10336], Loss: 0.0591\n",
      "Epoch [3/5], Step [138/10336], Loss: 0.5581\n",
      "Epoch [3/5], Step [140/10336], Loss: 0.6817\n",
      "Epoch [3/5], Step [142/10336], Loss: 0.7667\n",
      "Epoch [3/5], Step [144/10336], Loss: 0.0727\n",
      "Epoch [3/5], Step [146/10336], Loss: 0.4924\n",
      "Epoch [3/5], Step [148/10336], Loss: 1.1441\n",
      "Epoch [3/5], Step [150/10336], Loss: 1.1031\n",
      "Epoch [3/5], Step [152/10336], Loss: 0.1074\n",
      "Epoch [3/5], Step [154/10336], Loss: 1.7040\n",
      "Epoch [3/5], Step [156/10336], Loss: 0.4449\n",
      "Epoch [3/5], Step [158/10336], Loss: 3.1074\n",
      "Epoch [3/5], Step [160/10336], Loss: 0.0120\n",
      "Epoch [3/5], Step [162/10336], Loss: 3.8511\n",
      "Epoch [3/5], Step [164/10336], Loss: 0.2029\n",
      "Epoch [3/5], Step [166/10336], Loss: 0.0114\n",
      "Epoch [3/5], Step [168/10336], Loss: 0.7859\n",
      "Epoch [3/5], Step [170/10336], Loss: 0.6456\n",
      "Epoch [3/5], Step [172/10336], Loss: 0.1613\n",
      "Epoch [3/5], Step [174/10336], Loss: 0.0359\n",
      "Epoch [3/5], Step [176/10336], Loss: 0.4730\n",
      "Epoch [3/5], Step [178/10336], Loss: 0.0527\n",
      "Epoch [3/5], Step [180/10336], Loss: 0.0780\n",
      "Epoch [3/5], Step [182/10336], Loss: 0.8237\n",
      "Epoch [3/5], Step [184/10336], Loss: 0.0775\n",
      "Epoch [3/5], Step [186/10336], Loss: 2.5061\n",
      "Epoch [3/5], Step [188/10336], Loss: 0.0428\n",
      "Epoch [3/5], Step [190/10336], Loss: 0.0517\n",
      "Epoch [3/5], Step [192/10336], Loss: 2.1713\n",
      "Epoch [3/5], Step [194/10336], Loss: 0.0933\n",
      "Epoch [3/5], Step [196/10336], Loss: 0.2249\n",
      "Epoch [3/5], Step [198/10336], Loss: 0.5493\n",
      "Epoch [3/5], Step [200/10336], Loss: 1.2536\n",
      "Epoch [3/5], Step [202/10336], Loss: 1.9636\n",
      "Epoch [3/5], Step [204/10336], Loss: 0.0161\n",
      "Epoch [3/5], Step [206/10336], Loss: 0.0314\n",
      "Epoch [3/5], Step [208/10336], Loss: 3.5216\n",
      "Epoch [3/5], Step [210/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [212/10336], Loss: 0.6932\n",
      "Epoch [3/5], Step [214/10336], Loss: 0.8080\n",
      "Epoch [3/5], Step [216/10336], Loss: 2.2934\n",
      "Epoch [3/5], Step [218/10336], Loss: 1.5220\n",
      "Epoch [3/5], Step [220/10336], Loss: 2.6799\n",
      "Epoch [3/5], Step [222/10336], Loss: 0.3188\n",
      "Epoch [3/5], Step [224/10336], Loss: 0.7612\n",
      "Epoch [3/5], Step [226/10336], Loss: 1.9363\n",
      "Epoch [3/5], Step [228/10336], Loss: 0.1072\n",
      "Epoch [3/5], Step [230/10336], Loss: 0.5015\n",
      "Epoch [3/5], Step [232/10336], Loss: 0.1370\n",
      "Epoch [3/5], Step [234/10336], Loss: 0.7531\n",
      "Epoch [3/5], Step [236/10336], Loss: 3.0586\n",
      "Epoch [3/5], Step [238/10336], Loss: 0.3147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [240/10336], Loss: 0.0954\n",
      "Epoch [3/5], Step [242/10336], Loss: 0.0995\n",
      "Epoch [3/5], Step [244/10336], Loss: 0.0794\n",
      "Epoch [3/5], Step [246/10336], Loss: 1.4047\n",
      "Epoch [3/5], Step [248/10336], Loss: 0.0227\n",
      "Epoch [3/5], Step [250/10336], Loss: 2.9471\n",
      "Epoch [3/5], Step [252/10336], Loss: 0.0665\n",
      "Epoch [3/5], Step [254/10336], Loss: 0.3575\n",
      "Epoch [3/5], Step [256/10336], Loss: 0.6640\n",
      "Epoch [3/5], Step [258/10336], Loss: 0.1865\n",
      "Epoch [3/5], Step [260/10336], Loss: 0.5498\n",
      "Epoch [3/5], Step [262/10336], Loss: 0.5700\n",
      "Epoch [3/5], Step [264/10336], Loss: 0.2756\n",
      "Epoch [3/5], Step [266/10336], Loss: 1.6747\n",
      "Epoch [3/5], Step [268/10336], Loss: 3.3916\n",
      "Epoch [3/5], Step [270/10336], Loss: 0.3929\n",
      "Epoch [3/5], Step [272/10336], Loss: 0.1712\n",
      "Epoch [3/5], Step [274/10336], Loss: 0.0104\n",
      "Epoch [3/5], Step [276/10336], Loss: 0.4532\n",
      "Epoch [3/5], Step [278/10336], Loss: 2.3425\n",
      "Epoch [3/5], Step [280/10336], Loss: 0.6319\n",
      "Epoch [3/5], Step [282/10336], Loss: 0.0069\n",
      "Epoch [3/5], Step [284/10336], Loss: 0.0198\n",
      "Epoch [3/5], Step [286/10336], Loss: 0.3521\n",
      "Epoch [3/5], Step [288/10336], Loss: 2.5027\n",
      "Epoch [3/5], Step [290/10336], Loss: 0.2001\n",
      "Epoch [3/5], Step [292/10336], Loss: 0.0223\n",
      "Epoch [3/5], Step [294/10336], Loss: 0.1481\n",
      "Epoch [3/5], Step [296/10336], Loss: 0.0696\n",
      "Epoch [3/5], Step [298/10336], Loss: 0.0122\n",
      "Epoch [3/5], Step [300/10336], Loss: 0.2351\n",
      "Epoch [3/5], Step [302/10336], Loss: 0.2488\n",
      "Epoch [3/5], Step [304/10336], Loss: 3.1277\n",
      "Epoch [3/5], Step [306/10336], Loss: 0.0850\n",
      "Epoch [3/5], Step [308/10336], Loss: 1.6297\n",
      "Epoch [3/5], Step [310/10336], Loss: 0.9282\n",
      "Epoch [3/5], Step [312/10336], Loss: 0.7654\n",
      "Epoch [3/5], Step [314/10336], Loss: 0.6678\n",
      "Epoch [3/5], Step [316/10336], Loss: 0.0473\n",
      "Epoch [3/5], Step [318/10336], Loss: 0.0044\n",
      "Epoch [3/5], Step [320/10336], Loss: 2.0722\n",
      "Epoch [3/5], Step [322/10336], Loss: 1.4820\n",
      "Epoch [3/5], Step [324/10336], Loss: 0.5204\n",
      "Epoch [3/5], Step [326/10336], Loss: 0.2928\n",
      "Epoch [3/5], Step [328/10336], Loss: 0.5767\n",
      "Epoch [3/5], Step [330/10336], Loss: 4.1549\n",
      "Epoch [3/5], Step [332/10336], Loss: 2.7812\n",
      "Epoch [3/5], Step [334/10336], Loss: 0.0395\n",
      "Epoch [3/5], Step [336/10336], Loss: 0.0288\n",
      "Epoch [3/5], Step [338/10336], Loss: 1.1058\n",
      "Epoch [3/5], Step [340/10336], Loss: 1.3793\n",
      "Epoch [3/5], Step [342/10336], Loss: 0.9268\n",
      "Epoch [3/5], Step [344/10336], Loss: 0.4914\n",
      "Epoch [3/5], Step [346/10336], Loss: 0.1096\n",
      "Epoch [3/5], Step [348/10336], Loss: 0.1837\n",
      "Epoch [3/5], Step [350/10336], Loss: 0.2361\n",
      "Epoch [3/5], Step [352/10336], Loss: 0.3841\n",
      "Epoch [3/5], Step [354/10336], Loss: 0.1779\n",
      "Epoch [3/5], Step [356/10336], Loss: 0.0455\n",
      "Epoch [3/5], Step [358/10336], Loss: 0.0496\n",
      "Epoch [3/5], Step [360/10336], Loss: 0.0941\n",
      "Epoch [3/5], Step [362/10336], Loss: 0.9790\n",
      "Epoch [3/5], Step [364/10336], Loss: 0.1375\n",
      "Epoch [3/5], Step [366/10336], Loss: 1.3889\n",
      "Epoch [3/5], Step [368/10336], Loss: 0.7423\n",
      "Epoch [3/5], Step [370/10336], Loss: 0.5215\n",
      "Epoch [3/5], Step [372/10336], Loss: 0.1605\n",
      "Epoch [3/5], Step [374/10336], Loss: 0.4823\n",
      "Epoch [3/5], Step [376/10336], Loss: 0.3246\n",
      "Epoch [3/5], Step [378/10336], Loss: 0.2771\n",
      "Epoch [3/5], Step [380/10336], Loss: 0.0011\n",
      "Epoch [3/5], Step [382/10336], Loss: 0.0069\n",
      "Epoch [3/5], Step [384/10336], Loss: 1.6453\n",
      "Epoch [3/5], Step [386/10336], Loss: 1.1996\n",
      "Epoch [3/5], Step [388/10336], Loss: 0.9238\n",
      "Epoch [3/5], Step [390/10336], Loss: 0.0437\n",
      "Epoch [3/5], Step [392/10336], Loss: 1.4034\n",
      "Epoch [3/5], Step [394/10336], Loss: 0.0374\n",
      "Epoch [3/5], Step [396/10336], Loss: 1.8140\n",
      "Epoch [3/5], Step [398/10336], Loss: 0.8106\n",
      "Epoch [3/5], Step [400/10336], Loss: 0.0795\n",
      "Epoch [3/5], Step [402/10336], Loss: 0.6046\n",
      "Epoch [3/5], Step [404/10336], Loss: 0.0134\n",
      "Epoch [3/5], Step [406/10336], Loss: 0.3184\n",
      "Epoch [3/5], Step [408/10336], Loss: 0.3492\n",
      "Epoch [3/5], Step [410/10336], Loss: 0.0024\n",
      "Epoch [3/5], Step [412/10336], Loss: 0.6660\n",
      "Epoch [3/5], Step [414/10336], Loss: 1.7198\n",
      "Epoch [3/5], Step [416/10336], Loss: 0.4442\n",
      "Epoch [3/5], Step [418/10336], Loss: 0.0668\n",
      "Epoch [3/5], Step [420/10336], Loss: 1.0553\n",
      "Epoch [3/5], Step [422/10336], Loss: 0.4453\n",
      "Epoch [3/5], Step [424/10336], Loss: 1.1346\n",
      "Epoch [3/5], Step [426/10336], Loss: 1.4269\n",
      "Epoch [3/5], Step [428/10336], Loss: 0.0348\n",
      "Epoch [3/5], Step [430/10336], Loss: 0.0418\n",
      "Epoch [3/5], Step [432/10336], Loss: 0.0108\n",
      "Epoch [3/5], Step [434/10336], Loss: 0.0321\n",
      "Epoch [3/5], Step [436/10336], Loss: 0.0485\n",
      "Epoch [3/5], Step [438/10336], Loss: 0.5751\n",
      "Epoch [3/5], Step [440/10336], Loss: 0.0282\n",
      "Epoch [3/5], Step [442/10336], Loss: 0.5914\n",
      "Epoch [3/5], Step [444/10336], Loss: 0.1369\n",
      "Epoch [3/5], Step [446/10336], Loss: 0.9579\n",
      "Epoch [3/5], Step [448/10336], Loss: 0.1125\n",
      "Epoch [3/5], Step [450/10336], Loss: 4.0526\n",
      "Epoch [3/5], Step [452/10336], Loss: 0.0671\n",
      "Epoch [3/5], Step [454/10336], Loss: 0.4240\n",
      "Epoch [3/5], Step [456/10336], Loss: 0.0250\n",
      "Epoch [3/5], Step [458/10336], Loss: 0.4052\n",
      "Epoch [3/5], Step [460/10336], Loss: 0.0367\n",
      "Epoch [3/5], Step [462/10336], Loss: 1.0383\n",
      "Epoch [3/5], Step [464/10336], Loss: 0.1293\n",
      "Epoch [3/5], Step [466/10336], Loss: 2.4484\n",
      "Epoch [3/5], Step [468/10336], Loss: 0.5531\n",
      "Epoch [3/5], Step [470/10336], Loss: 0.0720\n",
      "Epoch [3/5], Step [472/10336], Loss: 0.1489\n",
      "Epoch [3/5], Step [474/10336], Loss: 2.0283\n",
      "Epoch [3/5], Step [476/10336], Loss: 1.7008\n",
      "Epoch [3/5], Step [478/10336], Loss: 0.0849\n",
      "Epoch [3/5], Step [480/10336], Loss: 0.1375\n",
      "Epoch [3/5], Step [482/10336], Loss: 0.0071\n",
      "Epoch [3/5], Step [484/10336], Loss: 0.0164\n",
      "Epoch [3/5], Step [486/10336], Loss: 0.0080\n",
      "Epoch [3/5], Step [488/10336], Loss: 0.3644\n",
      "Epoch [3/5], Step [490/10336], Loss: 0.0330\n",
      "Epoch [3/5], Step [492/10336], Loss: 0.0511\n",
      "Epoch [3/5], Step [494/10336], Loss: 0.5287\n",
      "Epoch [3/5], Step [496/10336], Loss: 0.2350\n",
      "Epoch [3/5], Step [498/10336], Loss: 0.5405\n",
      "Epoch [3/5], Step [500/10336], Loss: 0.0590\n",
      "Epoch [3/5], Step [502/10336], Loss: 1.3454\n",
      "Epoch [3/5], Step [504/10336], Loss: 0.5519\n",
      "Epoch [3/5], Step [506/10336], Loss: 1.1334\n",
      "Epoch [3/5], Step [508/10336], Loss: 0.4255\n",
      "Epoch [3/5], Step [510/10336], Loss: 0.2713\n",
      "Epoch [3/5], Step [512/10336], Loss: 1.3357\n",
      "Epoch [3/5], Step [514/10336], Loss: 0.7465\n",
      "Epoch [3/5], Step [516/10336], Loss: 0.0264\n",
      "Epoch [3/5], Step [518/10336], Loss: 0.3976\n",
      "Epoch [3/5], Step [520/10336], Loss: 0.3615\n",
      "Epoch [3/5], Step [522/10336], Loss: 0.1365\n",
      "Epoch [3/5], Step [524/10336], Loss: 0.2939\n",
      "Epoch [3/5], Step [526/10336], Loss: 1.2312\n",
      "Epoch [3/5], Step [528/10336], Loss: 1.8196\n",
      "Epoch [3/5], Step [530/10336], Loss: 1.7454\n",
      "Epoch [3/5], Step [532/10336], Loss: 1.9988\n",
      "Epoch [3/5], Step [534/10336], Loss: 1.2680\n",
      "Epoch [3/5], Step [536/10336], Loss: 0.0268\n",
      "Epoch [3/5], Step [538/10336], Loss: 0.2815\n",
      "Epoch [3/5], Step [540/10336], Loss: 0.0934\n",
      "Epoch [3/5], Step [542/10336], Loss: 3.8415\n",
      "Epoch [3/5], Step [544/10336], Loss: 0.0255\n",
      "Epoch [3/5], Step [546/10336], Loss: 1.1029\n",
      "Epoch [3/5], Step [548/10336], Loss: 0.9219\n",
      "Epoch [3/5], Step [550/10336], Loss: 0.2062\n",
      "Epoch [3/5], Step [552/10336], Loss: 0.0117\n",
      "Epoch [3/5], Step [554/10336], Loss: 3.8304\n",
      "Epoch [3/5], Step [556/10336], Loss: 0.9364\n",
      "Epoch [3/5], Step [558/10336], Loss: 0.1150\n",
      "Epoch [3/5], Step [560/10336], Loss: 0.6064\n",
      "Epoch [3/5], Step [562/10336], Loss: 0.0217\n",
      "Epoch [3/5], Step [564/10336], Loss: 0.9189\n",
      "Epoch [3/5], Step [566/10336], Loss: 1.2233\n",
      "Epoch [3/5], Step [568/10336], Loss: 0.6062\n",
      "Epoch [3/5], Step [570/10336], Loss: 1.3451\n",
      "Epoch [3/5], Step [572/10336], Loss: 1.2792\n",
      "Epoch [3/5], Step [574/10336], Loss: 1.3869\n",
      "Epoch [3/5], Step [576/10336], Loss: 0.2343\n",
      "Epoch [3/5], Step [578/10336], Loss: 0.8527\n",
      "Epoch [3/5], Step [580/10336], Loss: 1.6762\n",
      "Epoch [3/5], Step [582/10336], Loss: 0.7670\n",
      "Epoch [3/5], Step [584/10336], Loss: 0.1806\n",
      "Epoch [3/5], Step [586/10336], Loss: 0.0107\n",
      "Epoch [3/5], Step [588/10336], Loss: 0.0275\n",
      "Epoch [3/5], Step [590/10336], Loss: 0.0375\n",
      "Epoch [3/5], Step [592/10336], Loss: 1.2857\n",
      "Epoch [3/5], Step [594/10336], Loss: 3.5289\n",
      "Epoch [3/5], Step [596/10336], Loss: 0.4655\n",
      "Epoch [3/5], Step [598/10336], Loss: 1.3824\n",
      "Epoch [3/5], Step [600/10336], Loss: 0.8938\n",
      "Epoch [3/5], Step [602/10336], Loss: 0.1792\n",
      "Epoch [3/5], Step [604/10336], Loss: 2.6644\n",
      "Epoch [3/5], Step [606/10336], Loss: 0.2912\n",
      "Epoch [3/5], Step [608/10336], Loss: 2.0737\n",
      "Epoch [3/5], Step [610/10336], Loss: 3.3863\n",
      "Epoch [3/5], Step [612/10336], Loss: 2.2453\n",
      "Epoch [3/5], Step [614/10336], Loss: 1.0821\n",
      "Epoch [3/5], Step [616/10336], Loss: 1.0963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [618/10336], Loss: 0.1549\n",
      "Epoch [3/5], Step [620/10336], Loss: 0.0240\n",
      "Epoch [3/5], Step [622/10336], Loss: 0.4598\n",
      "Epoch [3/5], Step [624/10336], Loss: 0.5297\n",
      "Epoch [3/5], Step [626/10336], Loss: 1.0068\n",
      "Epoch [3/5], Step [628/10336], Loss: 0.3579\n",
      "Epoch [3/5], Step [630/10336], Loss: 0.5454\n",
      "Epoch [3/5], Step [632/10336], Loss: 0.8425\n",
      "Epoch [3/5], Step [634/10336], Loss: 0.3056\n",
      "Epoch [3/5], Step [636/10336], Loss: 0.4893\n",
      "Epoch [3/5], Step [638/10336], Loss: 1.6772\n",
      "Epoch [3/5], Step [640/10336], Loss: 0.2547\n",
      "Epoch [3/5], Step [642/10336], Loss: 0.0790\n",
      "Epoch [3/5], Step [644/10336], Loss: 0.0090\n",
      "Epoch [3/5], Step [646/10336], Loss: 0.4694\n",
      "Epoch [3/5], Step [648/10336], Loss: 0.8894\n",
      "Epoch [3/5], Step [650/10336], Loss: 0.2952\n",
      "Epoch [3/5], Step [652/10336], Loss: 0.1705\n",
      "Epoch [3/5], Step [654/10336], Loss: 2.9522\n",
      "Epoch [3/5], Step [656/10336], Loss: 1.3905\n",
      "Epoch [3/5], Step [658/10336], Loss: 1.5338\n",
      "Epoch [3/5], Step [660/10336], Loss: 2.7825\n",
      "Epoch [3/5], Step [662/10336], Loss: 1.8238\n",
      "Epoch [3/5], Step [664/10336], Loss: 1.0381\n",
      "Epoch [3/5], Step [666/10336], Loss: 0.5232\n",
      "Epoch [3/5], Step [668/10336], Loss: 0.4263\n",
      "Epoch [3/5], Step [670/10336], Loss: 0.6620\n",
      "Epoch [3/5], Step [672/10336], Loss: 0.5354\n",
      "Epoch [3/5], Step [674/10336], Loss: 0.0627\n",
      "Epoch [3/5], Step [676/10336], Loss: 1.1269\n",
      "Epoch [3/5], Step [678/10336], Loss: 2.3967\n",
      "Epoch [3/5], Step [680/10336], Loss: 0.9600\n",
      "Epoch [3/5], Step [682/10336], Loss: 2.6797\n",
      "Epoch [3/5], Step [684/10336], Loss: 0.0088\n",
      "Epoch [3/5], Step [686/10336], Loss: 0.2650\n",
      "Epoch [3/5], Step [688/10336], Loss: 1.3206\n",
      "Epoch [3/5], Step [690/10336], Loss: 1.3132\n",
      "Epoch [3/5], Step [692/10336], Loss: 2.1641\n",
      "Epoch [3/5], Step [694/10336], Loss: 0.0404\n",
      "Epoch [3/5], Step [696/10336], Loss: 2.3122\n",
      "Epoch [3/5], Step [698/10336], Loss: 1.1814\n",
      "Epoch [3/5], Step [700/10336], Loss: 0.1359\n",
      "Epoch [3/5], Step [702/10336], Loss: 0.2654\n",
      "Epoch [3/5], Step [704/10336], Loss: 0.0394\n",
      "Epoch [3/5], Step [706/10336], Loss: 2.1640\n",
      "Epoch [3/5], Step [708/10336], Loss: 0.3318\n",
      "Epoch [3/5], Step [710/10336], Loss: 0.1890\n",
      "Epoch [3/5], Step [712/10336], Loss: 0.3855\n",
      "Epoch [3/5], Step [714/10336], Loss: 1.8423\n",
      "Epoch [3/5], Step [716/10336], Loss: 0.1512\n",
      "Epoch [3/5], Step [718/10336], Loss: 0.0526\n",
      "Epoch [3/5], Step [720/10336], Loss: 1.0398\n",
      "Epoch [3/5], Step [722/10336], Loss: 0.6790\n",
      "Epoch [3/5], Step [724/10336], Loss: 2.6528\n",
      "Epoch [3/5], Step [726/10336], Loss: 2.3540\n",
      "Epoch [3/5], Step [728/10336], Loss: 0.6915\n",
      "Epoch [3/5], Step [730/10336], Loss: 0.0422\n",
      "Epoch [3/5], Step [732/10336], Loss: 1.1930\n",
      "Epoch [3/5], Step [734/10336], Loss: 1.2781\n",
      "Epoch [3/5], Step [736/10336], Loss: 0.1593\n",
      "Epoch [3/5], Step [738/10336], Loss: 0.1585\n",
      "Epoch [3/5], Step [740/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [742/10336], Loss: 0.3175\n",
      "Epoch [3/5], Step [744/10336], Loss: 0.6128\n",
      "Epoch [3/5], Step [746/10336], Loss: 3.5060\n",
      "Epoch [3/5], Step [748/10336], Loss: 0.0107\n",
      "Epoch [3/5], Step [750/10336], Loss: 0.2392\n",
      "Epoch [3/5], Step [752/10336], Loss: 1.0892\n",
      "Epoch [3/5], Step [754/10336], Loss: 2.2957\n",
      "Epoch [3/5], Step [756/10336], Loss: 2.6761\n",
      "Epoch [3/5], Step [758/10336], Loss: 0.0179\n",
      "Epoch [3/5], Step [760/10336], Loss: 1.4728\n",
      "Epoch [3/5], Step [762/10336], Loss: 0.1024\n",
      "Epoch [3/5], Step [764/10336], Loss: 1.1887\n",
      "Epoch [3/5], Step [766/10336], Loss: 1.2264\n",
      "Epoch [3/5], Step [768/10336], Loss: 0.2610\n",
      "Epoch [3/5], Step [770/10336], Loss: 0.8563\n",
      "Epoch [3/5], Step [772/10336], Loss: 1.0360\n",
      "Epoch [3/5], Step [774/10336], Loss: 0.6897\n",
      "Epoch [3/5], Step [776/10336], Loss: 0.0099\n",
      "Epoch [3/5], Step [778/10336], Loss: 0.6639\n",
      "Epoch [3/5], Step [780/10336], Loss: 0.1666\n",
      "Epoch [3/5], Step [782/10336], Loss: 0.0345\n",
      "Epoch [3/5], Step [784/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [786/10336], Loss: 1.0557\n",
      "Epoch [3/5], Step [788/10336], Loss: 3.7443\n",
      "Epoch [3/5], Step [790/10336], Loss: 0.8940\n",
      "Epoch [3/5], Step [792/10336], Loss: 0.2181\n",
      "Epoch [3/5], Step [794/10336], Loss: 0.1486\n",
      "Epoch [3/5], Step [796/10336], Loss: 0.2425\n",
      "Epoch [3/5], Step [798/10336], Loss: 0.0239\n",
      "Epoch [3/5], Step [800/10336], Loss: 1.6890\n",
      "Epoch [3/5], Step [802/10336], Loss: 1.6989\n",
      "Epoch [3/5], Step [804/10336], Loss: 0.3475\n",
      "Epoch [3/5], Step [806/10336], Loss: 0.7604\n",
      "Epoch [3/5], Step [808/10336], Loss: 0.5234\n",
      "Epoch [3/5], Step [810/10336], Loss: 1.3530\n",
      "Epoch [3/5], Step [812/10336], Loss: 1.7527\n",
      "Epoch [3/5], Step [814/10336], Loss: 0.2273\n",
      "Epoch [3/5], Step [816/10336], Loss: 0.8731\n",
      "Epoch [3/5], Step [818/10336], Loss: 0.1167\n",
      "Epoch [3/5], Step [820/10336], Loss: 0.2042\n",
      "Epoch [3/5], Step [822/10336], Loss: 1.2221\n",
      "Epoch [3/5], Step [824/10336], Loss: 0.5461\n",
      "Epoch [3/5], Step [826/10336], Loss: 0.8959\n",
      "Epoch [3/5], Step [828/10336], Loss: 0.0467\n",
      "Epoch [3/5], Step [830/10336], Loss: 0.2609\n",
      "Epoch [3/5], Step [832/10336], Loss: 0.2470\n",
      "Epoch [3/5], Step [834/10336], Loss: 0.0783\n",
      "Epoch [3/5], Step [836/10336], Loss: 3.1466\n",
      "Epoch [3/5], Step [838/10336], Loss: 1.6387\n",
      "Epoch [3/5], Step [840/10336], Loss: 1.6761\n",
      "Epoch [3/5], Step [842/10336], Loss: 0.1844\n",
      "Epoch [3/5], Step [844/10336], Loss: 0.0149\n",
      "Epoch [3/5], Step [846/10336], Loss: 3.5356\n",
      "Epoch [3/5], Step [848/10336], Loss: 0.0428\n",
      "Epoch [3/5], Step [850/10336], Loss: 1.4482\n",
      "Epoch [3/5], Step [852/10336], Loss: 0.3245\n",
      "Epoch [3/5], Step [854/10336], Loss: 0.0722\n",
      "Epoch [3/5], Step [856/10336], Loss: 0.1871\n",
      "Epoch [3/5], Step [858/10336], Loss: 0.1466\n",
      "Epoch [3/5], Step [860/10336], Loss: 0.0456\n",
      "Epoch [3/5], Step [862/10336], Loss: 0.9603\n",
      "Epoch [3/5], Step [864/10336], Loss: 0.1328\n",
      "Epoch [3/5], Step [866/10336], Loss: 0.1642\n",
      "Epoch [3/5], Step [868/10336], Loss: 0.6292\n",
      "Epoch [3/5], Step [870/10336], Loss: 1.2330\n",
      "Epoch [3/5], Step [872/10336], Loss: 0.0268\n",
      "Epoch [3/5], Step [874/10336], Loss: 1.3600\n",
      "Epoch [3/5], Step [876/10336], Loss: 3.0144\n",
      "Epoch [3/5], Step [878/10336], Loss: 2.2272\n",
      "Epoch [3/5], Step [880/10336], Loss: 5.4147\n",
      "Epoch [3/5], Step [882/10336], Loss: 2.4557\n",
      "Epoch [3/5], Step [884/10336], Loss: 1.0168\n",
      "Epoch [3/5], Step [886/10336], Loss: 1.1234\n",
      "Epoch [3/5], Step [888/10336], Loss: 1.0969\n",
      "Epoch [3/5], Step [890/10336], Loss: 1.9892\n",
      "Epoch [3/5], Step [892/10336], Loss: 0.1732\n",
      "Epoch [3/5], Step [894/10336], Loss: 1.4073\n",
      "Epoch [3/5], Step [896/10336], Loss: 0.7804\n",
      "Epoch [3/5], Step [898/10336], Loss: 0.0754\n",
      "Epoch [3/5], Step [900/10336], Loss: 0.9491\n",
      "Epoch [3/5], Step [902/10336], Loss: 0.8696\n",
      "Epoch [3/5], Step [904/10336], Loss: 0.9985\n",
      "Epoch [3/5], Step [906/10336], Loss: 0.1744\n",
      "Epoch [3/5], Step [908/10336], Loss: 1.4710\n",
      "Epoch [3/5], Step [910/10336], Loss: 0.1630\n",
      "Epoch [3/5], Step [912/10336], Loss: 1.0500\n",
      "Epoch [3/5], Step [914/10336], Loss: 1.4908\n",
      "Epoch [3/5], Step [916/10336], Loss: 0.8404\n",
      "Epoch [3/5], Step [918/10336], Loss: 0.3192\n",
      "Epoch [3/5], Step [920/10336], Loss: 0.0726\n",
      "Epoch [3/5], Step [922/10336], Loss: 0.5950\n",
      "Epoch [3/5], Step [924/10336], Loss: 4.9674\n",
      "Epoch [3/5], Step [926/10336], Loss: 1.4439\n",
      "Epoch [3/5], Step [928/10336], Loss: 0.7516\n",
      "Epoch [3/5], Step [930/10336], Loss: 0.1650\n",
      "Epoch [3/5], Step [932/10336], Loss: 0.0040\n",
      "Epoch [3/5], Step [934/10336], Loss: 0.9455\n",
      "Epoch [3/5], Step [936/10336], Loss: 1.0358\n",
      "Epoch [3/5], Step [938/10336], Loss: 1.8856\n",
      "Epoch [3/5], Step [940/10336], Loss: 1.1527\n",
      "Epoch [3/5], Step [942/10336], Loss: 0.0677\n",
      "Epoch [3/5], Step [944/10336], Loss: 0.0092\n",
      "Epoch [3/5], Step [946/10336], Loss: 0.2157\n",
      "Epoch [3/5], Step [948/10336], Loss: 0.0227\n",
      "Epoch [3/5], Step [950/10336], Loss: 0.0469\n",
      "Epoch [3/5], Step [952/10336], Loss: 0.6761\n",
      "Epoch [3/5], Step [954/10336], Loss: 0.9483\n",
      "Epoch [3/5], Step [956/10336], Loss: 0.0853\n",
      "Epoch [3/5], Step [958/10336], Loss: 0.1256\n",
      "Epoch [3/5], Step [960/10336], Loss: 0.0789\n",
      "Epoch [3/5], Step [962/10336], Loss: 0.1754\n",
      "Epoch [3/5], Step [964/10336], Loss: 3.5066\n",
      "Epoch [3/5], Step [966/10336], Loss: 0.6667\n",
      "Epoch [3/5], Step [968/10336], Loss: 2.1816\n",
      "Epoch [3/5], Step [970/10336], Loss: 1.9042\n",
      "Epoch [3/5], Step [972/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [974/10336], Loss: 0.0069\n",
      "Epoch [3/5], Step [976/10336], Loss: 0.8950\n",
      "Epoch [3/5], Step [978/10336], Loss: 0.1573\n",
      "Epoch [3/5], Step [980/10336], Loss: 1.6054\n",
      "Epoch [3/5], Step [982/10336], Loss: 1.0517\n",
      "Epoch [3/5], Step [984/10336], Loss: 4.6576\n",
      "Epoch [3/5], Step [986/10336], Loss: 2.1846\n",
      "Epoch [3/5], Step [988/10336], Loss: 0.7657\n",
      "Epoch [3/5], Step [990/10336], Loss: 0.1119\n",
      "Epoch [3/5], Step [992/10336], Loss: 2.1671\n",
      "Epoch [3/5], Step [994/10336], Loss: 0.2567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [996/10336], Loss: 0.0510\n",
      "Epoch [3/5], Step [998/10336], Loss: 0.9990\n",
      "Epoch [3/5], Step [1000/10336], Loss: 0.9342\n",
      "Epoch [3/5], Step [1002/10336], Loss: 1.0677\n",
      "Epoch [3/5], Step [1004/10336], Loss: 0.0171\n",
      "Epoch [3/5], Step [1006/10336], Loss: 1.3417\n",
      "Epoch [3/5], Step [1008/10336], Loss: 0.5957\n",
      "Epoch [3/5], Step [1010/10336], Loss: 0.0640\n",
      "Epoch [3/5], Step [1012/10336], Loss: 0.8389\n",
      "Epoch [3/5], Step [1014/10336], Loss: 0.0485\n",
      "Epoch [3/5], Step [1016/10336], Loss: 0.3553\n",
      "Epoch [3/5], Step [1018/10336], Loss: 0.0231\n",
      "Epoch [3/5], Step [1020/10336], Loss: 2.2191\n",
      "Epoch [3/5], Step [1022/10336], Loss: 1.8399\n",
      "Epoch [3/5], Step [1024/10336], Loss: 2.3959\n",
      "Epoch [3/5], Step [1026/10336], Loss: 0.3666\n",
      "Epoch [3/5], Step [1028/10336], Loss: 1.0221\n",
      "Epoch [3/5], Step [1030/10336], Loss: 1.2566\n",
      "Epoch [3/5], Step [1032/10336], Loss: 0.1041\n",
      "Epoch [3/5], Step [1034/10336], Loss: 0.5324\n",
      "Epoch [3/5], Step [1036/10336], Loss: 0.3020\n",
      "Epoch [3/5], Step [1038/10336], Loss: 4.1218\n",
      "Epoch [3/5], Step [1040/10336], Loss: 3.6781\n",
      "Epoch [3/5], Step [1042/10336], Loss: 2.8702\n",
      "Epoch [3/5], Step [1044/10336], Loss: 0.2822\n",
      "Epoch [3/5], Step [1046/10336], Loss: 2.1969\n",
      "Epoch [3/5], Step [1048/10336], Loss: 0.2781\n",
      "Epoch [3/5], Step [1050/10336], Loss: 0.2272\n",
      "Epoch [3/5], Step [1052/10336], Loss: 1.8206\n",
      "Epoch [3/5], Step [1054/10336], Loss: 0.3394\n",
      "Epoch [3/5], Step [1056/10336], Loss: 0.8814\n",
      "Epoch [3/5], Step [1058/10336], Loss: 1.3745\n",
      "Epoch [3/5], Step [1060/10336], Loss: 0.8486\n",
      "Epoch [3/5], Step [1062/10336], Loss: 0.5278\n",
      "Epoch [3/5], Step [1064/10336], Loss: 0.6689\n",
      "Epoch [3/5], Step [1066/10336], Loss: 0.0212\n",
      "Epoch [3/5], Step [1068/10336], Loss: 0.0792\n",
      "Epoch [3/5], Step [1070/10336], Loss: 0.9507\n",
      "Epoch [3/5], Step [1072/10336], Loss: 0.4843\n",
      "Epoch [3/5], Step [1074/10336], Loss: 3.3810\n",
      "Epoch [3/5], Step [1076/10336], Loss: 1.8074\n",
      "Epoch [3/5], Step [1078/10336], Loss: 0.0180\n",
      "Epoch [3/5], Step [1080/10336], Loss: 1.2152\n",
      "Epoch [3/5], Step [1082/10336], Loss: 0.0235\n",
      "Epoch [3/5], Step [1084/10336], Loss: 0.0045\n",
      "Epoch [3/5], Step [1086/10336], Loss: 0.1345\n",
      "Epoch [3/5], Step [1088/10336], Loss: 0.9122\n",
      "Epoch [3/5], Step [1090/10336], Loss: 0.4911\n",
      "Epoch [3/5], Step [1092/10336], Loss: 1.1513\n",
      "Epoch [3/5], Step [1094/10336], Loss: 1.4181\n",
      "Epoch [3/5], Step [1096/10336], Loss: 0.7550\n",
      "Epoch [3/5], Step [1098/10336], Loss: 0.0709\n",
      "Epoch [3/5], Step [1100/10336], Loss: 1.9157\n",
      "Epoch [3/5], Step [1102/10336], Loss: 0.0510\n",
      "Epoch [3/5], Step [1104/10336], Loss: 0.2552\n",
      "Epoch [3/5], Step [1106/10336], Loss: 0.0726\n",
      "Epoch [3/5], Step [1108/10336], Loss: 1.4204\n",
      "Epoch [3/5], Step [1110/10336], Loss: 1.1588\n",
      "Epoch [3/5], Step [1112/10336], Loss: 0.0045\n",
      "Epoch [3/5], Step [1114/10336], Loss: 0.5336\n",
      "Epoch [3/5], Step [1116/10336], Loss: 1.5759\n",
      "Epoch [3/5], Step [1118/10336], Loss: 2.4668\n",
      "Epoch [3/5], Step [1120/10336], Loss: 0.2281\n",
      "Epoch [3/5], Step [1122/10336], Loss: 0.6935\n",
      "Epoch [3/5], Step [1124/10336], Loss: 0.6214\n",
      "Epoch [3/5], Step [1126/10336], Loss: 0.0539\n",
      "Epoch [3/5], Step [1128/10336], Loss: 0.6177\n",
      "Epoch [3/5], Step [1130/10336], Loss: 0.0093\n",
      "Epoch [3/5], Step [1132/10336], Loss: 0.7347\n",
      "Epoch [3/5], Step [1134/10336], Loss: 0.1558\n",
      "Epoch [3/5], Step [1136/10336], Loss: 0.8922\n",
      "Epoch [3/5], Step [1138/10336], Loss: 0.5179\n",
      "Epoch [3/5], Step [1140/10336], Loss: 0.0069\n",
      "Epoch [3/5], Step [1142/10336], Loss: 0.1493\n",
      "Epoch [3/5], Step [1144/10336], Loss: 0.8929\n",
      "Epoch [3/5], Step [1146/10336], Loss: 0.4616\n",
      "Epoch [3/5], Step [1148/10336], Loss: 0.0171\n",
      "Epoch [3/5], Step [1150/10336], Loss: 0.0102\n",
      "Epoch [3/5], Step [1152/10336], Loss: 0.1043\n",
      "Epoch [3/5], Step [1154/10336], Loss: 0.0570\n",
      "Epoch [3/5], Step [1156/10336], Loss: 0.2970\n",
      "Epoch [3/5], Step [1158/10336], Loss: 0.1154\n",
      "Epoch [3/5], Step [1160/10336], Loss: 0.1735\n",
      "Epoch [3/5], Step [1162/10336], Loss: 0.1316\n",
      "Epoch [3/5], Step [1164/10336], Loss: 0.6816\n",
      "Epoch [3/5], Step [1166/10336], Loss: 1.1506\n",
      "Epoch [3/5], Step [1168/10336], Loss: 0.1395\n",
      "Epoch [3/5], Step [1170/10336], Loss: 0.2370\n",
      "Epoch [3/5], Step [1172/10336], Loss: 0.0261\n",
      "Epoch [3/5], Step [1174/10336], Loss: 1.3644\n",
      "Epoch [3/5], Step [1176/10336], Loss: 0.2940\n",
      "Epoch [3/5], Step [1178/10336], Loss: 0.0145\n",
      "Epoch [3/5], Step [1180/10336], Loss: 0.3218\n",
      "Epoch [3/5], Step [1182/10336], Loss: 0.0453\n",
      "Epoch [3/5], Step [1184/10336], Loss: 0.2781\n",
      "Epoch [3/5], Step [1186/10336], Loss: 0.0162\n",
      "Epoch [3/5], Step [1188/10336], Loss: 0.1571\n",
      "Epoch [3/5], Step [1190/10336], Loss: 1.5492\n",
      "Epoch [3/5], Step [1192/10336], Loss: 1.7417\n",
      "Epoch [3/5], Step [1194/10336], Loss: 0.6673\n",
      "Epoch [3/5], Step [1196/10336], Loss: 0.0154\n",
      "Epoch [3/5], Step [1198/10336], Loss: 1.2860\n",
      "Epoch [3/5], Step [1200/10336], Loss: 0.6831\n",
      "Epoch [3/5], Step [1202/10336], Loss: 1.3942\n",
      "Epoch [3/5], Step [1204/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [1206/10336], Loss: 0.0140\n",
      "Epoch [3/5], Step [1208/10336], Loss: 0.6139\n",
      "Epoch [3/5], Step [1210/10336], Loss: 0.0418\n",
      "Epoch [3/5], Step [1212/10336], Loss: 0.7871\n",
      "Epoch [3/5], Step [1214/10336], Loss: 0.8956\n",
      "Epoch [3/5], Step [1216/10336], Loss: 2.3272\n",
      "Epoch [3/5], Step [1218/10336], Loss: 0.8566\n",
      "Epoch [3/5], Step [1220/10336], Loss: 0.6415\n",
      "Epoch [3/5], Step [1222/10336], Loss: 0.3128\n",
      "Epoch [3/5], Step [1224/10336], Loss: 3.5799\n",
      "Epoch [3/5], Step [1226/10336], Loss: 0.0782\n",
      "Epoch [3/5], Step [1228/10336], Loss: 0.0350\n",
      "Epoch [3/5], Step [1230/10336], Loss: 0.0514\n",
      "Epoch [3/5], Step [1232/10336], Loss: 0.0020\n",
      "Epoch [3/5], Step [1234/10336], Loss: 0.5828\n",
      "Epoch [3/5], Step [1236/10336], Loss: 2.6601\n",
      "Epoch [3/5], Step [1238/10336], Loss: 1.3429\n",
      "Epoch [3/5], Step [1240/10336], Loss: 1.2209\n",
      "Epoch [3/5], Step [1242/10336], Loss: 1.9462\n",
      "Epoch [3/5], Step [1244/10336], Loss: 0.7657\n",
      "Epoch [3/5], Step [1246/10336], Loss: 1.3647\n",
      "Epoch [3/5], Step [1248/10336], Loss: 0.2541\n",
      "Epoch [3/5], Step [1250/10336], Loss: 0.9385\n",
      "Epoch [3/5], Step [1252/10336], Loss: 0.1612\n",
      "Epoch [3/5], Step [1254/10336], Loss: 0.2524\n",
      "Epoch [3/5], Step [1256/10336], Loss: 0.0471\n",
      "Epoch [3/5], Step [1258/10336], Loss: 3.3994\n",
      "Epoch [3/5], Step [1260/10336], Loss: 0.0152\n",
      "Epoch [3/5], Step [1262/10336], Loss: 0.2713\n",
      "Epoch [3/5], Step [1264/10336], Loss: 2.5860\n",
      "Epoch [3/5], Step [1266/10336], Loss: 0.0428\n",
      "Epoch [3/5], Step [1268/10336], Loss: 1.3828\n",
      "Epoch [3/5], Step [1270/10336], Loss: 5.8456\n",
      "Epoch [3/5], Step [1272/10336], Loss: 0.0375\n",
      "Epoch [3/5], Step [1274/10336], Loss: 2.2377\n",
      "Epoch [3/5], Step [1276/10336], Loss: 0.0045\n",
      "Epoch [3/5], Step [1278/10336], Loss: 0.0605\n",
      "Epoch [3/5], Step [1280/10336], Loss: 0.0187\n",
      "Epoch [3/5], Step [1282/10336], Loss: 0.2971\n",
      "Epoch [3/5], Step [1284/10336], Loss: 1.2028\n",
      "Epoch [3/5], Step [1286/10336], Loss: 0.0318\n",
      "Epoch [3/5], Step [1288/10336], Loss: 2.9150\n",
      "Epoch [3/5], Step [1290/10336], Loss: 0.0072\n",
      "Epoch [3/5], Step [1292/10336], Loss: 0.1209\n",
      "Epoch [3/5], Step [1294/10336], Loss: 1.0933\n",
      "Epoch [3/5], Step [1296/10336], Loss: 0.2567\n",
      "Epoch [3/5], Step [1298/10336], Loss: 1.1506\n",
      "Epoch [3/5], Step [1300/10336], Loss: 0.1010\n",
      "Epoch [3/5], Step [1302/10336], Loss: 1.5673\n",
      "Epoch [3/5], Step [1304/10336], Loss: 0.3291\n",
      "Epoch [3/5], Step [1306/10336], Loss: 0.0211\n",
      "Epoch [3/5], Step [1308/10336], Loss: 0.4248\n",
      "Epoch [3/5], Step [1310/10336], Loss: 0.1525\n",
      "Epoch [3/5], Step [1312/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [1314/10336], Loss: 1.0836\n",
      "Epoch [3/5], Step [1316/10336], Loss: 0.1778\n",
      "Epoch [3/5], Step [1318/10336], Loss: 0.1648\n",
      "Epoch [3/5], Step [1320/10336], Loss: 0.3705\n",
      "Epoch [3/5], Step [1322/10336], Loss: 0.6626\n",
      "Epoch [3/5], Step [1324/10336], Loss: 2.0447\n",
      "Epoch [3/5], Step [1326/10336], Loss: 0.9211\n",
      "Epoch [3/5], Step [1328/10336], Loss: 0.7999\n",
      "Epoch [3/5], Step [1330/10336], Loss: 0.1542\n",
      "Epoch [3/5], Step [1332/10336], Loss: 0.2121\n",
      "Epoch [3/5], Step [1334/10336], Loss: 0.3845\n",
      "Epoch [3/5], Step [1336/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [1338/10336], Loss: 0.3140\n",
      "Epoch [3/5], Step [1340/10336], Loss: 0.3093\n",
      "Epoch [3/5], Step [1342/10336], Loss: 0.0333\n",
      "Epoch [3/5], Step [1344/10336], Loss: 1.1550\n",
      "Epoch [3/5], Step [1346/10336], Loss: 0.8342\n",
      "Epoch [3/5], Step [1348/10336], Loss: 2.5331\n",
      "Epoch [3/5], Step [1350/10336], Loss: 0.1816\n",
      "Epoch [3/5], Step [1352/10336], Loss: 0.0913\n",
      "Epoch [3/5], Step [1354/10336], Loss: 0.1222\n",
      "Epoch [3/5], Step [1356/10336], Loss: 1.1401\n",
      "Epoch [3/5], Step [1358/10336], Loss: 0.5243\n",
      "Epoch [3/5], Step [1360/10336], Loss: 1.9314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [1362/10336], Loss: 0.1623\n",
      "Epoch [3/5], Step [1364/10336], Loss: 0.0652\n",
      "Epoch [3/5], Step [1366/10336], Loss: 1.1499\n",
      "Epoch [3/5], Step [1368/10336], Loss: 0.2297\n",
      "Epoch [3/5], Step [1370/10336], Loss: 0.2727\n",
      "Epoch [3/5], Step [1372/10336], Loss: 1.4279\n",
      "Epoch [3/5], Step [1374/10336], Loss: 0.0875\n",
      "Epoch [3/5], Step [1376/10336], Loss: 0.0634\n",
      "Epoch [3/5], Step [1378/10336], Loss: 0.6048\n",
      "Epoch [3/5], Step [1380/10336], Loss: 1.5814\n",
      "Epoch [3/5], Step [1382/10336], Loss: 4.4278\n",
      "Epoch [3/5], Step [1384/10336], Loss: 1.6158\n",
      "Epoch [3/5], Step [1386/10336], Loss: 0.3639\n",
      "Epoch [3/5], Step [1388/10336], Loss: 0.2356\n",
      "Epoch [3/5], Step [1390/10336], Loss: 0.0908\n",
      "Epoch [3/5], Step [1392/10336], Loss: 2.7334\n",
      "Epoch [3/5], Step [1394/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [1396/10336], Loss: 0.0892\n",
      "Epoch [3/5], Step [1398/10336], Loss: 0.3699\n",
      "Epoch [3/5], Step [1400/10336], Loss: 0.0029\n",
      "Epoch [3/5], Step [1402/10336], Loss: 0.9187\n",
      "Epoch [3/5], Step [1404/10336], Loss: 0.5823\n",
      "Epoch [3/5], Step [1406/10336], Loss: 1.9408\n",
      "Epoch [3/5], Step [1408/10336], Loss: 0.7437\n",
      "Epoch [3/5], Step [1410/10336], Loss: 0.1849\n",
      "Epoch [3/5], Step [1412/10336], Loss: 0.1598\n",
      "Epoch [3/5], Step [1414/10336], Loss: 0.2842\n",
      "Epoch [3/5], Step [1416/10336], Loss: 1.1225\n",
      "Epoch [3/5], Step [1418/10336], Loss: 1.3633\n",
      "Epoch [3/5], Step [1420/10336], Loss: 0.6699\n",
      "Epoch [3/5], Step [1422/10336], Loss: 0.5235\n",
      "Epoch [3/5], Step [1424/10336], Loss: 0.0107\n",
      "Epoch [3/5], Step [1426/10336], Loss: 0.1817\n",
      "Epoch [3/5], Step [1428/10336], Loss: 0.0477\n",
      "Epoch [3/5], Step [1430/10336], Loss: 1.4254\n",
      "Epoch [3/5], Step [1432/10336], Loss: 0.3178\n",
      "Epoch [3/5], Step [1434/10336], Loss: 0.0189\n",
      "Epoch [3/5], Step [1436/10336], Loss: 0.0298\n",
      "Epoch [3/5], Step [1438/10336], Loss: 0.4417\n",
      "Epoch [3/5], Step [1440/10336], Loss: 0.3692\n",
      "Epoch [3/5], Step [1442/10336], Loss: 0.0061\n",
      "Epoch [3/5], Step [1444/10336], Loss: 0.0145\n",
      "Epoch [3/5], Step [1446/10336], Loss: 1.4537\n",
      "Epoch [3/5], Step [1448/10336], Loss: 0.3655\n",
      "Epoch [3/5], Step [1450/10336], Loss: 0.3420\n",
      "Epoch [3/5], Step [1452/10336], Loss: 0.0326\n",
      "Epoch [3/5], Step [1454/10336], Loss: 0.0163\n",
      "Epoch [3/5], Step [1456/10336], Loss: 2.1857\n",
      "Epoch [3/5], Step [1458/10336], Loss: 2.0249\n",
      "Epoch [3/5], Step [1460/10336], Loss: 0.1057\n",
      "Epoch [3/5], Step [1462/10336], Loss: 0.0853\n",
      "Epoch [3/5], Step [1464/10336], Loss: 0.7379\n",
      "Epoch [3/5], Step [1466/10336], Loss: 2.1078\n",
      "Epoch [3/5], Step [1468/10336], Loss: 1.2624\n",
      "Epoch [3/5], Step [1470/10336], Loss: 1.1233\n",
      "Epoch [3/5], Step [1472/10336], Loss: 0.0062\n",
      "Epoch [3/5], Step [1474/10336], Loss: 1.3655\n",
      "Epoch [3/5], Step [1476/10336], Loss: 0.0414\n",
      "Epoch [3/5], Step [1478/10336], Loss: 0.1207\n",
      "Epoch [3/5], Step [1480/10336], Loss: 0.0054\n",
      "Epoch [3/5], Step [1482/10336], Loss: 0.0552\n",
      "Epoch [3/5], Step [1484/10336], Loss: 1.9891\n",
      "Epoch [3/5], Step [1486/10336], Loss: 0.1190\n",
      "Epoch [3/5], Step [1488/10336], Loss: 0.2127\n",
      "Epoch [3/5], Step [1490/10336], Loss: 2.3867\n",
      "Epoch [3/5], Step [1492/10336], Loss: 1.7170\n",
      "Epoch [3/5], Step [1494/10336], Loss: 0.2774\n",
      "Epoch [3/5], Step [1496/10336], Loss: 0.9783\n",
      "Epoch [3/5], Step [1498/10336], Loss: 0.8641\n",
      "Epoch [3/5], Step [1500/10336], Loss: 0.5679\n",
      "Epoch [3/5], Step [1502/10336], Loss: 0.0099\n",
      "Epoch [3/5], Step [1504/10336], Loss: 0.0744\n",
      "Epoch [3/5], Step [1506/10336], Loss: 0.2520\n",
      "Epoch [3/5], Step [1508/10336], Loss: 0.2039\n",
      "Epoch [3/5], Step [1510/10336], Loss: 0.0217\n",
      "Epoch [3/5], Step [1512/10336], Loss: 0.2894\n",
      "Epoch [3/5], Step [1514/10336], Loss: 0.1467\n",
      "Epoch [3/5], Step [1516/10336], Loss: 0.2824\n",
      "Epoch [3/5], Step [1518/10336], Loss: 0.8355\n",
      "Epoch [3/5], Step [1520/10336], Loss: 0.8496\n",
      "Epoch [3/5], Step [1522/10336], Loss: 0.0532\n",
      "Epoch [3/5], Step [1524/10336], Loss: 2.3102\n",
      "Epoch [3/5], Step [1526/10336], Loss: 0.5902\n",
      "Epoch [3/5], Step [1528/10336], Loss: 0.2619\n",
      "Epoch [3/5], Step [1530/10336], Loss: 0.0498\n",
      "Epoch [3/5], Step [1532/10336], Loss: 0.0288\n",
      "Epoch [3/5], Step [1534/10336], Loss: 0.2648\n",
      "Epoch [3/5], Step [1536/10336], Loss: 1.5670\n",
      "Epoch [3/5], Step [1538/10336], Loss: 0.6370\n",
      "Epoch [3/5], Step [1540/10336], Loss: 1.6186\n",
      "Epoch [3/5], Step [1542/10336], Loss: 0.2036\n",
      "Epoch [3/5], Step [1544/10336], Loss: 0.0127\n",
      "Epoch [3/5], Step [1546/10336], Loss: 1.6924\n",
      "Epoch [3/5], Step [1548/10336], Loss: 0.2633\n",
      "Epoch [3/5], Step [1550/10336], Loss: 1.1470\n",
      "Epoch [3/5], Step [1552/10336], Loss: 2.3255\n",
      "Epoch [3/5], Step [1554/10336], Loss: 4.6323\n",
      "Epoch [3/5], Step [1556/10336], Loss: 1.3612\n",
      "Epoch [3/5], Step [1558/10336], Loss: 0.3965\n",
      "Epoch [3/5], Step [1560/10336], Loss: 2.6275\n",
      "Epoch [3/5], Step [1562/10336], Loss: 2.9546\n",
      "Epoch [3/5], Step [1564/10336], Loss: 0.0050\n",
      "Epoch [3/5], Step [1566/10336], Loss: 0.1604\n",
      "Epoch [3/5], Step [1568/10336], Loss: 0.0245\n",
      "Epoch [3/5], Step [1570/10336], Loss: 0.7167\n",
      "Epoch [3/5], Step [1572/10336], Loss: 0.1803\n",
      "Epoch [3/5], Step [1574/10336], Loss: 0.0207\n",
      "Epoch [3/5], Step [1576/10336], Loss: 0.1023\n",
      "Epoch [3/5], Step [1578/10336], Loss: 0.0335\n",
      "Epoch [3/5], Step [1580/10336], Loss: 1.0546\n",
      "Epoch [3/5], Step [1582/10336], Loss: 0.1234\n",
      "Epoch [3/5], Step [1584/10336], Loss: 0.1537\n",
      "Epoch [3/5], Step [1586/10336], Loss: 1.2210\n",
      "Epoch [3/5], Step [1588/10336], Loss: 2.1095\n",
      "Epoch [3/5], Step [1590/10336], Loss: 0.0091\n",
      "Epoch [3/5], Step [1592/10336], Loss: 0.0789\n",
      "Epoch [3/5], Step [1594/10336], Loss: 0.2496\n",
      "Epoch [3/5], Step [1596/10336], Loss: 0.9238\n",
      "Epoch [3/5], Step [1598/10336], Loss: 2.9184\n",
      "Epoch [3/5], Step [1600/10336], Loss: 0.0401\n",
      "Epoch [3/5], Step [1602/10336], Loss: 1.3050\n",
      "Epoch [3/5], Step [1604/10336], Loss: 2.0712\n",
      "Epoch [3/5], Step [1606/10336], Loss: 0.0638\n",
      "Epoch [3/5], Step [1608/10336], Loss: 0.0266\n",
      "Epoch [3/5], Step [1610/10336], Loss: 1.9937\n",
      "Epoch [3/5], Step [1612/10336], Loss: 2.1652\n",
      "Epoch [3/5], Step [1614/10336], Loss: 0.0141\n",
      "Epoch [3/5], Step [1616/10336], Loss: 0.3985\n",
      "Epoch [3/5], Step [1618/10336], Loss: 0.2798\n",
      "Epoch [3/5], Step [1620/10336], Loss: 0.3144\n",
      "Epoch [3/5], Step [1622/10336], Loss: 0.0640\n",
      "Epoch [3/5], Step [1624/10336], Loss: 0.9495\n",
      "Epoch [3/5], Step [1626/10336], Loss: 0.2261\n",
      "Epoch [3/5], Step [1628/10336], Loss: 0.2422\n",
      "Epoch [3/5], Step [1630/10336], Loss: 0.3333\n",
      "Epoch [3/5], Step [1632/10336], Loss: 0.9844\n",
      "Epoch [3/5], Step [1634/10336], Loss: 0.3183\n",
      "Epoch [3/5], Step [1636/10336], Loss: 5.2878\n",
      "Epoch [3/5], Step [1638/10336], Loss: 0.6202\n",
      "Epoch [3/5], Step [1640/10336], Loss: 0.2126\n",
      "Epoch [3/5], Step [1642/10336], Loss: 2.7838\n",
      "Epoch [3/5], Step [1644/10336], Loss: 0.1180\n",
      "Epoch [3/5], Step [1646/10336], Loss: 1.9213\n",
      "Epoch [3/5], Step [1648/10336], Loss: 0.4740\n",
      "Epoch [3/5], Step [1650/10336], Loss: 0.1301\n",
      "Epoch [3/5], Step [1652/10336], Loss: 0.2363\n",
      "Epoch [3/5], Step [1654/10336], Loss: 1.1780\n",
      "Epoch [3/5], Step [1656/10336], Loss: 0.0004\n",
      "Epoch [3/5], Step [1658/10336], Loss: 0.0165\n",
      "Epoch [3/5], Step [1660/10336], Loss: 0.8465\n",
      "Epoch [3/5], Step [1662/10336], Loss: 1.4521\n",
      "Epoch [3/5], Step [1664/10336], Loss: 3.3518\n",
      "Epoch [3/5], Step [1666/10336], Loss: 0.0164\n",
      "Epoch [3/5], Step [1668/10336], Loss: 0.1317\n",
      "Epoch [3/5], Step [1670/10336], Loss: 0.0468\n",
      "Epoch [3/5], Step [1672/10336], Loss: 0.5146\n",
      "Epoch [3/5], Step [1674/10336], Loss: 0.1922\n",
      "Epoch [3/5], Step [1676/10336], Loss: 0.0812\n",
      "Epoch [3/5], Step [1678/10336], Loss: 0.9006\n",
      "Epoch [3/5], Step [1680/10336], Loss: 0.1162\n",
      "Epoch [3/5], Step [1682/10336], Loss: 3.0856\n",
      "Epoch [3/5], Step [1684/10336], Loss: 0.7155\n",
      "Epoch [3/5], Step [1686/10336], Loss: 2.5395\n",
      "Epoch [3/5], Step [1688/10336], Loss: 0.0165\n",
      "Epoch [3/5], Step [1690/10336], Loss: 0.7460\n",
      "Epoch [3/5], Step [1692/10336], Loss: 0.2030\n",
      "Epoch [3/5], Step [1694/10336], Loss: 2.2090\n",
      "Epoch [3/5], Step [1696/10336], Loss: 0.4013\n",
      "Epoch [3/5], Step [1698/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [1700/10336], Loss: 2.4795\n",
      "Epoch [3/5], Step [1702/10336], Loss: 1.4187\n",
      "Epoch [3/5], Step [1704/10336], Loss: 0.0208\n",
      "Epoch [3/5], Step [1706/10336], Loss: 0.0360\n",
      "Epoch [3/5], Step [1708/10336], Loss: 1.4793\n",
      "Epoch [3/5], Step [1710/10336], Loss: 2.6529\n",
      "Epoch [3/5], Step [1712/10336], Loss: 0.1513\n",
      "Epoch [3/5], Step [1714/10336], Loss: 4.0008\n",
      "Epoch [3/5], Step [1716/10336], Loss: 0.1519\n",
      "Epoch [3/5], Step [1718/10336], Loss: 1.9541\n",
      "Epoch [3/5], Step [1720/10336], Loss: 0.0444\n",
      "Epoch [3/5], Step [1722/10336], Loss: 0.6397\n",
      "Epoch [3/5], Step [1724/10336], Loss: 0.3400\n",
      "Epoch [3/5], Step [1726/10336], Loss: 0.6483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [1728/10336], Loss: 4.2272\n",
      "Epoch [3/5], Step [1730/10336], Loss: 0.7318\n",
      "Epoch [3/5], Step [1732/10336], Loss: 0.0450\n",
      "Epoch [3/5], Step [1734/10336], Loss: 0.1030\n",
      "Epoch [3/5], Step [1736/10336], Loss: 0.1911\n",
      "Epoch [3/5], Step [1738/10336], Loss: 0.7289\n",
      "Epoch [3/5], Step [1740/10336], Loss: 0.0363\n",
      "Epoch [3/5], Step [1742/10336], Loss: 0.0888\n",
      "Epoch [3/5], Step [1744/10336], Loss: 0.0061\n",
      "Epoch [3/5], Step [1746/10336], Loss: 0.3321\n",
      "Epoch [3/5], Step [1748/10336], Loss: 0.0458\n",
      "Epoch [3/5], Step [1750/10336], Loss: 4.2546\n",
      "Epoch [3/5], Step [1752/10336], Loss: 0.0437\n",
      "Epoch [3/5], Step [1754/10336], Loss: 0.0108\n",
      "Epoch [3/5], Step [1756/10336], Loss: 0.2442\n",
      "Epoch [3/5], Step [1758/10336], Loss: 0.0985\n",
      "Epoch [3/5], Step [1760/10336], Loss: 1.1428\n",
      "Epoch [3/5], Step [1762/10336], Loss: 0.1892\n",
      "Epoch [3/5], Step [1764/10336], Loss: 0.1132\n",
      "Epoch [3/5], Step [1766/10336], Loss: 0.3730\n",
      "Epoch [3/5], Step [1768/10336], Loss: 0.0363\n",
      "Epoch [3/5], Step [1770/10336], Loss: 0.0362\n",
      "Epoch [3/5], Step [1772/10336], Loss: 0.1626\n",
      "Epoch [3/5], Step [1774/10336], Loss: 0.0913\n",
      "Epoch [3/5], Step [1776/10336], Loss: 3.3970\n",
      "Epoch [3/5], Step [1778/10336], Loss: 1.4943\n",
      "Epoch [3/5], Step [1780/10336], Loss: 0.0208\n",
      "Epoch [3/5], Step [1782/10336], Loss: 0.1752\n",
      "Epoch [3/5], Step [1784/10336], Loss: 2.6095\n",
      "Epoch [3/5], Step [1786/10336], Loss: 0.9425\n",
      "Epoch [3/5], Step [1788/10336], Loss: 1.8695\n",
      "Epoch [3/5], Step [1790/10336], Loss: 0.0259\n",
      "Epoch [3/5], Step [1792/10336], Loss: 0.7228\n",
      "Epoch [3/5], Step [1794/10336], Loss: 0.1735\n",
      "Epoch [3/5], Step [1796/10336], Loss: 0.9462\n",
      "Epoch [3/5], Step [1798/10336], Loss: 1.8508\n",
      "Epoch [3/5], Step [1800/10336], Loss: 0.0786\n",
      "Epoch [3/5], Step [1802/10336], Loss: 0.2304\n",
      "Epoch [3/5], Step [1804/10336], Loss: 0.0465\n",
      "Epoch [3/5], Step [1806/10336], Loss: 5.0357\n",
      "Epoch [3/5], Step [1808/10336], Loss: 0.0229\n",
      "Epoch [3/5], Step [1810/10336], Loss: 1.2457\n",
      "Epoch [3/5], Step [1812/10336], Loss: 0.0121\n",
      "Epoch [3/5], Step [1814/10336], Loss: 0.5661\n",
      "Epoch [3/5], Step [1816/10336], Loss: 1.2647\n",
      "Epoch [3/5], Step [1818/10336], Loss: 0.5642\n",
      "Epoch [3/5], Step [1820/10336], Loss: 0.2416\n",
      "Epoch [3/5], Step [1822/10336], Loss: 0.2617\n",
      "Epoch [3/5], Step [1824/10336], Loss: 0.0141\n",
      "Epoch [3/5], Step [1826/10336], Loss: 0.1228\n",
      "Epoch [3/5], Step [1828/10336], Loss: 2.1891\n",
      "Epoch [3/5], Step [1830/10336], Loss: 0.0254\n",
      "Epoch [3/5], Step [1832/10336], Loss: 0.5040\n",
      "Epoch [3/5], Step [1834/10336], Loss: 0.5312\n",
      "Epoch [3/5], Step [1836/10336], Loss: 3.2545\n",
      "Epoch [3/5], Step [1838/10336], Loss: 0.0736\n",
      "Epoch [3/5], Step [1840/10336], Loss: 0.0554\n",
      "Epoch [3/5], Step [1842/10336], Loss: 0.4556\n",
      "Epoch [3/5], Step [1844/10336], Loss: 1.0190\n",
      "Epoch [3/5], Step [1846/10336], Loss: 0.0166\n",
      "Epoch [3/5], Step [1848/10336], Loss: 0.0436\n",
      "Epoch [3/5], Step [1850/10336], Loss: 0.6787\n",
      "Epoch [3/5], Step [1852/10336], Loss: 1.8836\n",
      "Epoch [3/5], Step [1854/10336], Loss: 1.6231\n",
      "Epoch [3/5], Step [1856/10336], Loss: 0.0605\n",
      "Epoch [3/5], Step [1858/10336], Loss: 0.1090\n",
      "Epoch [3/5], Step [1860/10336], Loss: 0.1627\n",
      "Epoch [3/5], Step [1862/10336], Loss: 0.0271\n",
      "Epoch [3/5], Step [1864/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [1866/10336], Loss: 1.2956\n",
      "Epoch [3/5], Step [1868/10336], Loss: 0.2078\n",
      "Epoch [3/5], Step [1870/10336], Loss: 1.7789\n",
      "Epoch [3/5], Step [1872/10336], Loss: 1.7568\n",
      "Epoch [3/5], Step [1874/10336], Loss: 0.0859\n",
      "Epoch [3/5], Step [1876/10336], Loss: 0.0657\n",
      "Epoch [3/5], Step [1878/10336], Loss: 0.1249\n",
      "Epoch [3/5], Step [1880/10336], Loss: 0.1228\n",
      "Epoch [3/5], Step [1882/10336], Loss: 0.1008\n",
      "Epoch [3/5], Step [1884/10336], Loss: 0.3613\n",
      "Epoch [3/5], Step [1886/10336], Loss: 1.7232\n",
      "Epoch [3/5], Step [1888/10336], Loss: 1.8556\n",
      "Epoch [3/5], Step [1890/10336], Loss: 0.0421\n",
      "Epoch [3/5], Step [1892/10336], Loss: 2.9366\n",
      "Epoch [3/5], Step [1894/10336], Loss: 0.0093\n",
      "Epoch [3/5], Step [1896/10336], Loss: 0.0100\n",
      "Epoch [3/5], Step [1898/10336], Loss: 0.1009\n",
      "Epoch [3/5], Step [1900/10336], Loss: 0.2493\n",
      "Epoch [3/5], Step [1902/10336], Loss: 0.9936\n",
      "Epoch [3/5], Step [1904/10336], Loss: 0.2132\n",
      "Epoch [3/5], Step [1906/10336], Loss: 1.5947\n",
      "Epoch [3/5], Step [1908/10336], Loss: 0.0734\n",
      "Epoch [3/5], Step [1910/10336], Loss: 0.6492\n",
      "Epoch [3/5], Step [1912/10336], Loss: 1.5758\n",
      "Epoch [3/5], Step [1914/10336], Loss: 1.4651\n",
      "Epoch [3/5], Step [1916/10336], Loss: 0.3275\n",
      "Epoch [3/5], Step [1918/10336], Loss: 0.1469\n",
      "Epoch [3/5], Step [1920/10336], Loss: 0.0021\n",
      "Epoch [3/5], Step [1922/10336], Loss: 0.6068\n",
      "Epoch [3/5], Step [1924/10336], Loss: 1.8173\n",
      "Epoch [3/5], Step [1926/10336], Loss: 0.3095\n",
      "Epoch [3/5], Step [1928/10336], Loss: 0.1969\n",
      "Epoch [3/5], Step [1930/10336], Loss: 0.3868\n",
      "Epoch [3/5], Step [1932/10336], Loss: 1.1605\n",
      "Epoch [3/5], Step [1934/10336], Loss: 0.0040\n",
      "Epoch [3/5], Step [1936/10336], Loss: 1.8547\n",
      "Epoch [3/5], Step [1938/10336], Loss: 1.4616\n",
      "Epoch [3/5], Step [1940/10336], Loss: 0.5747\n",
      "Epoch [3/5], Step [1942/10336], Loss: 0.2428\n",
      "Epoch [3/5], Step [1944/10336], Loss: 0.0096\n",
      "Epoch [3/5], Step [1946/10336], Loss: 2.1933\n",
      "Epoch [3/5], Step [1948/10336], Loss: 1.3647\n",
      "Epoch [3/5], Step [1950/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [1952/10336], Loss: 0.8948\n",
      "Epoch [3/5], Step [1954/10336], Loss: 0.7837\n",
      "Epoch [3/5], Step [1956/10336], Loss: 0.0025\n",
      "Epoch [3/5], Step [1958/10336], Loss: 1.6788\n",
      "Epoch [3/5], Step [1960/10336], Loss: 0.0079\n",
      "Epoch [3/5], Step [1962/10336], Loss: 3.5087\n",
      "Epoch [3/5], Step [1964/10336], Loss: 0.3423\n",
      "Epoch [3/5], Step [1966/10336], Loss: 0.2588\n",
      "Epoch [3/5], Step [1968/10336], Loss: 0.3247\n",
      "Epoch [3/5], Step [1970/10336], Loss: 0.1662\n",
      "Epoch [3/5], Step [1972/10336], Loss: 0.4818\n",
      "Epoch [3/5], Step [1974/10336], Loss: 0.1100\n",
      "Epoch [3/5], Step [1976/10336], Loss: 0.9929\n",
      "Epoch [3/5], Step [1978/10336], Loss: 2.1206\n",
      "Epoch [3/5], Step [1980/10336], Loss: 0.5165\n",
      "Epoch [3/5], Step [1982/10336], Loss: 2.6420\n",
      "Epoch [3/5], Step [1984/10336], Loss: 0.1424\n",
      "Epoch [3/5], Step [1986/10336], Loss: 0.1232\n",
      "Epoch [3/5], Step [1988/10336], Loss: 0.0158\n",
      "Epoch [3/5], Step [1990/10336], Loss: 0.3507\n",
      "Epoch [3/5], Step [1992/10336], Loss: 0.3341\n",
      "Epoch [3/5], Step [1994/10336], Loss: 1.2588\n",
      "Epoch [3/5], Step [1996/10336], Loss: 0.0014\n",
      "Epoch [3/5], Step [1998/10336], Loss: 0.2677\n",
      "Epoch [3/5], Step [2000/10336], Loss: 1.1892\n",
      "Epoch [3/5], Step [2002/10336], Loss: 2.0480\n",
      "Epoch [3/5], Step [2004/10336], Loss: 1.4442\n",
      "Epoch [3/5], Step [2006/10336], Loss: 0.3963\n",
      "Epoch [3/5], Step [2008/10336], Loss: 0.9050\n",
      "Epoch [3/5], Step [2010/10336], Loss: 3.1329\n",
      "Epoch [3/5], Step [2012/10336], Loss: 0.3263\n",
      "Epoch [3/5], Step [2014/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [2016/10336], Loss: 0.2198\n",
      "Epoch [3/5], Step [2018/10336], Loss: 0.0489\n",
      "Epoch [3/5], Step [2020/10336], Loss: 0.2968\n",
      "Epoch [3/5], Step [2022/10336], Loss: 0.5783\n",
      "Epoch [3/5], Step [2024/10336], Loss: 0.0844\n",
      "Epoch [3/5], Step [2026/10336], Loss: 1.2571\n",
      "Epoch [3/5], Step [2028/10336], Loss: 1.9990\n",
      "Epoch [3/5], Step [2030/10336], Loss: 0.8513\n",
      "Epoch [3/5], Step [2032/10336], Loss: 1.7327\n",
      "Epoch [3/5], Step [2034/10336], Loss: 0.4164\n",
      "Epoch [3/5], Step [2036/10336], Loss: 0.4516\n",
      "Epoch [3/5], Step [2038/10336], Loss: 0.4006\n",
      "Epoch [3/5], Step [2040/10336], Loss: 2.4643\n",
      "Epoch [3/5], Step [2042/10336], Loss: 2.1449\n",
      "Epoch [3/5], Step [2044/10336], Loss: 0.0114\n",
      "Epoch [3/5], Step [2046/10336], Loss: 0.0034\n",
      "Epoch [3/5], Step [2048/10336], Loss: 2.9091\n",
      "Epoch [3/5], Step [2050/10336], Loss: 1.6194\n",
      "Epoch [3/5], Step [2052/10336], Loss: 0.8119\n",
      "Epoch [3/5], Step [2054/10336], Loss: 0.2424\n",
      "Epoch [3/5], Step [2056/10336], Loss: 0.2922\n",
      "Epoch [3/5], Step [2058/10336], Loss: 1.6969\n",
      "Epoch [3/5], Step [2060/10336], Loss: 0.1697\n",
      "Epoch [3/5], Step [2062/10336], Loss: 1.1106\n",
      "Epoch [3/5], Step [2064/10336], Loss: 0.1061\n",
      "Epoch [3/5], Step [2066/10336], Loss: 2.4592\n",
      "Epoch [3/5], Step [2068/10336], Loss: 0.0800\n",
      "Epoch [3/5], Step [2070/10336], Loss: 0.8826\n",
      "Epoch [3/5], Step [2072/10336], Loss: 0.0209\n",
      "Epoch [3/5], Step [2074/10336], Loss: 0.7489\n",
      "Epoch [3/5], Step [2076/10336], Loss: 2.1432\n",
      "Epoch [3/5], Step [2078/10336], Loss: 0.1790\n",
      "Epoch [3/5], Step [2080/10336], Loss: 0.1695\n",
      "Epoch [3/5], Step [2082/10336], Loss: 0.0096\n",
      "Epoch [3/5], Step [2084/10336], Loss: 0.0400\n",
      "Epoch [3/5], Step [2086/10336], Loss: 0.0053\n",
      "Epoch [3/5], Step [2088/10336], Loss: 0.0239\n",
      "Epoch [3/5], Step [2090/10336], Loss: 0.3539\n",
      "Epoch [3/5], Step [2092/10336], Loss: 0.0052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [2094/10336], Loss: 0.2271\n",
      "Epoch [3/5], Step [2096/10336], Loss: 0.3674\n",
      "Epoch [3/5], Step [2098/10336], Loss: 0.0965\n",
      "Epoch [3/5], Step [2100/10336], Loss: 0.4669\n",
      "Epoch [3/5], Step [2102/10336], Loss: 1.6145\n",
      "Epoch [3/5], Step [2104/10336], Loss: 0.0770\n",
      "Epoch [3/5], Step [2106/10336], Loss: 0.1709\n",
      "Epoch [3/5], Step [2108/10336], Loss: 0.0532\n",
      "Epoch [3/5], Step [2110/10336], Loss: 1.0471\n",
      "Epoch [3/5], Step [2112/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [2114/10336], Loss: 0.9398\n",
      "Epoch [3/5], Step [2116/10336], Loss: 1.4893\n",
      "Epoch [3/5], Step [2118/10336], Loss: 1.7739\n",
      "Epoch [3/5], Step [2120/10336], Loss: 0.0094\n",
      "Epoch [3/5], Step [2122/10336], Loss: 0.1304\n",
      "Epoch [3/5], Step [2124/10336], Loss: 0.9986\n",
      "Epoch [3/5], Step [2126/10336], Loss: 0.7285\n",
      "Epoch [3/5], Step [2128/10336], Loss: 0.0423\n",
      "Epoch [3/5], Step [2130/10336], Loss: 0.6867\n",
      "Epoch [3/5], Step [2132/10336], Loss: 0.5448\n",
      "Epoch [3/5], Step [2134/10336], Loss: 0.0162\n",
      "Epoch [3/5], Step [2136/10336], Loss: 0.0065\n",
      "Epoch [3/5], Step [2138/10336], Loss: 1.2002\n",
      "Epoch [3/5], Step [2140/10336], Loss: 0.2574\n",
      "Epoch [3/5], Step [2142/10336], Loss: 0.0582\n",
      "Epoch [3/5], Step [2144/10336], Loss: 0.1001\n",
      "Epoch [3/5], Step [2146/10336], Loss: 1.0446\n",
      "Epoch [3/5], Step [2148/10336], Loss: 0.4473\n",
      "Epoch [3/5], Step [2150/10336], Loss: 0.0120\n",
      "Epoch [3/5], Step [2152/10336], Loss: 0.0128\n",
      "Epoch [3/5], Step [2154/10336], Loss: 1.7301\n",
      "Epoch [3/5], Step [2156/10336], Loss: 1.5215\n",
      "Epoch [3/5], Step [2158/10336], Loss: 0.1398\n",
      "Epoch [3/5], Step [2160/10336], Loss: 0.0538\n",
      "Epoch [3/5], Step [2162/10336], Loss: 0.7089\n",
      "Epoch [3/5], Step [2164/10336], Loss: 0.4292\n",
      "Epoch [3/5], Step [2166/10336], Loss: 0.1091\n",
      "Epoch [3/5], Step [2168/10336], Loss: 0.3068\n",
      "Epoch [3/5], Step [2170/10336], Loss: 0.3377\n",
      "Epoch [3/5], Step [2172/10336], Loss: 2.1421\n",
      "Epoch [3/5], Step [2174/10336], Loss: 0.0494\n",
      "Epoch [3/5], Step [2176/10336], Loss: 0.0198\n",
      "Epoch [3/5], Step [2178/10336], Loss: 0.0043\n",
      "Epoch [3/5], Step [2180/10336], Loss: 0.5070\n",
      "Epoch [3/5], Step [2182/10336], Loss: 0.2667\n",
      "Epoch [3/5], Step [2184/10336], Loss: 0.2262\n",
      "Epoch [3/5], Step [2186/10336], Loss: 0.2422\n",
      "Epoch [3/5], Step [2188/10336], Loss: 0.5484\n",
      "Epoch [3/5], Step [2190/10336], Loss: 0.3972\n",
      "Epoch [3/5], Step [2192/10336], Loss: 0.1799\n",
      "Epoch [3/5], Step [2194/10336], Loss: 0.0625\n",
      "Epoch [3/5], Step [2196/10336], Loss: 0.2199\n",
      "Epoch [3/5], Step [2198/10336], Loss: 0.0749\n",
      "Epoch [3/5], Step [2200/10336], Loss: 0.0056\n",
      "Epoch [3/5], Step [2202/10336], Loss: 0.3586\n",
      "Epoch [3/5], Step [2204/10336], Loss: 0.6140\n",
      "Epoch [3/5], Step [2206/10336], Loss: 0.3416\n",
      "Epoch [3/5], Step [2208/10336], Loss: 1.2375\n",
      "Epoch [3/5], Step [2210/10336], Loss: 0.6128\n",
      "Epoch [3/5], Step [2212/10336], Loss: 0.0499\n",
      "Epoch [3/5], Step [2214/10336], Loss: 0.5737\n",
      "Epoch [3/5], Step [2216/10336], Loss: 0.8682\n",
      "Epoch [3/5], Step [2218/10336], Loss: 0.7327\n",
      "Epoch [3/5], Step [2220/10336], Loss: 0.4878\n",
      "Epoch [3/5], Step [2222/10336], Loss: 0.2259\n",
      "Epoch [3/5], Step [2224/10336], Loss: 4.1450\n",
      "Epoch [3/5], Step [2226/10336], Loss: 0.1305\n",
      "Epoch [3/5], Step [2228/10336], Loss: 0.4445\n",
      "Epoch [3/5], Step [2230/10336], Loss: 0.1504\n",
      "Epoch [3/5], Step [2232/10336], Loss: 1.0160\n",
      "Epoch [3/5], Step [2234/10336], Loss: 0.1263\n",
      "Epoch [3/5], Step [2236/10336], Loss: 0.0460\n",
      "Epoch [3/5], Step [2238/10336], Loss: 1.8631\n",
      "Epoch [3/5], Step [2240/10336], Loss: 0.0533\n",
      "Epoch [3/5], Step [2242/10336], Loss: 0.5499\n",
      "Epoch [3/5], Step [2244/10336], Loss: 0.1141\n",
      "Epoch [3/5], Step [2246/10336], Loss: 0.0746\n",
      "Epoch [3/5], Step [2248/10336], Loss: 0.7039\n",
      "Epoch [3/5], Step [2250/10336], Loss: 0.4341\n",
      "Epoch [3/5], Step [2252/10336], Loss: 0.7705\n",
      "Epoch [3/5], Step [2254/10336], Loss: 0.3786\n",
      "Epoch [3/5], Step [2256/10336], Loss: 1.1198\n",
      "Epoch [3/5], Step [2258/10336], Loss: 0.1715\n",
      "Epoch [3/5], Step [2260/10336], Loss: 0.4836\n",
      "Epoch [3/5], Step [2262/10336], Loss: 0.3481\n",
      "Epoch [3/5], Step [2264/10336], Loss: 0.3219\n",
      "Epoch [3/5], Step [2266/10336], Loss: 0.9870\n",
      "Epoch [3/5], Step [2268/10336], Loss: 4.9085\n",
      "Epoch [3/5], Step [2270/10336], Loss: 0.1829\n",
      "Epoch [3/5], Step [2272/10336], Loss: 1.1223\n",
      "Epoch [3/5], Step [2274/10336], Loss: 0.0804\n",
      "Epoch [3/5], Step [2276/10336], Loss: 0.1641\n",
      "Epoch [3/5], Step [2278/10336], Loss: 0.7071\n",
      "Epoch [3/5], Step [2280/10336], Loss: 0.3565\n",
      "Epoch [3/5], Step [2282/10336], Loss: 0.0450\n",
      "Epoch [3/5], Step [2284/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [2286/10336], Loss: 0.6515\n",
      "Epoch [3/5], Step [2288/10336], Loss: 2.9018\n",
      "Epoch [3/5], Step [2290/10336], Loss: 1.1904\n",
      "Epoch [3/5], Step [2292/10336], Loss: 0.0048\n",
      "Epoch [3/5], Step [2294/10336], Loss: 0.0795\n",
      "Epoch [3/5], Step [2296/10336], Loss: 0.0133\n",
      "Epoch [3/5], Step [2298/10336], Loss: 1.3558\n",
      "Epoch [3/5], Step [2300/10336], Loss: 0.0167\n",
      "Epoch [3/5], Step [2302/10336], Loss: 0.1417\n",
      "Epoch [3/5], Step [2304/10336], Loss: 0.0346\n",
      "Epoch [3/5], Step [2306/10336], Loss: 0.1511\n",
      "Epoch [3/5], Step [2308/10336], Loss: 0.0194\n",
      "Epoch [3/5], Step [2310/10336], Loss: 0.2273\n",
      "Epoch [3/5], Step [2312/10336], Loss: 0.0743\n",
      "Epoch [3/5], Step [2314/10336], Loss: 0.0764\n",
      "Epoch [3/5], Step [2316/10336], Loss: 0.1505\n",
      "Epoch [3/5], Step [2318/10336], Loss: 0.4885\n",
      "Epoch [3/5], Step [2320/10336], Loss: 0.0259\n",
      "Epoch [3/5], Step [2322/10336], Loss: 0.3849\n",
      "Epoch [3/5], Step [2324/10336], Loss: 1.5947\n",
      "Epoch [3/5], Step [2326/10336], Loss: 0.1341\n",
      "Epoch [3/5], Step [2328/10336], Loss: 2.1693\n",
      "Epoch [3/5], Step [2330/10336], Loss: 0.1310\n",
      "Epoch [3/5], Step [2332/10336], Loss: 0.0601\n",
      "Epoch [3/5], Step [2334/10336], Loss: 0.0693\n",
      "Epoch [3/5], Step [2336/10336], Loss: 0.2381\n",
      "Epoch [3/5], Step [2338/10336], Loss: 0.5359\n",
      "Epoch [3/5], Step [2340/10336], Loss: 0.0333\n",
      "Epoch [3/5], Step [2342/10336], Loss: 1.4200\n",
      "Epoch [3/5], Step [2344/10336], Loss: 0.0367\n",
      "Epoch [3/5], Step [2346/10336], Loss: 0.8698\n",
      "Epoch [3/5], Step [2348/10336], Loss: 3.1600\n",
      "Epoch [3/5], Step [2350/10336], Loss: 0.0705\n",
      "Epoch [3/5], Step [2352/10336], Loss: 1.9011\n",
      "Epoch [3/5], Step [2354/10336], Loss: 0.5540\n",
      "Epoch [3/5], Step [2356/10336], Loss: 0.7021\n",
      "Epoch [3/5], Step [2358/10336], Loss: 1.4932\n",
      "Epoch [3/5], Step [2360/10336], Loss: 1.3631\n",
      "Epoch [3/5], Step [2362/10336], Loss: 2.4098\n",
      "Epoch [3/5], Step [2364/10336], Loss: 0.0915\n",
      "Epoch [3/5], Step [2366/10336], Loss: 2.2765\n",
      "Epoch [3/5], Step [2368/10336], Loss: 0.4792\n",
      "Epoch [3/5], Step [2370/10336], Loss: 0.0781\n",
      "Epoch [3/5], Step [2372/10336], Loss: 0.1093\n",
      "Epoch [3/5], Step [2374/10336], Loss: 1.1259\n",
      "Epoch [3/5], Step [2376/10336], Loss: 0.3533\n",
      "Epoch [3/5], Step [2378/10336], Loss: 0.0617\n",
      "Epoch [3/5], Step [2380/10336], Loss: 0.1104\n",
      "Epoch [3/5], Step [2382/10336], Loss: 1.6276\n",
      "Epoch [3/5], Step [2384/10336], Loss: 0.6275\n",
      "Epoch [3/5], Step [2386/10336], Loss: 0.2035\n",
      "Epoch [3/5], Step [2388/10336], Loss: 1.7728\n",
      "Epoch [3/5], Step [2390/10336], Loss: 1.3279\n",
      "Epoch [3/5], Step [2392/10336], Loss: 0.0313\n",
      "Epoch [3/5], Step [2394/10336], Loss: 1.5088\n",
      "Epoch [3/5], Step [2396/10336], Loss: 1.7151\n",
      "Epoch [3/5], Step [2398/10336], Loss: 0.0855\n",
      "Epoch [3/5], Step [2400/10336], Loss: 1.9810\n",
      "Epoch [3/5], Step [2402/10336], Loss: 0.0439\n",
      "Epoch [3/5], Step [2404/10336], Loss: 0.6305\n",
      "Epoch [3/5], Step [2406/10336], Loss: 0.7834\n",
      "Epoch [3/5], Step [2408/10336], Loss: 0.8837\n",
      "Epoch [3/5], Step [2410/10336], Loss: 0.3418\n",
      "Epoch [3/5], Step [2412/10336], Loss: 2.7540\n",
      "Epoch [3/5], Step [2414/10336], Loss: 3.1663\n",
      "Epoch [3/5], Step [2416/10336], Loss: 0.7170\n",
      "Epoch [3/5], Step [2418/10336], Loss: 0.0928\n",
      "Epoch [3/5], Step [2420/10336], Loss: 0.6216\n",
      "Epoch [3/5], Step [2422/10336], Loss: 1.3200\n",
      "Epoch [3/5], Step [2424/10336], Loss: 1.2570\n",
      "Epoch [3/5], Step [2426/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [2428/10336], Loss: 0.7661\n",
      "Epoch [3/5], Step [2430/10336], Loss: 0.0168\n",
      "Epoch [3/5], Step [2432/10336], Loss: 0.1251\n",
      "Epoch [3/5], Step [2434/10336], Loss: 0.3036\n",
      "Epoch [3/5], Step [2436/10336], Loss: 0.0522\n",
      "Epoch [3/5], Step [2438/10336], Loss: 0.0419\n",
      "Epoch [3/5], Step [2440/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [2442/10336], Loss: 0.4831\n",
      "Epoch [3/5], Step [2444/10336], Loss: 0.2526\n",
      "Epoch [3/5], Step [2446/10336], Loss: 1.4356\n",
      "Epoch [3/5], Step [2448/10336], Loss: 0.1029\n",
      "Epoch [3/5], Step [2450/10336], Loss: 3.6843\n",
      "Epoch [3/5], Step [2452/10336], Loss: 0.8293\n",
      "Epoch [3/5], Step [2454/10336], Loss: 0.9426\n",
      "Epoch [3/5], Step [2456/10336], Loss: 0.0055\n",
      "Epoch [3/5], Step [2458/10336], Loss: 0.3036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [2460/10336], Loss: 1.9063\n",
      "Epoch [3/5], Step [2462/10336], Loss: 0.7444\n",
      "Epoch [3/5], Step [2464/10336], Loss: 0.0577\n",
      "Epoch [3/5], Step [2466/10336], Loss: 1.3240\n",
      "Epoch [3/5], Step [2468/10336], Loss: 0.1453\n",
      "Epoch [3/5], Step [2470/10336], Loss: 1.3134\n",
      "Epoch [3/5], Step [2472/10336], Loss: 2.7601\n",
      "Epoch [3/5], Step [2474/10336], Loss: 1.5820\n",
      "Epoch [3/5], Step [2476/10336], Loss: 0.4463\n",
      "Epoch [3/5], Step [2478/10336], Loss: 2.7513\n",
      "Epoch [3/5], Step [2480/10336], Loss: 0.5475\n",
      "Epoch [3/5], Step [2482/10336], Loss: 0.2475\n",
      "Epoch [3/5], Step [2484/10336], Loss: 0.8823\n",
      "Epoch [3/5], Step [2486/10336], Loss: 1.1258\n",
      "Epoch [3/5], Step [2488/10336], Loss: 1.5467\n",
      "Epoch [3/5], Step [2490/10336], Loss: 0.0081\n",
      "Epoch [3/5], Step [2492/10336], Loss: 0.1684\n",
      "Epoch [3/5], Step [2494/10336], Loss: 0.2065\n",
      "Epoch [3/5], Step [2496/10336], Loss: 6.5682\n",
      "Epoch [3/5], Step [2498/10336], Loss: 0.0224\n",
      "Epoch [3/5], Step [2500/10336], Loss: 2.3726\n",
      "Epoch [3/5], Step [2502/10336], Loss: 1.0702\n",
      "Epoch [3/5], Step [2504/10336], Loss: 1.9422\n",
      "Epoch [3/5], Step [2506/10336], Loss: 0.3356\n",
      "Epoch [3/5], Step [2508/10336], Loss: 0.8371\n",
      "Epoch [3/5], Step [2510/10336], Loss: 0.3424\n",
      "Epoch [3/5], Step [2512/10336], Loss: 0.0545\n",
      "Epoch [3/5], Step [2514/10336], Loss: 0.1197\n",
      "Epoch [3/5], Step [2516/10336], Loss: 0.0423\n",
      "Epoch [3/5], Step [2518/10336], Loss: 0.9180\n",
      "Epoch [3/5], Step [2520/10336], Loss: 0.4334\n",
      "Epoch [3/5], Step [2522/10336], Loss: 0.0023\n",
      "Epoch [3/5], Step [2524/10336], Loss: 0.0193\n",
      "Epoch [3/5], Step [2526/10336], Loss: 1.5187\n",
      "Epoch [3/5], Step [2528/10336], Loss: 0.1503\n",
      "Epoch [3/5], Step [2530/10336], Loss: 0.0533\n",
      "Epoch [3/5], Step [2532/10336], Loss: 0.0049\n",
      "Epoch [3/5], Step [2534/10336], Loss: 0.1703\n",
      "Epoch [3/5], Step [2536/10336], Loss: 0.4701\n",
      "Epoch [3/5], Step [2538/10336], Loss: 0.8523\n",
      "Epoch [3/5], Step [2540/10336], Loss: 0.0290\n",
      "Epoch [3/5], Step [2542/10336], Loss: 0.0437\n",
      "Epoch [3/5], Step [2544/10336], Loss: 1.0848\n",
      "Epoch [3/5], Step [2546/10336], Loss: 1.7165\n",
      "Epoch [3/5], Step [2548/10336], Loss: 0.0350\n",
      "Epoch [3/5], Step [2550/10336], Loss: 0.0186\n",
      "Epoch [3/5], Step [2552/10336], Loss: 0.0249\n",
      "Epoch [3/5], Step [2554/10336], Loss: 0.5080\n",
      "Epoch [3/5], Step [2556/10336], Loss: 0.1230\n",
      "Epoch [3/5], Step [2558/10336], Loss: 0.8590\n",
      "Epoch [3/5], Step [2560/10336], Loss: 0.2573\n",
      "Epoch [3/5], Step [2562/10336], Loss: 0.6676\n",
      "Epoch [3/5], Step [2564/10336], Loss: 0.3298\n",
      "Epoch [3/5], Step [2566/10336], Loss: 0.6863\n",
      "Epoch [3/5], Step [2568/10336], Loss: 1.0703\n",
      "Epoch [3/5], Step [2570/10336], Loss: 0.2602\n",
      "Epoch [3/5], Step [2572/10336], Loss: 0.2816\n",
      "Epoch [3/5], Step [2574/10336], Loss: 0.4586\n",
      "Epoch [3/5], Step [2576/10336], Loss: 0.2077\n",
      "Epoch [3/5], Step [2578/10336], Loss: 0.6760\n",
      "Epoch [3/5], Step [2580/10336], Loss: 0.0327\n",
      "Epoch [3/5], Step [2582/10336], Loss: 2.2517\n",
      "Epoch [3/5], Step [2584/10336], Loss: 0.0324\n",
      "Epoch [3/5], Step [2586/10336], Loss: 0.3009\n",
      "Epoch [3/5], Step [2588/10336], Loss: 0.0165\n",
      "Epoch [3/5], Step [2590/10336], Loss: 0.0624\n",
      "Epoch [3/5], Step [2592/10336], Loss: 0.4880\n",
      "Epoch [3/5], Step [2594/10336], Loss: 0.0135\n",
      "Epoch [3/5], Step [2596/10336], Loss: 0.4224\n",
      "Epoch [3/5], Step [2598/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [2600/10336], Loss: 1.7148\n",
      "Epoch [3/5], Step [2602/10336], Loss: 1.0128\n",
      "Epoch [3/5], Step [2604/10336], Loss: 3.6514\n",
      "Epoch [3/5], Step [2606/10336], Loss: 0.1955\n",
      "Epoch [3/5], Step [2608/10336], Loss: 0.1058\n",
      "Epoch [3/5], Step [2610/10336], Loss: 1.5484\n",
      "Epoch [3/5], Step [2612/10336], Loss: 0.0020\n",
      "Epoch [3/5], Step [2614/10336], Loss: 1.0137\n",
      "Epoch [3/5], Step [2616/10336], Loss: 0.9576\n",
      "Epoch [3/5], Step [2618/10336], Loss: 0.2038\n",
      "Epoch [3/5], Step [2620/10336], Loss: 0.9073\n",
      "Epoch [3/5], Step [2622/10336], Loss: 0.0687\n",
      "Epoch [3/5], Step [2624/10336], Loss: 0.8159\n",
      "Epoch [3/5], Step [2626/10336], Loss: 0.2671\n",
      "Epoch [3/5], Step [2628/10336], Loss: 1.4379\n",
      "Epoch [3/5], Step [2630/10336], Loss: 0.2310\n",
      "Epoch [3/5], Step [2632/10336], Loss: 2.1269\n",
      "Epoch [3/5], Step [2634/10336], Loss: 1.0588\n",
      "Epoch [3/5], Step [2636/10336], Loss: 0.5251\n",
      "Epoch [3/5], Step [2638/10336], Loss: 2.7697\n",
      "Epoch [3/5], Step [2640/10336], Loss: 0.0070\n",
      "Epoch [3/5], Step [2642/10336], Loss: 1.0175\n",
      "Epoch [3/5], Step [2644/10336], Loss: 0.1704\n",
      "Epoch [3/5], Step [2646/10336], Loss: 0.1674\n",
      "Epoch [3/5], Step [2648/10336], Loss: 1.2598\n",
      "Epoch [3/5], Step [2650/10336], Loss: 1.9479\n",
      "Epoch [3/5], Step [2652/10336], Loss: 1.7107\n",
      "Epoch [3/5], Step [2654/10336], Loss: 0.0679\n",
      "Epoch [3/5], Step [2656/10336], Loss: 1.9547\n",
      "Epoch [3/5], Step [2658/10336], Loss: 0.4803\n",
      "Epoch [3/5], Step [2660/10336], Loss: 0.0306\n",
      "Epoch [3/5], Step [2662/10336], Loss: 0.8009\n",
      "Epoch [3/5], Step [2664/10336], Loss: 0.0749\n",
      "Epoch [3/5], Step [2666/10336], Loss: 1.3707\n",
      "Epoch [3/5], Step [2668/10336], Loss: 0.0312\n",
      "Epoch [3/5], Step [2670/10336], Loss: 0.4775\n",
      "Epoch [3/5], Step [2672/10336], Loss: 2.5464\n",
      "Epoch [3/5], Step [2674/10336], Loss: 1.5401\n",
      "Epoch [3/5], Step [2676/10336], Loss: 0.0557\n",
      "Epoch [3/5], Step [2678/10336], Loss: 1.3041\n",
      "Epoch [3/5], Step [2680/10336], Loss: 1.1546\n",
      "Epoch [3/5], Step [2682/10336], Loss: 0.0242\n",
      "Epoch [3/5], Step [2684/10336], Loss: 0.3411\n",
      "Epoch [3/5], Step [2686/10336], Loss: 1.7115\n",
      "Epoch [3/5], Step [2688/10336], Loss: 0.1428\n",
      "Epoch [3/5], Step [2690/10336], Loss: 0.2280\n",
      "Epoch [3/5], Step [2692/10336], Loss: 0.2494\n",
      "Epoch [3/5], Step [2694/10336], Loss: 1.6682\n",
      "Epoch [3/5], Step [2696/10336], Loss: 0.0068\n",
      "Epoch [3/5], Step [2698/10336], Loss: 1.9570\n",
      "Epoch [3/5], Step [2700/10336], Loss: 0.6441\n",
      "Epoch [3/5], Step [2702/10336], Loss: 0.8180\n",
      "Epoch [3/5], Step [2704/10336], Loss: 0.4368\n",
      "Epoch [3/5], Step [2706/10336], Loss: 1.0453\n",
      "Epoch [3/5], Step [2708/10336], Loss: 2.0570\n",
      "Epoch [3/5], Step [2710/10336], Loss: 2.2647\n",
      "Epoch [3/5], Step [2712/10336], Loss: 0.8448\n",
      "Epoch [3/5], Step [2714/10336], Loss: 0.6388\n",
      "Epoch [3/5], Step [2716/10336], Loss: 0.0480\n",
      "Epoch [3/5], Step [2718/10336], Loss: 0.1155\n",
      "Epoch [3/5], Step [2720/10336], Loss: 0.0108\n",
      "Epoch [3/5], Step [2722/10336], Loss: 0.4276\n",
      "Epoch [3/5], Step [2724/10336], Loss: 0.5913\n",
      "Epoch [3/5], Step [2726/10336], Loss: 0.0487\n",
      "Epoch [3/5], Step [2728/10336], Loss: 0.8932\n",
      "Epoch [3/5], Step [2730/10336], Loss: 0.4265\n",
      "Epoch [3/5], Step [2732/10336], Loss: 0.0062\n",
      "Epoch [3/5], Step [2734/10336], Loss: 0.1707\n",
      "Epoch [3/5], Step [2736/10336], Loss: 0.2148\n",
      "Epoch [3/5], Step [2738/10336], Loss: 0.1039\n",
      "Epoch [3/5], Step [2740/10336], Loss: 0.0940\n",
      "Epoch [3/5], Step [2742/10336], Loss: 1.4484\n",
      "Epoch [3/5], Step [2744/10336], Loss: 1.0528\n",
      "Epoch [3/5], Step [2746/10336], Loss: 0.7444\n",
      "Epoch [3/5], Step [2748/10336], Loss: 0.1082\n",
      "Epoch [3/5], Step [2750/10336], Loss: 0.4975\n",
      "Epoch [3/5], Step [2752/10336], Loss: 1.5203\n",
      "Epoch [3/5], Step [2754/10336], Loss: 0.6878\n",
      "Epoch [3/5], Step [2756/10336], Loss: 2.8371\n",
      "Epoch [3/5], Step [2758/10336], Loss: 2.2944\n",
      "Epoch [3/5], Step [2760/10336], Loss: 1.5797\n",
      "Epoch [3/5], Step [2762/10336], Loss: 0.0091\n",
      "Epoch [3/5], Step [2764/10336], Loss: 0.0125\n",
      "Epoch [3/5], Step [2766/10336], Loss: 0.9503\n",
      "Epoch [3/5], Step [2768/10336], Loss: 0.0790\n",
      "Epoch [3/5], Step [2770/10336], Loss: 0.1651\n",
      "Epoch [3/5], Step [2772/10336], Loss: 0.1543\n",
      "Epoch [3/5], Step [2774/10336], Loss: 1.1159\n",
      "Epoch [3/5], Step [2776/10336], Loss: 0.0074\n",
      "Epoch [3/5], Step [2778/10336], Loss: 1.1332\n",
      "Epoch [3/5], Step [2780/10336], Loss: 0.0171\n",
      "Epoch [3/5], Step [2782/10336], Loss: 0.9227\n",
      "Epoch [3/5], Step [2784/10336], Loss: 0.4780\n",
      "Epoch [3/5], Step [2786/10336], Loss: 0.9168\n",
      "Epoch [3/5], Step [2788/10336], Loss: 0.1740\n",
      "Epoch [3/5], Step [2790/10336], Loss: 2.3544\n",
      "Epoch [3/5], Step [2792/10336], Loss: 0.0866\n",
      "Epoch [3/5], Step [2794/10336], Loss: 1.7596\n",
      "Epoch [3/5], Step [2796/10336], Loss: 0.1049\n",
      "Epoch [3/5], Step [2798/10336], Loss: 0.7190\n",
      "Epoch [3/5], Step [2800/10336], Loss: 2.4907\n",
      "Epoch [3/5], Step [2802/10336], Loss: 2.5779\n",
      "Epoch [3/5], Step [2804/10336], Loss: 2.1531\n",
      "Epoch [3/5], Step [2806/10336], Loss: 0.0908\n",
      "Epoch [3/5], Step [2808/10336], Loss: 0.0109\n",
      "Epoch [3/5], Step [2810/10336], Loss: 0.2871\n",
      "Epoch [3/5], Step [2812/10336], Loss: 0.0612\n",
      "Epoch [3/5], Step [2814/10336], Loss: 1.5069\n",
      "Epoch [3/5], Step [2816/10336], Loss: 1.6247\n",
      "Epoch [3/5], Step [2818/10336], Loss: 0.2930\n",
      "Epoch [3/5], Step [2820/10336], Loss: 0.0686\n",
      "Epoch [3/5], Step [2822/10336], Loss: 0.0768\n",
      "Epoch [3/5], Step [2824/10336], Loss: 1.3039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [2826/10336], Loss: 5.6296\n",
      "Epoch [3/5], Step [2828/10336], Loss: 0.0491\n",
      "Epoch [3/5], Step [2830/10336], Loss: 0.0155\n",
      "Epoch [3/5], Step [2832/10336], Loss: 0.5855\n",
      "Epoch [3/5], Step [2834/10336], Loss: 0.0145\n",
      "Epoch [3/5], Step [2836/10336], Loss: 0.6508\n",
      "Epoch [3/5], Step [2838/10336], Loss: 1.2583\n",
      "Epoch [3/5], Step [2840/10336], Loss: 0.0037\n",
      "Epoch [3/5], Step [2842/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [2844/10336], Loss: 1.3637\n",
      "Epoch [3/5], Step [2846/10336], Loss: 1.0258\n",
      "Epoch [3/5], Step [2848/10336], Loss: 0.4234\n",
      "Epoch [3/5], Step [2850/10336], Loss: 4.0735\n",
      "Epoch [3/5], Step [2852/10336], Loss: 0.1077\n",
      "Epoch [3/5], Step [2854/10336], Loss: 0.0281\n",
      "Epoch [3/5], Step [2856/10336], Loss: 0.0134\n",
      "Epoch [3/5], Step [2858/10336], Loss: 0.0161\n",
      "Epoch [3/5], Step [2860/10336], Loss: 0.0118\n",
      "Epoch [3/5], Step [2862/10336], Loss: 1.2017\n",
      "Epoch [3/5], Step [2864/10336], Loss: 0.2555\n",
      "Epoch [3/5], Step [2866/10336], Loss: 0.6353\n",
      "Epoch [3/5], Step [2868/10336], Loss: 0.3113\n",
      "Epoch [3/5], Step [2870/10336], Loss: 0.0279\n",
      "Epoch [3/5], Step [2872/10336], Loss: 0.2585\n",
      "Epoch [3/5], Step [2874/10336], Loss: 0.1749\n",
      "Epoch [3/5], Step [2876/10336], Loss: 0.0087\n",
      "Epoch [3/5], Step [2878/10336], Loss: 0.3120\n",
      "Epoch [3/5], Step [2880/10336], Loss: 0.2711\n",
      "Epoch [3/5], Step [2882/10336], Loss: 0.0501\n",
      "Epoch [3/5], Step [2884/10336], Loss: 0.0722\n",
      "Epoch [3/5], Step [2886/10336], Loss: 0.4845\n",
      "Epoch [3/5], Step [2888/10336], Loss: 1.6607\n",
      "Epoch [3/5], Step [2890/10336], Loss: 0.1241\n",
      "Epoch [3/5], Step [2892/10336], Loss: 0.9395\n",
      "Epoch [3/5], Step [2894/10336], Loss: 0.0280\n",
      "Epoch [3/5], Step [2896/10336], Loss: 1.9214\n",
      "Epoch [3/5], Step [2898/10336], Loss: 0.0031\n",
      "Epoch [3/5], Step [2900/10336], Loss: 0.9275\n",
      "Epoch [3/5], Step [2902/10336], Loss: 0.2194\n",
      "Epoch [3/5], Step [2904/10336], Loss: 0.5136\n",
      "Epoch [3/5], Step [2906/10336], Loss: 0.0907\n",
      "Epoch [3/5], Step [2908/10336], Loss: 0.0062\n",
      "Epoch [3/5], Step [2910/10336], Loss: 2.3762\n",
      "Epoch [3/5], Step [2912/10336], Loss: 0.1131\n",
      "Epoch [3/5], Step [2914/10336], Loss: 3.4342\n",
      "Epoch [3/5], Step [2916/10336], Loss: 0.0164\n",
      "Epoch [3/5], Step [2918/10336], Loss: 0.2435\n",
      "Epoch [3/5], Step [2920/10336], Loss: 1.1743\n",
      "Epoch [3/5], Step [2922/10336], Loss: 1.9719\n",
      "Epoch [3/5], Step [2924/10336], Loss: 2.5595\n",
      "Epoch [3/5], Step [2926/10336], Loss: 0.0196\n",
      "Epoch [3/5], Step [2928/10336], Loss: 2.7049\n",
      "Epoch [3/5], Step [2930/10336], Loss: 0.0138\n",
      "Epoch [3/5], Step [2932/10336], Loss: 2.9029\n",
      "Epoch [3/5], Step [2934/10336], Loss: 1.3458\n",
      "Epoch [3/5], Step [2936/10336], Loss: 0.0030\n",
      "Epoch [3/5], Step [2938/10336], Loss: 1.7859\n",
      "Epoch [3/5], Step [2940/10336], Loss: 0.9361\n",
      "Epoch [3/5], Step [2942/10336], Loss: 0.0272\n",
      "Epoch [3/5], Step [2944/10336], Loss: 0.0938\n",
      "Epoch [3/5], Step [2946/10336], Loss: 1.6833\n",
      "Epoch [3/5], Step [2948/10336], Loss: 0.2307\n",
      "Epoch [3/5], Step [2950/10336], Loss: 0.1325\n",
      "Epoch [3/5], Step [2952/10336], Loss: 0.0732\n",
      "Epoch [3/5], Step [2954/10336], Loss: 0.3063\n",
      "Epoch [3/5], Step [2956/10336], Loss: 2.0199\n",
      "Epoch [3/5], Step [2958/10336], Loss: 0.0051\n",
      "Epoch [3/5], Step [2960/10336], Loss: 2.7478\n",
      "Epoch [3/5], Step [2962/10336], Loss: 0.1899\n",
      "Epoch [3/5], Step [2964/10336], Loss: 0.1894\n",
      "Epoch [3/5], Step [2966/10336], Loss: 0.4223\n",
      "Epoch [3/5], Step [2968/10336], Loss: 0.0082\n",
      "Epoch [3/5], Step [2970/10336], Loss: 0.9325\n",
      "Epoch [3/5], Step [2972/10336], Loss: 0.0850\n",
      "Epoch [3/5], Step [2974/10336], Loss: 0.4890\n",
      "Epoch [3/5], Step [2976/10336], Loss: 0.4371\n",
      "Epoch [3/5], Step [2978/10336], Loss: 0.5285\n",
      "Epoch [3/5], Step [2980/10336], Loss: 0.9476\n",
      "Epoch [3/5], Step [2982/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [2984/10336], Loss: 1.6449\n",
      "Epoch [3/5], Step [2986/10336], Loss: 1.0781\n",
      "Epoch [3/5], Step [2988/10336], Loss: 0.1753\n",
      "Epoch [3/5], Step [2990/10336], Loss: 0.9491\n",
      "Epoch [3/5], Step [2992/10336], Loss: 0.2312\n",
      "Epoch [3/5], Step [2994/10336], Loss: 0.5360\n",
      "Epoch [3/5], Step [2996/10336], Loss: 0.0414\n",
      "Epoch [3/5], Step [2998/10336], Loss: 5.6360\n",
      "Epoch [3/5], Step [3000/10336], Loss: 0.6912\n",
      "Epoch [3/5], Step [3002/10336], Loss: 0.1185\n",
      "Epoch [3/5], Step [3004/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [3006/10336], Loss: 0.0130\n",
      "Epoch [3/5], Step [3008/10336], Loss: 0.5000\n",
      "Epoch [3/5], Step [3010/10336], Loss: 5.1271\n",
      "Epoch [3/5], Step [3012/10336], Loss: 0.3861\n",
      "Epoch [3/5], Step [3014/10336], Loss: 0.0677\n",
      "Epoch [3/5], Step [3016/10336], Loss: 1.7117\n",
      "Epoch [3/5], Step [3018/10336], Loss: 0.7718\n",
      "Epoch [3/5], Step [3020/10336], Loss: 0.5131\n",
      "Epoch [3/5], Step [3022/10336], Loss: 1.7034\n",
      "Epoch [3/5], Step [3024/10336], Loss: 1.2753\n",
      "Epoch [3/5], Step [3026/10336], Loss: 2.0921\n",
      "Epoch [3/5], Step [3028/10336], Loss: 0.1011\n",
      "Epoch [3/5], Step [3030/10336], Loss: 0.0822\n",
      "Epoch [3/5], Step [3032/10336], Loss: 0.0170\n",
      "Epoch [3/5], Step [3034/10336], Loss: 0.3456\n",
      "Epoch [3/5], Step [3036/10336], Loss: 0.1395\n",
      "Epoch [3/5], Step [3038/10336], Loss: 0.3483\n",
      "Epoch [3/5], Step [3040/10336], Loss: 1.8574\n",
      "Epoch [3/5], Step [3042/10336], Loss: 0.0150\n",
      "Epoch [3/5], Step [3044/10336], Loss: 0.4741\n",
      "Epoch [3/5], Step [3046/10336], Loss: 0.0599\n",
      "Epoch [3/5], Step [3048/10336], Loss: 1.5121\n",
      "Epoch [3/5], Step [3050/10336], Loss: 1.6597\n",
      "Epoch [3/5], Step [3052/10336], Loss: 0.1608\n",
      "Epoch [3/5], Step [3054/10336], Loss: 0.4364\n",
      "Epoch [3/5], Step [3056/10336], Loss: 3.3832\n",
      "Epoch [3/5], Step [3058/10336], Loss: 0.4802\n",
      "Epoch [3/5], Step [3060/10336], Loss: 2.1494\n",
      "Epoch [3/5], Step [3062/10336], Loss: 0.0602\n",
      "Epoch [3/5], Step [3064/10336], Loss: 0.7597\n",
      "Epoch [3/5], Step [3066/10336], Loss: 0.0092\n",
      "Epoch [3/5], Step [3068/10336], Loss: 0.8926\n",
      "Epoch [3/5], Step [3070/10336], Loss: 2.7416\n",
      "Epoch [3/5], Step [3072/10336], Loss: 1.0313\n",
      "Epoch [3/5], Step [3074/10336], Loss: 0.2619\n",
      "Epoch [3/5], Step [3076/10336], Loss: 0.1970\n",
      "Epoch [3/5], Step [3078/10336], Loss: 0.0056\n",
      "Epoch [3/5], Step [3080/10336], Loss: 1.9449\n",
      "Epoch [3/5], Step [3082/10336], Loss: 0.6876\n",
      "Epoch [3/5], Step [3084/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [3086/10336], Loss: 0.0241\n",
      "Epoch [3/5], Step [3088/10336], Loss: 0.9618\n",
      "Epoch [3/5], Step [3090/10336], Loss: 0.3584\n",
      "Epoch [3/5], Step [3092/10336], Loss: 1.3155\n",
      "Epoch [3/5], Step [3094/10336], Loss: 1.5899\n",
      "Epoch [3/5], Step [3096/10336], Loss: 0.0448\n",
      "Epoch [3/5], Step [3098/10336], Loss: 0.7045\n",
      "Epoch [3/5], Step [3100/10336], Loss: 0.0094\n",
      "Epoch [3/5], Step [3102/10336], Loss: 0.3490\n",
      "Epoch [3/5], Step [3104/10336], Loss: 1.1582\n",
      "Epoch [3/5], Step [3106/10336], Loss: 2.0866\n",
      "Epoch [3/5], Step [3108/10336], Loss: 0.5802\n",
      "Epoch [3/5], Step [3110/10336], Loss: 1.0191\n",
      "Epoch [3/5], Step [3112/10336], Loss: 0.8952\n",
      "Epoch [3/5], Step [3114/10336], Loss: 0.0064\n",
      "Epoch [3/5], Step [3116/10336], Loss: 0.3036\n",
      "Epoch [3/5], Step [3118/10336], Loss: 4.4905\n",
      "Epoch [3/5], Step [3120/10336], Loss: 0.0072\n",
      "Epoch [3/5], Step [3122/10336], Loss: 0.0185\n",
      "Epoch [3/5], Step [3124/10336], Loss: 0.2401\n",
      "Epoch [3/5], Step [3126/10336], Loss: 0.5943\n",
      "Epoch [3/5], Step [3128/10336], Loss: 0.1492\n",
      "Epoch [3/5], Step [3130/10336], Loss: 0.4554\n",
      "Epoch [3/5], Step [3132/10336], Loss: 0.4672\n",
      "Epoch [3/5], Step [3134/10336], Loss: 0.6037\n",
      "Epoch [3/5], Step [3136/10336], Loss: 0.9580\n",
      "Epoch [3/5], Step [3138/10336], Loss: 0.1588\n",
      "Epoch [3/5], Step [3140/10336], Loss: 0.6515\n",
      "Epoch [3/5], Step [3142/10336], Loss: 1.5872\n",
      "Epoch [3/5], Step [3144/10336], Loss: 4.0315\n",
      "Epoch [3/5], Step [3146/10336], Loss: 2.5189\n",
      "Epoch [3/5], Step [3148/10336], Loss: 0.4007\n",
      "Epoch [3/5], Step [3150/10336], Loss: 3.5646\n",
      "Epoch [3/5], Step [3152/10336], Loss: 0.1555\n",
      "Epoch [3/5], Step [3154/10336], Loss: 1.1299\n",
      "Epoch [3/5], Step [3156/10336], Loss: 0.1183\n",
      "Epoch [3/5], Step [3158/10336], Loss: 1.2579\n",
      "Epoch [3/5], Step [3160/10336], Loss: 0.5928\n",
      "Epoch [3/5], Step [3162/10336], Loss: 2.2091\n",
      "Epoch [3/5], Step [3164/10336], Loss: 0.0497\n",
      "Epoch [3/5], Step [3166/10336], Loss: 2.4359\n",
      "Epoch [3/5], Step [3168/10336], Loss: 0.0299\n",
      "Epoch [3/5], Step [3170/10336], Loss: 0.8859\n",
      "Epoch [3/5], Step [3172/10336], Loss: 0.2723\n",
      "Epoch [3/5], Step [3174/10336], Loss: 0.7587\n",
      "Epoch [3/5], Step [3176/10336], Loss: 1.5705\n",
      "Epoch [3/5], Step [3178/10336], Loss: 0.3566\n",
      "Epoch [3/5], Step [3180/10336], Loss: 1.7148\n",
      "Epoch [3/5], Step [3182/10336], Loss: 0.2461\n",
      "Epoch [3/5], Step [3184/10336], Loss: 1.2101\n",
      "Epoch [3/5], Step [3186/10336], Loss: 0.0492\n",
      "Epoch [3/5], Step [3188/10336], Loss: 0.1051\n",
      "Epoch [3/5], Step [3190/10336], Loss: 0.7476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [3192/10336], Loss: 0.0163\n",
      "Epoch [3/5], Step [3194/10336], Loss: 0.9132\n",
      "Epoch [3/5], Step [3196/10336], Loss: 0.6693\n",
      "Epoch [3/5], Step [3198/10336], Loss: 0.0338\n",
      "Epoch [3/5], Step [3200/10336], Loss: 1.4623\n",
      "Epoch [3/5], Step [3202/10336], Loss: 0.1339\n",
      "Epoch [3/5], Step [3204/10336], Loss: 1.2555\n",
      "Epoch [3/5], Step [3206/10336], Loss: 0.6102\n",
      "Epoch [3/5], Step [3208/10336], Loss: 1.6288\n",
      "Epoch [3/5], Step [3210/10336], Loss: 0.1634\n",
      "Epoch [3/5], Step [3212/10336], Loss: 4.7393\n",
      "Epoch [3/5], Step [3214/10336], Loss: 0.1465\n",
      "Epoch [3/5], Step [3216/10336], Loss: 0.9596\n",
      "Epoch [3/5], Step [3218/10336], Loss: 0.6626\n",
      "Epoch [3/5], Step [3220/10336], Loss: 0.0215\n",
      "Epoch [3/5], Step [3222/10336], Loss: 0.1585\n",
      "Epoch [3/5], Step [3224/10336], Loss: 0.9211\n",
      "Epoch [3/5], Step [3226/10336], Loss: 0.1285\n",
      "Epoch [3/5], Step [3228/10336], Loss: 2.1226\n",
      "Epoch [3/5], Step [3230/10336], Loss: 2.2986\n",
      "Epoch [3/5], Step [3232/10336], Loss: 0.2284\n",
      "Epoch [3/5], Step [3234/10336], Loss: 0.0724\n",
      "Epoch [3/5], Step [3236/10336], Loss: 0.2617\n",
      "Epoch [3/5], Step [3238/10336], Loss: 1.0024\n",
      "Epoch [3/5], Step [3240/10336], Loss: 1.7977\n",
      "Epoch [3/5], Step [3242/10336], Loss: 0.2136\n",
      "Epoch [3/5], Step [3244/10336], Loss: 0.5511\n",
      "Epoch [3/5], Step [3246/10336], Loss: 0.1365\n",
      "Epoch [3/5], Step [3248/10336], Loss: 0.1048\n",
      "Epoch [3/5], Step [3250/10336], Loss: 0.6864\n",
      "Epoch [3/5], Step [3252/10336], Loss: 1.3718\n",
      "Epoch [3/5], Step [3254/10336], Loss: 2.2597\n",
      "Epoch [3/5], Step [3256/10336], Loss: 1.9034\n",
      "Epoch [3/5], Step [3258/10336], Loss: 0.1824\n",
      "Epoch [3/5], Step [3260/10336], Loss: 0.6166\n",
      "Epoch [3/5], Step [3262/10336], Loss: 0.9736\n",
      "Epoch [3/5], Step [3264/10336], Loss: 0.1437\n",
      "Epoch [3/5], Step [3266/10336], Loss: 1.1596\n",
      "Epoch [3/5], Step [3268/10336], Loss: 0.5675\n",
      "Epoch [3/5], Step [3270/10336], Loss: 0.1222\n",
      "Epoch [3/5], Step [3272/10336], Loss: 0.8701\n",
      "Epoch [3/5], Step [3274/10336], Loss: 0.3071\n",
      "Epoch [3/5], Step [3276/10336], Loss: 1.2729\n",
      "Epoch [3/5], Step [3278/10336], Loss: 0.5982\n",
      "Epoch [3/5], Step [3280/10336], Loss: 0.3945\n",
      "Epoch [3/5], Step [3282/10336], Loss: 1.5787\n",
      "Epoch [3/5], Step [3284/10336], Loss: 1.2536\n",
      "Epoch [3/5], Step [3286/10336], Loss: 0.8824\n",
      "Epoch [3/5], Step [3288/10336], Loss: 6.3501\n",
      "Epoch [3/5], Step [3290/10336], Loss: 2.6234\n",
      "Epoch [3/5], Step [3292/10336], Loss: 0.2935\n",
      "Epoch [3/5], Step [3294/10336], Loss: 0.9652\n",
      "Epoch [3/5], Step [3296/10336], Loss: 2.1066\n",
      "Epoch [3/5], Step [3298/10336], Loss: 0.1794\n",
      "Epoch [3/5], Step [3300/10336], Loss: 1.2475\n",
      "Epoch [3/5], Step [3302/10336], Loss: 1.6260\n",
      "Epoch [3/5], Step [3304/10336], Loss: 0.3328\n",
      "Epoch [3/5], Step [3306/10336], Loss: 0.0486\n",
      "Epoch [3/5], Step [3308/10336], Loss: 1.1751\n",
      "Epoch [3/5], Step [3310/10336], Loss: 0.2932\n",
      "Epoch [3/5], Step [3312/10336], Loss: 1.5612\n",
      "Epoch [3/5], Step [3314/10336], Loss: 0.1751\n",
      "Epoch [3/5], Step [3316/10336], Loss: 0.3346\n",
      "Epoch [3/5], Step [3318/10336], Loss: 0.8240\n",
      "Epoch [3/5], Step [3320/10336], Loss: 0.4125\n",
      "Epoch [3/5], Step [3322/10336], Loss: 0.1648\n",
      "Epoch [3/5], Step [3324/10336], Loss: 2.3358\n",
      "Epoch [3/5], Step [3326/10336], Loss: 0.0045\n",
      "Epoch [3/5], Step [3328/10336], Loss: 0.0894\n",
      "Epoch [3/5], Step [3330/10336], Loss: 0.0888\n",
      "Epoch [3/5], Step [3332/10336], Loss: 1.9175\n",
      "Epoch [3/5], Step [3334/10336], Loss: 0.1159\n",
      "Epoch [3/5], Step [3336/10336], Loss: 1.9547\n",
      "Epoch [3/5], Step [3338/10336], Loss: 0.2575\n",
      "Epoch [3/5], Step [3340/10336], Loss: 0.0108\n",
      "Epoch [3/5], Step [3342/10336], Loss: 0.0036\n",
      "Epoch [3/5], Step [3344/10336], Loss: 0.1083\n",
      "Epoch [3/5], Step [3346/10336], Loss: 1.4337\n",
      "Epoch [3/5], Step [3348/10336], Loss: 0.4183\n",
      "Epoch [3/5], Step [3350/10336], Loss: 0.2724\n",
      "Epoch [3/5], Step [3352/10336], Loss: 2.4510\n",
      "Epoch [3/5], Step [3354/10336], Loss: 0.5139\n",
      "Epoch [3/5], Step [3356/10336], Loss: 0.3381\n",
      "Epoch [3/5], Step [3358/10336], Loss: 0.0899\n",
      "Epoch [3/5], Step [3360/10336], Loss: 0.0118\n",
      "Epoch [3/5], Step [3362/10336], Loss: 2.3141\n",
      "Epoch [3/5], Step [3364/10336], Loss: 1.2445\n",
      "Epoch [3/5], Step [3366/10336], Loss: 1.9037\n",
      "Epoch [3/5], Step [3368/10336], Loss: 0.1890\n",
      "Epoch [3/5], Step [3370/10336], Loss: 0.3458\n",
      "Epoch [3/5], Step [3372/10336], Loss: 0.6079\n",
      "Epoch [3/5], Step [3374/10336], Loss: 0.6788\n",
      "Epoch [3/5], Step [3376/10336], Loss: 0.1283\n",
      "Epoch [3/5], Step [3378/10336], Loss: 0.9928\n",
      "Epoch [3/5], Step [3380/10336], Loss: 0.5346\n",
      "Epoch [3/5], Step [3382/10336], Loss: 2.0253\n",
      "Epoch [3/5], Step [3384/10336], Loss: 0.4078\n",
      "Epoch [3/5], Step [3386/10336], Loss: 1.2376\n",
      "Epoch [3/5], Step [3388/10336], Loss: 0.0025\n",
      "Epoch [3/5], Step [3390/10336], Loss: 0.0126\n",
      "Epoch [3/5], Step [3392/10336], Loss: 0.0298\n",
      "Epoch [3/5], Step [3394/10336], Loss: 0.0034\n",
      "Epoch [3/5], Step [3396/10336], Loss: 0.3985\n",
      "Epoch [3/5], Step [3398/10336], Loss: 0.0905\n",
      "Epoch [3/5], Step [3400/10336], Loss: 1.2903\n",
      "Epoch [3/5], Step [3402/10336], Loss: 2.9782\n",
      "Epoch [3/5], Step [3404/10336], Loss: 0.1144\n",
      "Epoch [3/5], Step [3406/10336], Loss: 0.1762\n",
      "Epoch [3/5], Step [3408/10336], Loss: 1.3540\n",
      "Epoch [3/5], Step [3410/10336], Loss: 0.1457\n",
      "Epoch [3/5], Step [3412/10336], Loss: 0.2356\n",
      "Epoch [3/5], Step [3414/10336], Loss: 0.0286\n",
      "Epoch [3/5], Step [3416/10336], Loss: 2.7037\n",
      "Epoch [3/5], Step [3418/10336], Loss: 3.2534\n",
      "Epoch [3/5], Step [3420/10336], Loss: 0.0529\n",
      "Epoch [3/5], Step [3422/10336], Loss: 0.0031\n",
      "Epoch [3/5], Step [3424/10336], Loss: 0.8568\n",
      "Epoch [3/5], Step [3426/10336], Loss: 0.5254\n",
      "Epoch [3/5], Step [3428/10336], Loss: 2.6435\n",
      "Epoch [3/5], Step [3430/10336], Loss: 0.2842\n",
      "Epoch [3/5], Step [3432/10336], Loss: 2.5012\n",
      "Epoch [3/5], Step [3434/10336], Loss: 0.1317\n",
      "Epoch [3/5], Step [3436/10336], Loss: 0.5230\n",
      "Epoch [3/5], Step [3438/10336], Loss: 0.0170\n",
      "Epoch [3/5], Step [3440/10336], Loss: 0.0132\n",
      "Epoch [3/5], Step [3442/10336], Loss: 0.4947\n",
      "Epoch [3/5], Step [3444/10336], Loss: 0.0607\n",
      "Epoch [3/5], Step [3446/10336], Loss: 0.0115\n",
      "Epoch [3/5], Step [3448/10336], Loss: 0.0302\n",
      "Epoch [3/5], Step [3450/10336], Loss: 1.1103\n",
      "Epoch [3/5], Step [3452/10336], Loss: 1.0640\n",
      "Epoch [3/5], Step [3454/10336], Loss: 0.0406\n",
      "Epoch [3/5], Step [3456/10336], Loss: 1.7291\n",
      "Epoch [3/5], Step [3458/10336], Loss: 0.4413\n",
      "Epoch [3/5], Step [3460/10336], Loss: 0.0346\n",
      "Epoch [3/5], Step [3462/10336], Loss: 0.0198\n",
      "Epoch [3/5], Step [3464/10336], Loss: 0.0061\n",
      "Epoch [3/5], Step [3466/10336], Loss: 0.1896\n",
      "Epoch [3/5], Step [3468/10336], Loss: 0.6269\n",
      "Epoch [3/5], Step [3470/10336], Loss: 1.9984\n",
      "Epoch [3/5], Step [3472/10336], Loss: 0.4144\n",
      "Epoch [3/5], Step [3474/10336], Loss: 0.0030\n",
      "Epoch [3/5], Step [3476/10336], Loss: 0.1171\n",
      "Epoch [3/5], Step [3478/10336], Loss: 0.8351\n",
      "Epoch [3/5], Step [3480/10336], Loss: 1.1436\n",
      "Epoch [3/5], Step [3482/10336], Loss: 0.0505\n",
      "Epoch [3/5], Step [3484/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [3486/10336], Loss: 0.5426\n",
      "Epoch [3/5], Step [3488/10336], Loss: 0.0104\n",
      "Epoch [3/5], Step [3490/10336], Loss: 0.7493\n",
      "Epoch [3/5], Step [3492/10336], Loss: 0.0438\n",
      "Epoch [3/5], Step [3494/10336], Loss: 0.8749\n",
      "Epoch [3/5], Step [3496/10336], Loss: 0.7292\n",
      "Epoch [3/5], Step [3498/10336], Loss: 2.0329\n",
      "Epoch [3/5], Step [3500/10336], Loss: 0.0505\n",
      "Epoch [3/5], Step [3502/10336], Loss: 0.0315\n",
      "Epoch [3/5], Step [3504/10336], Loss: 1.4539\n",
      "Epoch [3/5], Step [3506/10336], Loss: 0.0451\n",
      "Epoch [3/5], Step [3508/10336], Loss: 0.0111\n",
      "Epoch [3/5], Step [3510/10336], Loss: 0.4995\n",
      "Epoch [3/5], Step [3512/10336], Loss: 2.0779\n",
      "Epoch [3/5], Step [3514/10336], Loss: 0.0500\n",
      "Epoch [3/5], Step [3516/10336], Loss: 0.0296\n",
      "Epoch [3/5], Step [3518/10336], Loss: 0.0079\n",
      "Epoch [3/5], Step [3520/10336], Loss: 0.8343\n",
      "Epoch [3/5], Step [3522/10336], Loss: 2.3394\n",
      "Epoch [3/5], Step [3524/10336], Loss: 0.0513\n",
      "Epoch [3/5], Step [3526/10336], Loss: 0.0053\n",
      "Epoch [3/5], Step [3528/10336], Loss: 0.3642\n",
      "Epoch [3/5], Step [3530/10336], Loss: 0.4839\n",
      "Epoch [3/5], Step [3532/10336], Loss: 2.7130\n",
      "Epoch [3/5], Step [3534/10336], Loss: 2.1871\n",
      "Epoch [3/5], Step [3536/10336], Loss: 2.0575\n",
      "Epoch [3/5], Step [3538/10336], Loss: 3.7348\n",
      "Epoch [3/5], Step [3540/10336], Loss: 1.3287\n",
      "Epoch [3/5], Step [3542/10336], Loss: 4.3195\n",
      "Epoch [3/5], Step [3544/10336], Loss: 1.7172\n",
      "Epoch [3/5], Step [3546/10336], Loss: 0.0517\n",
      "Epoch [3/5], Step [3548/10336], Loss: 0.4232\n",
      "Epoch [3/5], Step [3550/10336], Loss: 0.2831\n",
      "Epoch [3/5], Step [3552/10336], Loss: 0.1387\n",
      "Epoch [3/5], Step [3554/10336], Loss: 1.2956\n",
      "Epoch [3/5], Step [3556/10336], Loss: 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [3558/10336], Loss: 0.0256\n",
      "Epoch [3/5], Step [3560/10336], Loss: 0.5039\n",
      "Epoch [3/5], Step [3562/10336], Loss: 2.1444\n",
      "Epoch [3/5], Step [3564/10336], Loss: 0.3203\n",
      "Epoch [3/5], Step [3566/10336], Loss: 1.8923\n",
      "Epoch [3/5], Step [3568/10336], Loss: 0.9095\n",
      "Epoch [3/5], Step [3570/10336], Loss: 1.7812\n",
      "Epoch [3/5], Step [3572/10336], Loss: 0.0789\n",
      "Epoch [3/5], Step [3574/10336], Loss: 1.7047\n",
      "Epoch [3/5], Step [3576/10336], Loss: 0.2384\n",
      "Epoch [3/5], Step [3578/10336], Loss: 0.3997\n",
      "Epoch [3/5], Step [3580/10336], Loss: 0.9090\n",
      "Epoch [3/5], Step [3582/10336], Loss: 0.0156\n",
      "Epoch [3/5], Step [3584/10336], Loss: 1.7660\n",
      "Epoch [3/5], Step [3586/10336], Loss: 2.8534\n",
      "Epoch [3/5], Step [3588/10336], Loss: 1.7953\n",
      "Epoch [3/5], Step [3590/10336], Loss: 1.3041\n",
      "Epoch [3/5], Step [3592/10336], Loss: 0.1500\n",
      "Epoch [3/5], Step [3594/10336], Loss: 0.9484\n",
      "Epoch [3/5], Step [3596/10336], Loss: 1.3648\n",
      "Epoch [3/5], Step [3598/10336], Loss: 0.0081\n",
      "Epoch [3/5], Step [3600/10336], Loss: 0.1938\n",
      "Epoch [3/5], Step [3602/10336], Loss: 2.1230\n",
      "Epoch [3/5], Step [3604/10336], Loss: 0.6860\n",
      "Epoch [3/5], Step [3606/10336], Loss: 1.5165\n",
      "Epoch [3/5], Step [3608/10336], Loss: 0.0206\n",
      "Epoch [3/5], Step [3610/10336], Loss: 1.5710\n",
      "Epoch [3/5], Step [3612/10336], Loss: 1.0276\n",
      "Epoch [3/5], Step [3614/10336], Loss: 0.0315\n",
      "Epoch [3/5], Step [3616/10336], Loss: 0.0638\n",
      "Epoch [3/5], Step [3618/10336], Loss: 3.1267\n",
      "Epoch [3/5], Step [3620/10336], Loss: 2.1155\n",
      "Epoch [3/5], Step [3622/10336], Loss: 0.8210\n",
      "Epoch [3/5], Step [3624/10336], Loss: 2.1571\n",
      "Epoch [3/5], Step [3626/10336], Loss: 0.9196\n",
      "Epoch [3/5], Step [3628/10336], Loss: 1.5278\n",
      "Epoch [3/5], Step [3630/10336], Loss: 0.9357\n",
      "Epoch [3/5], Step [3632/10336], Loss: 0.3587\n",
      "Epoch [3/5], Step [3634/10336], Loss: 0.3840\n",
      "Epoch [3/5], Step [3636/10336], Loss: 0.5428\n",
      "Epoch [3/5], Step [3638/10336], Loss: 0.0494\n",
      "Epoch [3/5], Step [3640/10336], Loss: 0.0538\n",
      "Epoch [3/5], Step [3642/10336], Loss: 0.7416\n",
      "Epoch [3/5], Step [3644/10336], Loss: 2.1409\n",
      "Epoch [3/5], Step [3646/10336], Loss: 0.8506\n",
      "Epoch [3/5], Step [3648/10336], Loss: 2.9188\n",
      "Epoch [3/5], Step [3650/10336], Loss: 0.1907\n",
      "Epoch [3/5], Step [3652/10336], Loss: 0.0863\n",
      "Epoch [3/5], Step [3654/10336], Loss: 0.0940\n",
      "Epoch [3/5], Step [3656/10336], Loss: 0.3222\n",
      "Epoch [3/5], Step [3658/10336], Loss: 0.4756\n",
      "Epoch [3/5], Step [3660/10336], Loss: 0.4444\n",
      "Epoch [3/5], Step [3662/10336], Loss: 0.4251\n",
      "Epoch [3/5], Step [3664/10336], Loss: 2.4884\n",
      "Epoch [3/5], Step [3666/10336], Loss: 0.3104\n",
      "Epoch [3/5], Step [3668/10336], Loss: 1.1263\n",
      "Epoch [3/5], Step [3670/10336], Loss: 1.9268\n",
      "Epoch [3/5], Step [3672/10336], Loss: 0.1131\n",
      "Epoch [3/5], Step [3674/10336], Loss: 0.2550\n",
      "Epoch [3/5], Step [3676/10336], Loss: 0.6001\n",
      "Epoch [3/5], Step [3678/10336], Loss: 0.0292\n",
      "Epoch [3/5], Step [3680/10336], Loss: 0.0536\n",
      "Epoch [3/5], Step [3682/10336], Loss: 0.2169\n",
      "Epoch [3/5], Step [3684/10336], Loss: 0.1220\n",
      "Epoch [3/5], Step [3686/10336], Loss: 4.1890\n",
      "Epoch [3/5], Step [3688/10336], Loss: 0.0063\n",
      "Epoch [3/5], Step [3690/10336], Loss: 0.2612\n",
      "Epoch [3/5], Step [3692/10336], Loss: 0.7084\n",
      "Epoch [3/5], Step [3694/10336], Loss: 1.2181\n",
      "Epoch [3/5], Step [3696/10336], Loss: 1.4840\n",
      "Epoch [3/5], Step [3698/10336], Loss: 1.0455\n",
      "Epoch [3/5], Step [3700/10336], Loss: 1.1932\n",
      "Epoch [3/5], Step [3702/10336], Loss: 0.6037\n",
      "Epoch [3/5], Step [3704/10336], Loss: 1.1727\n",
      "Epoch [3/5], Step [3706/10336], Loss: 0.3214\n",
      "Epoch [3/5], Step [3708/10336], Loss: 1.0604\n",
      "Epoch [3/5], Step [3710/10336], Loss: 1.6825\n",
      "Epoch [3/5], Step [3712/10336], Loss: 0.5164\n",
      "Epoch [3/5], Step [3714/10336], Loss: 4.1546\n",
      "Epoch [3/5], Step [3716/10336], Loss: 0.7204\n",
      "Epoch [3/5], Step [3718/10336], Loss: 0.2350\n",
      "Epoch [3/5], Step [3720/10336], Loss: 1.3423\n",
      "Epoch [3/5], Step [3722/10336], Loss: 1.1585\n",
      "Epoch [3/5], Step [3724/10336], Loss: 0.3181\n",
      "Epoch [3/5], Step [3726/10336], Loss: 1.2494\n",
      "Epoch [3/5], Step [3728/10336], Loss: 0.1022\n",
      "Epoch [3/5], Step [3730/10336], Loss: 2.0473\n",
      "Epoch [3/5], Step [3732/10336], Loss: 0.1473\n",
      "Epoch [3/5], Step [3734/10336], Loss: 1.3842\n",
      "Epoch [3/5], Step [3736/10336], Loss: 0.7668\n",
      "Epoch [3/5], Step [3738/10336], Loss: 1.6649\n",
      "Epoch [3/5], Step [3740/10336], Loss: 1.4036\n",
      "Epoch [3/5], Step [3742/10336], Loss: 1.8463\n",
      "Epoch [3/5], Step [3744/10336], Loss: 2.0218\n",
      "Epoch [3/5], Step [3746/10336], Loss: 0.0128\n",
      "Epoch [3/5], Step [3748/10336], Loss: 0.0778\n",
      "Epoch [3/5], Step [3750/10336], Loss: 1.1135\n",
      "Epoch [3/5], Step [3752/10336], Loss: 0.1269\n",
      "Epoch [3/5], Step [3754/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [3756/10336], Loss: 1.2575\n",
      "Epoch [3/5], Step [3758/10336], Loss: 0.2097\n",
      "Epoch [3/5], Step [3760/10336], Loss: 0.1945\n",
      "Epoch [3/5], Step [3762/10336], Loss: 1.3516\n",
      "Epoch [3/5], Step [3764/10336], Loss: 0.0116\n",
      "Epoch [3/5], Step [3766/10336], Loss: 0.3933\n",
      "Epoch [3/5], Step [3768/10336], Loss: 0.0074\n",
      "Epoch [3/5], Step [3770/10336], Loss: 0.0831\n",
      "Epoch [3/5], Step [3772/10336], Loss: 1.7737\n",
      "Epoch [3/5], Step [3774/10336], Loss: 0.2909\n",
      "Epoch [3/5], Step [3776/10336], Loss: 0.0090\n",
      "Epoch [3/5], Step [3778/10336], Loss: 2.0513\n",
      "Epoch [3/5], Step [3780/10336], Loss: 0.8034\n",
      "Epoch [3/5], Step [3782/10336], Loss: 1.6847\n",
      "Epoch [3/5], Step [3784/10336], Loss: 0.6970\n",
      "Epoch [3/5], Step [3786/10336], Loss: 0.1347\n",
      "Epoch [3/5], Step [3788/10336], Loss: 1.7514\n",
      "Epoch [3/5], Step [3790/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [3792/10336], Loss: 0.2028\n",
      "Epoch [3/5], Step [3794/10336], Loss: 1.0533\n",
      "Epoch [3/5], Step [3796/10336], Loss: 3.5153\n",
      "Epoch [3/5], Step [3798/10336], Loss: 0.1867\n",
      "Epoch [3/5], Step [3800/10336], Loss: 0.0572\n",
      "Epoch [3/5], Step [3802/10336], Loss: 0.0936\n",
      "Epoch [3/5], Step [3804/10336], Loss: 0.0324\n",
      "Epoch [3/5], Step [3806/10336], Loss: 0.1769\n",
      "Epoch [3/5], Step [3808/10336], Loss: 0.2663\n",
      "Epoch [3/5], Step [3810/10336], Loss: 0.4208\n",
      "Epoch [3/5], Step [3812/10336], Loss: 1.5040\n",
      "Epoch [3/5], Step [3814/10336], Loss: 0.4297\n",
      "Epoch [3/5], Step [3816/10336], Loss: 1.3446\n",
      "Epoch [3/5], Step [3818/10336], Loss: 0.1994\n",
      "Epoch [3/5], Step [3820/10336], Loss: 0.3287\n",
      "Epoch [3/5], Step [3822/10336], Loss: 4.2407\n",
      "Epoch [3/5], Step [3824/10336], Loss: 1.0631\n",
      "Epoch [3/5], Step [3826/10336], Loss: 0.1878\n",
      "Epoch [3/5], Step [3828/10336], Loss: 0.3706\n",
      "Epoch [3/5], Step [3830/10336], Loss: 2.6759\n",
      "Epoch [3/5], Step [3832/10336], Loss: 2.3151\n",
      "Epoch [3/5], Step [3834/10336], Loss: 0.7265\n",
      "Epoch [3/5], Step [3836/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [3838/10336], Loss: 0.3338\n",
      "Epoch [3/5], Step [3840/10336], Loss: 0.0136\n",
      "Epoch [3/5], Step [3842/10336], Loss: 1.4903\n",
      "Epoch [3/5], Step [3844/10336], Loss: 0.4390\n",
      "Epoch [3/5], Step [3846/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [3848/10336], Loss: 0.0119\n",
      "Epoch [3/5], Step [3850/10336], Loss: 0.1043\n",
      "Epoch [3/5], Step [3852/10336], Loss: 0.1202\n",
      "Epoch [3/5], Step [3854/10336], Loss: 0.0038\n",
      "Epoch [3/5], Step [3856/10336], Loss: 1.0047\n",
      "Epoch [3/5], Step [3858/10336], Loss: 0.9660\n",
      "Epoch [3/5], Step [3860/10336], Loss: 4.0272\n",
      "Epoch [3/5], Step [3862/10336], Loss: 0.5585\n",
      "Epoch [3/5], Step [3864/10336], Loss: 1.4658\n",
      "Epoch [3/5], Step [3866/10336], Loss: 0.0057\n",
      "Epoch [3/5], Step [3868/10336], Loss: 2.3397\n",
      "Epoch [3/5], Step [3870/10336], Loss: 1.6991\n",
      "Epoch [3/5], Step [3872/10336], Loss: 0.5370\n",
      "Epoch [3/5], Step [3874/10336], Loss: 0.5709\n",
      "Epoch [3/5], Step [3876/10336], Loss: 0.0757\n",
      "Epoch [3/5], Step [3878/10336], Loss: 1.5833\n",
      "Epoch [3/5], Step [3880/10336], Loss: 0.0243\n",
      "Epoch [3/5], Step [3882/10336], Loss: 0.0446\n",
      "Epoch [3/5], Step [3884/10336], Loss: 1.2823\n",
      "Epoch [3/5], Step [3886/10336], Loss: 1.6139\n",
      "Epoch [3/5], Step [3888/10336], Loss: 2.1638\n",
      "Epoch [3/5], Step [3890/10336], Loss: 0.0119\n",
      "Epoch [3/5], Step [3892/10336], Loss: 1.3105\n",
      "Epoch [3/5], Step [3894/10336], Loss: 1.2885\n",
      "Epoch [3/5], Step [3896/10336], Loss: 1.3154\n",
      "Epoch [3/5], Step [3898/10336], Loss: 0.2740\n",
      "Epoch [3/5], Step [3900/10336], Loss: 0.2255\n",
      "Epoch [3/5], Step [3902/10336], Loss: 0.4842\n",
      "Epoch [3/5], Step [3904/10336], Loss: 0.0238\n",
      "Epoch [3/5], Step [3906/10336], Loss: 1.0086\n",
      "Epoch [3/5], Step [3908/10336], Loss: 0.3077\n",
      "Epoch [3/5], Step [3910/10336], Loss: 0.0385\n",
      "Epoch [3/5], Step [3912/10336], Loss: 1.6280\n",
      "Epoch [3/5], Step [3914/10336], Loss: 1.2022\n",
      "Epoch [3/5], Step [3916/10336], Loss: 0.4707\n",
      "Epoch [3/5], Step [3918/10336], Loss: 0.6561\n",
      "Epoch [3/5], Step [3920/10336], Loss: 1.1539\n",
      "Epoch [3/5], Step [3922/10336], Loss: 0.6297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [3924/10336], Loss: 2.5225\n",
      "Epoch [3/5], Step [3926/10336], Loss: 0.0312\n",
      "Epoch [3/5], Step [3928/10336], Loss: 0.2328\n",
      "Epoch [3/5], Step [3930/10336], Loss: 0.0028\n",
      "Epoch [3/5], Step [3932/10336], Loss: 0.0034\n",
      "Epoch [3/5], Step [3934/10336], Loss: 0.3549\n",
      "Epoch [3/5], Step [3936/10336], Loss: 1.6910\n",
      "Epoch [3/5], Step [3938/10336], Loss: 0.3788\n",
      "Epoch [3/5], Step [3940/10336], Loss: 0.1824\n",
      "Epoch [3/5], Step [3942/10336], Loss: 0.0286\n",
      "Epoch [3/5], Step [3944/10336], Loss: 0.5003\n",
      "Epoch [3/5], Step [3946/10336], Loss: 0.7614\n",
      "Epoch [3/5], Step [3948/10336], Loss: 0.3468\n",
      "Epoch [3/5], Step [3950/10336], Loss: 0.1911\n",
      "Epoch [3/5], Step [3952/10336], Loss: 1.5636\n",
      "Epoch [3/5], Step [3954/10336], Loss: 0.1459\n",
      "Epoch [3/5], Step [3956/10336], Loss: 0.9851\n",
      "Epoch [3/5], Step [3958/10336], Loss: 0.0075\n",
      "Epoch [3/5], Step [3960/10336], Loss: 0.0169\n",
      "Epoch [3/5], Step [3962/10336], Loss: 5.5060\n",
      "Epoch [3/5], Step [3964/10336], Loss: 0.1858\n",
      "Epoch [3/5], Step [3966/10336], Loss: 2.9250\n",
      "Epoch [3/5], Step [3968/10336], Loss: 0.0834\n",
      "Epoch [3/5], Step [3970/10336], Loss: 0.9013\n",
      "Epoch [3/5], Step [3972/10336], Loss: 0.0840\n",
      "Epoch [3/5], Step [3974/10336], Loss: 1.3087\n",
      "Epoch [3/5], Step [3976/10336], Loss: 0.5098\n",
      "Epoch [3/5], Step [3978/10336], Loss: 0.3410\n",
      "Epoch [3/5], Step [3980/10336], Loss: 1.7924\n",
      "Epoch [3/5], Step [3982/10336], Loss: 1.6866\n",
      "Epoch [3/5], Step [3984/10336], Loss: 2.1967\n",
      "Epoch [3/5], Step [3986/10336], Loss: 0.0430\n",
      "Epoch [3/5], Step [3988/10336], Loss: 0.0312\n",
      "Epoch [3/5], Step [3990/10336], Loss: 0.2969\n",
      "Epoch [3/5], Step [3992/10336], Loss: 0.2924\n",
      "Epoch [3/5], Step [3994/10336], Loss: 1.7576\n",
      "Epoch [3/5], Step [3996/10336], Loss: 0.0589\n",
      "Epoch [3/5], Step [3998/10336], Loss: 0.0522\n",
      "Epoch [3/5], Step [4000/10336], Loss: 0.8667\n",
      "Epoch [3/5], Step [4002/10336], Loss: 0.1061\n",
      "Epoch [3/5], Step [4004/10336], Loss: 0.0921\n",
      "Epoch [3/5], Step [4006/10336], Loss: 1.8235\n",
      "Epoch [3/5], Step [4008/10336], Loss: 0.0007\n",
      "Epoch [3/5], Step [4010/10336], Loss: 2.1085\n",
      "Epoch [3/5], Step [4012/10336], Loss: 0.0028\n",
      "Epoch [3/5], Step [4014/10336], Loss: 0.0110\n",
      "Epoch [3/5], Step [4016/10336], Loss: 0.4519\n",
      "Epoch [3/5], Step [4018/10336], Loss: 0.9785\n",
      "Epoch [3/5], Step [4020/10336], Loss: 3.2508\n",
      "Epoch [3/5], Step [4022/10336], Loss: 0.0070\n",
      "Epoch [3/5], Step [4024/10336], Loss: 0.6906\n",
      "Epoch [3/5], Step [4026/10336], Loss: 0.6995\n",
      "Epoch [3/5], Step [4028/10336], Loss: 1.1397\n",
      "Epoch [3/5], Step [4030/10336], Loss: 0.1090\n",
      "Epoch [3/5], Step [4032/10336], Loss: 0.9516\n",
      "Epoch [3/5], Step [4034/10336], Loss: 1.2437\n",
      "Epoch [3/5], Step [4036/10336], Loss: 1.4005\n",
      "Epoch [3/5], Step [4038/10336], Loss: 0.6921\n",
      "Epoch [3/5], Step [4040/10336], Loss: 0.8305\n",
      "Epoch [3/5], Step [4042/10336], Loss: 3.2438\n",
      "Epoch [3/5], Step [4044/10336], Loss: 0.4612\n",
      "Epoch [3/5], Step [4046/10336], Loss: 0.0548\n",
      "Epoch [3/5], Step [4048/10336], Loss: 0.4330\n",
      "Epoch [3/5], Step [4050/10336], Loss: 0.2538\n",
      "Epoch [3/5], Step [4052/10336], Loss: 1.0857\n",
      "Epoch [3/5], Step [4054/10336], Loss: 1.1228\n",
      "Epoch [3/5], Step [4056/10336], Loss: 0.5263\n",
      "Epoch [3/5], Step [4058/10336], Loss: 0.0555\n",
      "Epoch [3/5], Step [4060/10336], Loss: 0.8932\n",
      "Epoch [3/5], Step [4062/10336], Loss: 0.1481\n",
      "Epoch [3/5], Step [4064/10336], Loss: 0.0390\n",
      "Epoch [3/5], Step [4066/10336], Loss: 1.9084\n",
      "Epoch [3/5], Step [4068/10336], Loss: 0.0618\n",
      "Epoch [3/5], Step [4070/10336], Loss: 2.5018\n",
      "Epoch [3/5], Step [4072/10336], Loss: 0.8881\n",
      "Epoch [3/5], Step [4074/10336], Loss: 0.0159\n",
      "Epoch [3/5], Step [4076/10336], Loss: 0.0398\n",
      "Epoch [3/5], Step [4078/10336], Loss: 1.6947\n",
      "Epoch [3/5], Step [4080/10336], Loss: 0.2485\n",
      "Epoch [3/5], Step [4082/10336], Loss: 0.0295\n",
      "Epoch [3/5], Step [4084/10336], Loss: 0.3738\n",
      "Epoch [3/5], Step [4086/10336], Loss: 0.0247\n",
      "Epoch [3/5], Step [4088/10336], Loss: 0.7092\n",
      "Epoch [3/5], Step [4090/10336], Loss: 0.0032\n",
      "Epoch [3/5], Step [4092/10336], Loss: 0.2490\n",
      "Epoch [3/5], Step [4094/10336], Loss: 0.0674\n",
      "Epoch [3/5], Step [4096/10336], Loss: 0.0155\n",
      "Epoch [3/5], Step [4098/10336], Loss: 0.0478\n",
      "Epoch [3/5], Step [4100/10336], Loss: 0.0497\n",
      "Epoch [3/5], Step [4102/10336], Loss: 0.0374\n",
      "Epoch [3/5], Step [4104/10336], Loss: 3.1139\n",
      "Epoch [3/5], Step [4106/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [4108/10336], Loss: 0.0189\n",
      "Epoch [3/5], Step [4110/10336], Loss: 3.1516\n",
      "Epoch [3/5], Step [4112/10336], Loss: 3.2331\n",
      "Epoch [3/5], Step [4114/10336], Loss: 0.3333\n",
      "Epoch [3/5], Step [4116/10336], Loss: 0.0175\n",
      "Epoch [3/5], Step [4118/10336], Loss: 2.1545\n",
      "Epoch [3/5], Step [4120/10336], Loss: 3.2395\n",
      "Epoch [3/5], Step [4122/10336], Loss: 0.2806\n",
      "Epoch [3/5], Step [4124/10336], Loss: 0.7453\n",
      "Epoch [3/5], Step [4126/10336], Loss: 0.1555\n",
      "Epoch [3/5], Step [4128/10336], Loss: 1.8329\n",
      "Epoch [3/5], Step [4130/10336], Loss: 0.2021\n",
      "Epoch [3/5], Step [4132/10336], Loss: 1.7227\n",
      "Epoch [3/5], Step [4134/10336], Loss: 1.7519\n",
      "Epoch [3/5], Step [4136/10336], Loss: 1.9101\n",
      "Epoch [3/5], Step [4138/10336], Loss: 0.9304\n",
      "Epoch [3/5], Step [4140/10336], Loss: 0.1020\n",
      "Epoch [3/5], Step [4142/10336], Loss: 0.2152\n",
      "Epoch [3/5], Step [4144/10336], Loss: 0.3081\n",
      "Epoch [3/5], Step [4146/10336], Loss: 1.8197\n",
      "Epoch [3/5], Step [4148/10336], Loss: 1.8180\n",
      "Epoch [3/5], Step [4150/10336], Loss: 0.0321\n",
      "Epoch [3/5], Step [4152/10336], Loss: 1.8214\n",
      "Epoch [3/5], Step [4154/10336], Loss: 2.0241\n",
      "Epoch [3/5], Step [4156/10336], Loss: 1.5178\n",
      "Epoch [3/5], Step [4158/10336], Loss: 0.0085\n",
      "Epoch [3/5], Step [4160/10336], Loss: 0.5035\n",
      "Epoch [3/5], Step [4162/10336], Loss: 3.3243\n",
      "Epoch [3/5], Step [4164/10336], Loss: 0.0489\n",
      "Epoch [3/5], Step [4166/10336], Loss: 0.5235\n",
      "Epoch [3/5], Step [4168/10336], Loss: 1.7062\n",
      "Epoch [3/5], Step [4170/10336], Loss: 0.0097\n",
      "Epoch [3/5], Step [4172/10336], Loss: 1.6352\n",
      "Epoch [3/5], Step [4174/10336], Loss: 0.0415\n",
      "Epoch [3/5], Step [4176/10336], Loss: 3.2454\n",
      "Epoch [3/5], Step [4178/10336], Loss: 0.3342\n",
      "Epoch [3/5], Step [4180/10336], Loss: 2.0956\n",
      "Epoch [3/5], Step [4182/10336], Loss: 0.4156\n",
      "Epoch [3/5], Step [4184/10336], Loss: 2.2828\n",
      "Epoch [3/5], Step [4186/10336], Loss: 0.0753\n",
      "Epoch [3/5], Step [4188/10336], Loss: 0.0074\n",
      "Epoch [3/5], Step [4190/10336], Loss: 0.0095\n",
      "Epoch [3/5], Step [4192/10336], Loss: 1.1031\n",
      "Epoch [3/5], Step [4194/10336], Loss: 0.0204\n",
      "Epoch [3/5], Step [4196/10336], Loss: 0.2264\n",
      "Epoch [3/5], Step [4198/10336], Loss: 1.6363\n",
      "Epoch [3/5], Step [4200/10336], Loss: 0.1410\n",
      "Epoch [3/5], Step [4202/10336], Loss: 1.3634\n",
      "Epoch [3/5], Step [4204/10336], Loss: 2.0975\n",
      "Epoch [3/5], Step [4206/10336], Loss: 2.0395\n",
      "Epoch [3/5], Step [4208/10336], Loss: 0.4217\n",
      "Epoch [3/5], Step [4210/10336], Loss: 0.3266\n",
      "Epoch [3/5], Step [4212/10336], Loss: 0.6890\n",
      "Epoch [3/5], Step [4214/10336], Loss: 0.4131\n",
      "Epoch [3/5], Step [4216/10336], Loss: 0.2180\n",
      "Epoch [3/5], Step [4218/10336], Loss: 0.1197\n",
      "Epoch [3/5], Step [4220/10336], Loss: 1.2965\n",
      "Epoch [3/5], Step [4222/10336], Loss: 0.3790\n",
      "Epoch [3/5], Step [4224/10336], Loss: 0.2383\n",
      "Epoch [3/5], Step [4226/10336], Loss: 1.0797\n",
      "Epoch [3/5], Step [4228/10336], Loss: 0.0354\n",
      "Epoch [3/5], Step [4230/10336], Loss: 0.7899\n",
      "Epoch [3/5], Step [4232/10336], Loss: 3.4449\n",
      "Epoch [3/5], Step [4234/10336], Loss: 0.3776\n",
      "Epoch [3/5], Step [4236/10336], Loss: 0.4633\n",
      "Epoch [3/5], Step [4238/10336], Loss: 1.0557\n",
      "Epoch [3/5], Step [4240/10336], Loss: 0.0048\n",
      "Epoch [3/5], Step [4242/10336], Loss: 3.4938\n",
      "Epoch [3/5], Step [4244/10336], Loss: 0.1548\n",
      "Epoch [3/5], Step [4246/10336], Loss: 0.1886\n",
      "Epoch [3/5], Step [4248/10336], Loss: 0.4270\n",
      "Epoch [3/5], Step [4250/10336], Loss: 0.6380\n",
      "Epoch [3/5], Step [4252/10336], Loss: 0.0152\n",
      "Epoch [3/5], Step [4254/10336], Loss: 1.0122\n",
      "Epoch [3/5], Step [4256/10336], Loss: 0.9911\n",
      "Epoch [3/5], Step [4258/10336], Loss: 0.4910\n",
      "Epoch [3/5], Step [4260/10336], Loss: 1.1726\n",
      "Epoch [3/5], Step [4262/10336], Loss: 0.1454\n",
      "Epoch [3/5], Step [4264/10336], Loss: 1.5392\n",
      "Epoch [3/5], Step [4266/10336], Loss: 0.3975\n",
      "Epoch [3/5], Step [4268/10336], Loss: 0.0496\n",
      "Epoch [3/5], Step [4270/10336], Loss: 0.3613\n",
      "Epoch [3/5], Step [4272/10336], Loss: 2.4324\n",
      "Epoch [3/5], Step [4274/10336], Loss: 2.6703\n",
      "Epoch [3/5], Step [4276/10336], Loss: 0.3838\n",
      "Epoch [3/5], Step [4278/10336], Loss: 0.0203\n",
      "Epoch [3/5], Step [4280/10336], Loss: 1.0472\n",
      "Epoch [3/5], Step [4282/10336], Loss: 0.1769\n",
      "Epoch [3/5], Step [4284/10336], Loss: 1.2172\n",
      "Epoch [3/5], Step [4286/10336], Loss: 0.8999\n",
      "Epoch [3/5], Step [4288/10336], Loss: 0.8131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [4290/10336], Loss: 4.5599\n",
      "Epoch [3/5], Step [4292/10336], Loss: 1.5354\n",
      "Epoch [3/5], Step [4294/10336], Loss: 0.0725\n",
      "Epoch [3/5], Step [4296/10336], Loss: 1.3322\n",
      "Epoch [3/5], Step [4298/10336], Loss: 1.2793\n",
      "Epoch [3/5], Step [4300/10336], Loss: 0.9125\n",
      "Epoch [3/5], Step [4302/10336], Loss: 1.8264\n",
      "Epoch [3/5], Step [4304/10336], Loss: 0.0107\n",
      "Epoch [3/5], Step [4306/10336], Loss: 1.3269\n",
      "Epoch [3/5], Step [4308/10336], Loss: 0.0121\n",
      "Epoch [3/5], Step [4310/10336], Loss: 0.3342\n",
      "Epoch [3/5], Step [4312/10336], Loss: 0.8022\n",
      "Epoch [3/5], Step [4314/10336], Loss: 0.9057\n",
      "Epoch [3/5], Step [4316/10336], Loss: 1.5754\n",
      "Epoch [3/5], Step [4318/10336], Loss: 1.1369\n",
      "Epoch [3/5], Step [4320/10336], Loss: 0.3831\n",
      "Epoch [3/5], Step [4322/10336], Loss: 0.0339\n",
      "Epoch [3/5], Step [4324/10336], Loss: 0.2695\n",
      "Epoch [3/5], Step [4326/10336], Loss: 0.0373\n",
      "Epoch [3/5], Step [4328/10336], Loss: 0.7726\n",
      "Epoch [3/5], Step [4330/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [4332/10336], Loss: 4.2667\n",
      "Epoch [3/5], Step [4334/10336], Loss: 1.0124\n",
      "Epoch [3/5], Step [4336/10336], Loss: 0.2861\n",
      "Epoch [3/5], Step [4338/10336], Loss: 2.5366\n",
      "Epoch [3/5], Step [4340/10336], Loss: 0.0263\n",
      "Epoch [3/5], Step [4342/10336], Loss: 0.0228\n",
      "Epoch [3/5], Step [4344/10336], Loss: 1.7253\n",
      "Epoch [3/5], Step [4346/10336], Loss: 0.4097\n",
      "Epoch [3/5], Step [4348/10336], Loss: 0.3445\n",
      "Epoch [3/5], Step [4350/10336], Loss: 0.1551\n",
      "Epoch [3/5], Step [4352/10336], Loss: 1.8041\n",
      "Epoch [3/5], Step [4354/10336], Loss: 0.1681\n",
      "Epoch [3/5], Step [4356/10336], Loss: 0.1861\n",
      "Epoch [3/5], Step [4358/10336], Loss: 1.4674\n",
      "Epoch [3/5], Step [4360/10336], Loss: 0.3974\n",
      "Epoch [3/5], Step [4362/10336], Loss: 0.0228\n",
      "Epoch [3/5], Step [4364/10336], Loss: 1.2733\n",
      "Epoch [3/5], Step [4366/10336], Loss: 0.4148\n",
      "Epoch [3/5], Step [4368/10336], Loss: 0.1050\n",
      "Epoch [3/5], Step [4370/10336], Loss: 0.6035\n",
      "Epoch [3/5], Step [4372/10336], Loss: 3.4300\n",
      "Epoch [3/5], Step [4374/10336], Loss: 0.1452\n",
      "Epoch [3/5], Step [4376/10336], Loss: 0.7515\n",
      "Epoch [3/5], Step [4378/10336], Loss: 1.2091\n",
      "Epoch [3/5], Step [4380/10336], Loss: 1.9371\n",
      "Epoch [3/5], Step [4382/10336], Loss: 2.9094\n",
      "Epoch [3/5], Step [4384/10336], Loss: 0.0235\n",
      "Epoch [3/5], Step [4386/10336], Loss: 0.0264\n",
      "Epoch [3/5], Step [4388/10336], Loss: 1.2995\n",
      "Epoch [3/5], Step [4390/10336], Loss: 0.2335\n",
      "Epoch [3/5], Step [4392/10336], Loss: 0.0698\n",
      "Epoch [3/5], Step [4394/10336], Loss: 0.5465\n",
      "Epoch [3/5], Step [4396/10336], Loss: 1.2860\n",
      "Epoch [3/5], Step [4398/10336], Loss: 0.6651\n",
      "Epoch [3/5], Step [4400/10336], Loss: 2.3893\n",
      "Epoch [3/5], Step [4402/10336], Loss: 2.0992\n",
      "Epoch [3/5], Step [4404/10336], Loss: 0.5050\n",
      "Epoch [3/5], Step [4406/10336], Loss: 0.1348\n",
      "Epoch [3/5], Step [4408/10336], Loss: 0.7739\n",
      "Epoch [3/5], Step [4410/10336], Loss: 0.1300\n",
      "Epoch [3/5], Step [4412/10336], Loss: 0.6884\n",
      "Epoch [3/5], Step [4414/10336], Loss: 0.0605\n",
      "Epoch [3/5], Step [4416/10336], Loss: 0.5185\n",
      "Epoch [3/5], Step [4418/10336], Loss: 0.4993\n",
      "Epoch [3/5], Step [4420/10336], Loss: 0.1133\n",
      "Epoch [3/5], Step [4422/10336], Loss: 1.8901\n",
      "Epoch [3/5], Step [4424/10336], Loss: 0.6026\n",
      "Epoch [3/5], Step [4426/10336], Loss: 0.0020\n",
      "Epoch [3/5], Step [4428/10336], Loss: 1.0208\n",
      "Epoch [3/5], Step [4430/10336], Loss: 0.2627\n",
      "Epoch [3/5], Step [4432/10336], Loss: 0.0027\n",
      "Epoch [3/5], Step [4434/10336], Loss: 2.1084\n",
      "Epoch [3/5], Step [4436/10336], Loss: 1.5990\n",
      "Epoch [3/5], Step [4438/10336], Loss: 0.0882\n",
      "Epoch [3/5], Step [4440/10336], Loss: 1.8116\n",
      "Epoch [3/5], Step [4442/10336], Loss: 1.7981\n",
      "Epoch [3/5], Step [4444/10336], Loss: 2.3367\n",
      "Epoch [3/5], Step [4446/10336], Loss: 0.0316\n",
      "Epoch [3/5], Step [4448/10336], Loss: 1.5376\n",
      "Epoch [3/5], Step [4450/10336], Loss: 0.0434\n",
      "Epoch [3/5], Step [4452/10336], Loss: 0.3776\n",
      "Epoch [3/5], Step [4454/10336], Loss: 0.6781\n",
      "Epoch [3/5], Step [4456/10336], Loss: 0.6578\n",
      "Epoch [3/5], Step [4458/10336], Loss: 0.5742\n",
      "Epoch [3/5], Step [4460/10336], Loss: 1.5139\n",
      "Epoch [3/5], Step [4462/10336], Loss: 0.8916\n",
      "Epoch [3/5], Step [4464/10336], Loss: 0.0857\n",
      "Epoch [3/5], Step [4466/10336], Loss: 0.1253\n",
      "Epoch [3/5], Step [4468/10336], Loss: 0.6003\n",
      "Epoch [3/5], Step [4470/10336], Loss: 0.0525\n",
      "Epoch [3/5], Step [4472/10336], Loss: 0.0076\n",
      "Epoch [3/5], Step [4474/10336], Loss: 0.7481\n",
      "Epoch [3/5], Step [4476/10336], Loss: 0.9071\n",
      "Epoch [3/5], Step [4478/10336], Loss: 0.0665\n",
      "Epoch [3/5], Step [4480/10336], Loss: 0.0771\n",
      "Epoch [3/5], Step [4482/10336], Loss: 0.5593\n",
      "Epoch [3/5], Step [4484/10336], Loss: 1.1699\n",
      "Epoch [3/5], Step [4486/10336], Loss: 0.0915\n",
      "Epoch [3/5], Step [4488/10336], Loss: 3.1788\n",
      "Epoch [3/5], Step [4490/10336], Loss: 0.9290\n",
      "Epoch [3/5], Step [4492/10336], Loss: 0.1971\n",
      "Epoch [3/5], Step [4494/10336], Loss: 0.1116\n",
      "Epoch [3/5], Step [4496/10336], Loss: 0.2259\n",
      "Epoch [3/5], Step [4498/10336], Loss: 0.0131\n",
      "Epoch [3/5], Step [4500/10336], Loss: 0.0136\n",
      "Epoch [3/5], Step [4502/10336], Loss: 0.4130\n",
      "Epoch [3/5], Step [4504/10336], Loss: 0.3256\n",
      "Epoch [3/5], Step [4506/10336], Loss: 0.3403\n",
      "Epoch [3/5], Step [4508/10336], Loss: 1.6941\n",
      "Epoch [3/5], Step [4510/10336], Loss: 0.0128\n",
      "Epoch [3/5], Step [4512/10336], Loss: 1.2404\n",
      "Epoch [3/5], Step [4514/10336], Loss: 0.2226\n",
      "Epoch [3/5], Step [4516/10336], Loss: 0.5304\n",
      "Epoch [3/5], Step [4518/10336], Loss: 0.3340\n",
      "Epoch [3/5], Step [4520/10336], Loss: 0.2074\n",
      "Epoch [3/5], Step [4522/10336], Loss: 0.7751\n",
      "Epoch [3/5], Step [4524/10336], Loss: 2.7686\n",
      "Epoch [3/5], Step [4526/10336], Loss: 0.6398\n",
      "Epoch [3/5], Step [4528/10336], Loss: 0.8906\n",
      "Epoch [3/5], Step [4530/10336], Loss: 1.3943\n",
      "Epoch [3/5], Step [4532/10336], Loss: 0.0178\n",
      "Epoch [3/5], Step [4534/10336], Loss: 3.5739\n",
      "Epoch [3/5], Step [4536/10336], Loss: 0.0151\n",
      "Epoch [3/5], Step [4538/10336], Loss: 1.3555\n",
      "Epoch [3/5], Step [4540/10336], Loss: 1.8474\n",
      "Epoch [3/5], Step [4542/10336], Loss: 0.0403\n",
      "Epoch [3/5], Step [4544/10336], Loss: 0.1605\n",
      "Epoch [3/5], Step [4546/10336], Loss: 0.0305\n",
      "Epoch [3/5], Step [4548/10336], Loss: 1.4613\n",
      "Epoch [3/5], Step [4550/10336], Loss: 0.0174\n",
      "Epoch [3/5], Step [4552/10336], Loss: 1.0955\n",
      "Epoch [3/5], Step [4554/10336], Loss: 0.0212\n",
      "Epoch [3/5], Step [4556/10336], Loss: 0.7901\n",
      "Epoch [3/5], Step [4558/10336], Loss: 0.0039\n",
      "Epoch [3/5], Step [4560/10336], Loss: 1.1287\n",
      "Epoch [3/5], Step [4562/10336], Loss: 0.1531\n",
      "Epoch [3/5], Step [4564/10336], Loss: 0.0133\n",
      "Epoch [3/5], Step [4566/10336], Loss: 0.0062\n",
      "Epoch [3/5], Step [4568/10336], Loss: 0.7438\n",
      "Epoch [3/5], Step [4570/10336], Loss: 1.3915\n",
      "Epoch [3/5], Step [4572/10336], Loss: 2.1361\n",
      "Epoch [3/5], Step [4574/10336], Loss: 0.0204\n",
      "Epoch [3/5], Step [4576/10336], Loss: 0.0937\n",
      "Epoch [3/5], Step [4578/10336], Loss: 1.1028\n",
      "Epoch [3/5], Step [4580/10336], Loss: 0.0335\n",
      "Epoch [3/5], Step [4582/10336], Loss: 0.7772\n",
      "Epoch [3/5], Step [4584/10336], Loss: 0.9651\n",
      "Epoch [3/5], Step [4586/10336], Loss: 1.3907\n",
      "Epoch [3/5], Step [4588/10336], Loss: 0.1516\n",
      "Epoch [3/5], Step [4590/10336], Loss: 0.2873\n",
      "Epoch [3/5], Step [4592/10336], Loss: 0.1629\n",
      "Epoch [3/5], Step [4594/10336], Loss: 0.2513\n",
      "Epoch [3/5], Step [4596/10336], Loss: 0.0062\n",
      "Epoch [3/5], Step [4598/10336], Loss: 0.1360\n",
      "Epoch [3/5], Step [4600/10336], Loss: 0.0606\n",
      "Epoch [3/5], Step [4602/10336], Loss: 0.1362\n",
      "Epoch [3/5], Step [4604/10336], Loss: 0.6127\n",
      "Epoch [3/5], Step [4606/10336], Loss: 1.6335\n",
      "Epoch [3/5], Step [4608/10336], Loss: 0.1804\n",
      "Epoch [3/5], Step [4610/10336], Loss: 0.0129\n",
      "Epoch [3/5], Step [4612/10336], Loss: 0.0290\n",
      "Epoch [3/5], Step [4614/10336], Loss: 0.9415\n",
      "Epoch [3/5], Step [4616/10336], Loss: 0.3590\n",
      "Epoch [3/5], Step [4618/10336], Loss: 0.0450\n",
      "Epoch [3/5], Step [4620/10336], Loss: 1.1777\n",
      "Epoch [3/5], Step [4622/10336], Loss: 0.0382\n",
      "Epoch [3/5], Step [4624/10336], Loss: 4.2470\n",
      "Epoch [3/5], Step [4626/10336], Loss: 0.6194\n",
      "Epoch [3/5], Step [4628/10336], Loss: 1.7644\n",
      "Epoch [3/5], Step [4630/10336], Loss: 0.0681\n",
      "Epoch [3/5], Step [4632/10336], Loss: 0.3347\n",
      "Epoch [3/5], Step [4634/10336], Loss: 0.0127\n",
      "Epoch [3/5], Step [4636/10336], Loss: 0.3563\n",
      "Epoch [3/5], Step [4638/10336], Loss: 0.1005\n",
      "Epoch [3/5], Step [4640/10336], Loss: 1.9760\n",
      "Epoch [3/5], Step [4642/10336], Loss: 0.5125\n",
      "Epoch [3/5], Step [4644/10336], Loss: 0.0548\n",
      "Epoch [3/5], Step [4646/10336], Loss: 1.6400\n",
      "Epoch [3/5], Step [4648/10336], Loss: 0.9588\n",
      "Epoch [3/5], Step [4650/10336], Loss: 1.4399\n",
      "Epoch [3/5], Step [4652/10336], Loss: 0.3941\n",
      "Epoch [3/5], Step [4654/10336], Loss: 0.0347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [4656/10336], Loss: 0.7216\n",
      "Epoch [3/5], Step [4658/10336], Loss: 0.0056\n",
      "Epoch [3/5], Step [4660/10336], Loss: 1.0320\n",
      "Epoch [3/5], Step [4662/10336], Loss: 0.0179\n",
      "Epoch [3/5], Step [4664/10336], Loss: 0.5682\n",
      "Epoch [3/5], Step [4666/10336], Loss: 0.0693\n",
      "Epoch [3/5], Step [4668/10336], Loss: 1.8608\n",
      "Epoch [3/5], Step [4670/10336], Loss: 0.0954\n",
      "Epoch [3/5], Step [4672/10336], Loss: 0.0074\n",
      "Epoch [3/5], Step [4674/10336], Loss: 0.2662\n",
      "Epoch [3/5], Step [4676/10336], Loss: 0.0140\n",
      "Epoch [3/5], Step [4678/10336], Loss: 0.0421\n",
      "Epoch [3/5], Step [4680/10336], Loss: 0.0016\n",
      "Epoch [3/5], Step [4682/10336], Loss: 3.0393\n",
      "Epoch [3/5], Step [4684/10336], Loss: 0.4769\n",
      "Epoch [3/5], Step [4686/10336], Loss: 0.0587\n",
      "Epoch [3/5], Step [4688/10336], Loss: 0.0470\n",
      "Epoch [3/5], Step [4690/10336], Loss: 0.3312\n",
      "Epoch [3/5], Step [4692/10336], Loss: 1.0592\n",
      "Epoch [3/5], Step [4694/10336], Loss: 0.0457\n",
      "Epoch [3/5], Step [4696/10336], Loss: 0.0154\n",
      "Epoch [3/5], Step [4698/10336], Loss: 0.4944\n",
      "Epoch [3/5], Step [4700/10336], Loss: 0.0151\n",
      "Epoch [3/5], Step [4702/10336], Loss: 0.3719\n",
      "Epoch [3/5], Step [4704/10336], Loss: 1.2155\n",
      "Epoch [3/5], Step [4706/10336], Loss: 2.6669\n",
      "Epoch [3/5], Step [4708/10336], Loss: 1.3229\n",
      "Epoch [3/5], Step [4710/10336], Loss: 0.0334\n",
      "Epoch [3/5], Step [4712/10336], Loss: 0.0105\n",
      "Epoch [3/5], Step [4714/10336], Loss: 0.6877\n",
      "Epoch [3/5], Step [4716/10336], Loss: 0.1631\n",
      "Epoch [3/5], Step [4718/10336], Loss: 0.0690\n",
      "Epoch [3/5], Step [4720/10336], Loss: 0.4443\n",
      "Epoch [3/5], Step [4722/10336], Loss: 1.2801\n",
      "Epoch [3/5], Step [4724/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [4726/10336], Loss: 1.3436\n",
      "Epoch [3/5], Step [4728/10336], Loss: 2.1906\n",
      "Epoch [3/5], Step [4730/10336], Loss: 1.9051\n",
      "Epoch [3/5], Step [4732/10336], Loss: 0.0951\n",
      "Epoch [3/5], Step [4734/10336], Loss: 0.1300\n",
      "Epoch [3/5], Step [4736/10336], Loss: 0.9266\n",
      "Epoch [3/5], Step [4738/10336], Loss: 1.3109\n",
      "Epoch [3/5], Step [4740/10336], Loss: 0.0029\n",
      "Epoch [3/5], Step [4742/10336], Loss: 0.1263\n",
      "Epoch [3/5], Step [4744/10336], Loss: 0.6430\n",
      "Epoch [3/5], Step [4746/10336], Loss: 0.3402\n",
      "Epoch [3/5], Step [4748/10336], Loss: 0.1641\n",
      "Epoch [3/5], Step [4750/10336], Loss: 1.8885\n",
      "Epoch [3/5], Step [4752/10336], Loss: 0.0484\n",
      "Epoch [3/5], Step [4754/10336], Loss: 0.0856\n",
      "Epoch [3/5], Step [4756/10336], Loss: 0.0140\n",
      "Epoch [3/5], Step [4758/10336], Loss: 0.0447\n",
      "Epoch [3/5], Step [4760/10336], Loss: 1.5732\n",
      "Epoch [3/5], Step [4762/10336], Loss: 1.1890\n",
      "Epoch [3/5], Step [4764/10336], Loss: 0.4671\n",
      "Epoch [3/5], Step [4766/10336], Loss: 2.2851\n",
      "Epoch [3/5], Step [4768/10336], Loss: 1.2936\n",
      "Epoch [3/5], Step [4770/10336], Loss: 0.2525\n",
      "Epoch [3/5], Step [4772/10336], Loss: 0.0439\n",
      "Epoch [3/5], Step [4774/10336], Loss: 0.5907\n",
      "Epoch [3/5], Step [4776/10336], Loss: 0.1941\n",
      "Epoch [3/5], Step [4778/10336], Loss: 0.2962\n",
      "Epoch [3/5], Step [4780/10336], Loss: 0.4352\n",
      "Epoch [3/5], Step [4782/10336], Loss: 0.0529\n",
      "Epoch [3/5], Step [4784/10336], Loss: 0.1869\n",
      "Epoch [3/5], Step [4786/10336], Loss: 0.1280\n",
      "Epoch [3/5], Step [4788/10336], Loss: 0.4123\n",
      "Epoch [3/5], Step [4790/10336], Loss: 0.5991\n",
      "Epoch [3/5], Step [4792/10336], Loss: 4.2085\n",
      "Epoch [3/5], Step [4794/10336], Loss: 0.7721\n",
      "Epoch [3/5], Step [4796/10336], Loss: 0.5298\n",
      "Epoch [3/5], Step [4798/10336], Loss: 0.3255\n",
      "Epoch [3/5], Step [4800/10336], Loss: 0.3223\n",
      "Epoch [3/5], Step [4802/10336], Loss: 0.3936\n",
      "Epoch [3/5], Step [4804/10336], Loss: 0.7705\n",
      "Epoch [3/5], Step [4806/10336], Loss: 0.0016\n",
      "Epoch [3/5], Step [4808/10336], Loss: 0.1318\n",
      "Epoch [3/5], Step [4810/10336], Loss: 1.9348\n",
      "Epoch [3/5], Step [4812/10336], Loss: 1.7234\n",
      "Epoch [3/5], Step [4814/10336], Loss: 2.4001\n",
      "Epoch [3/5], Step [4816/10336], Loss: 0.0463\n",
      "Epoch [3/5], Step [4818/10336], Loss: 0.0429\n",
      "Epoch [3/5], Step [4820/10336], Loss: 0.4867\n",
      "Epoch [3/5], Step [4822/10336], Loss: 0.9358\n",
      "Epoch [3/5], Step [4824/10336], Loss: 0.0354\n",
      "Epoch [3/5], Step [4826/10336], Loss: 0.4371\n",
      "Epoch [3/5], Step [4828/10336], Loss: 0.5509\n",
      "Epoch [3/5], Step [4830/10336], Loss: 0.9188\n",
      "Epoch [3/5], Step [4832/10336], Loss: 1.6112\n",
      "Epoch [3/5], Step [4834/10336], Loss: 1.6802\n",
      "Epoch [3/5], Step [4836/10336], Loss: 0.1037\n",
      "Epoch [3/5], Step [4838/10336], Loss: 0.0062\n",
      "Epoch [3/5], Step [4840/10336], Loss: 1.1728\n",
      "Epoch [3/5], Step [4842/10336], Loss: 0.0710\n",
      "Epoch [3/5], Step [4844/10336], Loss: 1.1100\n",
      "Epoch [3/5], Step [4846/10336], Loss: 0.9918\n",
      "Epoch [3/5], Step [4848/10336], Loss: 2.5133\n",
      "Epoch [3/5], Step [4850/10336], Loss: 0.1229\n",
      "Epoch [3/5], Step [4852/10336], Loss: 1.4549\n",
      "Epoch [3/5], Step [4854/10336], Loss: 0.7612\n",
      "Epoch [3/5], Step [4856/10336], Loss: 1.9756\n",
      "Epoch [3/5], Step [4858/10336], Loss: 0.4415\n",
      "Epoch [3/5], Step [4860/10336], Loss: 1.3136\n",
      "Epoch [3/5], Step [4862/10336], Loss: 0.7398\n",
      "Epoch [3/5], Step [4864/10336], Loss: 0.0148\n",
      "Epoch [3/5], Step [4866/10336], Loss: 0.4020\n",
      "Epoch [3/5], Step [4868/10336], Loss: 0.6685\n",
      "Epoch [3/5], Step [4870/10336], Loss: 0.0031\n",
      "Epoch [3/5], Step [4872/10336], Loss: 0.1978\n",
      "Epoch [3/5], Step [4874/10336], Loss: 0.5214\n",
      "Epoch [3/5], Step [4876/10336], Loss: 0.9543\n",
      "Epoch [3/5], Step [4878/10336], Loss: 0.2215\n",
      "Epoch [3/5], Step [4880/10336], Loss: 1.4293\n",
      "Epoch [3/5], Step [4882/10336], Loss: 0.8269\n",
      "Epoch [3/5], Step [4884/10336], Loss: 2.9983\n",
      "Epoch [3/5], Step [4886/10336], Loss: 0.5814\n",
      "Epoch [3/5], Step [4888/10336], Loss: 2.6268\n",
      "Epoch [3/5], Step [4890/10336], Loss: 1.6675\n",
      "Epoch [3/5], Step [4892/10336], Loss: 0.7411\n",
      "Epoch [3/5], Step [4894/10336], Loss: 0.2963\n",
      "Epoch [3/5], Step [4896/10336], Loss: 0.5274\n",
      "Epoch [3/5], Step [4898/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [4900/10336], Loss: 0.3558\n",
      "Epoch [3/5], Step [4902/10336], Loss: 0.3706\n",
      "Epoch [3/5], Step [4904/10336], Loss: 0.3654\n",
      "Epoch [3/5], Step [4906/10336], Loss: 1.9735\n",
      "Epoch [3/5], Step [4908/10336], Loss: 1.2899\n",
      "Epoch [3/5], Step [4910/10336], Loss: 0.0478\n",
      "Epoch [3/5], Step [4912/10336], Loss: 0.2258\n",
      "Epoch [3/5], Step [4914/10336], Loss: 0.0514\n",
      "Epoch [3/5], Step [4916/10336], Loss: 0.0532\n",
      "Epoch [3/5], Step [4918/10336], Loss: 2.0156\n",
      "Epoch [3/5], Step [4920/10336], Loss: 1.2685\n",
      "Epoch [3/5], Step [4922/10336], Loss: 0.0738\n",
      "Epoch [3/5], Step [4924/10336], Loss: 0.5570\n",
      "Epoch [3/5], Step [4926/10336], Loss: 0.2180\n",
      "Epoch [3/5], Step [4928/10336], Loss: 0.3544\n",
      "Epoch [3/5], Step [4930/10336], Loss: 0.2347\n",
      "Epoch [3/5], Step [4932/10336], Loss: 0.1360\n",
      "Epoch [3/5], Step [4934/10336], Loss: 1.2360\n",
      "Epoch [3/5], Step [4936/10336], Loss: 0.8399\n",
      "Epoch [3/5], Step [4938/10336], Loss: 0.7237\n",
      "Epoch [3/5], Step [4940/10336], Loss: 1.4748\n",
      "Epoch [3/5], Step [4942/10336], Loss: 0.0833\n",
      "Epoch [3/5], Step [4944/10336], Loss: 1.0917\n",
      "Epoch [3/5], Step [4946/10336], Loss: 1.0774\n",
      "Epoch [3/5], Step [4948/10336], Loss: 0.1005\n",
      "Epoch [3/5], Step [4950/10336], Loss: 0.3249\n",
      "Epoch [3/5], Step [4952/10336], Loss: 0.1059\n",
      "Epoch [3/5], Step [4954/10336], Loss: 0.4375\n",
      "Epoch [3/5], Step [4956/10336], Loss: 0.1675\n",
      "Epoch [3/5], Step [4958/10336], Loss: 0.0625\n",
      "Epoch [3/5], Step [4960/10336], Loss: 0.0947\n",
      "Epoch [3/5], Step [4962/10336], Loss: 0.8222\n",
      "Epoch [3/5], Step [4964/10336], Loss: 0.2976\n",
      "Epoch [3/5], Step [4966/10336], Loss: 0.4165\n",
      "Epoch [3/5], Step [4968/10336], Loss: 0.5156\n",
      "Epoch [3/5], Step [4970/10336], Loss: 4.0100\n",
      "Epoch [3/5], Step [4972/10336], Loss: 0.5699\n",
      "Epoch [3/5], Step [4974/10336], Loss: 0.1929\n",
      "Epoch [3/5], Step [4976/10336], Loss: 0.0207\n",
      "Epoch [3/5], Step [4978/10336], Loss: 1.1993\n",
      "Epoch [3/5], Step [4980/10336], Loss: 5.8460\n",
      "Epoch [3/5], Step [4982/10336], Loss: 0.6595\n",
      "Epoch [3/5], Step [4984/10336], Loss: 0.4608\n",
      "Epoch [3/5], Step [4986/10336], Loss: 1.4496\n",
      "Epoch [3/5], Step [4988/10336], Loss: 0.8979\n",
      "Epoch [3/5], Step [4990/10336], Loss: 0.3939\n",
      "Epoch [3/5], Step [4992/10336], Loss: 0.0566\n",
      "Epoch [3/5], Step [4994/10336], Loss: 0.1355\n",
      "Epoch [3/5], Step [4996/10336], Loss: 0.0064\n",
      "Epoch [3/5], Step [4998/10336], Loss: 0.7160\n",
      "Epoch [3/5], Step [5000/10336], Loss: 1.1984\n",
      "Epoch [3/5], Step [5002/10336], Loss: 0.7804\n",
      "Epoch [3/5], Step [5004/10336], Loss: 0.0300\n",
      "Epoch [3/5], Step [5006/10336], Loss: 0.0738\n",
      "Epoch [3/5], Step [5008/10336], Loss: 1.4789\n",
      "Epoch [3/5], Step [5010/10336], Loss: 0.0971\n",
      "Epoch [3/5], Step [5012/10336], Loss: 0.0181\n",
      "Epoch [3/5], Step [5014/10336], Loss: 1.6449\n",
      "Epoch [3/5], Step [5016/10336], Loss: 0.0046\n",
      "Epoch [3/5], Step [5018/10336], Loss: 0.4613\n",
      "Epoch [3/5], Step [5020/10336], Loss: 0.0788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [5022/10336], Loss: 0.1698\n",
      "Epoch [3/5], Step [5024/10336], Loss: 0.0273\n",
      "Epoch [3/5], Step [5026/10336], Loss: 0.0368\n",
      "Epoch [3/5], Step [5028/10336], Loss: 2.6983\n",
      "Epoch [3/5], Step [5030/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [5032/10336], Loss: 0.1447\n",
      "Epoch [3/5], Step [5034/10336], Loss: 0.0286\n",
      "Epoch [3/5], Step [5036/10336], Loss: 0.3910\n",
      "Epoch [3/5], Step [5038/10336], Loss: 0.0224\n",
      "Epoch [3/5], Step [5040/10336], Loss: 0.0903\n",
      "Epoch [3/5], Step [5042/10336], Loss: 0.7261\n",
      "Epoch [3/5], Step [5044/10336], Loss: 2.2785\n",
      "Epoch [3/5], Step [5046/10336], Loss: 1.9422\n",
      "Epoch [3/5], Step [5048/10336], Loss: 0.5837\n",
      "Epoch [3/5], Step [5050/10336], Loss: 0.1319\n",
      "Epoch [3/5], Step [5052/10336], Loss: 0.0030\n",
      "Epoch [3/5], Step [5054/10336], Loss: 2.2087\n",
      "Epoch [3/5], Step [5056/10336], Loss: 0.0751\n",
      "Epoch [3/5], Step [5058/10336], Loss: 0.6521\n",
      "Epoch [3/5], Step [5060/10336], Loss: 0.0557\n",
      "Epoch [3/5], Step [5062/10336], Loss: 0.5819\n",
      "Epoch [3/5], Step [5064/10336], Loss: 2.1526\n",
      "Epoch [3/5], Step [5066/10336], Loss: 0.7278\n",
      "Epoch [3/5], Step [5068/10336], Loss: 0.3739\n",
      "Epoch [3/5], Step [5070/10336], Loss: 0.5628\n",
      "Epoch [3/5], Step [5072/10336], Loss: 0.0318\n",
      "Epoch [3/5], Step [5074/10336], Loss: 1.0432\n",
      "Epoch [3/5], Step [5076/10336], Loss: 0.4146\n",
      "Epoch [3/5], Step [5078/10336], Loss: 1.2213\n",
      "Epoch [3/5], Step [5080/10336], Loss: 1.1528\n",
      "Epoch [3/5], Step [5082/10336], Loss: 0.3187\n",
      "Epoch [3/5], Step [5084/10336], Loss: 1.8346\n",
      "Epoch [3/5], Step [5086/10336], Loss: 5.6649\n",
      "Epoch [3/5], Step [5088/10336], Loss: 0.0022\n",
      "Epoch [3/5], Step [5090/10336], Loss: 0.2770\n",
      "Epoch [3/5], Step [5092/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [5094/10336], Loss: 0.0326\n",
      "Epoch [3/5], Step [5096/10336], Loss: 0.0094\n",
      "Epoch [3/5], Step [5098/10336], Loss: 0.0536\n",
      "Epoch [3/5], Step [5100/10336], Loss: 0.6510\n",
      "Epoch [3/5], Step [5102/10336], Loss: 1.7118\n",
      "Epoch [3/5], Step [5104/10336], Loss: 0.0546\n",
      "Epoch [3/5], Step [5106/10336], Loss: 0.4066\n",
      "Epoch [3/5], Step [5108/10336], Loss: 0.2827\n",
      "Epoch [3/5], Step [5110/10336], Loss: 0.0898\n",
      "Epoch [3/5], Step [5112/10336], Loss: 1.1472\n",
      "Epoch [3/5], Step [5114/10336], Loss: 0.1069\n",
      "Epoch [3/5], Step [5116/10336], Loss: 0.6831\n",
      "Epoch [3/5], Step [5118/10336], Loss: 0.0943\n",
      "Epoch [3/5], Step [5120/10336], Loss: 0.8565\n",
      "Epoch [3/5], Step [5122/10336], Loss: 0.2141\n",
      "Epoch [3/5], Step [5124/10336], Loss: 0.2755\n",
      "Epoch [3/5], Step [5126/10336], Loss: 0.1246\n",
      "Epoch [3/5], Step [5128/10336], Loss: 1.0078\n",
      "Epoch [3/5], Step [5130/10336], Loss: 0.0093\n",
      "Epoch [3/5], Step [5132/10336], Loss: 0.0762\n",
      "Epoch [3/5], Step [5134/10336], Loss: 2.8810\n",
      "Epoch [3/5], Step [5136/10336], Loss: 0.2099\n",
      "Epoch [3/5], Step [5138/10336], Loss: 2.2300\n",
      "Epoch [3/5], Step [5140/10336], Loss: 0.0564\n",
      "Epoch [3/5], Step [5142/10336], Loss: 0.1001\n",
      "Epoch [3/5], Step [5144/10336], Loss: 0.6239\n",
      "Epoch [3/5], Step [5146/10336], Loss: 3.0023\n",
      "Epoch [3/5], Step [5148/10336], Loss: 3.1837\n",
      "Epoch [3/5], Step [5150/10336], Loss: 0.0071\n",
      "Epoch [3/5], Step [5152/10336], Loss: 0.1663\n",
      "Epoch [3/5], Step [5154/10336], Loss: 0.0418\n",
      "Epoch [3/5], Step [5156/10336], Loss: 0.2599\n",
      "Epoch [3/5], Step [5158/10336], Loss: 0.0371\n",
      "Epoch [3/5], Step [5160/10336], Loss: 0.0103\n",
      "Epoch [3/5], Step [5162/10336], Loss: 0.2657\n",
      "Epoch [3/5], Step [5164/10336], Loss: 0.6125\n",
      "Epoch [3/5], Step [5166/10336], Loss: 0.1973\n",
      "Epoch [3/5], Step [5168/10336], Loss: 1.5765\n",
      "Epoch [3/5], Step [5170/10336], Loss: 0.0258\n",
      "Epoch [3/5], Step [5172/10336], Loss: 0.0112\n",
      "Epoch [3/5], Step [5174/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [5176/10336], Loss: 0.2093\n",
      "Epoch [3/5], Step [5178/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [5180/10336], Loss: 0.0134\n",
      "Epoch [3/5], Step [5182/10336], Loss: 0.5172\n",
      "Epoch [3/5], Step [5184/10336], Loss: 1.3124\n",
      "Epoch [3/5], Step [5186/10336], Loss: 0.0275\n",
      "Epoch [3/5], Step [5188/10336], Loss: 0.0001\n",
      "Epoch [3/5], Step [5190/10336], Loss: 2.1396\n",
      "Epoch [3/5], Step [5192/10336], Loss: 0.5613\n",
      "Epoch [3/5], Step [5194/10336], Loss: 0.0002\n",
      "Epoch [3/5], Step [5196/10336], Loss: 0.4249\n",
      "Epoch [3/5], Step [5198/10336], Loss: 0.0219\n",
      "Epoch [3/5], Step [5200/10336], Loss: 2.8387\n",
      "Epoch [3/5], Step [5202/10336], Loss: 0.2584\n",
      "Epoch [3/5], Step [5204/10336], Loss: 2.4436\n",
      "Epoch [3/5], Step [5206/10336], Loss: 0.3021\n",
      "Epoch [3/5], Step [5208/10336], Loss: 2.2635\n",
      "Epoch [3/5], Step [5210/10336], Loss: 0.1465\n",
      "Epoch [3/5], Step [5212/10336], Loss: 0.1458\n",
      "Epoch [3/5], Step [5214/10336], Loss: 2.6714\n",
      "Epoch [3/5], Step [5216/10336], Loss: 1.0070\n",
      "Epoch [3/5], Step [5218/10336], Loss: 0.2613\n",
      "Epoch [3/5], Step [5220/10336], Loss: 0.3029\n",
      "Epoch [3/5], Step [5222/10336], Loss: 0.8267\n",
      "Epoch [3/5], Step [5224/10336], Loss: 0.8300\n",
      "Epoch [3/5], Step [5226/10336], Loss: 2.0213\n",
      "Epoch [3/5], Step [5228/10336], Loss: 1.7368\n",
      "Epoch [3/5], Step [5230/10336], Loss: 1.3563\n",
      "Epoch [3/5], Step [5232/10336], Loss: 2.2178\n",
      "Epoch [3/5], Step [5234/10336], Loss: 0.5955\n",
      "Epoch [3/5], Step [5236/10336], Loss: 0.6653\n",
      "Epoch [3/5], Step [5238/10336], Loss: 0.7896\n",
      "Epoch [3/5], Step [5240/10336], Loss: 0.1467\n",
      "Epoch [3/5], Step [5242/10336], Loss: 0.3267\n",
      "Epoch [3/5], Step [5244/10336], Loss: 1.3694\n",
      "Epoch [3/5], Step [5246/10336], Loss: 0.4261\n",
      "Epoch [3/5], Step [5248/10336], Loss: 2.0229\n",
      "Epoch [3/5], Step [5250/10336], Loss: 0.2379\n",
      "Epoch [3/5], Step [5252/10336], Loss: 0.4458\n",
      "Epoch [3/5], Step [5254/10336], Loss: 1.0922\n",
      "Epoch [3/5], Step [5256/10336], Loss: 0.3025\n",
      "Epoch [3/5], Step [5258/10336], Loss: 3.9350\n",
      "Epoch [3/5], Step [5260/10336], Loss: 0.2138\n",
      "Epoch [3/5], Step [5262/10336], Loss: 0.0193\n",
      "Epoch [3/5], Step [5264/10336], Loss: 0.0008\n",
      "Epoch [3/5], Step [5266/10336], Loss: 0.0379\n",
      "Epoch [3/5], Step [5268/10336], Loss: 0.0233\n",
      "Epoch [3/5], Step [5270/10336], Loss: 0.9746\n",
      "Epoch [3/5], Step [5272/10336], Loss: 0.1657\n",
      "Epoch [3/5], Step [5274/10336], Loss: 2.6347\n",
      "Epoch [3/5], Step [5276/10336], Loss: 0.0080\n",
      "Epoch [3/5], Step [5278/10336], Loss: 2.0139\n",
      "Epoch [3/5], Step [5280/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [5282/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [5284/10336], Loss: 0.0146\n",
      "Epoch [3/5], Step [5286/10336], Loss: 1.0456\n",
      "Epoch [3/5], Step [5288/10336], Loss: 0.7289\n",
      "Epoch [3/5], Step [5290/10336], Loss: 2.4291\n",
      "Epoch [3/5], Step [5292/10336], Loss: 1.6223\n",
      "Epoch [3/5], Step [5294/10336], Loss: 2.1279\n",
      "Epoch [3/5], Step [5296/10336], Loss: 0.7169\n",
      "Epoch [3/5], Step [5298/10336], Loss: 0.5015\n",
      "Epoch [3/5], Step [5300/10336], Loss: 0.7085\n",
      "Epoch [3/5], Step [5302/10336], Loss: 0.7173\n",
      "Epoch [3/5], Step [5304/10336], Loss: 0.1742\n",
      "Epoch [3/5], Step [5306/10336], Loss: 3.1007\n",
      "Epoch [3/5], Step [5308/10336], Loss: 0.7624\n",
      "Epoch [3/5], Step [5310/10336], Loss: 3.9916\n",
      "Epoch [3/5], Step [5312/10336], Loss: 1.1017\n",
      "Epoch [3/5], Step [5314/10336], Loss: 1.6250\n",
      "Epoch [3/5], Step [5316/10336], Loss: 0.1847\n",
      "Epoch [3/5], Step [5318/10336], Loss: 0.9766\n",
      "Epoch [3/5], Step [5320/10336], Loss: 0.0404\n",
      "Epoch [3/5], Step [5322/10336], Loss: 0.7218\n",
      "Epoch [3/5], Step [5324/10336], Loss: 1.3389\n",
      "Epoch [3/5], Step [5326/10336], Loss: 0.0457\n",
      "Epoch [3/5], Step [5328/10336], Loss: 0.0883\n",
      "Epoch [3/5], Step [5330/10336], Loss: 0.0851\n",
      "Epoch [3/5], Step [5332/10336], Loss: 0.0170\n",
      "Epoch [3/5], Step [5334/10336], Loss: 2.0273\n",
      "Epoch [3/5], Step [5336/10336], Loss: 2.1668\n",
      "Epoch [3/5], Step [5338/10336], Loss: 2.1319\n",
      "Epoch [3/5], Step [5340/10336], Loss: 1.5978\n",
      "Epoch [3/5], Step [5342/10336], Loss: 0.0061\n",
      "Epoch [3/5], Step [5344/10336], Loss: 1.0005\n",
      "Epoch [3/5], Step [5346/10336], Loss: 0.2608\n",
      "Epoch [3/5], Step [5348/10336], Loss: 3.9544\n",
      "Epoch [3/5], Step [5350/10336], Loss: 0.2073\n",
      "Epoch [3/5], Step [5352/10336], Loss: 0.5472\n",
      "Epoch [3/5], Step [5354/10336], Loss: 0.1256\n",
      "Epoch [3/5], Step [5356/10336], Loss: 0.4753\n",
      "Epoch [3/5], Step [5358/10336], Loss: 2.2512\n",
      "Epoch [3/5], Step [5360/10336], Loss: 1.0178\n",
      "Epoch [3/5], Step [5362/10336], Loss: 1.9452\n",
      "Epoch [3/5], Step [5364/10336], Loss: 0.0391\n",
      "Epoch [3/5], Step [5366/10336], Loss: 0.1798\n",
      "Epoch [3/5], Step [5368/10336], Loss: 1.9895\n",
      "Epoch [3/5], Step [5370/10336], Loss: 0.3639\n",
      "Epoch [3/5], Step [5372/10336], Loss: 0.9150\n",
      "Epoch [3/5], Step [5374/10336], Loss: 2.0941\n",
      "Epoch [3/5], Step [5376/10336], Loss: 0.9155\n",
      "Epoch [3/5], Step [5378/10336], Loss: 0.0479\n",
      "Epoch [3/5], Step [5380/10336], Loss: 1.9082\n",
      "Epoch [3/5], Step [5382/10336], Loss: 0.0716\n",
      "Epoch [3/5], Step [5384/10336], Loss: 0.0709\n",
      "Epoch [3/5], Step [5386/10336], Loss: 0.0128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [5388/10336], Loss: 0.5285\n",
      "Epoch [3/5], Step [5390/10336], Loss: 2.0882\n",
      "Epoch [3/5], Step [5392/10336], Loss: 0.3389\n",
      "Epoch [3/5], Step [5394/10336], Loss: 0.7608\n",
      "Epoch [3/5], Step [5396/10336], Loss: 0.0109\n",
      "Epoch [3/5], Step [5398/10336], Loss: 0.1069\n",
      "Epoch [3/5], Step [5400/10336], Loss: 0.0729\n",
      "Epoch [3/5], Step [5402/10336], Loss: 0.0084\n",
      "Epoch [3/5], Step [5404/10336], Loss: 0.0050\n",
      "Epoch [3/5], Step [5406/10336], Loss: 0.0464\n",
      "Epoch [3/5], Step [5408/10336], Loss: 0.3543\n",
      "Epoch [3/5], Step [5410/10336], Loss: 0.1122\n",
      "Epoch [3/5], Step [5412/10336], Loss: 0.1590\n",
      "Epoch [3/5], Step [5414/10336], Loss: 0.0388\n",
      "Epoch [3/5], Step [5416/10336], Loss: 0.0668\n",
      "Epoch [3/5], Step [5418/10336], Loss: 1.0807\n",
      "Epoch [3/5], Step [5420/10336], Loss: 0.3386\n",
      "Epoch [3/5], Step [5422/10336], Loss: 0.6328\n",
      "Epoch [3/5], Step [5424/10336], Loss: 0.0837\n",
      "Epoch [3/5], Step [5426/10336], Loss: 0.0093\n",
      "Epoch [3/5], Step [5428/10336], Loss: 1.2360\n",
      "Epoch [3/5], Step [5430/10336], Loss: 0.6204\n",
      "Epoch [3/5], Step [5432/10336], Loss: 0.0525\n",
      "Epoch [3/5], Step [5434/10336], Loss: 0.3774\n",
      "Epoch [3/5], Step [5436/10336], Loss: 0.0642\n",
      "Epoch [3/5], Step [5438/10336], Loss: 0.8393\n",
      "Epoch [3/5], Step [5440/10336], Loss: 0.4365\n",
      "Epoch [3/5], Step [5442/10336], Loss: 0.0168\n",
      "Epoch [3/5], Step [5444/10336], Loss: 0.0561\n",
      "Epoch [3/5], Step [5446/10336], Loss: 0.2180\n",
      "Epoch [3/5], Step [5448/10336], Loss: 0.1093\n",
      "Epoch [3/5], Step [5450/10336], Loss: 0.1684\n",
      "Epoch [3/5], Step [5452/10336], Loss: 2.4395\n",
      "Epoch [3/5], Step [5454/10336], Loss: 0.0415\n",
      "Epoch [3/5], Step [5456/10336], Loss: 0.1851\n",
      "Epoch [3/5], Step [5458/10336], Loss: 1.7126\n",
      "Epoch [3/5], Step [5460/10336], Loss: 0.0513\n",
      "Epoch [3/5], Step [5462/10336], Loss: 0.0154\n",
      "Epoch [3/5], Step [5464/10336], Loss: 1.1279\n",
      "Epoch [3/5], Step [5466/10336], Loss: 2.0375\n",
      "Epoch [3/5], Step [5468/10336], Loss: 0.3315\n",
      "Epoch [3/5], Step [5470/10336], Loss: 0.0003\n",
      "Epoch [3/5], Step [5472/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [5474/10336], Loss: 0.0501\n",
      "Epoch [3/5], Step [5476/10336], Loss: 0.9755\n",
      "Epoch [3/5], Step [5478/10336], Loss: 0.9366\n",
      "Epoch [3/5], Step [5480/10336], Loss: 0.1701\n",
      "Epoch [3/5], Step [5482/10336], Loss: 0.2251\n",
      "Epoch [3/5], Step [5484/10336], Loss: 0.0238\n",
      "Epoch [3/5], Step [5486/10336], Loss: 0.3336\n",
      "Epoch [3/5], Step [5488/10336], Loss: 0.4536\n",
      "Epoch [3/5], Step [5490/10336], Loss: 0.3529\n",
      "Epoch [3/5], Step [5492/10336], Loss: 0.0396\n",
      "Epoch [3/5], Step [5494/10336], Loss: 0.8138\n",
      "Epoch [3/5], Step [5496/10336], Loss: 1.8272\n",
      "Epoch [3/5], Step [5498/10336], Loss: 0.0043\n",
      "Epoch [3/5], Step [5500/10336], Loss: 0.0053\n",
      "Epoch [3/5], Step [5502/10336], Loss: 0.0230\n",
      "Epoch [3/5], Step [5504/10336], Loss: 4.1062\n",
      "Epoch [3/5], Step [5506/10336], Loss: 0.0036\n",
      "Epoch [3/5], Step [5508/10336], Loss: 0.7500\n",
      "Epoch [3/5], Step [5510/10336], Loss: 0.7440\n",
      "Epoch [3/5], Step [5512/10336], Loss: 3.1345\n",
      "Epoch [3/5], Step [5514/10336], Loss: 0.3125\n",
      "Epoch [3/5], Step [5516/10336], Loss: 0.6523\n",
      "Epoch [3/5], Step [5518/10336], Loss: 0.1077\n",
      "Epoch [3/5], Step [5520/10336], Loss: 0.1014\n",
      "Epoch [3/5], Step [5522/10336], Loss: 0.7356\n",
      "Epoch [3/5], Step [5524/10336], Loss: 1.1733\n",
      "Epoch [3/5], Step [5526/10336], Loss: 0.1294\n",
      "Epoch [3/5], Step [5528/10336], Loss: 1.0721\n",
      "Epoch [3/5], Step [5530/10336], Loss: 0.7506\n",
      "Epoch [3/5], Step [5532/10336], Loss: 0.2806\n",
      "Epoch [3/5], Step [5534/10336], Loss: 0.4497\n",
      "Epoch [3/5], Step [5536/10336], Loss: 0.1532\n",
      "Epoch [3/5], Step [5538/10336], Loss: 0.1150\n",
      "Epoch [3/5], Step [5540/10336], Loss: 0.5544\n",
      "Epoch [3/5], Step [5542/10336], Loss: 1.1114\n",
      "Epoch [3/5], Step [5544/10336], Loss: 1.0587\n",
      "Epoch [3/5], Step [5546/10336], Loss: 0.0114\n",
      "Epoch [3/5], Step [5548/10336], Loss: 0.2283\n",
      "Epoch [3/5], Step [5550/10336], Loss: 0.1786\n",
      "Epoch [3/5], Step [5552/10336], Loss: 0.0785\n",
      "Epoch [3/5], Step [5554/10336], Loss: 0.0764\n",
      "Epoch [3/5], Step [5556/10336], Loss: 1.1383\n",
      "Epoch [3/5], Step [5558/10336], Loss: 0.0232\n",
      "Epoch [3/5], Step [5560/10336], Loss: 0.3421\n",
      "Epoch [3/5], Step [5562/10336], Loss: 0.8885\n",
      "Epoch [3/5], Step [5564/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [5566/10336], Loss: 2.7591\n",
      "Epoch [3/5], Step [5568/10336], Loss: 0.1100\n",
      "Epoch [3/5], Step [5570/10336], Loss: 0.2237\n",
      "Epoch [3/5], Step [5572/10336], Loss: 2.7817\n",
      "Epoch [3/5], Step [5574/10336], Loss: 1.3705\n",
      "Epoch [3/5], Step [5576/10336], Loss: 3.1223\n",
      "Epoch [3/5], Step [5578/10336], Loss: 0.3127\n",
      "Epoch [3/5], Step [5580/10336], Loss: 0.0606\n",
      "Epoch [3/5], Step [5582/10336], Loss: 0.0659\n",
      "Epoch [3/5], Step [5584/10336], Loss: 0.5331\n",
      "Epoch [3/5], Step [5586/10336], Loss: 0.0939\n",
      "Epoch [3/5], Step [5588/10336], Loss: 0.5650\n",
      "Epoch [3/5], Step [5590/10336], Loss: 0.1130\n",
      "Epoch [3/5], Step [5592/10336], Loss: 1.0396\n",
      "Epoch [3/5], Step [5594/10336], Loss: 0.1401\n",
      "Epoch [3/5], Step [5596/10336], Loss: 0.3063\n",
      "Epoch [3/5], Step [5598/10336], Loss: 1.3981\n",
      "Epoch [3/5], Step [5600/10336], Loss: 3.1581\n",
      "Epoch [3/5], Step [5602/10336], Loss: 0.0492\n",
      "Epoch [3/5], Step [5604/10336], Loss: 5.3434\n",
      "Epoch [3/5], Step [5606/10336], Loss: 0.0054\n",
      "Epoch [3/5], Step [5608/10336], Loss: 1.2572\n",
      "Epoch [3/5], Step [5610/10336], Loss: 0.8565\n",
      "Epoch [3/5], Step [5612/10336], Loss: 0.5200\n",
      "Epoch [3/5], Step [5614/10336], Loss: 0.1639\n",
      "Epoch [3/5], Step [5616/10336], Loss: 3.5908\n",
      "Epoch [3/5], Step [5618/10336], Loss: 3.4439\n",
      "Epoch [3/5], Step [5620/10336], Loss: 1.8484\n",
      "Epoch [3/5], Step [5622/10336], Loss: 0.0276\n",
      "Epoch [3/5], Step [5624/10336], Loss: 1.3731\n",
      "Epoch [3/5], Step [5626/10336], Loss: 0.0126\n",
      "Epoch [3/5], Step [5628/10336], Loss: 0.6913\n",
      "Epoch [3/5], Step [5630/10336], Loss: 0.0808\n",
      "Epoch [3/5], Step [5632/10336], Loss: 0.1593\n",
      "Epoch [3/5], Step [5634/10336], Loss: 0.1020\n",
      "Epoch [3/5], Step [5636/10336], Loss: 0.0930\n",
      "Epoch [3/5], Step [5638/10336], Loss: 0.6815\n",
      "Epoch [3/5], Step [5640/10336], Loss: 2.9119\n",
      "Epoch [3/5], Step [5642/10336], Loss: 1.6315\n",
      "Epoch [3/5], Step [5644/10336], Loss: 1.7299\n",
      "Epoch [3/5], Step [5646/10336], Loss: 2.0781\n",
      "Epoch [3/5], Step [5648/10336], Loss: 0.1383\n",
      "Epoch [3/5], Step [5650/10336], Loss: 0.1884\n",
      "Epoch [3/5], Step [5652/10336], Loss: 0.8799\n",
      "Epoch [3/5], Step [5654/10336], Loss: 4.3660\n",
      "Epoch [3/5], Step [5656/10336], Loss: 0.2088\n",
      "Epoch [3/5], Step [5658/10336], Loss: 0.2708\n",
      "Epoch [3/5], Step [5660/10336], Loss: 0.0426\n",
      "Epoch [3/5], Step [5662/10336], Loss: 0.0310\n",
      "Epoch [3/5], Step [5664/10336], Loss: 0.2200\n",
      "Epoch [3/5], Step [5666/10336], Loss: 1.7136\n",
      "Epoch [3/5], Step [5668/10336], Loss: 1.1054\n",
      "Epoch [3/5], Step [5670/10336], Loss: 0.0278\n",
      "Epoch [3/5], Step [5672/10336], Loss: 0.0527\n",
      "Epoch [3/5], Step [5674/10336], Loss: 0.7475\n",
      "Epoch [3/5], Step [5676/10336], Loss: 0.0393\n",
      "Epoch [3/5], Step [5678/10336], Loss: 0.7160\n",
      "Epoch [3/5], Step [5680/10336], Loss: 4.2926\n",
      "Epoch [3/5], Step [5682/10336], Loss: 0.2205\n",
      "Epoch [3/5], Step [5684/10336], Loss: 1.3202\n",
      "Epoch [3/5], Step [5686/10336], Loss: 1.3878\n",
      "Epoch [3/5], Step [5688/10336], Loss: 0.1763\n",
      "Epoch [3/5], Step [5690/10336], Loss: 2.0588\n",
      "Epoch [3/5], Step [5692/10336], Loss: 0.2742\n",
      "Epoch [3/5], Step [5694/10336], Loss: 1.1549\n",
      "Epoch [3/5], Step [5696/10336], Loss: 1.4907\n",
      "Epoch [3/5], Step [5698/10336], Loss: 0.4287\n",
      "Epoch [3/5], Step [5700/10336], Loss: 0.2402\n",
      "Epoch [3/5], Step [5702/10336], Loss: 0.2203\n",
      "Epoch [3/5], Step [5704/10336], Loss: 0.0357\n",
      "Epoch [3/5], Step [5706/10336], Loss: 0.0082\n",
      "Epoch [3/5], Step [5708/10336], Loss: 0.2747\n",
      "Epoch [3/5], Step [5710/10336], Loss: 0.2788\n",
      "Epoch [3/5], Step [5712/10336], Loss: 0.0058\n",
      "Epoch [3/5], Step [5714/10336], Loss: 0.2482\n",
      "Epoch [3/5], Step [5716/10336], Loss: 0.5360\n",
      "Epoch [3/5], Step [5718/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [5720/10336], Loss: 0.0180\n",
      "Epoch [3/5], Step [5722/10336], Loss: 0.1671\n",
      "Epoch [3/5], Step [5724/10336], Loss: 0.7598\n",
      "Epoch [3/5], Step [5726/10336], Loss: 0.0207\n",
      "Epoch [3/5], Step [5728/10336], Loss: 1.5656\n",
      "Epoch [3/5], Step [5730/10336], Loss: 0.5629\n",
      "Epoch [3/5], Step [5732/10336], Loss: 0.7545\n",
      "Epoch [3/5], Step [5734/10336], Loss: 0.0036\n",
      "Epoch [3/5], Step [5736/10336], Loss: 0.0364\n",
      "Epoch [3/5], Step [5738/10336], Loss: 2.3487\n",
      "Epoch [3/5], Step [5740/10336], Loss: 0.9552\n",
      "Epoch [3/5], Step [5742/10336], Loss: 0.7956\n",
      "Epoch [3/5], Step [5744/10336], Loss: 2.3217\n",
      "Epoch [3/5], Step [5746/10336], Loss: 3.3749\n",
      "Epoch [3/5], Step [5748/10336], Loss: 3.1097\n",
      "Epoch [3/5], Step [5750/10336], Loss: 1.0831\n",
      "Epoch [3/5], Step [5752/10336], Loss: 0.9703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [5754/10336], Loss: 0.1003\n",
      "Epoch [3/5], Step [5756/10336], Loss: 0.3216\n",
      "Epoch [3/5], Step [5758/10336], Loss: 0.0179\n",
      "Epoch [3/5], Step [5760/10336], Loss: 0.3336\n",
      "Epoch [3/5], Step [5762/10336], Loss: 0.0032\n",
      "Epoch [3/5], Step [5764/10336], Loss: 0.3891\n",
      "Epoch [3/5], Step [5766/10336], Loss: 0.0094\n",
      "Epoch [3/5], Step [5768/10336], Loss: 0.0703\n",
      "Epoch [3/5], Step [5770/10336], Loss: 0.0063\n",
      "Epoch [3/5], Step [5772/10336], Loss: 0.5894\n",
      "Epoch [3/5], Step [5774/10336], Loss: 0.0210\n",
      "Epoch [3/5], Step [5776/10336], Loss: 0.0099\n",
      "Epoch [3/5], Step [5778/10336], Loss: 0.6763\n",
      "Epoch [3/5], Step [5780/10336], Loss: 1.2409\n",
      "Epoch [3/5], Step [5782/10336], Loss: 0.6735\n",
      "Epoch [3/5], Step [5784/10336], Loss: 0.0226\n",
      "Epoch [3/5], Step [5786/10336], Loss: 0.1199\n",
      "Epoch [3/5], Step [5788/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [5790/10336], Loss: 0.2426\n",
      "Epoch [3/5], Step [5792/10336], Loss: 0.1600\n",
      "Epoch [3/5], Step [5794/10336], Loss: 0.4109\n",
      "Epoch [3/5], Step [5796/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [5798/10336], Loss: 0.1635\n",
      "Epoch [3/5], Step [5800/10336], Loss: 0.8192\n",
      "Epoch [3/5], Step [5802/10336], Loss: 0.4115\n",
      "Epoch [3/5], Step [5804/10336], Loss: 1.0988\n",
      "Epoch [3/5], Step [5806/10336], Loss: 0.9913\n",
      "Epoch [3/5], Step [5808/10336], Loss: 0.0075\n",
      "Epoch [3/5], Step [5810/10336], Loss: 0.0517\n",
      "Epoch [3/5], Step [5812/10336], Loss: 0.0320\n",
      "Epoch [3/5], Step [5814/10336], Loss: 0.5947\n",
      "Epoch [3/5], Step [5816/10336], Loss: 0.2696\n",
      "Epoch [3/5], Step [5818/10336], Loss: 0.8227\n",
      "Epoch [3/5], Step [5820/10336], Loss: 1.3221\n",
      "Epoch [3/5], Step [5822/10336], Loss: 0.5495\n",
      "Epoch [3/5], Step [5824/10336], Loss: 0.6625\n",
      "Epoch [3/5], Step [5826/10336], Loss: 1.3528\n",
      "Epoch [3/5], Step [5828/10336], Loss: 1.6180\n",
      "Epoch [3/5], Step [5830/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [5832/10336], Loss: 0.5077\n",
      "Epoch [3/5], Step [5834/10336], Loss: 1.0051\n",
      "Epoch [3/5], Step [5836/10336], Loss: 0.0155\n",
      "Epoch [3/5], Step [5838/10336], Loss: 2.5328\n",
      "Epoch [3/5], Step [5840/10336], Loss: 0.9696\n",
      "Epoch [3/5], Step [5842/10336], Loss: 1.6810\n",
      "Epoch [3/5], Step [5844/10336], Loss: 1.2734\n",
      "Epoch [3/5], Step [5846/10336], Loss: 0.0668\n",
      "Epoch [3/5], Step [5848/10336], Loss: 0.3497\n",
      "Epoch [3/5], Step [5850/10336], Loss: 0.8898\n",
      "Epoch [3/5], Step [5852/10336], Loss: 0.5624\n",
      "Epoch [3/5], Step [5854/10336], Loss: 0.0238\n",
      "Epoch [3/5], Step [5856/10336], Loss: 0.0075\n",
      "Epoch [3/5], Step [5858/10336], Loss: 0.0428\n",
      "Epoch [3/5], Step [5860/10336], Loss: 0.8356\n",
      "Epoch [3/5], Step [5862/10336], Loss: 0.0206\n",
      "Epoch [3/5], Step [5864/10336], Loss: 0.0046\n",
      "Epoch [3/5], Step [5866/10336], Loss: 0.2159\n",
      "Epoch [3/5], Step [5868/10336], Loss: 0.1499\n",
      "Epoch [3/5], Step [5870/10336], Loss: 0.0023\n",
      "Epoch [3/5], Step [5872/10336], Loss: 0.0600\n",
      "Epoch [3/5], Step [5874/10336], Loss: 0.0590\n",
      "Epoch [3/5], Step [5876/10336], Loss: 0.1011\n",
      "Epoch [3/5], Step [5878/10336], Loss: 2.1868\n",
      "Epoch [3/5], Step [5880/10336], Loss: 1.5798\n",
      "Epoch [3/5], Step [5882/10336], Loss: 1.0461\n",
      "Epoch [3/5], Step [5884/10336], Loss: 0.0059\n",
      "Epoch [3/5], Step [5886/10336], Loss: 0.0654\n",
      "Epoch [3/5], Step [5888/10336], Loss: 0.9557\n",
      "Epoch [3/5], Step [5890/10336], Loss: 0.3231\n",
      "Epoch [3/5], Step [5892/10336], Loss: 2.3084\n",
      "Epoch [3/5], Step [5894/10336], Loss: 0.3904\n",
      "Epoch [3/5], Step [5896/10336], Loss: 1.5995\n",
      "Epoch [3/5], Step [5898/10336], Loss: 0.2415\n",
      "Epoch [3/5], Step [5900/10336], Loss: 0.3375\n",
      "Epoch [3/5], Step [5902/10336], Loss: 0.7767\n",
      "Epoch [3/5], Step [5904/10336], Loss: 0.5636\n",
      "Epoch [3/5], Step [5906/10336], Loss: 0.3686\n",
      "Epoch [3/5], Step [5908/10336], Loss: 0.0746\n",
      "Epoch [3/5], Step [5910/10336], Loss: 0.1892\n",
      "Epoch [3/5], Step [5912/10336], Loss: 0.9648\n",
      "Epoch [3/5], Step [5914/10336], Loss: 2.8873\n",
      "Epoch [3/5], Step [5916/10336], Loss: 0.4135\n",
      "Epoch [3/5], Step [5918/10336], Loss: 0.1389\n",
      "Epoch [3/5], Step [5920/10336], Loss: 0.1053\n",
      "Epoch [3/5], Step [5922/10336], Loss: 0.2628\n",
      "Epoch [3/5], Step [5924/10336], Loss: 1.3920\n",
      "Epoch [3/5], Step [5926/10336], Loss: 0.1235\n",
      "Epoch [3/5], Step [5928/10336], Loss: 0.8344\n",
      "Epoch [3/5], Step [5930/10336], Loss: 0.3152\n",
      "Epoch [3/5], Step [5932/10336], Loss: 0.2953\n",
      "Epoch [3/5], Step [5934/10336], Loss: 0.9629\n",
      "Epoch [3/5], Step [5936/10336], Loss: 1.6660\n",
      "Epoch [3/5], Step [5938/10336], Loss: 0.1329\n",
      "Epoch [3/5], Step [5940/10336], Loss: 0.0380\n",
      "Epoch [3/5], Step [5942/10336], Loss: 0.0093\n",
      "Epoch [3/5], Step [5944/10336], Loss: 3.6194\n",
      "Epoch [3/5], Step [5946/10336], Loss: 0.2120\n",
      "Epoch [3/5], Step [5948/10336], Loss: 0.0111\n",
      "Epoch [3/5], Step [5950/10336], Loss: 1.1090\n",
      "Epoch [3/5], Step [5952/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [5954/10336], Loss: 0.1090\n",
      "Epoch [3/5], Step [5956/10336], Loss: 0.1693\n",
      "Epoch [3/5], Step [5958/10336], Loss: 1.5919\n",
      "Epoch [3/5], Step [5960/10336], Loss: 1.2803\n",
      "Epoch [3/5], Step [5962/10336], Loss: 0.3242\n",
      "Epoch [3/5], Step [5964/10336], Loss: 0.0037\n",
      "Epoch [3/5], Step [5966/10336], Loss: 0.0167\n",
      "Epoch [3/5], Step [5968/10336], Loss: 1.1379\n",
      "Epoch [3/5], Step [5970/10336], Loss: 0.8815\n",
      "Epoch [3/5], Step [5972/10336], Loss: 0.0821\n",
      "Epoch [3/5], Step [5974/10336], Loss: 0.1962\n",
      "Epoch [3/5], Step [5976/10336], Loss: 0.9282\n",
      "Epoch [3/5], Step [5978/10336], Loss: 0.2892\n",
      "Epoch [3/5], Step [5980/10336], Loss: 2.3083\n",
      "Epoch [3/5], Step [5982/10336], Loss: 1.4569\n",
      "Epoch [3/5], Step [5984/10336], Loss: 1.2901\n",
      "Epoch [3/5], Step [5986/10336], Loss: 0.4715\n",
      "Epoch [3/5], Step [5988/10336], Loss: 0.1457\n",
      "Epoch [3/5], Step [5990/10336], Loss: 0.1077\n",
      "Epoch [3/5], Step [5992/10336], Loss: 0.6450\n",
      "Epoch [3/5], Step [5994/10336], Loss: 0.8976\n",
      "Epoch [3/5], Step [5996/10336], Loss: 0.0384\n",
      "Epoch [3/5], Step [5998/10336], Loss: 0.3299\n",
      "Epoch [3/5], Step [6000/10336], Loss: 1.2742\n",
      "Epoch [3/5], Step [6002/10336], Loss: 0.8106\n",
      "Epoch [3/5], Step [6004/10336], Loss: 0.2177\n",
      "Epoch [3/5], Step [6006/10336], Loss: 0.0525\n",
      "Epoch [3/5], Step [6008/10336], Loss: 1.3836\n",
      "Epoch [3/5], Step [6010/10336], Loss: 1.2564\n",
      "Epoch [3/5], Step [6012/10336], Loss: 1.2757\n",
      "Epoch [3/5], Step [6014/10336], Loss: 0.0053\n",
      "Epoch [3/5], Step [6016/10336], Loss: 0.9404\n",
      "Epoch [3/5], Step [6018/10336], Loss: 0.3851\n",
      "Epoch [3/5], Step [6020/10336], Loss: 0.5950\n",
      "Epoch [3/5], Step [6022/10336], Loss: 0.5742\n",
      "Epoch [3/5], Step [6024/10336], Loss: 1.5983\n",
      "Epoch [3/5], Step [6026/10336], Loss: 0.1749\n",
      "Epoch [3/5], Step [6028/10336], Loss: 0.7459\n",
      "Epoch [3/5], Step [6030/10336], Loss: 0.2685\n",
      "Epoch [3/5], Step [6032/10336], Loss: 1.1195\n",
      "Epoch [3/5], Step [6034/10336], Loss: 0.0962\n",
      "Epoch [3/5], Step [6036/10336], Loss: 0.1301\n",
      "Epoch [3/5], Step [6038/10336], Loss: 0.7280\n",
      "Epoch [3/5], Step [6040/10336], Loss: 1.2428\n",
      "Epoch [3/5], Step [6042/10336], Loss: 1.2632\n",
      "Epoch [3/5], Step [6044/10336], Loss: 0.1244\n",
      "Epoch [3/5], Step [6046/10336], Loss: 0.5156\n",
      "Epoch [3/5], Step [6048/10336], Loss: 0.9909\n",
      "Epoch [3/5], Step [6050/10336], Loss: 0.9567\n",
      "Epoch [3/5], Step [6052/10336], Loss: 0.4546\n",
      "Epoch [3/5], Step [6054/10336], Loss: 0.0014\n",
      "Epoch [3/5], Step [6056/10336], Loss: 0.8261\n",
      "Epoch [3/5], Step [6058/10336], Loss: 0.9333\n",
      "Epoch [3/5], Step [6060/10336], Loss: 1.7587\n",
      "Epoch [3/5], Step [6062/10336], Loss: 2.6733\n",
      "Epoch [3/5], Step [6064/10336], Loss: 0.0982\n",
      "Epoch [3/5], Step [6066/10336], Loss: 0.0973\n",
      "Epoch [3/5], Step [6068/10336], Loss: 0.0021\n",
      "Epoch [3/5], Step [6070/10336], Loss: 0.4747\n",
      "Epoch [3/5], Step [6072/10336], Loss: 2.8616\n",
      "Epoch [3/5], Step [6074/10336], Loss: 0.2024\n",
      "Epoch [3/5], Step [6076/10336], Loss: 0.1302\n",
      "Epoch [3/5], Step [6078/10336], Loss: 0.8224\n",
      "Epoch [3/5], Step [6080/10336], Loss: 1.7852\n",
      "Epoch [3/5], Step [6082/10336], Loss: 0.1688\n",
      "Epoch [3/5], Step [6084/10336], Loss: 0.4278\n",
      "Epoch [3/5], Step [6086/10336], Loss: 0.6863\n",
      "Epoch [3/5], Step [6088/10336], Loss: 0.1165\n",
      "Epoch [3/5], Step [6090/10336], Loss: 1.4465\n",
      "Epoch [3/5], Step [6092/10336], Loss: 0.0990\n",
      "Epoch [3/5], Step [6094/10336], Loss: 0.0023\n",
      "Epoch [3/5], Step [6096/10336], Loss: 0.0127\n",
      "Epoch [3/5], Step [6098/10336], Loss: 0.2063\n",
      "Epoch [3/5], Step [6100/10336], Loss: 0.4820\n",
      "Epoch [3/5], Step [6102/10336], Loss: 0.4095\n",
      "Epoch [3/5], Step [6104/10336], Loss: 0.3677\n",
      "Epoch [3/5], Step [6106/10336], Loss: 2.6310\n",
      "Epoch [3/5], Step [6108/10336], Loss: 0.1949\n",
      "Epoch [3/5], Step [6110/10336], Loss: 0.4270\n",
      "Epoch [3/5], Step [6112/10336], Loss: 4.3922\n",
      "Epoch [3/5], Step [6114/10336], Loss: 0.0360\n",
      "Epoch [3/5], Step [6116/10336], Loss: 0.4578\n",
      "Epoch [3/5], Step [6118/10336], Loss: 0.9313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [6120/10336], Loss: 0.0268\n",
      "Epoch [3/5], Step [6122/10336], Loss: 0.5232\n",
      "Epoch [3/5], Step [6124/10336], Loss: 0.0140\n",
      "Epoch [3/5], Step [6126/10336], Loss: 0.6024\n",
      "Epoch [3/5], Step [6128/10336], Loss: 1.7024\n",
      "Epoch [3/5], Step [6130/10336], Loss: 0.4410\n",
      "Epoch [3/5], Step [6132/10336], Loss: 0.3674\n",
      "Epoch [3/5], Step [6134/10336], Loss: 0.0963\n",
      "Epoch [3/5], Step [6136/10336], Loss: 2.8729\n",
      "Epoch [3/5], Step [6138/10336], Loss: 0.0179\n",
      "Epoch [3/5], Step [6140/10336], Loss: 5.6847\n",
      "Epoch [3/5], Step [6142/10336], Loss: 0.0287\n",
      "Epoch [3/5], Step [6144/10336], Loss: 0.6593\n",
      "Epoch [3/5], Step [6146/10336], Loss: 0.0022\n",
      "Epoch [3/5], Step [6148/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [6150/10336], Loss: 0.2230\n",
      "Epoch [3/5], Step [6152/10336], Loss: 0.4740\n",
      "Epoch [3/5], Step [6154/10336], Loss: 0.0358\n",
      "Epoch [3/5], Step [6156/10336], Loss: 1.8715\n",
      "Epoch [3/5], Step [6158/10336], Loss: 0.7980\n",
      "Epoch [3/5], Step [6160/10336], Loss: 0.7013\n",
      "Epoch [3/5], Step [6162/10336], Loss: 1.1567\n",
      "Epoch [3/5], Step [6164/10336], Loss: 1.4672\n",
      "Epoch [3/5], Step [6166/10336], Loss: 3.7077\n",
      "Epoch [3/5], Step [6168/10336], Loss: 1.0268\n",
      "Epoch [3/5], Step [6170/10336], Loss: 0.8409\n",
      "Epoch [3/5], Step [6172/10336], Loss: 0.0395\n",
      "Epoch [3/5], Step [6174/10336], Loss: 1.7984\n",
      "Epoch [3/5], Step [6176/10336], Loss: 0.3146\n",
      "Epoch [3/5], Step [6178/10336], Loss: 0.8707\n",
      "Epoch [3/5], Step [6180/10336], Loss: 0.6180\n",
      "Epoch [3/5], Step [6182/10336], Loss: 0.4476\n",
      "Epoch [3/5], Step [6184/10336], Loss: 0.3579\n",
      "Epoch [3/5], Step [6186/10336], Loss: 0.3720\n",
      "Epoch [3/5], Step [6188/10336], Loss: 1.0844\n",
      "Epoch [3/5], Step [6190/10336], Loss: 0.7500\n",
      "Epoch [3/5], Step [6192/10336], Loss: 0.2899\n",
      "Epoch [3/5], Step [6194/10336], Loss: 1.3172\n",
      "Epoch [3/5], Step [6196/10336], Loss: 0.1533\n",
      "Epoch [3/5], Step [6198/10336], Loss: 1.4416\n",
      "Epoch [3/5], Step [6200/10336], Loss: 1.1615\n",
      "Epoch [3/5], Step [6202/10336], Loss: 0.0625\n",
      "Epoch [3/5], Step [6204/10336], Loss: 0.2679\n",
      "Epoch [3/5], Step [6206/10336], Loss: 0.8288\n",
      "Epoch [3/5], Step [6208/10336], Loss: 0.2498\n",
      "Epoch [3/5], Step [6210/10336], Loss: 0.4322\n",
      "Epoch [3/5], Step [6212/10336], Loss: 1.4541\n",
      "Epoch [3/5], Step [6214/10336], Loss: 0.1055\n",
      "Epoch [3/5], Step [6216/10336], Loss: 0.5467\n",
      "Epoch [3/5], Step [6218/10336], Loss: 2.8684\n",
      "Epoch [3/5], Step [6220/10336], Loss: 0.1702\n",
      "Epoch [3/5], Step [6222/10336], Loss: 0.3332\n",
      "Epoch [3/5], Step [6224/10336], Loss: 0.0449\n",
      "Epoch [3/5], Step [6226/10336], Loss: 0.1587\n",
      "Epoch [3/5], Step [6228/10336], Loss: 0.9225\n",
      "Epoch [3/5], Step [6230/10336], Loss: 0.1393\n",
      "Epoch [3/5], Step [6232/10336], Loss: 1.2344\n",
      "Epoch [3/5], Step [6234/10336], Loss: 0.1299\n",
      "Epoch [3/5], Step [6236/10336], Loss: 0.4520\n",
      "Epoch [3/5], Step [6238/10336], Loss: 0.2145\n",
      "Epoch [3/5], Step [6240/10336], Loss: 0.1098\n",
      "Epoch [3/5], Step [6242/10336], Loss: 0.0298\n",
      "Epoch [3/5], Step [6244/10336], Loss: 0.0406\n",
      "Epoch [3/5], Step [6246/10336], Loss: 0.6814\n",
      "Epoch [3/5], Step [6248/10336], Loss: 0.0063\n",
      "Epoch [3/5], Step [6250/10336], Loss: 0.8555\n",
      "Epoch [3/5], Step [6252/10336], Loss: 0.0738\n",
      "Epoch [3/5], Step [6254/10336], Loss: 1.8943\n",
      "Epoch [3/5], Step [6256/10336], Loss: 0.2781\n",
      "Epoch [3/5], Step [6258/10336], Loss: 0.4798\n",
      "Epoch [3/5], Step [6260/10336], Loss: 0.3872\n",
      "Epoch [3/5], Step [6262/10336], Loss: 0.0187\n",
      "Epoch [3/5], Step [6264/10336], Loss: 0.7808\n",
      "Epoch [3/5], Step [6266/10336], Loss: 0.0035\n",
      "Epoch [3/5], Step [6268/10336], Loss: 0.1590\n",
      "Epoch [3/5], Step [6270/10336], Loss: 0.5038\n",
      "Epoch [3/5], Step [6272/10336], Loss: 3.2269\n",
      "Epoch [3/5], Step [6274/10336], Loss: 0.0195\n",
      "Epoch [3/5], Step [6276/10336], Loss: 0.1764\n",
      "Epoch [3/5], Step [6278/10336], Loss: 0.0299\n",
      "Epoch [3/5], Step [6280/10336], Loss: 0.0145\n",
      "Epoch [3/5], Step [6282/10336], Loss: 0.0829\n",
      "Epoch [3/5], Step [6284/10336], Loss: 1.6184\n",
      "Epoch [3/5], Step [6286/10336], Loss: 0.0027\n",
      "Epoch [3/5], Step [6288/10336], Loss: 1.3694\n",
      "Epoch [3/5], Step [6290/10336], Loss: 1.7628\n",
      "Epoch [3/5], Step [6292/10336], Loss: 0.0392\n",
      "Epoch [3/5], Step [6294/10336], Loss: 0.0141\n",
      "Epoch [3/5], Step [6296/10336], Loss: 0.3018\n",
      "Epoch [3/5], Step [6298/10336], Loss: 0.8213\n",
      "Epoch [3/5], Step [6300/10336], Loss: 1.8610\n",
      "Epoch [3/5], Step [6302/10336], Loss: 3.4299\n",
      "Epoch [3/5], Step [6304/10336], Loss: 0.0283\n",
      "Epoch [3/5], Step [6306/10336], Loss: 0.3174\n",
      "Epoch [3/5], Step [6308/10336], Loss: 1.7324\n",
      "Epoch [3/5], Step [6310/10336], Loss: 2.6593\n",
      "Epoch [3/5], Step [6312/10336], Loss: 1.4286\n",
      "Epoch [3/5], Step [6314/10336], Loss: 0.9995\n",
      "Epoch [3/5], Step [6316/10336], Loss: 0.9819\n",
      "Epoch [3/5], Step [6318/10336], Loss: 1.2785\n",
      "Epoch [3/5], Step [6320/10336], Loss: 2.4125\n",
      "Epoch [3/5], Step [6322/10336], Loss: 0.0158\n",
      "Epoch [3/5], Step [6324/10336], Loss: 0.1095\n",
      "Epoch [3/5], Step [6326/10336], Loss: 1.8173\n",
      "Epoch [3/5], Step [6328/10336], Loss: 2.6998\n",
      "Epoch [3/5], Step [6330/10336], Loss: 0.0347\n",
      "Epoch [3/5], Step [6332/10336], Loss: 1.9234\n",
      "Epoch [3/5], Step [6334/10336], Loss: 0.5766\n",
      "Epoch [3/5], Step [6336/10336], Loss: 0.8823\n",
      "Epoch [3/5], Step [6338/10336], Loss: 0.5011\n",
      "Epoch [3/5], Step [6340/10336], Loss: 0.8527\n",
      "Epoch [3/5], Step [6342/10336], Loss: 1.7020\n",
      "Epoch [3/5], Step [6344/10336], Loss: 3.1703\n",
      "Epoch [3/5], Step [6346/10336], Loss: 1.0450\n",
      "Epoch [3/5], Step [6348/10336], Loss: 0.1602\n",
      "Epoch [3/5], Step [6350/10336], Loss: 2.1931\n",
      "Epoch [3/5], Step [6352/10336], Loss: 0.6590\n",
      "Epoch [3/5], Step [6354/10336], Loss: 1.2737\n",
      "Epoch [3/5], Step [6356/10336], Loss: 2.7566\n",
      "Epoch [3/5], Step [6358/10336], Loss: 2.4654\n",
      "Epoch [3/5], Step [6360/10336], Loss: 0.0847\n",
      "Epoch [3/5], Step [6362/10336], Loss: 0.5168\n",
      "Epoch [3/5], Step [6364/10336], Loss: 1.1755\n",
      "Epoch [3/5], Step [6366/10336], Loss: 0.3196\n",
      "Epoch [3/5], Step [6368/10336], Loss: 0.3606\n",
      "Epoch [3/5], Step [6370/10336], Loss: 0.0830\n",
      "Epoch [3/5], Step [6372/10336], Loss: 1.8366\n",
      "Epoch [3/5], Step [6374/10336], Loss: 0.8261\n",
      "Epoch [3/5], Step [6376/10336], Loss: 0.1242\n",
      "Epoch [3/5], Step [6378/10336], Loss: 0.6886\n",
      "Epoch [3/5], Step [6380/10336], Loss: 0.3362\n",
      "Epoch [3/5], Step [6382/10336], Loss: 0.0628\n",
      "Epoch [3/5], Step [6384/10336], Loss: 0.7607\n",
      "Epoch [3/5], Step [6386/10336], Loss: 0.0657\n",
      "Epoch [3/5], Step [6388/10336], Loss: 0.2099\n",
      "Epoch [3/5], Step [6390/10336], Loss: 1.1364\n",
      "Epoch [3/5], Step [6392/10336], Loss: 0.0447\n",
      "Epoch [3/5], Step [6394/10336], Loss: 0.0678\n",
      "Epoch [3/5], Step [6396/10336], Loss: 0.9934\n",
      "Epoch [3/5], Step [6398/10336], Loss: 0.5484\n",
      "Epoch [3/5], Step [6400/10336], Loss: 0.0266\n",
      "Epoch [3/5], Step [6402/10336], Loss: 0.0151\n",
      "Epoch [3/5], Step [6404/10336], Loss: 1.0633\n",
      "Epoch [3/5], Step [6406/10336], Loss: 0.1450\n",
      "Epoch [3/5], Step [6408/10336], Loss: 0.0850\n",
      "Epoch [3/5], Step [6410/10336], Loss: 0.0425\n",
      "Epoch [3/5], Step [6412/10336], Loss: 1.4224\n",
      "Epoch [3/5], Step [6414/10336], Loss: 0.8059\n",
      "Epoch [3/5], Step [6416/10336], Loss: 0.4121\n",
      "Epoch [3/5], Step [6418/10336], Loss: 0.8501\n",
      "Epoch [3/5], Step [6420/10336], Loss: 0.0177\n",
      "Epoch [3/5], Step [6422/10336], Loss: 0.0209\n",
      "Epoch [3/5], Step [6424/10336], Loss: 0.3795\n",
      "Epoch [3/5], Step [6426/10336], Loss: 0.5089\n",
      "Epoch [3/5], Step [6428/10336], Loss: 0.0256\n",
      "Epoch [3/5], Step [6430/10336], Loss: 0.3944\n",
      "Epoch [3/5], Step [6432/10336], Loss: 0.0408\n",
      "Epoch [3/5], Step [6434/10336], Loss: 0.7340\n",
      "Epoch [3/5], Step [6436/10336], Loss: 0.6103\n",
      "Epoch [3/5], Step [6438/10336], Loss: 0.8405\n",
      "Epoch [3/5], Step [6440/10336], Loss: 1.5697\n",
      "Epoch [3/5], Step [6442/10336], Loss: 0.1968\n",
      "Epoch [3/5], Step [6444/10336], Loss: 1.2227\n",
      "Epoch [3/5], Step [6446/10336], Loss: 0.2855\n",
      "Epoch [3/5], Step [6448/10336], Loss: 0.9122\n",
      "Epoch [3/5], Step [6450/10336], Loss: 0.0594\n",
      "Epoch [3/5], Step [6452/10336], Loss: 0.0582\n",
      "Epoch [3/5], Step [6454/10336], Loss: 0.0014\n",
      "Epoch [3/5], Step [6456/10336], Loss: 2.2478\n",
      "Epoch [3/5], Step [6458/10336], Loss: 0.4683\n",
      "Epoch [3/5], Step [6460/10336], Loss: 0.4026\n",
      "Epoch [3/5], Step [6462/10336], Loss: 0.6729\n",
      "Epoch [3/5], Step [6464/10336], Loss: 1.6228\n",
      "Epoch [3/5], Step [6466/10336], Loss: 1.4787\n",
      "Epoch [3/5], Step [6468/10336], Loss: 0.3477\n",
      "Epoch [3/5], Step [6470/10336], Loss: 0.3973\n",
      "Epoch [3/5], Step [6472/10336], Loss: 0.0927\n",
      "Epoch [3/5], Step [6474/10336], Loss: 0.8595\n",
      "Epoch [3/5], Step [6476/10336], Loss: 0.0334\n",
      "Epoch [3/5], Step [6478/10336], Loss: 0.1086\n",
      "Epoch [3/5], Step [6480/10336], Loss: 0.0166\n",
      "Epoch [3/5], Step [6482/10336], Loss: 0.0857\n",
      "Epoch [3/5], Step [6484/10336], Loss: 0.6565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [6486/10336], Loss: 0.0457\n",
      "Epoch [3/5], Step [6488/10336], Loss: 1.5278\n",
      "Epoch [3/5], Step [6490/10336], Loss: 0.2274\n",
      "Epoch [3/5], Step [6492/10336], Loss: 1.0339\n",
      "Epoch [3/5], Step [6494/10336], Loss: 5.1072\n",
      "Epoch [3/5], Step [6496/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [6498/10336], Loss: 0.6888\n",
      "Epoch [3/5], Step [6500/10336], Loss: 0.0789\n",
      "Epoch [3/5], Step [6502/10336], Loss: 0.1225\n",
      "Epoch [3/5], Step [6504/10336], Loss: 1.4074\n",
      "Epoch [3/5], Step [6506/10336], Loss: 0.0240\n",
      "Epoch [3/5], Step [6508/10336], Loss: 1.5959\n",
      "Epoch [3/5], Step [6510/10336], Loss: 1.6976\n",
      "Epoch [3/5], Step [6512/10336], Loss: 0.7935\n",
      "Epoch [3/5], Step [6514/10336], Loss: 0.6431\n",
      "Epoch [3/5], Step [6516/10336], Loss: 0.2694\n",
      "Epoch [3/5], Step [6518/10336], Loss: 0.0949\n",
      "Epoch [3/5], Step [6520/10336], Loss: 0.0432\n",
      "Epoch [3/5], Step [6522/10336], Loss: 0.2164\n",
      "Epoch [3/5], Step [6524/10336], Loss: 1.5199\n",
      "Epoch [3/5], Step [6526/10336], Loss: 2.6135\n",
      "Epoch [3/5], Step [6528/10336], Loss: 0.0218\n",
      "Epoch [3/5], Step [6530/10336], Loss: 0.0467\n",
      "Epoch [3/5], Step [6532/10336], Loss: 0.9251\n",
      "Epoch [3/5], Step [6534/10336], Loss: 0.4893\n",
      "Epoch [3/5], Step [6536/10336], Loss: 3.4971\n",
      "Epoch [3/5], Step [6538/10336], Loss: 0.4267\n",
      "Epoch [3/5], Step [6540/10336], Loss: 0.7042\n",
      "Epoch [3/5], Step [6542/10336], Loss: 0.2027\n",
      "Epoch [3/5], Step [6544/10336], Loss: 0.0230\n",
      "Epoch [3/5], Step [6546/10336], Loss: 0.8532\n",
      "Epoch [3/5], Step [6548/10336], Loss: 0.3073\n",
      "Epoch [3/5], Step [6550/10336], Loss: 0.0313\n",
      "Epoch [3/5], Step [6552/10336], Loss: 0.1129\n",
      "Epoch [3/5], Step [6554/10336], Loss: 0.0810\n",
      "Epoch [3/5], Step [6556/10336], Loss: 0.1646\n",
      "Epoch [3/5], Step [6558/10336], Loss: 0.7563\n",
      "Epoch [3/5], Step [6560/10336], Loss: 2.3427\n",
      "Epoch [3/5], Step [6562/10336], Loss: 0.7287\n",
      "Epoch [3/5], Step [6564/10336], Loss: 0.3439\n",
      "Epoch [3/5], Step [6566/10336], Loss: 0.7471\n",
      "Epoch [3/5], Step [6568/10336], Loss: 0.1026\n",
      "Epoch [3/5], Step [6570/10336], Loss: 0.2133\n",
      "Epoch [3/5], Step [6572/10336], Loss: 1.5353\n",
      "Epoch [3/5], Step [6574/10336], Loss: 0.0138\n",
      "Epoch [3/5], Step [6576/10336], Loss: 0.0627\n",
      "Epoch [3/5], Step [6578/10336], Loss: 1.6546\n",
      "Epoch [3/5], Step [6580/10336], Loss: 0.9004\n",
      "Epoch [3/5], Step [6582/10336], Loss: 0.0198\n",
      "Epoch [3/5], Step [6584/10336], Loss: 2.3394\n",
      "Epoch [3/5], Step [6586/10336], Loss: 1.9202\n",
      "Epoch [3/5], Step [6588/10336], Loss: 0.1482\n",
      "Epoch [3/5], Step [6590/10336], Loss: 3.0723\n",
      "Epoch [3/5], Step [6592/10336], Loss: 0.0849\n",
      "Epoch [3/5], Step [6594/10336], Loss: 0.0396\n",
      "Epoch [3/5], Step [6596/10336], Loss: 0.0510\n",
      "Epoch [3/5], Step [6598/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [6600/10336], Loss: 0.1000\n",
      "Epoch [3/5], Step [6602/10336], Loss: 0.4460\n",
      "Epoch [3/5], Step [6604/10336], Loss: 0.0715\n",
      "Epoch [3/5], Step [6606/10336], Loss: 1.1119\n",
      "Epoch [3/5], Step [6608/10336], Loss: 0.0074\n",
      "Epoch [3/5], Step [6610/10336], Loss: 1.7493\n",
      "Epoch [3/5], Step [6612/10336], Loss: 0.0408\n",
      "Epoch [3/5], Step [6614/10336], Loss: 0.6141\n",
      "Epoch [3/5], Step [6616/10336], Loss: 1.6895\n",
      "Epoch [3/5], Step [6618/10336], Loss: 0.0519\n",
      "Epoch [3/5], Step [6620/10336], Loss: 0.1275\n",
      "Epoch [3/5], Step [6622/10336], Loss: 0.0205\n",
      "Epoch [3/5], Step [6624/10336], Loss: 0.7258\n",
      "Epoch [3/5], Step [6626/10336], Loss: 0.4123\n",
      "Epoch [3/5], Step [6628/10336], Loss: 0.0049\n",
      "Epoch [3/5], Step [6630/10336], Loss: 0.8781\n",
      "Epoch [3/5], Step [6632/10336], Loss: 1.2571\n",
      "Epoch [3/5], Step [6634/10336], Loss: 0.3200\n",
      "Epoch [3/5], Step [6636/10336], Loss: 0.6449\n",
      "Epoch [3/5], Step [6638/10336], Loss: 1.1687\n",
      "Epoch [3/5], Step [6640/10336], Loss: 0.0279\n",
      "Epoch [3/5], Step [6642/10336], Loss: 0.3236\n",
      "Epoch [3/5], Step [6644/10336], Loss: 0.1012\n",
      "Epoch [3/5], Step [6646/10336], Loss: 0.9624\n",
      "Epoch [3/5], Step [6648/10336], Loss: 1.6815\n",
      "Epoch [3/5], Step [6650/10336], Loss: 0.3135\n",
      "Epoch [3/5], Step [6652/10336], Loss: 1.1406\n",
      "Epoch [3/5], Step [6654/10336], Loss: 0.1749\n",
      "Epoch [3/5], Step [6656/10336], Loss: 0.0784\n",
      "Epoch [3/5], Step [6658/10336], Loss: 1.3372\n",
      "Epoch [3/5], Step [6660/10336], Loss: 0.1028\n",
      "Epoch [3/5], Step [6662/10336], Loss: 2.3983\n",
      "Epoch [3/5], Step [6664/10336], Loss: 0.0740\n",
      "Epoch [3/5], Step [6666/10336], Loss: 2.8121\n",
      "Epoch [3/5], Step [6668/10336], Loss: 2.0707\n",
      "Epoch [3/5], Step [6670/10336], Loss: 0.4269\n",
      "Epoch [3/5], Step [6672/10336], Loss: 1.5632\n",
      "Epoch [3/5], Step [6674/10336], Loss: 4.2159\n",
      "Epoch [3/5], Step [6676/10336], Loss: 0.5814\n",
      "Epoch [3/5], Step [6678/10336], Loss: 2.1209\n",
      "Epoch [3/5], Step [6680/10336], Loss: 0.6296\n",
      "Epoch [3/5], Step [6682/10336], Loss: 0.2447\n",
      "Epoch [3/5], Step [6684/10336], Loss: 0.9137\n",
      "Epoch [3/5], Step [6686/10336], Loss: 0.5608\n",
      "Epoch [3/5], Step [6688/10336], Loss: 1.0949\n",
      "Epoch [3/5], Step [6690/10336], Loss: 1.6648\n",
      "Epoch [3/5], Step [6692/10336], Loss: 0.1373\n",
      "Epoch [3/5], Step [6694/10336], Loss: 0.4192\n",
      "Epoch [3/5], Step [6696/10336], Loss: 0.8145\n",
      "Epoch [3/5], Step [6698/10336], Loss: 0.7485\n",
      "Epoch [3/5], Step [6700/10336], Loss: 0.3375\n",
      "Epoch [3/5], Step [6702/10336], Loss: 0.2773\n",
      "Epoch [3/5], Step [6704/10336], Loss: 1.5577\n",
      "Epoch [3/5], Step [6706/10336], Loss: 0.0164\n",
      "Epoch [3/5], Step [6708/10336], Loss: 0.0051\n",
      "Epoch [3/5], Step [6710/10336], Loss: 0.8299\n",
      "Epoch [3/5], Step [6712/10336], Loss: 1.1455\n",
      "Epoch [3/5], Step [6714/10336], Loss: 1.9719\n",
      "Epoch [3/5], Step [6716/10336], Loss: 0.4931\n",
      "Epoch [3/5], Step [6718/10336], Loss: 0.9962\n",
      "Epoch [3/5], Step [6720/10336], Loss: 0.0816\n",
      "Epoch [3/5], Step [6722/10336], Loss: 1.1361\n",
      "Epoch [3/5], Step [6724/10336], Loss: 1.1081\n",
      "Epoch [3/5], Step [6726/10336], Loss: 1.0693\n",
      "Epoch [3/5], Step [6728/10336], Loss: 0.4201\n",
      "Epoch [3/5], Step [6730/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [6732/10336], Loss: 0.2037\n",
      "Epoch [3/5], Step [6734/10336], Loss: 0.0344\n",
      "Epoch [3/5], Step [6736/10336], Loss: 1.8643\n",
      "Epoch [3/5], Step [6738/10336], Loss: 2.0958\n",
      "Epoch [3/5], Step [6740/10336], Loss: 1.3728\n",
      "Epoch [3/5], Step [6742/10336], Loss: 2.3969\n",
      "Epoch [3/5], Step [6744/10336], Loss: 1.6513\n",
      "Epoch [3/5], Step [6746/10336], Loss: 0.0589\n",
      "Epoch [3/5], Step [6748/10336], Loss: 2.6193\n",
      "Epoch [3/5], Step [6750/10336], Loss: 0.0117\n",
      "Epoch [3/5], Step [6752/10336], Loss: 1.7636\n",
      "Epoch [3/5], Step [6754/10336], Loss: 0.0598\n",
      "Epoch [3/5], Step [6756/10336], Loss: 0.9686\n",
      "Epoch [3/5], Step [6758/10336], Loss: 0.8290\n",
      "Epoch [3/5], Step [6760/10336], Loss: 0.4348\n",
      "Epoch [3/5], Step [6762/10336], Loss: 1.5048\n",
      "Epoch [3/5], Step [6764/10336], Loss: 0.0107\n",
      "Epoch [3/5], Step [6766/10336], Loss: 0.2180\n",
      "Epoch [3/5], Step [6768/10336], Loss: 0.6373\n",
      "Epoch [3/5], Step [6770/10336], Loss: 0.6408\n",
      "Epoch [3/5], Step [6772/10336], Loss: 0.0714\n",
      "Epoch [3/5], Step [6774/10336], Loss: 0.9785\n",
      "Epoch [3/5], Step [6776/10336], Loss: 0.3965\n",
      "Epoch [3/5], Step [6778/10336], Loss: 0.0919\n",
      "Epoch [3/5], Step [6780/10336], Loss: 0.1261\n",
      "Epoch [3/5], Step [6782/10336], Loss: 0.5053\n",
      "Epoch [3/5], Step [6784/10336], Loss: 0.2147\n",
      "Epoch [3/5], Step [6786/10336], Loss: 0.0303\n",
      "Epoch [3/5], Step [6788/10336], Loss: 1.4822\n",
      "Epoch [3/5], Step [6790/10336], Loss: 0.0402\n",
      "Epoch [3/5], Step [6792/10336], Loss: 0.0027\n",
      "Epoch [3/5], Step [6794/10336], Loss: 0.0377\n",
      "Epoch [3/5], Step [6796/10336], Loss: 0.5244\n",
      "Epoch [3/5], Step [6798/10336], Loss: 0.4063\n",
      "Epoch [3/5], Step [6800/10336], Loss: 1.0112\n",
      "Epoch [3/5], Step [6802/10336], Loss: 0.4629\n",
      "Epoch [3/5], Step [6804/10336], Loss: 1.8002\n",
      "Epoch [3/5], Step [6806/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [6808/10336], Loss: 0.0055\n",
      "Epoch [3/5], Step [6810/10336], Loss: 1.7784\n",
      "Epoch [3/5], Step [6812/10336], Loss: 0.0149\n",
      "Epoch [3/5], Step [6814/10336], Loss: 0.0691\n",
      "Epoch [3/5], Step [6816/10336], Loss: 0.3676\n",
      "Epoch [3/5], Step [6818/10336], Loss: 0.3586\n",
      "Epoch [3/5], Step [6820/10336], Loss: 0.4751\n",
      "Epoch [3/5], Step [6822/10336], Loss: 0.1367\n",
      "Epoch [3/5], Step [6824/10336], Loss: 0.5723\n",
      "Epoch [3/5], Step [6826/10336], Loss: 0.5415\n",
      "Epoch [3/5], Step [6828/10336], Loss: 0.6774\n",
      "Epoch [3/5], Step [6830/10336], Loss: 1.2213\n",
      "Epoch [3/5], Step [6832/10336], Loss: 1.6068\n",
      "Epoch [3/5], Step [6834/10336], Loss: 0.3518\n",
      "Epoch [3/5], Step [6836/10336], Loss: 0.0023\n",
      "Epoch [3/5], Step [6838/10336], Loss: 0.0807\n",
      "Epoch [3/5], Step [6840/10336], Loss: 0.2215\n",
      "Epoch [3/5], Step [6842/10336], Loss: 0.6868\n",
      "Epoch [3/5], Step [6844/10336], Loss: 0.0384\n",
      "Epoch [3/5], Step [6846/10336], Loss: 0.3594\n",
      "Epoch [3/5], Step [6848/10336], Loss: 0.2440\n",
      "Epoch [3/5], Step [6850/10336], Loss: 1.8373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [6852/10336], Loss: 3.4284\n",
      "Epoch [3/5], Step [6854/10336], Loss: 0.1857\n",
      "Epoch [3/5], Step [6856/10336], Loss: 2.2611\n",
      "Epoch [3/5], Step [6858/10336], Loss: 0.5593\n",
      "Epoch [3/5], Step [6860/10336], Loss: 0.3141\n",
      "Epoch [3/5], Step [6862/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [6864/10336], Loss: 0.0306\n",
      "Epoch [3/5], Step [6866/10336], Loss: 2.3342\n",
      "Epoch [3/5], Step [6868/10336], Loss: 0.1448\n",
      "Epoch [3/5], Step [6870/10336], Loss: 0.5991\n",
      "Epoch [3/5], Step [6872/10336], Loss: 0.6816\n",
      "Epoch [3/5], Step [6874/10336], Loss: 2.3865\n",
      "Epoch [3/5], Step [6876/10336], Loss: 1.7969\n",
      "Epoch [3/5], Step [6878/10336], Loss: 0.9670\n",
      "Epoch [3/5], Step [6880/10336], Loss: 0.4126\n",
      "Epoch [3/5], Step [6882/10336], Loss: 0.0087\n",
      "Epoch [3/5], Step [6884/10336], Loss: 0.3464\n",
      "Epoch [3/5], Step [6886/10336], Loss: 0.0076\n",
      "Epoch [3/5], Step [6888/10336], Loss: 0.7334\n",
      "Epoch [3/5], Step [6890/10336], Loss: 0.0047\n",
      "Epoch [3/5], Step [6892/10336], Loss: 0.5930\n",
      "Epoch [3/5], Step [6894/10336], Loss: 0.1475\n",
      "Epoch [3/5], Step [6896/10336], Loss: 0.5309\n",
      "Epoch [3/5], Step [6898/10336], Loss: 0.7631\n",
      "Epoch [3/5], Step [6900/10336], Loss: 1.7484\n",
      "Epoch [3/5], Step [6902/10336], Loss: 0.5883\n",
      "Epoch [3/5], Step [6904/10336], Loss: 0.9244\n",
      "Epoch [3/5], Step [6906/10336], Loss: 4.0836\n",
      "Epoch [3/5], Step [6908/10336], Loss: 0.7994\n",
      "Epoch [3/5], Step [6910/10336], Loss: 0.2087\n",
      "Epoch [3/5], Step [6912/10336], Loss: 0.8169\n",
      "Epoch [3/5], Step [6914/10336], Loss: 0.4594\n",
      "Epoch [3/5], Step [6916/10336], Loss: 0.0515\n",
      "Epoch [3/5], Step [6918/10336], Loss: 1.3036\n",
      "Epoch [3/5], Step [6920/10336], Loss: 2.8961\n",
      "Epoch [3/5], Step [6922/10336], Loss: 1.1424\n",
      "Epoch [3/5], Step [6924/10336], Loss: 0.0135\n",
      "Epoch [3/5], Step [6926/10336], Loss: 0.5678\n",
      "Epoch [3/5], Step [6928/10336], Loss: 3.1495\n",
      "Epoch [3/5], Step [6930/10336], Loss: 1.6173\n",
      "Epoch [3/5], Step [6932/10336], Loss: 0.2928\n",
      "Epoch [3/5], Step [6934/10336], Loss: 0.8577\n",
      "Epoch [3/5], Step [6936/10336], Loss: 1.1058\n",
      "Epoch [3/5], Step [6938/10336], Loss: 0.0109\n",
      "Epoch [3/5], Step [6940/10336], Loss: 0.2996\n",
      "Epoch [3/5], Step [6942/10336], Loss: 0.3454\n",
      "Epoch [3/5], Step [6944/10336], Loss: 0.0426\n",
      "Epoch [3/5], Step [6946/10336], Loss: 1.2207\n",
      "Epoch [3/5], Step [6948/10336], Loss: 0.0613\n",
      "Epoch [3/5], Step [6950/10336], Loss: 0.1017\n",
      "Epoch [3/5], Step [6952/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [6954/10336], Loss: 0.3113\n",
      "Epoch [3/5], Step [6956/10336], Loss: 0.0461\n",
      "Epoch [3/5], Step [6958/10336], Loss: 0.3827\n",
      "Epoch [3/5], Step [6960/10336], Loss: 0.1572\n",
      "Epoch [3/5], Step [6962/10336], Loss: 2.4887\n",
      "Epoch [3/5], Step [6964/10336], Loss: 0.0190\n",
      "Epoch [3/5], Step [6966/10336], Loss: 0.2223\n",
      "Epoch [3/5], Step [6968/10336], Loss: 0.0014\n",
      "Epoch [3/5], Step [6970/10336], Loss: 1.2284\n",
      "Epoch [3/5], Step [6972/10336], Loss: 2.7581\n",
      "Epoch [3/5], Step [6974/10336], Loss: 0.7204\n",
      "Epoch [3/5], Step [6976/10336], Loss: 0.0344\n",
      "Epoch [3/5], Step [6978/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [6980/10336], Loss: 4.4440\n",
      "Epoch [3/5], Step [6982/10336], Loss: 0.0432\n",
      "Epoch [3/5], Step [6984/10336], Loss: 0.0390\n",
      "Epoch [3/5], Step [6986/10336], Loss: 0.9721\n",
      "Epoch [3/5], Step [6988/10336], Loss: 0.0159\n",
      "Epoch [3/5], Step [6990/10336], Loss: 0.5585\n",
      "Epoch [3/5], Step [6992/10336], Loss: 0.2565\n",
      "Epoch [3/5], Step [6994/10336], Loss: 2.3646\n",
      "Epoch [3/5], Step [6996/10336], Loss: 0.7906\n",
      "Epoch [3/5], Step [6998/10336], Loss: 0.8535\n",
      "Epoch [3/5], Step [7000/10336], Loss: 0.0827\n",
      "Epoch [3/5], Step [7002/10336], Loss: 0.9450\n",
      "Epoch [3/5], Step [7004/10336], Loss: 1.3090\n",
      "Epoch [3/5], Step [7006/10336], Loss: 1.6305\n",
      "Epoch [3/5], Step [7008/10336], Loss: 0.0008\n",
      "Epoch [3/5], Step [7010/10336], Loss: 0.2813\n",
      "Epoch [3/5], Step [7012/10336], Loss: 0.0328\n",
      "Epoch [3/5], Step [7014/10336], Loss: 0.0233\n",
      "Epoch [3/5], Step [7016/10336], Loss: 2.1105\n",
      "Epoch [3/5], Step [7018/10336], Loss: 0.0752\n",
      "Epoch [3/5], Step [7020/10336], Loss: 0.0378\n",
      "Epoch [3/5], Step [7022/10336], Loss: 0.1523\n",
      "Epoch [3/5], Step [7024/10336], Loss: 0.8248\n",
      "Epoch [3/5], Step [7026/10336], Loss: 0.2080\n",
      "Epoch [3/5], Step [7028/10336], Loss: 0.3570\n",
      "Epoch [3/5], Step [7030/10336], Loss: 0.4277\n",
      "Epoch [3/5], Step [7032/10336], Loss: 0.2851\n",
      "Epoch [3/5], Step [7034/10336], Loss: 0.2238\n",
      "Epoch [3/5], Step [7036/10336], Loss: 2.6459\n",
      "Epoch [3/5], Step [7038/10336], Loss: 0.7947\n",
      "Epoch [3/5], Step [7040/10336], Loss: 0.1336\n",
      "Epoch [3/5], Step [7042/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [7044/10336], Loss: 2.6834\n",
      "Epoch [3/5], Step [7046/10336], Loss: 0.0062\n",
      "Epoch [3/5], Step [7048/10336], Loss: 2.1453\n",
      "Epoch [3/5], Step [7050/10336], Loss: 1.6033\n",
      "Epoch [3/5], Step [7052/10336], Loss: 0.0660\n",
      "Epoch [3/5], Step [7054/10336], Loss: 0.7556\n",
      "Epoch [3/5], Step [7056/10336], Loss: 1.3303\n",
      "Epoch [3/5], Step [7058/10336], Loss: 1.3030\n",
      "Epoch [3/5], Step [7060/10336], Loss: 0.1181\n",
      "Epoch [3/5], Step [7062/10336], Loss: 0.0766\n",
      "Epoch [3/5], Step [7064/10336], Loss: 1.3976\n",
      "Epoch [3/5], Step [7066/10336], Loss: 0.0160\n",
      "Epoch [3/5], Step [7068/10336], Loss: 0.4007\n",
      "Epoch [3/5], Step [7070/10336], Loss: 0.5941\n",
      "Epoch [3/5], Step [7072/10336], Loss: 2.3044\n",
      "Epoch [3/5], Step [7074/10336], Loss: 1.7109\n",
      "Epoch [3/5], Step [7076/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [7078/10336], Loss: 1.0354\n",
      "Epoch [3/5], Step [7080/10336], Loss: 0.1259\n",
      "Epoch [3/5], Step [7082/10336], Loss: 2.9748\n",
      "Epoch [3/5], Step [7084/10336], Loss: 0.0095\n",
      "Epoch [3/5], Step [7086/10336], Loss: 0.0083\n",
      "Epoch [3/5], Step [7088/10336], Loss: 0.5465\n",
      "Epoch [3/5], Step [7090/10336], Loss: 0.3457\n",
      "Epoch [3/5], Step [7092/10336], Loss: 0.3139\n",
      "Epoch [3/5], Step [7094/10336], Loss: 0.5063\n",
      "Epoch [3/5], Step [7096/10336], Loss: 3.0013\n",
      "Epoch [3/5], Step [7098/10336], Loss: 0.3854\n",
      "Epoch [3/5], Step [7100/10336], Loss: 0.0359\n",
      "Epoch [3/5], Step [7102/10336], Loss: 1.3935\n",
      "Epoch [3/5], Step [7104/10336], Loss: 0.0446\n",
      "Epoch [3/5], Step [7106/10336], Loss: 0.2729\n",
      "Epoch [3/5], Step [7108/10336], Loss: 0.0300\n",
      "Epoch [3/5], Step [7110/10336], Loss: 1.1887\n",
      "Epoch [3/5], Step [7112/10336], Loss: 0.3004\n",
      "Epoch [3/5], Step [7114/10336], Loss: 2.3203\n",
      "Epoch [3/5], Step [7116/10336], Loss: 0.0659\n",
      "Epoch [3/5], Step [7118/10336], Loss: 3.6166\n",
      "Epoch [3/5], Step [7120/10336], Loss: 0.1825\n",
      "Epoch [3/5], Step [7122/10336], Loss: 1.3475\n",
      "Epoch [3/5], Step [7124/10336], Loss: 0.4999\n",
      "Epoch [3/5], Step [7126/10336], Loss: 0.3827\n",
      "Epoch [3/5], Step [7128/10336], Loss: 0.4317\n",
      "Epoch [3/5], Step [7130/10336], Loss: 0.6703\n",
      "Epoch [3/5], Step [7132/10336], Loss: 0.0932\n",
      "Epoch [3/5], Step [7134/10336], Loss: 1.4533\n",
      "Epoch [3/5], Step [7136/10336], Loss: 0.0419\n",
      "Epoch [3/5], Step [7138/10336], Loss: 1.4448\n",
      "Epoch [3/5], Step [7140/10336], Loss: 0.1775\n",
      "Epoch [3/5], Step [7142/10336], Loss: 0.4944\n",
      "Epoch [3/5], Step [7144/10336], Loss: 0.0080\n",
      "Epoch [3/5], Step [7146/10336], Loss: 0.0354\n",
      "Epoch [3/5], Step [7148/10336], Loss: 0.0277\n",
      "Epoch [3/5], Step [7150/10336], Loss: 0.0621\n",
      "Epoch [3/5], Step [7152/10336], Loss: 0.6017\n",
      "Epoch [3/5], Step [7154/10336], Loss: 0.0571\n",
      "Epoch [3/5], Step [7156/10336], Loss: 3.6726\n",
      "Epoch [3/5], Step [7158/10336], Loss: 0.8217\n",
      "Epoch [3/5], Step [7160/10336], Loss: 0.0156\n",
      "Epoch [3/5], Step [7162/10336], Loss: 0.3117\n",
      "Epoch [3/5], Step [7164/10336], Loss: 0.3352\n",
      "Epoch [3/5], Step [7166/10336], Loss: 0.3535\n",
      "Epoch [3/5], Step [7168/10336], Loss: 0.2758\n",
      "Epoch [3/5], Step [7170/10336], Loss: 0.0938\n",
      "Epoch [3/5], Step [7172/10336], Loss: 0.2108\n",
      "Epoch [3/5], Step [7174/10336], Loss: 0.0895\n",
      "Epoch [3/5], Step [7176/10336], Loss: 1.1471\n",
      "Epoch [3/5], Step [7178/10336], Loss: 0.0342\n",
      "Epoch [3/5], Step [7180/10336], Loss: 0.2296\n",
      "Epoch [3/5], Step [7182/10336], Loss: 0.0768\n",
      "Epoch [3/5], Step [7184/10336], Loss: 2.3977\n",
      "Epoch [3/5], Step [7186/10336], Loss: 0.3343\n",
      "Epoch [3/5], Step [7188/10336], Loss: 1.6596\n",
      "Epoch [3/5], Step [7190/10336], Loss: 0.5474\n",
      "Epoch [3/5], Step [7192/10336], Loss: 3.6750\n",
      "Epoch [3/5], Step [7194/10336], Loss: 0.8551\n",
      "Epoch [3/5], Step [7196/10336], Loss: 0.0496\n",
      "Epoch [3/5], Step [7198/10336], Loss: 0.1703\n",
      "Epoch [3/5], Step [7200/10336], Loss: 0.6068\n",
      "Epoch [3/5], Step [7202/10336], Loss: 0.0720\n",
      "Epoch [3/5], Step [7204/10336], Loss: 1.9542\n",
      "Epoch [3/5], Step [7206/10336], Loss: 1.4638\n",
      "Epoch [3/5], Step [7208/10336], Loss: 0.4965\n",
      "Epoch [3/5], Step [7210/10336], Loss: 0.2226\n",
      "Epoch [3/5], Step [7212/10336], Loss: 0.0037\n",
      "Epoch [3/5], Step [7214/10336], Loss: 0.7053\n",
      "Epoch [3/5], Step [7216/10336], Loss: 0.0099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [7218/10336], Loss: 1.6703\n",
      "Epoch [3/5], Step [7220/10336], Loss: 0.0183\n",
      "Epoch [3/5], Step [7222/10336], Loss: 0.6761\n",
      "Epoch [3/5], Step [7224/10336], Loss: 2.4711\n",
      "Epoch [3/5], Step [7226/10336], Loss: 0.2186\n",
      "Epoch [3/5], Step [7228/10336], Loss: 2.0659\n",
      "Epoch [3/5], Step [7230/10336], Loss: 0.0046\n",
      "Epoch [3/5], Step [7232/10336], Loss: 0.5470\n",
      "Epoch [3/5], Step [7234/10336], Loss: 0.0324\n",
      "Epoch [3/5], Step [7236/10336], Loss: 0.0103\n",
      "Epoch [3/5], Step [7238/10336], Loss: 2.3343\n",
      "Epoch [3/5], Step [7240/10336], Loss: 0.2918\n",
      "Epoch [3/5], Step [7242/10336], Loss: 0.2555\n",
      "Epoch [3/5], Step [7244/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [7246/10336], Loss: 0.0129\n",
      "Epoch [3/5], Step [7248/10336], Loss: 1.1831\n",
      "Epoch [3/5], Step [7250/10336], Loss: 0.0048\n",
      "Epoch [3/5], Step [7252/10336], Loss: 0.1136\n",
      "Epoch [3/5], Step [7254/10336], Loss: 0.9231\n",
      "Epoch [3/5], Step [7256/10336], Loss: 1.6957\n",
      "Epoch [3/5], Step [7258/10336], Loss: 1.7408\n",
      "Epoch [3/5], Step [7260/10336], Loss: 1.4771\n",
      "Epoch [3/5], Step [7262/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [7264/10336], Loss: 0.0030\n",
      "Epoch [3/5], Step [7266/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [7268/10336], Loss: 0.8528\n",
      "Epoch [3/5], Step [7270/10336], Loss: 0.0029\n",
      "Epoch [3/5], Step [7272/10336], Loss: 1.0928\n",
      "Epoch [3/5], Step [7274/10336], Loss: 0.0428\n",
      "Epoch [3/5], Step [7276/10336], Loss: 0.0644\n",
      "Epoch [3/5], Step [7278/10336], Loss: 0.3198\n",
      "Epoch [3/5], Step [7280/10336], Loss: 2.0731\n",
      "Epoch [3/5], Step [7282/10336], Loss: 0.0082\n",
      "Epoch [3/5], Step [7284/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [7286/10336], Loss: 0.9615\n",
      "Epoch [3/5], Step [7288/10336], Loss: 0.0260\n",
      "Epoch [3/5], Step [7290/10336], Loss: 0.0020\n",
      "Epoch [3/5], Step [7292/10336], Loss: 4.5503\n",
      "Epoch [3/5], Step [7294/10336], Loss: 0.0521\n",
      "Epoch [3/5], Step [7296/10336], Loss: 0.0608\n",
      "Epoch [3/5], Step [7298/10336], Loss: 2.1921\n",
      "Epoch [3/5], Step [7300/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [7302/10336], Loss: 0.5082\n",
      "Epoch [3/5], Step [7304/10336], Loss: 1.6818\n",
      "Epoch [3/5], Step [7306/10336], Loss: 0.3112\n",
      "Epoch [3/5], Step [7308/10336], Loss: 0.0111\n",
      "Epoch [3/5], Step [7310/10336], Loss: 0.0275\n",
      "Epoch [3/5], Step [7312/10336], Loss: 0.1021\n",
      "Epoch [3/5], Step [7314/10336], Loss: 1.5443\n",
      "Epoch [3/5], Step [7316/10336], Loss: 2.2070\n",
      "Epoch [3/5], Step [7318/10336], Loss: 0.1317\n",
      "Epoch [3/5], Step [7320/10336], Loss: 1.7005\n",
      "Epoch [3/5], Step [7322/10336], Loss: 0.0044\n",
      "Epoch [3/5], Step [7324/10336], Loss: 0.3140\n",
      "Epoch [3/5], Step [7326/10336], Loss: 0.0875\n",
      "Epoch [3/5], Step [7328/10336], Loss: 1.2331\n",
      "Epoch [3/5], Step [7330/10336], Loss: 2.3586\n",
      "Epoch [3/5], Step [7332/10336], Loss: 0.0848\n",
      "Epoch [3/5], Step [7334/10336], Loss: 1.4936\n",
      "Epoch [3/5], Step [7336/10336], Loss: 1.7553\n",
      "Epoch [3/5], Step [7338/10336], Loss: 0.2415\n",
      "Epoch [3/5], Step [7340/10336], Loss: 1.8795\n",
      "Epoch [3/5], Step [7342/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [7344/10336], Loss: 1.0829\n",
      "Epoch [3/5], Step [7346/10336], Loss: 0.2118\n",
      "Epoch [3/5], Step [7348/10336], Loss: 0.0831\n",
      "Epoch [3/5], Step [7350/10336], Loss: 0.6261\n",
      "Epoch [3/5], Step [7352/10336], Loss: 0.3925\n",
      "Epoch [3/5], Step [7354/10336], Loss: 1.0197\n",
      "Epoch [3/5], Step [7356/10336], Loss: 0.4743\n",
      "Epoch [3/5], Step [7358/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [7360/10336], Loss: 1.0744\n",
      "Epoch [3/5], Step [7362/10336], Loss: 2.1899\n",
      "Epoch [3/5], Step [7364/10336], Loss: 1.0334\n",
      "Epoch [3/5], Step [7366/10336], Loss: 1.3805\n",
      "Epoch [3/5], Step [7368/10336], Loss: 0.7783\n",
      "Epoch [3/5], Step [7370/10336], Loss: 0.0825\n",
      "Epoch [3/5], Step [7372/10336], Loss: 1.8258\n",
      "Epoch [3/5], Step [7374/10336], Loss: 0.0312\n",
      "Epoch [3/5], Step [7376/10336], Loss: 0.0241\n",
      "Epoch [3/5], Step [7378/10336], Loss: 1.5816\n",
      "Epoch [3/5], Step [7380/10336], Loss: 1.3175\n",
      "Epoch [3/5], Step [7382/10336], Loss: 1.0727\n",
      "Epoch [3/5], Step [7384/10336], Loss: 0.3595\n",
      "Epoch [3/5], Step [7386/10336], Loss: 0.0382\n",
      "Epoch [3/5], Step [7388/10336], Loss: 0.3533\n",
      "Epoch [3/5], Step [7390/10336], Loss: 0.0904\n",
      "Epoch [3/5], Step [7392/10336], Loss: 0.8583\n",
      "Epoch [3/5], Step [7394/10336], Loss: 0.0027\n",
      "Epoch [3/5], Step [7396/10336], Loss: 0.0097\n",
      "Epoch [3/5], Step [7398/10336], Loss: 0.0155\n",
      "Epoch [3/5], Step [7400/10336], Loss: 0.0924\n",
      "Epoch [3/5], Step [7402/10336], Loss: 0.1286\n",
      "Epoch [3/5], Step [7404/10336], Loss: 0.1173\n",
      "Epoch [3/5], Step [7406/10336], Loss: 3.9833\n",
      "Epoch [3/5], Step [7408/10336], Loss: 0.0576\n",
      "Epoch [3/5], Step [7410/10336], Loss: 0.2402\n",
      "Epoch [3/5], Step [7412/10336], Loss: 3.6951\n",
      "Epoch [3/5], Step [7414/10336], Loss: 1.5321\n",
      "Epoch [3/5], Step [7416/10336], Loss: 0.3378\n",
      "Epoch [3/5], Step [7418/10336], Loss: 0.7414\n",
      "Epoch [3/5], Step [7420/10336], Loss: 0.0035\n",
      "Epoch [3/5], Step [7422/10336], Loss: 4.0177\n",
      "Epoch [3/5], Step [7424/10336], Loss: 0.1821\n",
      "Epoch [3/5], Step [7426/10336], Loss: 1.8634\n",
      "Epoch [3/5], Step [7428/10336], Loss: 0.4337\n",
      "Epoch [3/5], Step [7430/10336], Loss: 0.9839\n",
      "Epoch [3/5], Step [7432/10336], Loss: 0.0548\n",
      "Epoch [3/5], Step [7434/10336], Loss: 2.7523\n",
      "Epoch [3/5], Step [7436/10336], Loss: 0.8883\n",
      "Epoch [3/5], Step [7438/10336], Loss: 0.2370\n",
      "Epoch [3/5], Step [7440/10336], Loss: 4.3610\n",
      "Epoch [3/5], Step [7442/10336], Loss: 0.7549\n",
      "Epoch [3/5], Step [7444/10336], Loss: 0.3966\n",
      "Epoch [3/5], Step [7446/10336], Loss: 0.5926\n",
      "Epoch [3/5], Step [7448/10336], Loss: 0.1285\n",
      "Epoch [3/5], Step [7450/10336], Loss: 0.0532\n",
      "Epoch [3/5], Step [7452/10336], Loss: 0.2839\n",
      "Epoch [3/5], Step [7454/10336], Loss: 3.3989\n",
      "Epoch [3/5], Step [7456/10336], Loss: 0.0404\n",
      "Epoch [3/5], Step [7458/10336], Loss: 0.3868\n",
      "Epoch [3/5], Step [7460/10336], Loss: 0.2011\n",
      "Epoch [3/5], Step [7462/10336], Loss: 0.5366\n",
      "Epoch [3/5], Step [7464/10336], Loss: 0.3291\n",
      "Epoch [3/5], Step [7466/10336], Loss: 0.3780\n",
      "Epoch [3/5], Step [7468/10336], Loss: 0.6070\n",
      "Epoch [3/5], Step [7470/10336], Loss: 0.3261\n",
      "Epoch [3/5], Step [7472/10336], Loss: 0.0883\n",
      "Epoch [3/5], Step [7474/10336], Loss: 0.0044\n",
      "Epoch [3/5], Step [7476/10336], Loss: 0.0111\n",
      "Epoch [3/5], Step [7478/10336], Loss: 0.0536\n",
      "Epoch [3/5], Step [7480/10336], Loss: 0.0232\n",
      "Epoch [3/5], Step [7482/10336], Loss: 0.0998\n",
      "Epoch [3/5], Step [7484/10336], Loss: 0.0054\n",
      "Epoch [3/5], Step [7486/10336], Loss: 0.0187\n",
      "Epoch [3/5], Step [7488/10336], Loss: 0.9353\n",
      "Epoch [3/5], Step [7490/10336], Loss: 2.0840\n",
      "Epoch [3/5], Step [7492/10336], Loss: 0.2045\n",
      "Epoch [3/5], Step [7494/10336], Loss: 1.1984\n",
      "Epoch [3/5], Step [7496/10336], Loss: 2.1359\n",
      "Epoch [3/5], Step [7498/10336], Loss: 0.0443\n",
      "Epoch [3/5], Step [7500/10336], Loss: 2.3864\n",
      "Epoch [3/5], Step [7502/10336], Loss: 0.3301\n",
      "Epoch [3/5], Step [7504/10336], Loss: 0.1114\n",
      "Epoch [3/5], Step [7506/10336], Loss: 0.1660\n",
      "Epoch [3/5], Step [7508/10336], Loss: 1.9873\n",
      "Epoch [3/5], Step [7510/10336], Loss: 0.8783\n",
      "Epoch [3/5], Step [7512/10336], Loss: 1.3306\n",
      "Epoch [3/5], Step [7514/10336], Loss: 0.0721\n",
      "Epoch [3/5], Step [7516/10336], Loss: 0.0119\n",
      "Epoch [3/5], Step [7518/10336], Loss: 1.4708\n",
      "Epoch [3/5], Step [7520/10336], Loss: 1.4141\n",
      "Epoch [3/5], Step [7522/10336], Loss: 1.6882\n",
      "Epoch [3/5], Step [7524/10336], Loss: 0.1482\n",
      "Epoch [3/5], Step [7526/10336], Loss: 0.0072\n",
      "Epoch [3/5], Step [7528/10336], Loss: 0.1248\n",
      "Epoch [3/5], Step [7530/10336], Loss: 0.1768\n",
      "Epoch [3/5], Step [7532/10336], Loss: 0.5325\n",
      "Epoch [3/5], Step [7534/10336], Loss: 2.3259\n",
      "Epoch [3/5], Step [7536/10336], Loss: 0.2158\n",
      "Epoch [3/5], Step [7538/10336], Loss: 0.0160\n",
      "Epoch [3/5], Step [7540/10336], Loss: 5.2326\n",
      "Epoch [3/5], Step [7542/10336], Loss: 0.4618\n",
      "Epoch [3/5], Step [7544/10336], Loss: 0.1971\n",
      "Epoch [3/5], Step [7546/10336], Loss: 0.3720\n",
      "Epoch [3/5], Step [7548/10336], Loss: 0.6933\n",
      "Epoch [3/5], Step [7550/10336], Loss: 0.3350\n",
      "Epoch [3/5], Step [7552/10336], Loss: 1.2691\n",
      "Epoch [3/5], Step [7554/10336], Loss: 0.5448\n",
      "Epoch [3/5], Step [7556/10336], Loss: 2.5215\n",
      "Epoch [3/5], Step [7558/10336], Loss: 0.0177\n",
      "Epoch [3/5], Step [7560/10336], Loss: 1.7075\n",
      "Epoch [3/5], Step [7562/10336], Loss: 0.2447\n",
      "Epoch [3/5], Step [7564/10336], Loss: 0.6578\n",
      "Epoch [3/5], Step [7566/10336], Loss: 0.6466\n",
      "Epoch [3/5], Step [7568/10336], Loss: 0.2344\n",
      "Epoch [3/5], Step [7570/10336], Loss: 0.9014\n",
      "Epoch [3/5], Step [7572/10336], Loss: 2.4224\n",
      "Epoch [3/5], Step [7574/10336], Loss: 1.2773\n",
      "Epoch [3/5], Step [7576/10336], Loss: 0.7330\n",
      "Epoch [3/5], Step [7578/10336], Loss: 0.8838\n",
      "Epoch [3/5], Step [7580/10336], Loss: 1.5273\n",
      "Epoch [3/5], Step [7582/10336], Loss: 0.0958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [7584/10336], Loss: 1.1795\n",
      "Epoch [3/5], Step [7586/10336], Loss: 0.5949\n",
      "Epoch [3/5], Step [7588/10336], Loss: 0.0789\n",
      "Epoch [3/5], Step [7590/10336], Loss: 0.0267\n",
      "Epoch [3/5], Step [7592/10336], Loss: 1.9856\n",
      "Epoch [3/5], Step [7594/10336], Loss: 0.1648\n",
      "Epoch [3/5], Step [7596/10336], Loss: 0.0613\n",
      "Epoch [3/5], Step [7598/10336], Loss: 0.0065\n",
      "Epoch [3/5], Step [7600/10336], Loss: 0.1173\n",
      "Epoch [3/5], Step [7602/10336], Loss: 0.0175\n",
      "Epoch [3/5], Step [7604/10336], Loss: 0.0221\n",
      "Epoch [3/5], Step [7606/10336], Loss: 0.9110\n",
      "Epoch [3/5], Step [7608/10336], Loss: 0.0388\n",
      "Epoch [3/5], Step [7610/10336], Loss: 0.8351\n",
      "Epoch [3/5], Step [7612/10336], Loss: 3.2667\n",
      "Epoch [3/5], Step [7614/10336], Loss: 0.3536\n",
      "Epoch [3/5], Step [7616/10336], Loss: 0.7743\n",
      "Epoch [3/5], Step [7618/10336], Loss: 0.1471\n",
      "Epoch [3/5], Step [7620/10336], Loss: 0.5979\n",
      "Epoch [3/5], Step [7622/10336], Loss: 2.2490\n",
      "Epoch [3/5], Step [7624/10336], Loss: 0.0190\n",
      "Epoch [3/5], Step [7626/10336], Loss: 0.0327\n",
      "Epoch [3/5], Step [7628/10336], Loss: 3.3727\n",
      "Epoch [3/5], Step [7630/10336], Loss: 0.6271\n",
      "Epoch [3/5], Step [7632/10336], Loss: 0.0237\n",
      "Epoch [3/5], Step [7634/10336], Loss: 1.7031\n",
      "Epoch [3/5], Step [7636/10336], Loss: 0.0344\n",
      "Epoch [3/5], Step [7638/10336], Loss: 0.4516\n",
      "Epoch [3/5], Step [7640/10336], Loss: 2.1044\n",
      "Epoch [3/5], Step [7642/10336], Loss: 2.0785\n",
      "Epoch [3/5], Step [7644/10336], Loss: 2.6682\n",
      "Epoch [3/5], Step [7646/10336], Loss: 0.5993\n",
      "Epoch [3/5], Step [7648/10336], Loss: 1.0325\n",
      "Epoch [3/5], Step [7650/10336], Loss: 1.4440\n",
      "Epoch [3/5], Step [7652/10336], Loss: 0.0715\n",
      "Epoch [3/5], Step [7654/10336], Loss: 1.8834\n",
      "Epoch [3/5], Step [7656/10336], Loss: 0.1659\n",
      "Epoch [3/5], Step [7658/10336], Loss: 2.4199\n",
      "Epoch [3/5], Step [7660/10336], Loss: 0.0813\n",
      "Epoch [3/5], Step [7662/10336], Loss: 0.3099\n",
      "Epoch [3/5], Step [7664/10336], Loss: 0.3614\n",
      "Epoch [3/5], Step [7666/10336], Loss: 0.0846\n",
      "Epoch [3/5], Step [7668/10336], Loss: 0.0970\n",
      "Epoch [3/5], Step [7670/10336], Loss: 0.2393\n",
      "Epoch [3/5], Step [7672/10336], Loss: 0.0077\n",
      "Epoch [3/5], Step [7674/10336], Loss: 1.3558\n",
      "Epoch [3/5], Step [7676/10336], Loss: 0.4330\n",
      "Epoch [3/5], Step [7678/10336], Loss: 0.0792\n",
      "Epoch [3/5], Step [7680/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [7682/10336], Loss: 0.4165\n",
      "Epoch [3/5], Step [7684/10336], Loss: 0.0057\n",
      "Epoch [3/5], Step [7686/10336], Loss: 0.3404\n",
      "Epoch [3/5], Step [7688/10336], Loss: 0.3011\n",
      "Epoch [3/5], Step [7690/10336], Loss: 3.5187\n",
      "Epoch [3/5], Step [7692/10336], Loss: 0.2184\n",
      "Epoch [3/5], Step [7694/10336], Loss: 0.0231\n",
      "Epoch [3/5], Step [7696/10336], Loss: 1.5535\n",
      "Epoch [3/5], Step [7698/10336], Loss: 0.9214\n",
      "Epoch [3/5], Step [7700/10336], Loss: 0.0729\n",
      "Epoch [3/5], Step [7702/10336], Loss: 2.2628\n",
      "Epoch [3/5], Step [7704/10336], Loss: 1.0478\n",
      "Epoch [3/5], Step [7706/10336], Loss: 1.5122\n",
      "Epoch [3/5], Step [7708/10336], Loss: 0.0263\n",
      "Epoch [3/5], Step [7710/10336], Loss: 0.6543\n",
      "Epoch [3/5], Step [7712/10336], Loss: 1.5963\n",
      "Epoch [3/5], Step [7714/10336], Loss: 0.0086\n",
      "Epoch [3/5], Step [7716/10336], Loss: 0.0586\n",
      "Epoch [3/5], Step [7718/10336], Loss: 2.1043\n",
      "Epoch [3/5], Step [7720/10336], Loss: 0.1839\n",
      "Epoch [3/5], Step [7722/10336], Loss: 0.1836\n",
      "Epoch [3/5], Step [7724/10336], Loss: 1.8193\n",
      "Epoch [3/5], Step [7726/10336], Loss: 0.0064\n",
      "Epoch [3/5], Step [7728/10336], Loss: 0.3144\n",
      "Epoch [3/5], Step [7730/10336], Loss: 0.9962\n",
      "Epoch [3/5], Step [7732/10336], Loss: 0.3193\n",
      "Epoch [3/5], Step [7734/10336], Loss: 0.9412\n",
      "Epoch [3/5], Step [7736/10336], Loss: 0.9407\n",
      "Epoch [3/5], Step [7738/10336], Loss: 1.0818\n",
      "Epoch [3/5], Step [7740/10336], Loss: 0.4220\n",
      "Epoch [3/5], Step [7742/10336], Loss: 0.4471\n",
      "Epoch [3/5], Step [7744/10336], Loss: 0.2153\n",
      "Epoch [3/5], Step [7746/10336], Loss: 0.5403\n",
      "Epoch [3/5], Step [7748/10336], Loss: 0.1114\n",
      "Epoch [3/5], Step [7750/10336], Loss: 0.3146\n",
      "Epoch [3/5], Step [7752/10336], Loss: 0.0671\n",
      "Epoch [3/5], Step [7754/10336], Loss: 0.3035\n",
      "Epoch [3/5], Step [7756/10336], Loss: 2.5460\n",
      "Epoch [3/5], Step [7758/10336], Loss: 1.4555\n",
      "Epoch [3/5], Step [7760/10336], Loss: 0.6054\n",
      "Epoch [3/5], Step [7762/10336], Loss: 1.4371\n",
      "Epoch [3/5], Step [7764/10336], Loss: 0.2628\n",
      "Epoch [3/5], Step [7766/10336], Loss: 0.6748\n",
      "Epoch [3/5], Step [7768/10336], Loss: 0.0372\n",
      "Epoch [3/5], Step [7770/10336], Loss: 0.0987\n",
      "Epoch [3/5], Step [7772/10336], Loss: 1.6106\n",
      "Epoch [3/5], Step [7774/10336], Loss: 0.0618\n",
      "Epoch [3/5], Step [7776/10336], Loss: 0.2818\n",
      "Epoch [3/5], Step [7778/10336], Loss: 0.0076\n",
      "Epoch [3/5], Step [7780/10336], Loss: 0.6989\n",
      "Epoch [3/5], Step [7782/10336], Loss: 0.0168\n",
      "Epoch [3/5], Step [7784/10336], Loss: 2.7166\n",
      "Epoch [3/5], Step [7786/10336], Loss: 2.1294\n",
      "Epoch [3/5], Step [7788/10336], Loss: 1.1694\n",
      "Epoch [3/5], Step [7790/10336], Loss: 0.6201\n",
      "Epoch [3/5], Step [7792/10336], Loss: 0.0120\n",
      "Epoch [3/5], Step [7794/10336], Loss: 0.2081\n",
      "Epoch [3/5], Step [7796/10336], Loss: 0.2003\n",
      "Epoch [3/5], Step [7798/10336], Loss: 0.3395\n",
      "Epoch [3/5], Step [7800/10336], Loss: 0.5245\n",
      "Epoch [3/5], Step [7802/10336], Loss: 0.6896\n",
      "Epoch [3/5], Step [7804/10336], Loss: 0.7644\n",
      "Epoch [3/5], Step [7806/10336], Loss: 2.0283\n",
      "Epoch [3/5], Step [7808/10336], Loss: 0.1488\n",
      "Epoch [3/5], Step [7810/10336], Loss: 1.1204\n",
      "Epoch [3/5], Step [7812/10336], Loss: 0.0577\n",
      "Epoch [3/5], Step [7814/10336], Loss: 0.9821\n",
      "Epoch [3/5], Step [7816/10336], Loss: 0.0239\n",
      "Epoch [3/5], Step [7818/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [7820/10336], Loss: 0.1931\n",
      "Epoch [3/5], Step [7822/10336], Loss: 0.9879\n",
      "Epoch [3/5], Step [7824/10336], Loss: 0.4186\n",
      "Epoch [3/5], Step [7826/10336], Loss: 0.3073\n",
      "Epoch [3/5], Step [7828/10336], Loss: 0.0635\n",
      "Epoch [3/5], Step [7830/10336], Loss: 0.0023\n",
      "Epoch [3/5], Step [7832/10336], Loss: 0.8732\n",
      "Epoch [3/5], Step [7834/10336], Loss: 0.9348\n",
      "Epoch [3/5], Step [7836/10336], Loss: 1.2550\n",
      "Epoch [3/5], Step [7838/10336], Loss: 0.1281\n",
      "Epoch [3/5], Step [7840/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [7842/10336], Loss: 0.0949\n",
      "Epoch [3/5], Step [7844/10336], Loss: 0.2160\n",
      "Epoch [3/5], Step [7846/10336], Loss: 0.1967\n",
      "Epoch [3/5], Step [7848/10336], Loss: 3.6138\n",
      "Epoch [3/5], Step [7850/10336], Loss: 0.6262\n",
      "Epoch [3/5], Step [7852/10336], Loss: 4.4101\n",
      "Epoch [3/5], Step [7854/10336], Loss: 1.0596\n",
      "Epoch [3/5], Step [7856/10336], Loss: 0.8755\n",
      "Epoch [3/5], Step [7858/10336], Loss: 0.3269\n",
      "Epoch [3/5], Step [7860/10336], Loss: 2.0771\n",
      "Epoch [3/5], Step [7862/10336], Loss: 1.4966\n",
      "Epoch [3/5], Step [7864/10336], Loss: 0.5536\n",
      "Epoch [3/5], Step [7866/10336], Loss: 0.1271\n",
      "Epoch [3/5], Step [7868/10336], Loss: 2.4255\n",
      "Epoch [3/5], Step [7870/10336], Loss: 0.0244\n",
      "Epoch [3/5], Step [7872/10336], Loss: 1.7428\n",
      "Epoch [3/5], Step [7874/10336], Loss: 0.2667\n",
      "Epoch [3/5], Step [7876/10336], Loss: 0.8435\n",
      "Epoch [3/5], Step [7878/10336], Loss: 0.3104\n",
      "Epoch [3/5], Step [7880/10336], Loss: 0.0687\n",
      "Epoch [3/5], Step [7882/10336], Loss: 4.0001\n",
      "Epoch [3/5], Step [7884/10336], Loss: 2.3251\n",
      "Epoch [3/5], Step [7886/10336], Loss: 0.2280\n",
      "Epoch [3/5], Step [7888/10336], Loss: 1.5320\n",
      "Epoch [3/5], Step [7890/10336], Loss: 1.1276\n",
      "Epoch [3/5], Step [7892/10336], Loss: 0.0169\n",
      "Epoch [3/5], Step [7894/10336], Loss: 0.4568\n",
      "Epoch [3/5], Step [7896/10336], Loss: 0.4133\n",
      "Epoch [3/5], Step [7898/10336], Loss: 0.6871\n",
      "Epoch [3/5], Step [7900/10336], Loss: 1.6358\n",
      "Epoch [3/5], Step [7902/10336], Loss: 0.0683\n",
      "Epoch [3/5], Step [7904/10336], Loss: 0.5524\n",
      "Epoch [3/5], Step [7906/10336], Loss: 0.0274\n",
      "Epoch [3/5], Step [7908/10336], Loss: 1.0411\n",
      "Epoch [3/5], Step [7910/10336], Loss: 0.0150\n",
      "Epoch [3/5], Step [7912/10336], Loss: 0.7464\n",
      "Epoch [3/5], Step [7914/10336], Loss: 3.1302\n",
      "Epoch [3/5], Step [7916/10336], Loss: 0.0102\n",
      "Epoch [3/5], Step [7918/10336], Loss: 0.6474\n",
      "Epoch [3/5], Step [7920/10336], Loss: 2.4638\n",
      "Epoch [3/5], Step [7922/10336], Loss: 0.0032\n",
      "Epoch [3/5], Step [7924/10336], Loss: 0.1438\n",
      "Epoch [3/5], Step [7926/10336], Loss: 0.0198\n",
      "Epoch [3/5], Step [7928/10336], Loss: 0.2182\n",
      "Epoch [3/5], Step [7930/10336], Loss: 0.0313\n",
      "Epoch [3/5], Step [7932/10336], Loss: 0.4972\n",
      "Epoch [3/5], Step [7934/10336], Loss: 0.0420\n",
      "Epoch [3/5], Step [7936/10336], Loss: 0.2412\n",
      "Epoch [3/5], Step [7938/10336], Loss: 0.1780\n",
      "Epoch [3/5], Step [7940/10336], Loss: 0.0064\n",
      "Epoch [3/5], Step [7942/10336], Loss: 0.0453\n",
      "Epoch [3/5], Step [7944/10336], Loss: 1.6220\n",
      "Epoch [3/5], Step [7946/10336], Loss: 1.1946\n",
      "Epoch [3/5], Step [7948/10336], Loss: 0.0006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [7950/10336], Loss: 0.1856\n",
      "Epoch [3/5], Step [7952/10336], Loss: 1.0617\n",
      "Epoch [3/5], Step [7954/10336], Loss: 0.5716\n",
      "Epoch [3/5], Step [7956/10336], Loss: 1.4186\n",
      "Epoch [3/5], Step [7958/10336], Loss: 0.2593\n",
      "Epoch [3/5], Step [7960/10336], Loss: 0.2962\n",
      "Epoch [3/5], Step [7962/10336], Loss: 0.0305\n",
      "Epoch [3/5], Step [7964/10336], Loss: 0.0652\n",
      "Epoch [3/5], Step [7966/10336], Loss: 0.3820\n",
      "Epoch [3/5], Step [7968/10336], Loss: 0.0142\n",
      "Epoch [3/5], Step [7970/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [7972/10336], Loss: 2.3443\n",
      "Epoch [3/5], Step [7974/10336], Loss: 0.0979\n",
      "Epoch [3/5], Step [7976/10336], Loss: 1.5023\n",
      "Epoch [3/5], Step [7978/10336], Loss: 0.2362\n",
      "Epoch [3/5], Step [7980/10336], Loss: 0.9658\n",
      "Epoch [3/5], Step [7982/10336], Loss: 0.0147\n",
      "Epoch [3/5], Step [7984/10336], Loss: 0.3544\n",
      "Epoch [3/5], Step [7986/10336], Loss: 0.0164\n",
      "Epoch [3/5], Step [7988/10336], Loss: 0.1239\n",
      "Epoch [3/5], Step [7990/10336], Loss: 1.3080\n",
      "Epoch [3/5], Step [7992/10336], Loss: 0.6571\n",
      "Epoch [3/5], Step [7994/10336], Loss: 3.7572\n",
      "Epoch [3/5], Step [7996/10336], Loss: 0.1327\n",
      "Epoch [3/5], Step [7998/10336], Loss: 0.1330\n",
      "Epoch [3/5], Step [8000/10336], Loss: 0.0099\n",
      "Epoch [3/5], Step [8002/10336], Loss: 0.3005\n",
      "Epoch [3/5], Step [8004/10336], Loss: 1.2425\n",
      "Epoch [3/5], Step [8006/10336], Loss: 0.4693\n",
      "Epoch [3/5], Step [8008/10336], Loss: 0.2429\n",
      "Epoch [3/5], Step [8010/10336], Loss: 0.7529\n",
      "Epoch [3/5], Step [8012/10336], Loss: 1.9237\n",
      "Epoch [3/5], Step [8014/10336], Loss: 0.3925\n",
      "Epoch [3/5], Step [8016/10336], Loss: 0.0918\n",
      "Epoch [3/5], Step [8018/10336], Loss: 1.3244\n",
      "Epoch [3/5], Step [8020/10336], Loss: 1.1517\n",
      "Epoch [3/5], Step [8022/10336], Loss: 1.8313\n",
      "Epoch [3/5], Step [8024/10336], Loss: 2.5190\n",
      "Epoch [3/5], Step [8026/10336], Loss: 0.8705\n",
      "Epoch [3/5], Step [8028/10336], Loss: 0.0112\n",
      "Epoch [3/5], Step [8030/10336], Loss: 0.2044\n",
      "Epoch [3/5], Step [8032/10336], Loss: 0.0238\n",
      "Epoch [3/5], Step [8034/10336], Loss: 1.5481\n",
      "Epoch [3/5], Step [8036/10336], Loss: 0.7742\n",
      "Epoch [3/5], Step [8038/10336], Loss: 0.0829\n",
      "Epoch [3/5], Step [8040/10336], Loss: 2.6347\n",
      "Epoch [3/5], Step [8042/10336], Loss: 0.7254\n",
      "Epoch [3/5], Step [8044/10336], Loss: 0.3150\n",
      "Epoch [3/5], Step [8046/10336], Loss: 0.1591\n",
      "Epoch [3/5], Step [8048/10336], Loss: 2.0735\n",
      "Epoch [3/5], Step [8050/10336], Loss: 0.2783\n",
      "Epoch [3/5], Step [8052/10336], Loss: 0.1186\n",
      "Epoch [3/5], Step [8054/10336], Loss: 0.1738\n",
      "Epoch [3/5], Step [8056/10336], Loss: 2.1213\n",
      "Epoch [3/5], Step [8058/10336], Loss: 0.0499\n",
      "Epoch [3/5], Step [8060/10336], Loss: 0.4611\n",
      "Epoch [3/5], Step [8062/10336], Loss: 1.1652\n",
      "Epoch [3/5], Step [8064/10336], Loss: 0.0073\n",
      "Epoch [3/5], Step [8066/10336], Loss: 0.2683\n",
      "Epoch [3/5], Step [8068/10336], Loss: 2.1217\n",
      "Epoch [3/5], Step [8070/10336], Loss: 1.3671\n",
      "Epoch [3/5], Step [8072/10336], Loss: 1.2806\n",
      "Epoch [3/5], Step [8074/10336], Loss: 0.0143\n",
      "Epoch [3/5], Step [8076/10336], Loss: 1.9443\n",
      "Epoch [3/5], Step [8078/10336], Loss: 0.4265\n",
      "Epoch [3/5], Step [8080/10336], Loss: 1.7766\n",
      "Epoch [3/5], Step [8082/10336], Loss: 0.7049\n",
      "Epoch [3/5], Step [8084/10336], Loss: 0.6787\n",
      "Epoch [3/5], Step [8086/10336], Loss: 0.1406\n",
      "Epoch [3/5], Step [8088/10336], Loss: 1.5242\n",
      "Epoch [3/5], Step [8090/10336], Loss: 0.3079\n",
      "Epoch [3/5], Step [8092/10336], Loss: 0.1436\n",
      "Epoch [3/5], Step [8094/10336], Loss: 0.0532\n",
      "Epoch [3/5], Step [8096/10336], Loss: 0.0442\n",
      "Epoch [3/5], Step [8098/10336], Loss: 0.0226\n",
      "Epoch [3/5], Step [8100/10336], Loss: 0.8005\n",
      "Epoch [3/5], Step [8102/10336], Loss: 0.0076\n",
      "Epoch [3/5], Step [8104/10336], Loss: 0.9804\n",
      "Epoch [3/5], Step [8106/10336], Loss: 2.7047\n",
      "Epoch [3/5], Step [8108/10336], Loss: 0.3778\n",
      "Epoch [3/5], Step [8110/10336], Loss: 1.5713\n",
      "Epoch [3/5], Step [8112/10336], Loss: 1.0020\n",
      "Epoch [3/5], Step [8114/10336], Loss: 0.0138\n",
      "Epoch [3/5], Step [8116/10336], Loss: 0.3134\n",
      "Epoch [3/5], Step [8118/10336], Loss: 0.1254\n",
      "Epoch [3/5], Step [8120/10336], Loss: 0.4072\n",
      "Epoch [3/5], Step [8122/10336], Loss: 2.2295\n",
      "Epoch [3/5], Step [8124/10336], Loss: 0.2112\n",
      "Epoch [3/5], Step [8126/10336], Loss: 0.0300\n",
      "Epoch [3/5], Step [8128/10336], Loss: 1.3346\n",
      "Epoch [3/5], Step [8130/10336], Loss: 1.5520\n",
      "Epoch [3/5], Step [8132/10336], Loss: 0.1210\n",
      "Epoch [3/5], Step [8134/10336], Loss: 0.1266\n",
      "Epoch [3/5], Step [8136/10336], Loss: 1.4198\n",
      "Epoch [3/5], Step [8138/10336], Loss: 0.5043\n",
      "Epoch [3/5], Step [8140/10336], Loss: 0.0588\n",
      "Epoch [3/5], Step [8142/10336], Loss: 0.1165\n",
      "Epoch [3/5], Step [8144/10336], Loss: 0.1212\n",
      "Epoch [3/5], Step [8146/10336], Loss: 0.7125\n",
      "Epoch [3/5], Step [8148/10336], Loss: 0.0056\n",
      "Epoch [3/5], Step [8150/10336], Loss: 1.0250\n",
      "Epoch [3/5], Step [8152/10336], Loss: 0.0083\n",
      "Epoch [3/5], Step [8154/10336], Loss: 0.6155\n",
      "Epoch [3/5], Step [8156/10336], Loss: 0.9761\n",
      "Epoch [3/5], Step [8158/10336], Loss: 0.0468\n",
      "Epoch [3/5], Step [8160/10336], Loss: 2.1071\n",
      "Epoch [3/5], Step [8162/10336], Loss: 0.4319\n",
      "Epoch [3/5], Step [8164/10336], Loss: 0.1979\n",
      "Epoch [3/5], Step [8166/10336], Loss: 0.0262\n",
      "Epoch [3/5], Step [8168/10336], Loss: 0.7255\n",
      "Epoch [3/5], Step [8170/10336], Loss: 1.8413\n",
      "Epoch [3/5], Step [8172/10336], Loss: 0.0050\n",
      "Epoch [3/5], Step [8174/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [8176/10336], Loss: 0.1086\n",
      "Epoch [3/5], Step [8178/10336], Loss: 0.5325\n",
      "Epoch [3/5], Step [8180/10336], Loss: 1.6272\n",
      "Epoch [3/5], Step [8182/10336], Loss: 1.4747\n",
      "Epoch [3/5], Step [8184/10336], Loss: 0.1159\n",
      "Epoch [3/5], Step [8186/10336], Loss: 1.0013\n",
      "Epoch [3/5], Step [8188/10336], Loss: 0.1564\n",
      "Epoch [3/5], Step [8190/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [8192/10336], Loss: 0.3221\n",
      "Epoch [3/5], Step [8194/10336], Loss: 0.7506\n",
      "Epoch [3/5], Step [8196/10336], Loss: 0.3291\n",
      "Epoch [3/5], Step [8198/10336], Loss: 0.4433\n",
      "Epoch [3/5], Step [8200/10336], Loss: 1.0717\n",
      "Epoch [3/5], Step [8202/10336], Loss: 0.1872\n",
      "Epoch [3/5], Step [8204/10336], Loss: 0.0051\n",
      "Epoch [3/5], Step [8206/10336], Loss: 0.5562\n",
      "Epoch [3/5], Step [8208/10336], Loss: 0.0098\n",
      "Epoch [3/5], Step [8210/10336], Loss: 2.2130\n",
      "Epoch [3/5], Step [8212/10336], Loss: 0.1240\n",
      "Epoch [3/5], Step [8214/10336], Loss: 0.4993\n",
      "Epoch [3/5], Step [8216/10336], Loss: 0.1660\n",
      "Epoch [3/5], Step [8218/10336], Loss: 0.0003\n",
      "Epoch [3/5], Step [8220/10336], Loss: 0.4082\n",
      "Epoch [3/5], Step [8222/10336], Loss: 0.0548\n",
      "Epoch [3/5], Step [8224/10336], Loss: 1.6127\n",
      "Epoch [3/5], Step [8226/10336], Loss: 0.0666\n",
      "Epoch [3/5], Step [8228/10336], Loss: 0.0175\n",
      "Epoch [3/5], Step [8230/10336], Loss: 2.3882\n",
      "Epoch [3/5], Step [8232/10336], Loss: 0.0509\n",
      "Epoch [3/5], Step [8234/10336], Loss: 0.5548\n",
      "Epoch [3/5], Step [8236/10336], Loss: 0.0050\n",
      "Epoch [3/5], Step [8238/10336], Loss: 0.3908\n",
      "Epoch [3/5], Step [8240/10336], Loss: 4.9101\n",
      "Epoch [3/5], Step [8242/10336], Loss: 0.2472\n",
      "Epoch [3/5], Step [8244/10336], Loss: 0.2709\n",
      "Epoch [3/5], Step [8246/10336], Loss: 2.4732\n",
      "Epoch [3/5], Step [8248/10336], Loss: 0.7972\n",
      "Epoch [3/5], Step [8250/10336], Loss: 0.1227\n",
      "Epoch [3/5], Step [8252/10336], Loss: 1.1035\n",
      "Epoch [3/5], Step [8254/10336], Loss: 0.0423\n",
      "Epoch [3/5], Step [8256/10336], Loss: 0.9262\n",
      "Epoch [3/5], Step [8258/10336], Loss: 0.7792\n",
      "Epoch [3/5], Step [8260/10336], Loss: 0.4868\n",
      "Epoch [3/5], Step [8262/10336], Loss: 0.4526\n",
      "Epoch [3/5], Step [8264/10336], Loss: 0.0063\n",
      "Epoch [3/5], Step [8266/10336], Loss: 0.2170\n",
      "Epoch [3/5], Step [8268/10336], Loss: 0.4707\n",
      "Epoch [3/5], Step [8270/10336], Loss: 1.0274\n",
      "Epoch [3/5], Step [8272/10336], Loss: 0.0420\n",
      "Epoch [3/5], Step [8274/10336], Loss: 2.5553\n",
      "Epoch [3/5], Step [8276/10336], Loss: 0.0034\n",
      "Epoch [3/5], Step [8278/10336], Loss: 0.0475\n",
      "Epoch [3/5], Step [8280/10336], Loss: 3.0916\n",
      "Epoch [3/5], Step [8282/10336], Loss: 3.9153\n",
      "Epoch [3/5], Step [8284/10336], Loss: 0.0544\n",
      "Epoch [3/5], Step [8286/10336], Loss: 0.0042\n",
      "Epoch [3/5], Step [8288/10336], Loss: 2.4956\n",
      "Epoch [3/5], Step [8290/10336], Loss: 0.0200\n",
      "Epoch [3/5], Step [8292/10336], Loss: 0.5171\n",
      "Epoch [3/5], Step [8294/10336], Loss: 0.3513\n",
      "Epoch [3/5], Step [8296/10336], Loss: 1.3603\n",
      "Epoch [3/5], Step [8298/10336], Loss: 1.7968\n",
      "Epoch [3/5], Step [8300/10336], Loss: 0.0421\n",
      "Epoch [3/5], Step [8302/10336], Loss: 1.8529\n",
      "Epoch [3/5], Step [8304/10336], Loss: 1.9338\n",
      "Epoch [3/5], Step [8306/10336], Loss: 0.4844\n",
      "Epoch [3/5], Step [8308/10336], Loss: 0.0528\n",
      "Epoch [3/5], Step [8310/10336], Loss: 1.4561\n",
      "Epoch [3/5], Step [8312/10336], Loss: 1.8170\n",
      "Epoch [3/5], Step [8314/10336], Loss: 0.9972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [8316/10336], Loss: 1.1396\n",
      "Epoch [3/5], Step [8318/10336], Loss: 0.3845\n",
      "Epoch [3/5], Step [8320/10336], Loss: 0.0020\n",
      "Epoch [3/5], Step [8322/10336], Loss: 2.0961\n",
      "Epoch [3/5], Step [8324/10336], Loss: 0.0905\n",
      "Epoch [3/5], Step [8326/10336], Loss: 0.8142\n",
      "Epoch [3/5], Step [8328/10336], Loss: 0.1260\n",
      "Epoch [3/5], Step [8330/10336], Loss: 0.1046\n",
      "Epoch [3/5], Step [8332/10336], Loss: 0.9219\n",
      "Epoch [3/5], Step [8334/10336], Loss: 0.1300\n",
      "Epoch [3/5], Step [8336/10336], Loss: 1.0220\n",
      "Epoch [3/5], Step [8338/10336], Loss: 0.1213\n",
      "Epoch [3/5], Step [8340/10336], Loss: 0.0081\n",
      "Epoch [3/5], Step [8342/10336], Loss: 0.0101\n",
      "Epoch [3/5], Step [8344/10336], Loss: 1.1330\n",
      "Epoch [3/5], Step [8346/10336], Loss: 0.3814\n",
      "Epoch [3/5], Step [8348/10336], Loss: 3.0565\n",
      "Epoch [3/5], Step [8350/10336], Loss: 1.0333\n",
      "Epoch [3/5], Step [8352/10336], Loss: 0.2886\n",
      "Epoch [3/5], Step [8354/10336], Loss: 0.8623\n",
      "Epoch [3/5], Step [8356/10336], Loss: 3.6669\n",
      "Epoch [3/5], Step [8358/10336], Loss: 0.1614\n",
      "Epoch [3/5], Step [8360/10336], Loss: 1.6798\n",
      "Epoch [3/5], Step [8362/10336], Loss: 1.2272\n",
      "Epoch [3/5], Step [8364/10336], Loss: 0.3190\n",
      "Epoch [3/5], Step [8366/10336], Loss: 2.1736\n",
      "Epoch [3/5], Step [8368/10336], Loss: 1.8361\n",
      "Epoch [3/5], Step [8370/10336], Loss: 0.0057\n",
      "Epoch [3/5], Step [8372/10336], Loss: 2.1592\n",
      "Epoch [3/5], Step [8374/10336], Loss: 0.3794\n",
      "Epoch [3/5], Step [8376/10336], Loss: 1.6982\n",
      "Epoch [3/5], Step [8378/10336], Loss: 0.2188\n",
      "Epoch [3/5], Step [8380/10336], Loss: 0.5585\n",
      "Epoch [3/5], Step [8382/10336], Loss: 1.9297\n",
      "Epoch [3/5], Step [8384/10336], Loss: 0.1262\n",
      "Epoch [3/5], Step [8386/10336], Loss: 0.0125\n",
      "Epoch [3/5], Step [8388/10336], Loss: 1.2586\n",
      "Epoch [3/5], Step [8390/10336], Loss: 0.8195\n",
      "Epoch [3/5], Step [8392/10336], Loss: 0.4425\n",
      "Epoch [3/5], Step [8394/10336], Loss: 0.1006\n",
      "Epoch [3/5], Step [8396/10336], Loss: 0.0489\n",
      "Epoch [3/5], Step [8398/10336], Loss: 0.9784\n",
      "Epoch [3/5], Step [8400/10336], Loss: 0.0253\n",
      "Epoch [3/5], Step [8402/10336], Loss: 0.9168\n",
      "Epoch [3/5], Step [8404/10336], Loss: 0.0777\n",
      "Epoch [3/5], Step [8406/10336], Loss: 0.4912\n",
      "Epoch [3/5], Step [8408/10336], Loss: 0.0194\n",
      "Epoch [3/5], Step [8410/10336], Loss: 0.0119\n",
      "Epoch [3/5], Step [8412/10336], Loss: 0.0894\n",
      "Epoch [3/5], Step [8414/10336], Loss: 0.1392\n",
      "Epoch [3/5], Step [8416/10336], Loss: 0.1020\n",
      "Epoch [3/5], Step [8418/10336], Loss: 0.8164\n",
      "Epoch [3/5], Step [8420/10336], Loss: 0.8591\n",
      "Epoch [3/5], Step [8422/10336], Loss: 0.1223\n",
      "Epoch [3/5], Step [8424/10336], Loss: 0.7311\n",
      "Epoch [3/5], Step [8426/10336], Loss: 0.5129\n",
      "Epoch [3/5], Step [8428/10336], Loss: 0.8803\n",
      "Epoch [3/5], Step [8430/10336], Loss: 0.0404\n",
      "Epoch [3/5], Step [8432/10336], Loss: 0.0032\n",
      "Epoch [3/5], Step [8434/10336], Loss: 0.5042\n",
      "Epoch [3/5], Step [8436/10336], Loss: 0.3774\n",
      "Epoch [3/5], Step [8438/10336], Loss: 1.9718\n",
      "Epoch [3/5], Step [8440/10336], Loss: 0.0168\n",
      "Epoch [3/5], Step [8442/10336], Loss: 0.0144\n",
      "Epoch [3/5], Step [8444/10336], Loss: 0.9923\n",
      "Epoch [3/5], Step [8446/10336], Loss: 0.4192\n",
      "Epoch [3/5], Step [8448/10336], Loss: 0.3444\n",
      "Epoch [3/5], Step [8450/10336], Loss: 1.5891\n",
      "Epoch [3/5], Step [8452/10336], Loss: 1.0037\n",
      "Epoch [3/5], Step [8454/10336], Loss: 3.2465\n",
      "Epoch [3/5], Step [8456/10336], Loss: 0.4702\n",
      "Epoch [3/5], Step [8458/10336], Loss: 2.2470\n",
      "Epoch [3/5], Step [8460/10336], Loss: 0.0476\n",
      "Epoch [3/5], Step [8462/10336], Loss: 1.8324\n",
      "Epoch [3/5], Step [8464/10336], Loss: 0.2224\n",
      "Epoch [3/5], Step [8466/10336], Loss: 1.1919\n",
      "Epoch [3/5], Step [8468/10336], Loss: 0.0297\n",
      "Epoch [3/5], Step [8470/10336], Loss: 3.3950\n",
      "Epoch [3/5], Step [8472/10336], Loss: 0.7399\n",
      "Epoch [3/5], Step [8474/10336], Loss: 1.3855\n",
      "Epoch [3/5], Step [8476/10336], Loss: 0.3849\n",
      "Epoch [3/5], Step [8478/10336], Loss: 0.0415\n",
      "Epoch [3/5], Step [8480/10336], Loss: 0.1005\n",
      "Epoch [3/5], Step [8482/10336], Loss: 1.0449\n",
      "Epoch [3/5], Step [8484/10336], Loss: 0.0611\n",
      "Epoch [3/5], Step [8486/10336], Loss: 0.7584\n",
      "Epoch [3/5], Step [8488/10336], Loss: 0.0158\n",
      "Epoch [3/5], Step [8490/10336], Loss: 0.5355\n",
      "Epoch [3/5], Step [8492/10336], Loss: 2.2436\n",
      "Epoch [3/5], Step [8494/10336], Loss: 0.6077\n",
      "Epoch [3/5], Step [8496/10336], Loss: 0.0474\n",
      "Epoch [3/5], Step [8498/10336], Loss: 0.3560\n",
      "Epoch [3/5], Step [8500/10336], Loss: 1.3799\n",
      "Epoch [3/5], Step [8502/10336], Loss: 0.6465\n",
      "Epoch [3/5], Step [8504/10336], Loss: 0.0600\n",
      "Epoch [3/5], Step [8506/10336], Loss: 2.1383\n",
      "Epoch [3/5], Step [8508/10336], Loss: 2.4213\n",
      "Epoch [3/5], Step [8510/10336], Loss: 0.7210\n",
      "Epoch [3/5], Step [8512/10336], Loss: 0.2585\n",
      "Epoch [3/5], Step [8514/10336], Loss: 1.2404\n",
      "Epoch [3/5], Step [8516/10336], Loss: 0.0166\n",
      "Epoch [3/5], Step [8518/10336], Loss: 1.7478\n",
      "Epoch [3/5], Step [8520/10336], Loss: 1.1583\n",
      "Epoch [3/5], Step [8522/10336], Loss: 1.4459\n",
      "Epoch [3/5], Step [8524/10336], Loss: 1.3104\n",
      "Epoch [3/5], Step [8526/10336], Loss: 1.3785\n",
      "Epoch [3/5], Step [8528/10336], Loss: 0.0041\n",
      "Epoch [3/5], Step [8530/10336], Loss: 3.3297\n",
      "Epoch [3/5], Step [8532/10336], Loss: 1.4497\n",
      "Epoch [3/5], Step [8534/10336], Loss: 1.0507\n",
      "Epoch [3/5], Step [8536/10336], Loss: 4.6886\n",
      "Epoch [3/5], Step [8538/10336], Loss: 0.2610\n",
      "Epoch [3/5], Step [8540/10336], Loss: 1.2324\n",
      "Epoch [3/5], Step [8542/10336], Loss: 0.1068\n",
      "Epoch [3/5], Step [8544/10336], Loss: 2.4530\n",
      "Epoch [3/5], Step [8546/10336], Loss: 1.1483\n",
      "Epoch [3/5], Step [8548/10336], Loss: 0.4124\n",
      "Epoch [3/5], Step [8550/10336], Loss: 1.0406\n",
      "Epoch [3/5], Step [8552/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [8554/10336], Loss: 0.0038\n",
      "Epoch [3/5], Step [8556/10336], Loss: 0.0813\n",
      "Epoch [3/5], Step [8558/10336], Loss: 0.0284\n",
      "Epoch [3/5], Step [8560/10336], Loss: 0.0076\n",
      "Epoch [3/5], Step [8562/10336], Loss: 0.1736\n",
      "Epoch [3/5], Step [8564/10336], Loss: 0.0219\n",
      "Epoch [3/5], Step [8566/10336], Loss: 4.6009\n",
      "Epoch [3/5], Step [8568/10336], Loss: 0.0381\n",
      "Epoch [3/5], Step [8570/10336], Loss: 0.6063\n",
      "Epoch [3/5], Step [8572/10336], Loss: 1.3007\n",
      "Epoch [3/5], Step [8574/10336], Loss: 3.4856\n",
      "Epoch [3/5], Step [8576/10336], Loss: 0.0191\n",
      "Epoch [3/5], Step [8578/10336], Loss: 0.5639\n",
      "Epoch [3/5], Step [8580/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [8582/10336], Loss: 0.3620\n",
      "Epoch [3/5], Step [8584/10336], Loss: 0.1284\n",
      "Epoch [3/5], Step [8586/10336], Loss: 0.2540\n",
      "Epoch [3/5], Step [8588/10336], Loss: 0.5081\n",
      "Epoch [3/5], Step [8590/10336], Loss: 0.6377\n",
      "Epoch [3/5], Step [8592/10336], Loss: 0.0622\n",
      "Epoch [3/5], Step [8594/10336], Loss: 0.7621\n",
      "Epoch [3/5], Step [8596/10336], Loss: 2.9122\n",
      "Epoch [3/5], Step [8598/10336], Loss: 0.2271\n",
      "Epoch [3/5], Step [8600/10336], Loss: 0.0124\n",
      "Epoch [3/5], Step [8602/10336], Loss: 0.2071\n",
      "Epoch [3/5], Step [8604/10336], Loss: 1.0688\n",
      "Epoch [3/5], Step [8606/10336], Loss: 0.1883\n",
      "Epoch [3/5], Step [8608/10336], Loss: 3.2782\n",
      "Epoch [3/5], Step [8610/10336], Loss: 0.0280\n",
      "Epoch [3/5], Step [8612/10336], Loss: 0.4074\n",
      "Epoch [3/5], Step [8614/10336], Loss: 0.3942\n",
      "Epoch [3/5], Step [8616/10336], Loss: 1.3084\n",
      "Epoch [3/5], Step [8618/10336], Loss: 0.1429\n",
      "Epoch [3/5], Step [8620/10336], Loss: 1.8635\n",
      "Epoch [3/5], Step [8622/10336], Loss: 0.0315\n",
      "Epoch [3/5], Step [8624/10336], Loss: 0.3733\n",
      "Epoch [3/5], Step [8626/10336], Loss: 0.3518\n",
      "Epoch [3/5], Step [8628/10336], Loss: 0.0415\n",
      "Epoch [3/5], Step [8630/10336], Loss: 0.5395\n",
      "Epoch [3/5], Step [8632/10336], Loss: 0.6998\n",
      "Epoch [3/5], Step [8634/10336], Loss: 0.1423\n",
      "Epoch [3/5], Step [8636/10336], Loss: 1.7569\n",
      "Epoch [3/5], Step [8638/10336], Loss: 1.8211\n",
      "Epoch [3/5], Step [8640/10336], Loss: 0.8066\n",
      "Epoch [3/5], Step [8642/10336], Loss: 0.0156\n",
      "Epoch [3/5], Step [8644/10336], Loss: 1.6895\n",
      "Epoch [3/5], Step [8646/10336], Loss: 0.7900\n",
      "Epoch [3/5], Step [8648/10336], Loss: 0.1928\n",
      "Epoch [3/5], Step [8650/10336], Loss: 0.1160\n",
      "Epoch [3/5], Step [8652/10336], Loss: 0.5720\n",
      "Epoch [3/5], Step [8654/10336], Loss: 0.3900\n",
      "Epoch [3/5], Step [8656/10336], Loss: 0.8079\n",
      "Epoch [3/5], Step [8658/10336], Loss: 0.3133\n",
      "Epoch [3/5], Step [8660/10336], Loss: 0.6777\n",
      "Epoch [3/5], Step [8662/10336], Loss: 0.7252\n",
      "Epoch [3/5], Step [8664/10336], Loss: 0.0006\n",
      "Epoch [3/5], Step [8666/10336], Loss: 0.0820\n",
      "Epoch [3/5], Step [8668/10336], Loss: 0.0214\n",
      "Epoch [3/5], Step [8670/10336], Loss: 1.3728\n",
      "Epoch [3/5], Step [8672/10336], Loss: 0.1118\n",
      "Epoch [3/5], Step [8674/10336], Loss: 3.6313\n",
      "Epoch [3/5], Step [8676/10336], Loss: 0.0087\n",
      "Epoch [3/5], Step [8678/10336], Loss: 1.8209\n",
      "Epoch [3/5], Step [8680/10336], Loss: 0.2754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [8682/10336], Loss: 0.0506\n",
      "Epoch [3/5], Step [8684/10336], Loss: 0.6679\n",
      "Epoch [3/5], Step [8686/10336], Loss: 0.3302\n",
      "Epoch [3/5], Step [8688/10336], Loss: 2.1928\n",
      "Epoch [3/5], Step [8690/10336], Loss: 0.9905\n",
      "Epoch [3/5], Step [8692/10336], Loss: 0.0297\n",
      "Epoch [3/5], Step [8694/10336], Loss: 0.1420\n",
      "Epoch [3/5], Step [8696/10336], Loss: 0.0397\n",
      "Epoch [3/5], Step [8698/10336], Loss: 2.2252\n",
      "Epoch [3/5], Step [8700/10336], Loss: 1.0294\n",
      "Epoch [3/5], Step [8702/10336], Loss: 0.0324\n",
      "Epoch [3/5], Step [8704/10336], Loss: 0.0650\n",
      "Epoch [3/5], Step [8706/10336], Loss: 0.0180\n",
      "Epoch [3/5], Step [8708/10336], Loss: 0.0017\n",
      "Epoch [3/5], Step [8710/10336], Loss: 1.7718\n",
      "Epoch [3/5], Step [8712/10336], Loss: 0.0040\n",
      "Epoch [3/5], Step [8714/10336], Loss: 0.0243\n",
      "Epoch [3/5], Step [8716/10336], Loss: 0.0395\n",
      "Epoch [3/5], Step [8718/10336], Loss: 0.1020\n",
      "Epoch [3/5], Step [8720/10336], Loss: 0.1862\n",
      "Epoch [3/5], Step [8722/10336], Loss: 1.9482\n",
      "Epoch [3/5], Step [8724/10336], Loss: 0.0220\n",
      "Epoch [3/5], Step [8726/10336], Loss: 2.6240\n",
      "Epoch [3/5], Step [8728/10336], Loss: 1.0918\n",
      "Epoch [3/5], Step [8730/10336], Loss: 0.3003\n",
      "Epoch [3/5], Step [8732/10336], Loss: 0.1661\n",
      "Epoch [3/5], Step [8734/10336], Loss: 1.9668\n",
      "Epoch [3/5], Step [8736/10336], Loss: 0.1276\n",
      "Epoch [3/5], Step [8738/10336], Loss: 0.0652\n",
      "Epoch [3/5], Step [8740/10336], Loss: 0.9141\n",
      "Epoch [3/5], Step [8742/10336], Loss: 0.3829\n",
      "Epoch [3/5], Step [8744/10336], Loss: 0.0807\n",
      "Epoch [3/5], Step [8746/10336], Loss: 0.0185\n",
      "Epoch [3/5], Step [8748/10336], Loss: 1.4856\n",
      "Epoch [3/5], Step [8750/10336], Loss: 1.4269\n",
      "Epoch [3/5], Step [8752/10336], Loss: 1.7016\n",
      "Epoch [3/5], Step [8754/10336], Loss: 2.0875\n",
      "Epoch [3/5], Step [8756/10336], Loss: 0.4895\n",
      "Epoch [3/5], Step [8758/10336], Loss: 1.1319\n",
      "Epoch [3/5], Step [8760/10336], Loss: 0.1010\n",
      "Epoch [3/5], Step [8762/10336], Loss: 0.0643\n",
      "Epoch [3/5], Step [8764/10336], Loss: 0.2015\n",
      "Epoch [3/5], Step [8766/10336], Loss: 0.4112\n",
      "Epoch [3/5], Step [8768/10336], Loss: 1.1505\n",
      "Epoch [3/5], Step [8770/10336], Loss: 0.1568\n",
      "Epoch [3/5], Step [8772/10336], Loss: 0.0059\n",
      "Epoch [3/5], Step [8774/10336], Loss: 0.6940\n",
      "Epoch [3/5], Step [8776/10336], Loss: 0.1685\n",
      "Epoch [3/5], Step [8778/10336], Loss: 0.1507\n",
      "Epoch [3/5], Step [8780/10336], Loss: 0.0104\n",
      "Epoch [3/5], Step [8782/10336], Loss: 1.0527\n",
      "Epoch [3/5], Step [8784/10336], Loss: 0.4191\n",
      "Epoch [3/5], Step [8786/10336], Loss: 0.0091\n",
      "Epoch [3/5], Step [8788/10336], Loss: 0.6495\n",
      "Epoch [3/5], Step [8790/10336], Loss: 0.0257\n",
      "Epoch [3/5], Step [8792/10336], Loss: 0.0101\n",
      "Epoch [3/5], Step [8794/10336], Loss: 0.1022\n",
      "Epoch [3/5], Step [8796/10336], Loss: 0.0078\n",
      "Epoch [3/5], Step [8798/10336], Loss: 1.1618\n",
      "Epoch [3/5], Step [8800/10336], Loss: 0.5556\n",
      "Epoch [3/5], Step [8802/10336], Loss: 0.1875\n",
      "Epoch [3/5], Step [8804/10336], Loss: 0.0036\n",
      "Epoch [3/5], Step [8806/10336], Loss: 0.7721\n",
      "Epoch [3/5], Step [8808/10336], Loss: 0.0312\n",
      "Epoch [3/5], Step [8810/10336], Loss: 0.8016\n",
      "Epoch [3/5], Step [8812/10336], Loss: 0.8367\n",
      "Epoch [3/5], Step [8814/10336], Loss: 0.7449\n",
      "Epoch [3/5], Step [8816/10336], Loss: 0.0150\n",
      "Epoch [3/5], Step [8818/10336], Loss: 0.7537\n",
      "Epoch [3/5], Step [8820/10336], Loss: 0.4775\n",
      "Epoch [3/5], Step [8822/10336], Loss: 0.7865\n",
      "Epoch [3/5], Step [8824/10336], Loss: 2.5499\n",
      "Epoch [3/5], Step [8826/10336], Loss: 0.1327\n",
      "Epoch [3/5], Step [8828/10336], Loss: 2.1174\n",
      "Epoch [3/5], Step [8830/10336], Loss: 1.3009\n",
      "Epoch [3/5], Step [8832/10336], Loss: 0.3752\n",
      "Epoch [3/5], Step [8834/10336], Loss: 0.5308\n",
      "Epoch [3/5], Step [8836/10336], Loss: 0.6427\n",
      "Epoch [3/5], Step [8838/10336], Loss: 0.0877\n",
      "Epoch [3/5], Step [8840/10336], Loss: 0.8043\n",
      "Epoch [3/5], Step [8842/10336], Loss: 2.2965\n",
      "Epoch [3/5], Step [8844/10336], Loss: 0.2864\n",
      "Epoch [3/5], Step [8846/10336], Loss: 5.1791\n",
      "Epoch [3/5], Step [8848/10336], Loss: 0.0445\n",
      "Epoch [3/5], Step [8850/10336], Loss: 0.1713\n",
      "Epoch [3/5], Step [8852/10336], Loss: 0.0431\n",
      "Epoch [3/5], Step [8854/10336], Loss: 0.9555\n",
      "Epoch [3/5], Step [8856/10336], Loss: 0.0725\n",
      "Epoch [3/5], Step [8858/10336], Loss: 1.1876\n",
      "Epoch [3/5], Step [8860/10336], Loss: 0.8958\n",
      "Epoch [3/5], Step [8862/10336], Loss: 0.3881\n",
      "Epoch [3/5], Step [8864/10336], Loss: 2.7412\n",
      "Epoch [3/5], Step [8866/10336], Loss: 0.0160\n",
      "Epoch [3/5], Step [8868/10336], Loss: 0.3693\n",
      "Epoch [3/5], Step [8870/10336], Loss: 0.2230\n",
      "Epoch [3/5], Step [8872/10336], Loss: 0.0033\n",
      "Epoch [3/5], Step [8874/10336], Loss: 1.8135\n",
      "Epoch [3/5], Step [8876/10336], Loss: 0.3162\n",
      "Epoch [3/5], Step [8878/10336], Loss: 1.3288\n",
      "Epoch [3/5], Step [8880/10336], Loss: 0.0237\n",
      "Epoch [3/5], Step [8882/10336], Loss: 2.0197\n",
      "Epoch [3/5], Step [8884/10336], Loss: 1.8847\n",
      "Epoch [3/5], Step [8886/10336], Loss: 0.7829\n",
      "Epoch [3/5], Step [8888/10336], Loss: 0.0038\n",
      "Epoch [3/5], Step [8890/10336], Loss: 0.5570\n",
      "Epoch [3/5], Step [8892/10336], Loss: 0.1633\n",
      "Epoch [3/5], Step [8894/10336], Loss: 0.4224\n",
      "Epoch [3/5], Step [8896/10336], Loss: 1.5804\n",
      "Epoch [3/5], Step [8898/10336], Loss: 0.0258\n",
      "Epoch [3/5], Step [8900/10336], Loss: 0.0081\n",
      "Epoch [3/5], Step [8902/10336], Loss: 0.3211\n",
      "Epoch [3/5], Step [8904/10336], Loss: 0.5133\n",
      "Epoch [3/5], Step [8906/10336], Loss: 2.5808\n",
      "Epoch [3/5], Step [8908/10336], Loss: 1.6224\n",
      "Epoch [3/5], Step [8910/10336], Loss: 0.0682\n",
      "Epoch [3/5], Step [8912/10336], Loss: 0.0171\n",
      "Epoch [3/5], Step [8914/10336], Loss: 0.0240\n",
      "Epoch [3/5], Step [8916/10336], Loss: 0.0264\n",
      "Epoch [3/5], Step [8918/10336], Loss: 0.4422\n",
      "Epoch [3/5], Step [8920/10336], Loss: 0.4956\n",
      "Epoch [3/5], Step [8922/10336], Loss: 0.1006\n",
      "Epoch [3/5], Step [8924/10336], Loss: 0.3759\n",
      "Epoch [3/5], Step [8926/10336], Loss: 0.0010\n",
      "Epoch [3/5], Step [8928/10336], Loss: 0.0172\n",
      "Epoch [3/5], Step [8930/10336], Loss: 3.0824\n",
      "Epoch [3/5], Step [8932/10336], Loss: 0.5639\n",
      "Epoch [3/5], Step [8934/10336], Loss: 0.2978\n",
      "Epoch [3/5], Step [8936/10336], Loss: 2.6823\n",
      "Epoch [3/5], Step [8938/10336], Loss: 0.0339\n",
      "Epoch [3/5], Step [8940/10336], Loss: 0.2503\n",
      "Epoch [3/5], Step [8942/10336], Loss: 0.0008\n",
      "Epoch [3/5], Step [8944/10336], Loss: 1.2805\n",
      "Epoch [3/5], Step [8946/10336], Loss: 0.1413\n",
      "Epoch [3/5], Step [8948/10336], Loss: 0.1415\n",
      "Epoch [3/5], Step [8950/10336], Loss: 0.8721\n",
      "Epoch [3/5], Step [8952/10336], Loss: 1.1873\n",
      "Epoch [3/5], Step [8954/10336], Loss: 1.5081\n",
      "Epoch [3/5], Step [8956/10336], Loss: 0.5469\n",
      "Epoch [3/5], Step [8958/10336], Loss: 0.1729\n",
      "Epoch [3/5], Step [8960/10336], Loss: 0.0850\n",
      "Epoch [3/5], Step [8962/10336], Loss: 1.6949\n",
      "Epoch [3/5], Step [8964/10336], Loss: 3.4164\n",
      "Epoch [3/5], Step [8966/10336], Loss: 0.0930\n",
      "Epoch [3/5], Step [8968/10336], Loss: 0.2287\n",
      "Epoch [3/5], Step [8970/10336], Loss: 0.5500\n",
      "Epoch [3/5], Step [8972/10336], Loss: 0.0012\n",
      "Epoch [3/5], Step [8974/10336], Loss: 0.0783\n",
      "Epoch [3/5], Step [8976/10336], Loss: 1.4414\n",
      "Epoch [3/5], Step [8978/10336], Loss: 0.3308\n",
      "Epoch [3/5], Step [8980/10336], Loss: 0.0828\n",
      "Epoch [3/5], Step [8982/10336], Loss: 0.4939\n",
      "Epoch [3/5], Step [8984/10336], Loss: 0.0249\n",
      "Epoch [3/5], Step [8986/10336], Loss: 0.0309\n",
      "Epoch [3/5], Step [8988/10336], Loss: 0.0009\n",
      "Epoch [3/5], Step [8990/10336], Loss: 0.0618\n",
      "Epoch [3/5], Step [8992/10336], Loss: 1.6193\n",
      "Epoch [3/5], Step [8994/10336], Loss: 0.1552\n",
      "Epoch [3/5], Step [8996/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [8998/10336], Loss: 0.0788\n",
      "Epoch [3/5], Step [9000/10336], Loss: 1.8810\n",
      "Epoch [3/5], Step [9002/10336], Loss: 2.9043\n",
      "Epoch [3/5], Step [9004/10336], Loss: 0.8417\n",
      "Epoch [3/5], Step [9006/10336], Loss: 0.0241\n",
      "Epoch [3/5], Step [9008/10336], Loss: 0.0974\n",
      "Epoch [3/5], Step [9010/10336], Loss: 1.3972\n",
      "Epoch [3/5], Step [9012/10336], Loss: 0.2166\n",
      "Epoch [3/5], Step [9014/10336], Loss: 3.6076\n",
      "Epoch [3/5], Step [9016/10336], Loss: 0.0808\n",
      "Epoch [3/5], Step [9018/10336], Loss: 0.0450\n",
      "Epoch [3/5], Step [9020/10336], Loss: 1.3096\n",
      "Epoch [3/5], Step [9022/10336], Loss: 0.0004\n",
      "Epoch [3/5], Step [9024/10336], Loss: 0.0566\n",
      "Epoch [3/5], Step [9026/10336], Loss: 0.0250\n",
      "Epoch [3/5], Step [9028/10336], Loss: 0.0443\n",
      "Epoch [3/5], Step [9030/10336], Loss: 0.0962\n",
      "Epoch [3/5], Step [9032/10336], Loss: 0.0591\n",
      "Epoch [3/5], Step [9034/10336], Loss: 0.7259\n",
      "Epoch [3/5], Step [9036/10336], Loss: 1.7391\n",
      "Epoch [3/5], Step [9038/10336], Loss: 0.4936\n",
      "Epoch [3/5], Step [9040/10336], Loss: 0.0159\n",
      "Epoch [3/5], Step [9042/10336], Loss: 0.7077\n",
      "Epoch [3/5], Step [9044/10336], Loss: 0.0987\n",
      "Epoch [3/5], Step [9046/10336], Loss: 0.3532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [9048/10336], Loss: 0.1407\n",
      "Epoch [3/5], Step [9050/10336], Loss: 1.9805\n",
      "Epoch [3/5], Step [9052/10336], Loss: 0.0037\n",
      "Epoch [3/5], Step [9054/10336], Loss: 1.5561\n",
      "Epoch [3/5], Step [9056/10336], Loss: 0.7946\n",
      "Epoch [3/5], Step [9058/10336], Loss: 0.1210\n",
      "Epoch [3/5], Step [9060/10336], Loss: 1.0334\n",
      "Epoch [3/5], Step [9062/10336], Loss: 1.5101\n",
      "Epoch [3/5], Step [9064/10336], Loss: 0.2099\n",
      "Epoch [3/5], Step [9066/10336], Loss: 0.2479\n",
      "Epoch [3/5], Step [9068/10336], Loss: 0.2943\n",
      "Epoch [3/5], Step [9070/10336], Loss: 0.0441\n",
      "Epoch [3/5], Step [9072/10336], Loss: 0.0642\n",
      "Epoch [3/5], Step [9074/10336], Loss: 3.5386\n",
      "Epoch [3/5], Step [9076/10336], Loss: 0.0626\n",
      "Epoch [3/5], Step [9078/10336], Loss: 1.5426\n",
      "Epoch [3/5], Step [9080/10336], Loss: 0.4698\n",
      "Epoch [3/5], Step [9082/10336], Loss: 1.6725\n",
      "Epoch [3/5], Step [9084/10336], Loss: 0.1103\n",
      "Epoch [3/5], Step [9086/10336], Loss: 0.2399\n",
      "Epoch [3/5], Step [9088/10336], Loss: 0.1348\n",
      "Epoch [3/5], Step [9090/10336], Loss: 1.3739\n",
      "Epoch [3/5], Step [9092/10336], Loss: 0.5384\n",
      "Epoch [3/5], Step [9094/10336], Loss: 0.0121\n",
      "Epoch [3/5], Step [9096/10336], Loss: 0.1358\n",
      "Epoch [3/5], Step [9098/10336], Loss: 0.0705\n",
      "Epoch [3/5], Step [9100/10336], Loss: 1.6838\n",
      "Epoch [3/5], Step [9102/10336], Loss: 0.3222\n",
      "Epoch [3/5], Step [9104/10336], Loss: 2.2640\n",
      "Epoch [3/5], Step [9106/10336], Loss: 0.2187\n",
      "Epoch [3/5], Step [9108/10336], Loss: 1.2783\n",
      "Epoch [3/5], Step [9110/10336], Loss: 1.4608\n",
      "Epoch [3/5], Step [9112/10336], Loss: 0.0142\n",
      "Epoch [3/5], Step [9114/10336], Loss: 2.2261\n",
      "Epoch [3/5], Step [9116/10336], Loss: 0.2781\n",
      "Epoch [3/5], Step [9118/10336], Loss: 0.2748\n",
      "Epoch [3/5], Step [9120/10336], Loss: 0.6916\n",
      "Epoch [3/5], Step [9122/10336], Loss: 0.0178\n",
      "Epoch [3/5], Step [9124/10336], Loss: 0.0002\n",
      "Epoch [3/5], Step [9126/10336], Loss: 3.2124\n",
      "Epoch [3/5], Step [9128/10336], Loss: 0.3542\n",
      "Epoch [3/5], Step [9130/10336], Loss: 1.4229\n",
      "Epoch [3/5], Step [9132/10336], Loss: 0.0052\n",
      "Epoch [3/5], Step [9134/10336], Loss: 0.0899\n",
      "Epoch [3/5], Step [9136/10336], Loss: 0.0013\n",
      "Epoch [3/5], Step [9138/10336], Loss: 1.4792\n",
      "Epoch [3/5], Step [9140/10336], Loss: 2.3457\n",
      "Epoch [3/5], Step [9142/10336], Loss: 0.0036\n",
      "Epoch [3/5], Step [9144/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [9146/10336], Loss: 0.7475\n",
      "Epoch [3/5], Step [9148/10336], Loss: 2.0020\n",
      "Epoch [3/5], Step [9150/10336], Loss: 0.8077\n",
      "Epoch [3/5], Step [9152/10336], Loss: 0.0956\n",
      "Epoch [3/5], Step [9154/10336], Loss: 0.3198\n",
      "Epoch [3/5], Step [9156/10336], Loss: 0.0993\n",
      "Epoch [3/5], Step [9158/10336], Loss: 0.0800\n",
      "Epoch [3/5], Step [9160/10336], Loss: 1.1425\n",
      "Epoch [3/5], Step [9162/10336], Loss: 0.6366\n",
      "Epoch [3/5], Step [9164/10336], Loss: 1.8290\n",
      "Epoch [3/5], Step [9166/10336], Loss: 2.3413\n",
      "Epoch [3/5], Step [9168/10336], Loss: 0.0005\n",
      "Epoch [3/5], Step [9170/10336], Loss: 0.2298\n",
      "Epoch [3/5], Step [9172/10336], Loss: 0.0118\n",
      "Epoch [3/5], Step [9174/10336], Loss: 0.0063\n",
      "Epoch [3/5], Step [9176/10336], Loss: 0.1245\n",
      "Epoch [3/5], Step [9178/10336], Loss: 4.2506\n",
      "Epoch [3/5], Step [9180/10336], Loss: 0.0683\n",
      "Epoch [3/5], Step [9182/10336], Loss: 2.2379\n",
      "Epoch [3/5], Step [9184/10336], Loss: 0.5679\n",
      "Epoch [3/5], Step [9186/10336], Loss: 1.1954\n",
      "Epoch [3/5], Step [9188/10336], Loss: 0.4546\n",
      "Epoch [3/5], Step [9190/10336], Loss: 1.2524\n",
      "Epoch [3/5], Step [9192/10336], Loss: 0.2141\n",
      "Epoch [3/5], Step [9194/10336], Loss: 0.0552\n",
      "Epoch [3/5], Step [9196/10336], Loss: 0.3799\n",
      "Epoch [3/5], Step [9198/10336], Loss: 0.0321\n",
      "Epoch [3/5], Step [9200/10336], Loss: 3.6888\n",
      "Epoch [3/5], Step [9202/10336], Loss: 0.9403\n",
      "Epoch [3/5], Step [9204/10336], Loss: 0.1015\n",
      "Epoch [3/5], Step [9206/10336], Loss: 1.7357\n",
      "Epoch [3/5], Step [9208/10336], Loss: 0.0086\n",
      "Epoch [3/5], Step [9210/10336], Loss: 0.0394\n",
      "Epoch [3/5], Step [9212/10336], Loss: 0.5448\n",
      "Epoch [3/5], Step [9214/10336], Loss: 0.6775\n",
      "Epoch [3/5], Step [9216/10336], Loss: 0.0176\n",
      "Epoch [3/5], Step [9218/10336], Loss: 0.8182\n",
      "Epoch [3/5], Step [9220/10336], Loss: 0.7952\n",
      "Epoch [3/5], Step [9222/10336], Loss: 0.2794\n",
      "Epoch [3/5], Step [9224/10336], Loss: 0.0081\n",
      "Epoch [3/5], Step [9226/10336], Loss: 1.7343\n",
      "Epoch [3/5], Step [9228/10336], Loss: 0.2872\n",
      "Epoch [3/5], Step [9230/10336], Loss: 1.8557\n",
      "Epoch [3/5], Step [9232/10336], Loss: 0.0537\n",
      "Epoch [3/5], Step [9234/10336], Loss: 0.0208\n",
      "Epoch [3/5], Step [9236/10336], Loss: 1.3062\n",
      "Epoch [3/5], Step [9238/10336], Loss: 3.1681\n",
      "Epoch [3/5], Step [9240/10336], Loss: 0.2537\n",
      "Epoch [3/5], Step [9242/10336], Loss: 0.0579\n",
      "Epoch [3/5], Step [9244/10336], Loss: 0.2442\n",
      "Epoch [3/5], Step [9246/10336], Loss: 3.4609\n",
      "Epoch [3/5], Step [9248/10336], Loss: 0.0070\n",
      "Epoch [3/5], Step [9250/10336], Loss: 2.0497\n",
      "Epoch [3/5], Step [9252/10336], Loss: 0.0086\n",
      "Epoch [3/5], Step [9254/10336], Loss: 1.2349\n",
      "Epoch [3/5], Step [9256/10336], Loss: 0.6871\n",
      "Epoch [3/5], Step [9258/10336], Loss: 0.1123\n",
      "Epoch [3/5], Step [9260/10336], Loss: 0.6067\n",
      "Epoch [3/5], Step [9262/10336], Loss: 0.6836\n",
      "Epoch [3/5], Step [9264/10336], Loss: 2.1162\n",
      "Epoch [3/5], Step [9266/10336], Loss: 0.9399\n",
      "Epoch [3/5], Step [9268/10336], Loss: 1.4858\n",
      "Epoch [3/5], Step [9270/10336], Loss: 0.9883\n",
      "Epoch [3/5], Step [9272/10336], Loss: 1.9523\n",
      "Epoch [3/5], Step [9274/10336], Loss: 1.5059\n",
      "Epoch [3/5], Step [9276/10336], Loss: 0.7770\n",
      "Epoch [3/5], Step [9278/10336], Loss: 0.5547\n",
      "Epoch [3/5], Step [9280/10336], Loss: 0.6693\n",
      "Epoch [3/5], Step [9282/10336], Loss: 1.6012\n",
      "Epoch [3/5], Step [9284/10336], Loss: 0.1565\n",
      "Epoch [3/5], Step [9286/10336], Loss: 1.1143\n",
      "Epoch [3/5], Step [9288/10336], Loss: 0.3022\n",
      "Epoch [3/5], Step [9290/10336], Loss: 0.0066\n",
      "Epoch [3/5], Step [9292/10336], Loss: 1.9070\n",
      "Epoch [3/5], Step [9294/10336], Loss: 1.0977\n",
      "Epoch [3/5], Step [9296/10336], Loss: 1.7592\n",
      "Epoch [3/5], Step [9298/10336], Loss: 1.7884\n",
      "Epoch [3/5], Step [9300/10336], Loss: 0.7126\n",
      "Epoch [3/5], Step [9302/10336], Loss: 2.5718\n",
      "Epoch [3/5], Step [9304/10336], Loss: 4.4880\n",
      "Epoch [3/5], Step [9306/10336], Loss: 0.5202\n",
      "Epoch [3/5], Step [9308/10336], Loss: 0.0348\n",
      "Epoch [3/5], Step [9310/10336], Loss: 0.3536\n",
      "Epoch [3/5], Step [9312/10336], Loss: 0.0676\n",
      "Epoch [3/5], Step [9314/10336], Loss: 0.7251\n",
      "Epoch [3/5], Step [9316/10336], Loss: 3.5915\n",
      "Epoch [3/5], Step [9318/10336], Loss: 0.1532\n",
      "Epoch [3/5], Step [9320/10336], Loss: 2.6658\n",
      "Epoch [3/5], Step [9322/10336], Loss: 2.3298\n",
      "Epoch [3/5], Step [9324/10336], Loss: 0.0465\n",
      "Epoch [3/5], Step [9326/10336], Loss: 0.0110\n",
      "Epoch [3/5], Step [9328/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [9330/10336], Loss: 0.1339\n",
      "Epoch [3/5], Step [9332/10336], Loss: 0.0053\n",
      "Epoch [3/5], Step [9334/10336], Loss: 0.1131\n",
      "Epoch [3/5], Step [9336/10336], Loss: 5.6718\n",
      "Epoch [3/5], Step [9338/10336], Loss: 0.1337\n",
      "Epoch [3/5], Step [9340/10336], Loss: 0.7264\n",
      "Epoch [3/5], Step [9342/10336], Loss: 2.3329\n",
      "Epoch [3/5], Step [9344/10336], Loss: 0.1105\n",
      "Epoch [3/5], Step [9346/10336], Loss: 1.7674\n",
      "Epoch [3/5], Step [9348/10336], Loss: 1.5517\n",
      "Epoch [3/5], Step [9350/10336], Loss: 0.5717\n",
      "Epoch [3/5], Step [9352/10336], Loss: 0.0934\n",
      "Epoch [3/5], Step [9354/10336], Loss: 0.6698\n",
      "Epoch [3/5], Step [9356/10336], Loss: 0.3156\n",
      "Epoch [3/5], Step [9358/10336], Loss: 3.3792\n",
      "Epoch [3/5], Step [9360/10336], Loss: 1.3749\n",
      "Epoch [3/5], Step [9362/10336], Loss: 1.1754\n",
      "Epoch [3/5], Step [9364/10336], Loss: 0.3221\n",
      "Epoch [3/5], Step [9366/10336], Loss: 0.0053\n",
      "Epoch [3/5], Step [9368/10336], Loss: 0.4583\n",
      "Epoch [3/5], Step [9370/10336], Loss: 0.0889\n",
      "Epoch [3/5], Step [9372/10336], Loss: 1.2915\n",
      "Epoch [3/5], Step [9374/10336], Loss: 0.0948\n",
      "Epoch [3/5], Step [9376/10336], Loss: 0.3478\n",
      "Epoch [3/5], Step [9378/10336], Loss: 0.0029\n",
      "Epoch [3/5], Step [9380/10336], Loss: 0.0022\n",
      "Epoch [3/5], Step [9382/10336], Loss: 1.9916\n",
      "Epoch [3/5], Step [9384/10336], Loss: 1.9216\n",
      "Epoch [3/5], Step [9386/10336], Loss: 1.2304\n",
      "Epoch [3/5], Step [9388/10336], Loss: 0.2349\n",
      "Epoch [3/5], Step [9390/10336], Loss: 1.0643\n",
      "Epoch [3/5], Step [9392/10336], Loss: 0.3570\n",
      "Epoch [3/5], Step [9394/10336], Loss: 1.0723\n",
      "Epoch [3/5], Step [9396/10336], Loss: 1.4474\n",
      "Epoch [3/5], Step [9398/10336], Loss: 2.4149\n",
      "Epoch [3/5], Step [9400/10336], Loss: 0.2021\n",
      "Epoch [3/5], Step [9402/10336], Loss: 0.4134\n",
      "Epoch [3/5], Step [9404/10336], Loss: 0.0254\n",
      "Epoch [3/5], Step [9406/10336], Loss: 0.0171\n",
      "Epoch [3/5], Step [9408/10336], Loss: 0.7245\n",
      "Epoch [3/5], Step [9410/10336], Loss: 3.5594\n",
      "Epoch [3/5], Step [9412/10336], Loss: 3.7446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [9414/10336], Loss: 0.0340\n",
      "Epoch [3/5], Step [9416/10336], Loss: 0.0059\n",
      "Epoch [3/5], Step [9418/10336], Loss: 0.3174\n",
      "Epoch [3/5], Step [9420/10336], Loss: 0.3380\n",
      "Epoch [3/5], Step [9422/10336], Loss: 0.8069\n",
      "Epoch [3/5], Step [9424/10336], Loss: 1.6969\n",
      "Epoch [3/5], Step [9426/10336], Loss: 0.5168\n",
      "Epoch [3/5], Step [9428/10336], Loss: 0.0031\n",
      "Epoch [3/5], Step [9430/10336], Loss: 1.8357\n",
      "Epoch [3/5], Step [9432/10336], Loss: 0.9647\n",
      "Epoch [3/5], Step [9434/10336], Loss: 1.1361\n",
      "Epoch [3/5], Step [9436/10336], Loss: 0.9028\n",
      "Epoch [3/5], Step [9438/10336], Loss: 0.0070\n",
      "Epoch [3/5], Step [9440/10336], Loss: 1.7088\n",
      "Epoch [3/5], Step [9442/10336], Loss: 0.0096\n",
      "Epoch [3/5], Step [9444/10336], Loss: 0.0197\n",
      "Epoch [3/5], Step [9446/10336], Loss: 1.2162\n",
      "Epoch [3/5], Step [9448/10336], Loss: 0.1192\n",
      "Epoch [3/5], Step [9450/10336], Loss: 0.1866\n",
      "Epoch [3/5], Step [9452/10336], Loss: 0.1572\n",
      "Epoch [3/5], Step [9454/10336], Loss: 0.0844\n",
      "Epoch [3/5], Step [9456/10336], Loss: 1.1030\n",
      "Epoch [3/5], Step [9458/10336], Loss: 1.6803\n",
      "Epoch [3/5], Step [9460/10336], Loss: 0.7014\n",
      "Epoch [3/5], Step [9462/10336], Loss: 2.8490\n",
      "Epoch [3/5], Step [9464/10336], Loss: 0.2378\n",
      "Epoch [3/5], Step [9466/10336], Loss: 1.1212\n",
      "Epoch [3/5], Step [9468/10336], Loss: 0.4289\n",
      "Epoch [3/5], Step [9470/10336], Loss: 0.8498\n",
      "Epoch [3/5], Step [9472/10336], Loss: 0.3305\n",
      "Epoch [3/5], Step [9474/10336], Loss: 0.1690\n",
      "Epoch [3/5], Step [9476/10336], Loss: 1.1780\n",
      "Epoch [3/5], Step [9478/10336], Loss: 1.4871\n",
      "Epoch [3/5], Step [9480/10336], Loss: 0.5335\n",
      "Epoch [3/5], Step [9482/10336], Loss: 0.9934\n",
      "Epoch [3/5], Step [9484/10336], Loss: 1.1511\n",
      "Epoch [3/5], Step [9486/10336], Loss: 0.3960\n",
      "Epoch [3/5], Step [9488/10336], Loss: 0.4225\n",
      "Epoch [3/5], Step [9490/10336], Loss: 0.9803\n",
      "Epoch [3/5], Step [9492/10336], Loss: 1.4448\n",
      "Epoch [3/5], Step [9494/10336], Loss: 0.8835\n",
      "Epoch [3/5], Step [9496/10336], Loss: 2.2856\n",
      "Epoch [3/5], Step [9498/10336], Loss: 2.5522\n",
      "Epoch [3/5], Step [9500/10336], Loss: 3.2094\n",
      "Epoch [3/5], Step [9502/10336], Loss: 0.0027\n",
      "Epoch [3/5], Step [9504/10336], Loss: 0.1022\n",
      "Epoch [3/5], Step [9506/10336], Loss: 1.6235\n",
      "Epoch [3/5], Step [9508/10336], Loss: 0.0499\n",
      "Epoch [3/5], Step [9510/10336], Loss: 0.0764\n",
      "Epoch [3/5], Step [9512/10336], Loss: 0.1805\n",
      "Epoch [3/5], Step [9514/10336], Loss: 1.5896\n",
      "Epoch [3/5], Step [9516/10336], Loss: 0.1200\n",
      "Epoch [3/5], Step [9518/10336], Loss: 0.3480\n",
      "Epoch [3/5], Step [9520/10336], Loss: 1.8540\n",
      "Epoch [3/5], Step [9522/10336], Loss: 1.5559\n",
      "Epoch [3/5], Step [9524/10336], Loss: 0.1073\n",
      "Epoch [3/5], Step [9526/10336], Loss: 0.0236\n",
      "Epoch [3/5], Step [9528/10336], Loss: 0.2325\n",
      "Epoch [3/5], Step [9530/10336], Loss: 0.0477\n",
      "Epoch [3/5], Step [9532/10336], Loss: 0.6864\n",
      "Epoch [3/5], Step [9534/10336], Loss: 0.0748\n",
      "Epoch [3/5], Step [9536/10336], Loss: 0.6693\n",
      "Epoch [3/5], Step [9538/10336], Loss: 1.5196\n",
      "Epoch [3/5], Step [9540/10336], Loss: 0.1924\n",
      "Epoch [3/5], Step [9542/10336], Loss: 0.0871\n",
      "Epoch [3/5], Step [9544/10336], Loss: 2.0665\n",
      "Epoch [3/5], Step [9546/10336], Loss: 0.2337\n",
      "Epoch [3/5], Step [9548/10336], Loss: 1.8496\n",
      "Epoch [3/5], Step [9550/10336], Loss: 0.2532\n",
      "Epoch [3/5], Step [9552/10336], Loss: 0.0037\n",
      "Epoch [3/5], Step [9554/10336], Loss: 0.7373\n",
      "Epoch [3/5], Step [9556/10336], Loss: 0.7259\n",
      "Epoch [3/5], Step [9558/10336], Loss: 0.0283\n",
      "Epoch [3/5], Step [9560/10336], Loss: 0.4057\n",
      "Epoch [3/5], Step [9562/10336], Loss: 0.1089\n",
      "Epoch [3/5], Step [9564/10336], Loss: 1.3485\n",
      "Epoch [3/5], Step [9566/10336], Loss: 0.0547\n",
      "Epoch [3/5], Step [9568/10336], Loss: 0.0833\n",
      "Epoch [3/5], Step [9570/10336], Loss: 1.4798\n",
      "Epoch [3/5], Step [9572/10336], Loss: 0.4490\n",
      "Epoch [3/5], Step [9574/10336], Loss: 0.1958\n",
      "Epoch [3/5], Step [9576/10336], Loss: 0.0374\n",
      "Epoch [3/5], Step [9578/10336], Loss: 0.0682\n",
      "Epoch [3/5], Step [9580/10336], Loss: 2.9806\n",
      "Epoch [3/5], Step [9582/10336], Loss: 0.2075\n",
      "Epoch [3/5], Step [9584/10336], Loss: 3.0835\n",
      "Epoch [3/5], Step [9586/10336], Loss: 0.0155\n",
      "Epoch [3/5], Step [9588/10336], Loss: 0.2386\n",
      "Epoch [3/5], Step [9590/10336], Loss: 0.0314\n",
      "Epoch [3/5], Step [9592/10336], Loss: 1.0343\n",
      "Epoch [3/5], Step [9594/10336], Loss: 0.3349\n",
      "Epoch [3/5], Step [9596/10336], Loss: 1.3212\n",
      "Epoch [3/5], Step [9598/10336], Loss: 2.4297\n",
      "Epoch [3/5], Step [9600/10336], Loss: 0.0048\n",
      "Epoch [3/5], Step [9602/10336], Loss: 0.0998\n",
      "Epoch [3/5], Step [9604/10336], Loss: 0.2903\n",
      "Epoch [3/5], Step [9606/10336], Loss: 0.0635\n",
      "Epoch [3/5], Step [9608/10336], Loss: 2.4584\n",
      "Epoch [3/5], Step [9610/10336], Loss: 0.7413\n",
      "Epoch [3/5], Step [9612/10336], Loss: 1.2375\n",
      "Epoch [3/5], Step [9614/10336], Loss: 0.0475\n",
      "Epoch [3/5], Step [9616/10336], Loss: 1.2667\n",
      "Epoch [3/5], Step [9618/10336], Loss: 0.2651\n",
      "Epoch [3/5], Step [9620/10336], Loss: 2.8803\n",
      "Epoch [3/5], Step [9622/10336], Loss: 0.7695\n",
      "Epoch [3/5], Step [9624/10336], Loss: 0.5841\n",
      "Epoch [3/5], Step [9626/10336], Loss: 1.7444\n",
      "Epoch [3/5], Step [9628/10336], Loss: 0.0205\n",
      "Epoch [3/5], Step [9630/10336], Loss: 0.5121\n",
      "Epoch [3/5], Step [9632/10336], Loss: 0.1153\n",
      "Epoch [3/5], Step [9634/10336], Loss: 0.1625\n",
      "Epoch [3/5], Step [9636/10336], Loss: 0.0062\n",
      "Epoch [3/5], Step [9638/10336], Loss: 0.0590\n",
      "Epoch [3/5], Step [9640/10336], Loss: 0.3872\n",
      "Epoch [3/5], Step [9642/10336], Loss: 0.3840\n",
      "Epoch [3/5], Step [9644/10336], Loss: 0.0218\n",
      "Epoch [3/5], Step [9646/10336], Loss: 0.7728\n",
      "Epoch [3/5], Step [9648/10336], Loss: 1.5502\n",
      "Epoch [3/5], Step [9650/10336], Loss: 0.0389\n",
      "Epoch [3/5], Step [9652/10336], Loss: 0.0997\n",
      "Epoch [3/5], Step [9654/10336], Loss: 0.1547\n",
      "Epoch [3/5], Step [9656/10336], Loss: 0.2389\n",
      "Epoch [3/5], Step [9658/10336], Loss: 0.3486\n",
      "Epoch [3/5], Step [9660/10336], Loss: 0.8256\n",
      "Epoch [3/5], Step [9662/10336], Loss: 1.0313\n",
      "Epoch [3/5], Step [9664/10336], Loss: 0.0004\n",
      "Epoch [3/5], Step [9666/10336], Loss: 0.1306\n",
      "Epoch [3/5], Step [9668/10336], Loss: 0.1565\n",
      "Epoch [3/5], Step [9670/10336], Loss: 0.0505\n",
      "Epoch [3/5], Step [9672/10336], Loss: 0.1488\n",
      "Epoch [3/5], Step [9674/10336], Loss: 0.3820\n",
      "Epoch [3/5], Step [9676/10336], Loss: 0.0251\n",
      "Epoch [3/5], Step [9678/10336], Loss: 0.0370\n",
      "Epoch [3/5], Step [9680/10336], Loss: 0.0016\n",
      "Epoch [3/5], Step [9682/10336], Loss: 0.0100\n",
      "Epoch [3/5], Step [9684/10336], Loss: 0.0162\n",
      "Epoch [3/5], Step [9686/10336], Loss: 0.0982\n",
      "Epoch [3/5], Step [9688/10336], Loss: 0.0143\n",
      "Epoch [3/5], Step [9690/10336], Loss: 0.8416\n",
      "Epoch [3/5], Step [9692/10336], Loss: 0.9616\n",
      "Epoch [3/5], Step [9694/10336], Loss: 0.0058\n",
      "Epoch [3/5], Step [9696/10336], Loss: 0.7195\n",
      "Epoch [3/5], Step [9698/10336], Loss: 0.7367\n",
      "Epoch [3/5], Step [9700/10336], Loss: 0.0182\n",
      "Epoch [3/5], Step [9702/10336], Loss: 0.3914\n",
      "Epoch [3/5], Step [9704/10336], Loss: 1.1225\n",
      "Epoch [3/5], Step [9706/10336], Loss: 0.7028\n",
      "Epoch [3/5], Step [9708/10336], Loss: 0.1065\n",
      "Epoch [3/5], Step [9710/10336], Loss: 1.1865\n",
      "Epoch [3/5], Step [9712/10336], Loss: 1.6779\n",
      "Epoch [3/5], Step [9714/10336], Loss: 2.0703\n",
      "Epoch [3/5], Step [9716/10336], Loss: 3.4317\n",
      "Epoch [3/5], Step [9718/10336], Loss: 0.0106\n",
      "Epoch [3/5], Step [9720/10336], Loss: 0.0645\n",
      "Epoch [3/5], Step [9722/10336], Loss: 0.0432\n",
      "Epoch [3/5], Step [9724/10336], Loss: 1.3714\n",
      "Epoch [3/5], Step [9726/10336], Loss: 0.0345\n",
      "Epoch [3/5], Step [9728/10336], Loss: 0.8311\n",
      "Epoch [3/5], Step [9730/10336], Loss: 0.0480\n",
      "Epoch [3/5], Step [9732/10336], Loss: 0.4931\n",
      "Epoch [3/5], Step [9734/10336], Loss: 2.5770\n",
      "Epoch [3/5], Step [9736/10336], Loss: 0.9269\n",
      "Epoch [3/5], Step [9738/10336], Loss: 1.2864\n",
      "Epoch [3/5], Step [9740/10336], Loss: 0.0332\n",
      "Epoch [3/5], Step [9742/10336], Loss: 0.0141\n",
      "Epoch [3/5], Step [9744/10336], Loss: 0.0733\n",
      "Epoch [3/5], Step [9746/10336], Loss: 0.0207\n",
      "Epoch [3/5], Step [9748/10336], Loss: 0.0049\n",
      "Epoch [3/5], Step [9750/10336], Loss: 0.2067\n",
      "Epoch [3/5], Step [9752/10336], Loss: 0.6724\n",
      "Epoch [3/5], Step [9754/10336], Loss: 0.5941\n",
      "Epoch [3/5], Step [9756/10336], Loss: 0.3525\n",
      "Epoch [3/5], Step [9758/10336], Loss: 2.7767\n",
      "Epoch [3/5], Step [9760/10336], Loss: 1.2775\n",
      "Epoch [3/5], Step [9762/10336], Loss: 1.4681\n",
      "Epoch [3/5], Step [9764/10336], Loss: 0.3496\n",
      "Epoch [3/5], Step [9766/10336], Loss: 0.2693\n",
      "Epoch [3/5], Step [9768/10336], Loss: 0.1373\n",
      "Epoch [3/5], Step [9770/10336], Loss: 1.2140\n",
      "Epoch [3/5], Step [9772/10336], Loss: 0.0971\n",
      "Epoch [3/5], Step [9774/10336], Loss: 0.7353\n",
      "Epoch [3/5], Step [9776/10336], Loss: 0.2524\n",
      "Epoch [3/5], Step [9778/10336], Loss: 0.0123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [9780/10336], Loss: 0.1571\n",
      "Epoch [3/5], Step [9782/10336], Loss: 0.0451\n",
      "Epoch [3/5], Step [9784/10336], Loss: 0.0468\n",
      "Epoch [3/5], Step [9786/10336], Loss: 0.6191\n",
      "Epoch [3/5], Step [9788/10336], Loss: 0.6579\n",
      "Epoch [3/5], Step [9790/10336], Loss: 0.0105\n",
      "Epoch [3/5], Step [9792/10336], Loss: 1.8397\n",
      "Epoch [3/5], Step [9794/10336], Loss: 1.7363\n",
      "Epoch [3/5], Step [9796/10336], Loss: 1.6676\n",
      "Epoch [3/5], Step [9798/10336], Loss: 2.9907\n",
      "Epoch [3/5], Step [9800/10336], Loss: 0.5416\n",
      "Epoch [3/5], Step [9802/10336], Loss: 0.4035\n",
      "Epoch [3/5], Step [9804/10336], Loss: 0.5685\n",
      "Epoch [3/5], Step [9806/10336], Loss: 0.0482\n",
      "Epoch [3/5], Step [9808/10336], Loss: 0.0272\n",
      "Epoch [3/5], Step [9810/10336], Loss: 0.5835\n",
      "Epoch [3/5], Step [9812/10336], Loss: 1.5938\n",
      "Epoch [3/5], Step [9814/10336], Loss: 1.7811\n",
      "Epoch [3/5], Step [9816/10336], Loss: 1.4612\n",
      "Epoch [3/5], Step [9818/10336], Loss: 0.1897\n",
      "Epoch [3/5], Step [9820/10336], Loss: 0.6006\n",
      "Epoch [3/5], Step [9822/10336], Loss: 0.0114\n",
      "Epoch [3/5], Step [9824/10336], Loss: 0.3121\n",
      "Epoch [3/5], Step [9826/10336], Loss: 0.7826\n",
      "Epoch [3/5], Step [9828/10336], Loss: 0.2269\n",
      "Epoch [3/5], Step [9830/10336], Loss: 0.0719\n",
      "Epoch [3/5], Step [9832/10336], Loss: 0.5164\n",
      "Epoch [3/5], Step [9834/10336], Loss: 0.5063\n",
      "Epoch [3/5], Step [9836/10336], Loss: 0.0849\n",
      "Epoch [3/5], Step [9838/10336], Loss: 0.6062\n",
      "Epoch [3/5], Step [9840/10336], Loss: 0.9357\n",
      "Epoch [3/5], Step [9842/10336], Loss: 0.9550\n",
      "Epoch [3/5], Step [9844/10336], Loss: 0.2685\n",
      "Epoch [3/5], Step [9846/10336], Loss: 1.6317\n",
      "Epoch [3/5], Step [9848/10336], Loss: 0.0745\n",
      "Epoch [3/5], Step [9850/10336], Loss: 0.6643\n",
      "Epoch [3/5], Step [9852/10336], Loss: 1.4876\n",
      "Epoch [3/5], Step [9854/10336], Loss: 1.2840\n",
      "Epoch [3/5], Step [9856/10336], Loss: 0.1345\n",
      "Epoch [3/5], Step [9858/10336], Loss: 0.0509\n",
      "Epoch [3/5], Step [9860/10336], Loss: 0.0402\n",
      "Epoch [3/5], Step [9862/10336], Loss: 0.2817\n",
      "Epoch [3/5], Step [9864/10336], Loss: 0.3100\n",
      "Epoch [3/5], Step [9866/10336], Loss: 0.4713\n",
      "Epoch [3/5], Step [9868/10336], Loss: 0.1544\n",
      "Epoch [3/5], Step [9870/10336], Loss: 0.0023\n",
      "Epoch [3/5], Step [9872/10336], Loss: 3.0642\n",
      "Epoch [3/5], Step [9874/10336], Loss: 1.5965\n",
      "Epoch [3/5], Step [9876/10336], Loss: 1.8140\n",
      "Epoch [3/5], Step [9878/10336], Loss: 0.9373\n",
      "Epoch [3/5], Step [9880/10336], Loss: 0.0045\n",
      "Epoch [3/5], Step [9882/10336], Loss: 0.2304\n",
      "Epoch [3/5], Step [9884/10336], Loss: 0.0151\n",
      "Epoch [3/5], Step [9886/10336], Loss: 0.0030\n",
      "Epoch [3/5], Step [9888/10336], Loss: 0.0271\n",
      "Epoch [3/5], Step [9890/10336], Loss: 0.8492\n",
      "Epoch [3/5], Step [9892/10336], Loss: 3.5208\n",
      "Epoch [3/5], Step [9894/10336], Loss: 0.2665\n",
      "Epoch [3/5], Step [9896/10336], Loss: 0.2317\n",
      "Epoch [3/5], Step [9898/10336], Loss: 0.8243\n",
      "Epoch [3/5], Step [9900/10336], Loss: 0.1044\n",
      "Epoch [3/5], Step [9902/10336], Loss: 0.0513\n",
      "Epoch [3/5], Step [9904/10336], Loss: 0.0437\n",
      "Epoch [3/5], Step [9906/10336], Loss: 2.4669\n",
      "Epoch [3/5], Step [9908/10336], Loss: 0.2629\n",
      "Epoch [3/5], Step [9910/10336], Loss: 1.1059\n",
      "Epoch [3/5], Step [9912/10336], Loss: 0.0124\n",
      "Epoch [3/5], Step [9914/10336], Loss: 0.0113\n",
      "Epoch [3/5], Step [9916/10336], Loss: 0.1995\n",
      "Epoch [3/5], Step [9918/10336], Loss: 1.7183\n",
      "Epoch [3/5], Step [9920/10336], Loss: 0.6127\n",
      "Epoch [3/5], Step [9922/10336], Loss: 0.0194\n",
      "Epoch [3/5], Step [9924/10336], Loss: 2.0551\n",
      "Epoch [3/5], Step [9926/10336], Loss: 0.9670\n",
      "Epoch [3/5], Step [9928/10336], Loss: 0.1591\n",
      "Epoch [3/5], Step [9930/10336], Loss: 0.0849\n",
      "Epoch [3/5], Step [9932/10336], Loss: 0.2352\n",
      "Epoch [3/5], Step [9934/10336], Loss: 0.8227\n",
      "Epoch [3/5], Step [9936/10336], Loss: 1.0394\n",
      "Epoch [3/5], Step [9938/10336], Loss: 0.9968\n",
      "Epoch [3/5], Step [9940/10336], Loss: 5.8581\n",
      "Epoch [3/5], Step [9942/10336], Loss: 0.4790\n",
      "Epoch [3/5], Step [9944/10336], Loss: 4.1147\n",
      "Epoch [3/5], Step [9946/10336], Loss: 0.1639\n",
      "Epoch [3/5], Step [9948/10336], Loss: 0.3831\n",
      "Epoch [3/5], Step [9950/10336], Loss: 1.3821\n",
      "Epoch [3/5], Step [9952/10336], Loss: 2.0060\n",
      "Epoch [3/5], Step [9954/10336], Loss: 0.9535\n",
      "Epoch [3/5], Step [9956/10336], Loss: 2.6241\n",
      "Epoch [3/5], Step [9958/10336], Loss: 3.7130\n",
      "Epoch [3/5], Step [9960/10336], Loss: 0.9492\n",
      "Epoch [3/5], Step [9962/10336], Loss: 1.5235\n",
      "Epoch [3/5], Step [9964/10336], Loss: 0.7225\n",
      "Epoch [3/5], Step [9966/10336], Loss: 0.1380\n",
      "Epoch [3/5], Step [9968/10336], Loss: 1.6216\n",
      "Epoch [3/5], Step [9970/10336], Loss: 0.0183\n",
      "Epoch [3/5], Step [9972/10336], Loss: 0.6315\n",
      "Epoch [3/5], Step [9974/10336], Loss: 1.2496\n",
      "Epoch [3/5], Step [9976/10336], Loss: 2.8635\n",
      "Epoch [3/5], Step [9978/10336], Loss: 0.5842\n",
      "Epoch [3/5], Step [9980/10336], Loss: 2.7838\n",
      "Epoch [3/5], Step [9982/10336], Loss: 0.1121\n",
      "Epoch [3/5], Step [9984/10336], Loss: 1.9567\n",
      "Epoch [3/5], Step [9986/10336], Loss: 0.5222\n",
      "Epoch [3/5], Step [9988/10336], Loss: 0.0067\n",
      "Epoch [3/5], Step [9990/10336], Loss: 0.8025\n",
      "Epoch [3/5], Step [9992/10336], Loss: 0.9881\n",
      "Epoch [3/5], Step [9994/10336], Loss: 0.7624\n",
      "Epoch [3/5], Step [9996/10336], Loss: 4.7885\n",
      "Epoch [3/5], Step [9998/10336], Loss: 0.1694\n",
      "Epoch [3/5], Step [10000/10336], Loss: 0.0411\n",
      "Epoch [3/5], Step [10002/10336], Loss: 0.1366\n",
      "Epoch [3/5], Step [10004/10336], Loss: 0.1473\n",
      "Epoch [3/5], Step [10006/10336], Loss: 0.6605\n",
      "Epoch [3/5], Step [10008/10336], Loss: 1.4601\n",
      "Epoch [3/5], Step [10010/10336], Loss: 0.1812\n",
      "Epoch [3/5], Step [10012/10336], Loss: 2.6507\n",
      "Epoch [3/5], Step [10014/10336], Loss: 0.0724\n",
      "Epoch [3/5], Step [10016/10336], Loss: 0.1158\n",
      "Epoch [3/5], Step [10018/10336], Loss: 0.3638\n",
      "Epoch [3/5], Step [10020/10336], Loss: 0.4692\n",
      "Epoch [3/5], Step [10022/10336], Loss: 1.2413\n",
      "Epoch [3/5], Step [10024/10336], Loss: 0.2965\n",
      "Epoch [3/5], Step [10026/10336], Loss: 0.0124\n",
      "Epoch [3/5], Step [10028/10336], Loss: 1.3160\n",
      "Epoch [3/5], Step [10030/10336], Loss: 0.1748\n",
      "Epoch [3/5], Step [10032/10336], Loss: 0.0018\n",
      "Epoch [3/5], Step [10034/10336], Loss: 0.2313\n",
      "Epoch [3/5], Step [10036/10336], Loss: 0.0172\n",
      "Epoch [3/5], Step [10038/10336], Loss: 0.0047\n",
      "Epoch [3/5], Step [10040/10336], Loss: 1.1113\n",
      "Epoch [3/5], Step [10042/10336], Loss: 0.4500\n",
      "Epoch [3/5], Step [10044/10336], Loss: 1.1984\n",
      "Epoch [3/5], Step [10046/10336], Loss: 0.1703\n",
      "Epoch [3/5], Step [10048/10336], Loss: 0.3671\n",
      "Epoch [3/5], Step [10050/10336], Loss: 0.5138\n",
      "Epoch [3/5], Step [10052/10336], Loss: 0.3630\n",
      "Epoch [3/5], Step [10054/10336], Loss: 0.6480\n",
      "Epoch [3/5], Step [10056/10336], Loss: 0.4932\n",
      "Epoch [3/5], Step [10058/10336], Loss: 1.6680\n",
      "Epoch [3/5], Step [10060/10336], Loss: 0.0270\n",
      "Epoch [3/5], Step [10062/10336], Loss: 1.8054\n",
      "Epoch [3/5], Step [10064/10336], Loss: 0.0080\n",
      "Epoch [3/5], Step [10066/10336], Loss: 0.5862\n",
      "Epoch [3/5], Step [10068/10336], Loss: 0.6042\n",
      "Epoch [3/5], Step [10070/10336], Loss: 1.3886\n",
      "Epoch [3/5], Step [10072/10336], Loss: 0.1270\n",
      "Epoch [3/5], Step [10074/10336], Loss: 0.0194\n",
      "Epoch [3/5], Step [10076/10336], Loss: 0.0357\n",
      "Epoch [3/5], Step [10078/10336], Loss: 0.9590\n",
      "Epoch [3/5], Step [10080/10336], Loss: 0.5491\n",
      "Epoch [3/5], Step [10082/10336], Loss: 2.7964\n",
      "Epoch [3/5], Step [10084/10336], Loss: 0.0100\n",
      "Epoch [3/5], Step [10086/10336], Loss: 3.3523\n",
      "Epoch [3/5], Step [10088/10336], Loss: 0.0133\n",
      "Epoch [3/5], Step [10090/10336], Loss: 0.0122\n",
      "Epoch [3/5], Step [10092/10336], Loss: 0.0427\n",
      "Epoch [3/5], Step [10094/10336], Loss: 1.6015\n",
      "Epoch [3/5], Step [10096/10336], Loss: 1.9544\n",
      "Epoch [3/5], Step [10098/10336], Loss: 0.0130\n",
      "Epoch [3/5], Step [10100/10336], Loss: 0.6642\n",
      "Epoch [3/5], Step [10102/10336], Loss: 0.4835\n",
      "Epoch [3/5], Step [10104/10336], Loss: 0.0019\n",
      "Epoch [3/5], Step [10106/10336], Loss: 0.5213\n",
      "Epoch [3/5], Step [10108/10336], Loss: 0.7179\n",
      "Epoch [3/5], Step [10110/10336], Loss: 0.4214\n",
      "Epoch [3/5], Step [10112/10336], Loss: 0.1346\n",
      "Epoch [3/5], Step [10114/10336], Loss: 1.1458\n",
      "Epoch [3/5], Step [10116/10336], Loss: 3.6231\n",
      "Epoch [3/5], Step [10118/10336], Loss: 0.0156\n",
      "Epoch [3/5], Step [10120/10336], Loss: 0.0037\n",
      "Epoch [3/5], Step [10122/10336], Loss: 0.0642\n",
      "Epoch [3/5], Step [10124/10336], Loss: 0.2669\n",
      "Epoch [3/5], Step [10126/10336], Loss: 0.8487\n",
      "Epoch [3/5], Step [10128/10336], Loss: 0.0048\n",
      "Epoch [3/5], Step [10130/10336], Loss: 1.6497\n",
      "Epoch [3/5], Step [10132/10336], Loss: 0.0265\n",
      "Epoch [3/5], Step [10134/10336], Loss: 1.5550\n",
      "Epoch [3/5], Step [10136/10336], Loss: 0.2447\n",
      "Epoch [3/5], Step [10138/10336], Loss: 0.2446\n",
      "Epoch [3/5], Step [10140/10336], Loss: 1.0729\n",
      "Epoch [3/5], Step [10142/10336], Loss: 1.2258\n",
      "Epoch [3/5], Step [10144/10336], Loss: 0.3422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [10146/10336], Loss: 1.0923\n",
      "Epoch [3/5], Step [10148/10336], Loss: 0.1549\n",
      "Epoch [3/5], Step [10150/10336], Loss: 0.0015\n",
      "Epoch [3/5], Step [10152/10336], Loss: 0.8620\n",
      "Epoch [3/5], Step [10154/10336], Loss: 0.4843\n",
      "Epoch [3/5], Step [10156/10336], Loss: 1.3445\n",
      "Epoch [3/5], Step [10158/10336], Loss: 0.1657\n",
      "Epoch [3/5], Step [10160/10336], Loss: 2.2213\n",
      "Epoch [3/5], Step [10162/10336], Loss: 0.6982\n",
      "Epoch [3/5], Step [10164/10336], Loss: 0.2395\n",
      "Epoch [3/5], Step [10166/10336], Loss: 0.0791\n",
      "Epoch [3/5], Step [10168/10336], Loss: 3.7866\n",
      "Epoch [3/5], Step [10170/10336], Loss: 0.5381\n",
      "Epoch [3/5], Step [10172/10336], Loss: 1.6283\n",
      "Epoch [3/5], Step [10174/10336], Loss: 1.1139\n",
      "Epoch [3/5], Step [10176/10336], Loss: 0.0595\n",
      "Epoch [3/5], Step [10178/10336], Loss: 0.4947\n",
      "Epoch [3/5], Step [10180/10336], Loss: 0.7596\n",
      "Epoch [3/5], Step [10182/10336], Loss: 0.0116\n",
      "Epoch [3/5], Step [10184/10336], Loss: 0.0229\n",
      "Epoch [3/5], Step [10186/10336], Loss: 0.0698\n",
      "Epoch [3/5], Step [10188/10336], Loss: 0.8353\n",
      "Epoch [3/5], Step [10190/10336], Loss: 3.4175\n",
      "Epoch [3/5], Step [10192/10336], Loss: 3.3748\n",
      "Epoch [3/5], Step [10194/10336], Loss: 3.3395\n",
      "Epoch [3/5], Step [10196/10336], Loss: 0.2685\n",
      "Epoch [3/5], Step [10198/10336], Loss: 0.4111\n",
      "Epoch [3/5], Step [10200/10336], Loss: 0.0164\n",
      "Epoch [3/5], Step [10202/10336], Loss: 0.7454\n",
      "Epoch [3/5], Step [10204/10336], Loss: 1.4914\n",
      "Epoch [3/5], Step [10206/10336], Loss: 0.0177\n",
      "Epoch [3/5], Step [10208/10336], Loss: 2.2221\n",
      "Epoch [3/5], Step [10210/10336], Loss: 0.0257\n",
      "Epoch [3/5], Step [10212/10336], Loss: 2.3113\n",
      "Epoch [3/5], Step [10214/10336], Loss: 1.1670\n",
      "Epoch [3/5], Step [10216/10336], Loss: 0.4593\n",
      "Epoch [3/5], Step [10218/10336], Loss: 0.0251\n",
      "Epoch [3/5], Step [10220/10336], Loss: 0.3402\n",
      "Epoch [3/5], Step [10222/10336], Loss: 2.1516\n",
      "Epoch [3/5], Step [10224/10336], Loss: 0.1427\n",
      "Epoch [3/5], Step [10226/10336], Loss: 3.2186\n",
      "Epoch [3/5], Step [10228/10336], Loss: 0.0193\n",
      "Epoch [3/5], Step [10230/10336], Loss: 0.0532\n",
      "Epoch [3/5], Step [10232/10336], Loss: 0.0432\n",
      "Epoch [3/5], Step [10234/10336], Loss: 0.4357\n",
      "Epoch [3/5], Step [10236/10336], Loss: 0.4034\n",
      "Epoch [3/5], Step [10238/10336], Loss: 0.3669\n",
      "Epoch [3/5], Step [10240/10336], Loss: 2.5115\n",
      "Epoch [3/5], Step [10242/10336], Loss: 0.4686\n",
      "Epoch [3/5], Step [10244/10336], Loss: 0.0524\n",
      "Epoch [3/5], Step [10246/10336], Loss: 0.4965\n",
      "Epoch [3/5], Step [10248/10336], Loss: 0.0431\n",
      "Epoch [3/5], Step [10250/10336], Loss: 0.0026\n",
      "Epoch [3/5], Step [10252/10336], Loss: 0.6376\n",
      "Epoch [3/5], Step [10254/10336], Loss: 0.2804\n",
      "Epoch [3/5], Step [10256/10336], Loss: 0.0144\n",
      "Epoch [3/5], Step [10258/10336], Loss: 0.2072\n",
      "Epoch [3/5], Step [10260/10336], Loss: 2.1661\n",
      "Epoch [3/5], Step [10262/10336], Loss: 1.8649\n",
      "Epoch [3/5], Step [10264/10336], Loss: 0.0028\n",
      "Epoch [3/5], Step [10266/10336], Loss: 0.0333\n",
      "Epoch [3/5], Step [10268/10336], Loss: 0.1998\n",
      "Epoch [3/5], Step [10270/10336], Loss: 0.5838\n",
      "Epoch [3/5], Step [10272/10336], Loss: 0.1723\n",
      "Epoch [3/5], Step [10274/10336], Loss: 0.1810\n",
      "Epoch [3/5], Step [10276/10336], Loss: 0.9448\n",
      "Epoch [3/5], Step [10278/10336], Loss: 0.4818\n",
      "Epoch [3/5], Step [10280/10336], Loss: 0.1907\n",
      "Epoch [3/5], Step [10282/10336], Loss: 1.2456\n",
      "Epoch [3/5], Step [10284/10336], Loss: 1.3790\n",
      "Epoch [3/5], Step [10286/10336], Loss: 0.1356\n",
      "Epoch [3/5], Step [10288/10336], Loss: 0.5900\n",
      "Epoch [3/5], Step [10290/10336], Loss: 1.8976\n",
      "Epoch [3/5], Step [10292/10336], Loss: 1.0471\n",
      "Epoch [3/5], Step [10294/10336], Loss: 0.0931\n",
      "Epoch [3/5], Step [10296/10336], Loss: 0.0438\n",
      "Epoch [3/5], Step [10298/10336], Loss: 2.2846\n",
      "Epoch [3/5], Step [10300/10336], Loss: 0.0140\n",
      "Epoch [3/5], Step [10302/10336], Loss: 2.1411\n",
      "Epoch [3/5], Step [10304/10336], Loss: 0.2930\n",
      "Epoch [3/5], Step [10306/10336], Loss: 2.9221\n",
      "Epoch [3/5], Step [10308/10336], Loss: 0.0434\n",
      "Epoch [3/5], Step [10310/10336], Loss: 0.0055\n",
      "Epoch [3/5], Step [10312/10336], Loss: 0.2494\n",
      "Epoch [3/5], Step [10314/10336], Loss: 0.5831\n",
      "Epoch [3/5], Step [10316/10336], Loss: 0.2784\n",
      "Epoch [3/5], Step [10318/10336], Loss: 0.0779\n",
      "Epoch [3/5], Step [10320/10336], Loss: 2.5866\n",
      "Epoch [3/5], Step [10322/10336], Loss: 0.1286\n",
      "Epoch [3/5], Step [10324/10336], Loss: 0.4029\n",
      "Epoch [3/5], Step [10326/10336], Loss: 0.0051\n",
      "Epoch [3/5], Step [10328/10336], Loss: 0.3153\n",
      "Epoch [3/5], Step [10330/10336], Loss: 2.6665\n",
      "Epoch [3/5], Step [10332/10336], Loss: 0.0734\n",
      "Epoch [3/5], Step [10334/10336], Loss: 0.0050\n",
      "Epoch [3/5], Step [10336/10336], Loss: 1.5653\n",
      "Epoch [4/5], Step [2/10336], Loss: 0.3028\n",
      "Epoch [4/5], Step [4/10336], Loss: 0.0576\n",
      "Epoch [4/5], Step [6/10336], Loss: 0.2847\n",
      "Epoch [4/5], Step [8/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [10/10336], Loss: 0.0856\n",
      "Epoch [4/5], Step [12/10336], Loss: 0.2998\n",
      "Epoch [4/5], Step [14/10336], Loss: 0.2041\n",
      "Epoch [4/5], Step [16/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [18/10336], Loss: 0.0358\n",
      "Epoch [4/5], Step [20/10336], Loss: 0.8381\n",
      "Epoch [4/5], Step [22/10336], Loss: 0.0403\n",
      "Epoch [4/5], Step [24/10336], Loss: 1.6886\n",
      "Epoch [4/5], Step [26/10336], Loss: 1.5066\n",
      "Epoch [4/5], Step [28/10336], Loss: 0.3042\n",
      "Epoch [4/5], Step [30/10336], Loss: 0.1636\n",
      "Epoch [4/5], Step [32/10336], Loss: 0.1390\n",
      "Epoch [4/5], Step [34/10336], Loss: 0.1152\n",
      "Epoch [4/5], Step [36/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [38/10336], Loss: 0.0220\n",
      "Epoch [4/5], Step [40/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [42/10336], Loss: 0.0830\n",
      "Epoch [4/5], Step [44/10336], Loss: 0.8473\n",
      "Epoch [4/5], Step [46/10336], Loss: 0.5715\n",
      "Epoch [4/5], Step [48/10336], Loss: 0.6514\n",
      "Epoch [4/5], Step [50/10336], Loss: 0.0117\n",
      "Epoch [4/5], Step [52/10336], Loss: 0.2189\n",
      "Epoch [4/5], Step [54/10336], Loss: 2.2972\n",
      "Epoch [4/5], Step [56/10336], Loss: 0.8798\n",
      "Epoch [4/5], Step [58/10336], Loss: 0.4778\n",
      "Epoch [4/5], Step [60/10336], Loss: 0.0029\n",
      "Epoch [4/5], Step [62/10336], Loss: 1.0093\n",
      "Epoch [4/5], Step [64/10336], Loss: 1.1067\n",
      "Epoch [4/5], Step [66/10336], Loss: 0.0595\n",
      "Epoch [4/5], Step [68/10336], Loss: 0.0149\n",
      "Epoch [4/5], Step [70/10336], Loss: 0.2773\n",
      "Epoch [4/5], Step [72/10336], Loss: 1.0017\n",
      "Epoch [4/5], Step [74/10336], Loss: 0.0161\n",
      "Epoch [4/5], Step [76/10336], Loss: 0.0393\n",
      "Epoch [4/5], Step [78/10336], Loss: 1.5443\n",
      "Epoch [4/5], Step [80/10336], Loss: 0.0814\n",
      "Epoch [4/5], Step [82/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [84/10336], Loss: 0.7087\n",
      "Epoch [4/5], Step [86/10336], Loss: 0.8780\n",
      "Epoch [4/5], Step [88/10336], Loss: 0.0097\n",
      "Epoch [4/5], Step [90/10336], Loss: 0.0570\n",
      "Epoch [4/5], Step [92/10336], Loss: 0.0196\n",
      "Epoch [4/5], Step [94/10336], Loss: 0.5053\n",
      "Epoch [4/5], Step [96/10336], Loss: 0.4523\n",
      "Epoch [4/5], Step [98/10336], Loss: 1.6068\n",
      "Epoch [4/5], Step [100/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [102/10336], Loss: 0.2775\n",
      "Epoch [4/5], Step [104/10336], Loss: 0.9094\n",
      "Epoch [4/5], Step [106/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [108/10336], Loss: 0.0768\n",
      "Epoch [4/5], Step [110/10336], Loss: 1.3686\n",
      "Epoch [4/5], Step [112/10336], Loss: 1.9716\n",
      "Epoch [4/5], Step [114/10336], Loss: 2.6836\n",
      "Epoch [4/5], Step [116/10336], Loss: 0.0075\n",
      "Epoch [4/5], Step [118/10336], Loss: 1.6651\n",
      "Epoch [4/5], Step [120/10336], Loss: 4.3669\n",
      "Epoch [4/5], Step [122/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [124/10336], Loss: 0.1960\n",
      "Epoch [4/5], Step [126/10336], Loss: 1.1371\n",
      "Epoch [4/5], Step [128/10336], Loss: 0.0617\n",
      "Epoch [4/5], Step [130/10336], Loss: 0.3072\n",
      "Epoch [4/5], Step [132/10336], Loss: 1.5311\n",
      "Epoch [4/5], Step [134/10336], Loss: 1.8057\n",
      "Epoch [4/5], Step [136/10336], Loss: 1.6996\n",
      "Epoch [4/5], Step [138/10336], Loss: 0.0677\n",
      "Epoch [4/5], Step [140/10336], Loss: 1.8189\n",
      "Epoch [4/5], Step [142/10336], Loss: 0.0709\n",
      "Epoch [4/5], Step [144/10336], Loss: 1.3928\n",
      "Epoch [4/5], Step [146/10336], Loss: 0.2888\n",
      "Epoch [4/5], Step [148/10336], Loss: 0.5436\n",
      "Epoch [4/5], Step [150/10336], Loss: 4.2786\n",
      "Epoch [4/5], Step [152/10336], Loss: 0.0645\n",
      "Epoch [4/5], Step [154/10336], Loss: 0.2427\n",
      "Epoch [4/5], Step [156/10336], Loss: 1.2365\n",
      "Epoch [4/5], Step [158/10336], Loss: 4.2806\n",
      "Epoch [4/5], Step [160/10336], Loss: 0.1115\n",
      "Epoch [4/5], Step [162/10336], Loss: 0.3039\n",
      "Epoch [4/5], Step [164/10336], Loss: 0.8270\n",
      "Epoch [4/5], Step [166/10336], Loss: 0.3628\n",
      "Epoch [4/5], Step [168/10336], Loss: 0.4889\n",
      "Epoch [4/5], Step [170/10336], Loss: 0.6406\n",
      "Epoch [4/5], Step [172/10336], Loss: 0.0111\n",
      "Epoch [4/5], Step [174/10336], Loss: 0.0779\n",
      "Epoch [4/5], Step [176/10336], Loss: 1.8275\n",
      "Epoch [4/5], Step [178/10336], Loss: 0.9538\n",
      "Epoch [4/5], Step [180/10336], Loss: 0.0023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [182/10336], Loss: 1.0243\n",
      "Epoch [4/5], Step [184/10336], Loss: 0.0145\n",
      "Epoch [4/5], Step [186/10336], Loss: 0.0879\n",
      "Epoch [4/5], Step [188/10336], Loss: 1.1240\n",
      "Epoch [4/5], Step [190/10336], Loss: 0.0132\n",
      "Epoch [4/5], Step [192/10336], Loss: 0.0085\n",
      "Epoch [4/5], Step [194/10336], Loss: 0.0991\n",
      "Epoch [4/5], Step [196/10336], Loss: 0.2249\n",
      "Epoch [4/5], Step [198/10336], Loss: 0.2805\n",
      "Epoch [4/5], Step [200/10336], Loss: 0.2098\n",
      "Epoch [4/5], Step [202/10336], Loss: 0.1463\n",
      "Epoch [4/5], Step [204/10336], Loss: 0.0306\n",
      "Epoch [4/5], Step [206/10336], Loss: 1.3875\n",
      "Epoch [4/5], Step [208/10336], Loss: 1.9547\n",
      "Epoch [4/5], Step [210/10336], Loss: 0.3604\n",
      "Epoch [4/5], Step [212/10336], Loss: 0.0412\n",
      "Epoch [4/5], Step [214/10336], Loss: 1.1279\n",
      "Epoch [4/5], Step [216/10336], Loss: 0.0484\n",
      "Epoch [4/5], Step [218/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [220/10336], Loss: 1.1046\n",
      "Epoch [4/5], Step [222/10336], Loss: 0.3981\n",
      "Epoch [4/5], Step [224/10336], Loss: 0.1050\n",
      "Epoch [4/5], Step [226/10336], Loss: 1.8961\n",
      "Epoch [4/5], Step [228/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [230/10336], Loss: 0.0086\n",
      "Epoch [4/5], Step [232/10336], Loss: 0.0468\n",
      "Epoch [4/5], Step [234/10336], Loss: 0.9291\n",
      "Epoch [4/5], Step [236/10336], Loss: 0.0349\n",
      "Epoch [4/5], Step [238/10336], Loss: 1.0470\n",
      "Epoch [4/5], Step [240/10336], Loss: 0.9153\n",
      "Epoch [4/5], Step [242/10336], Loss: 0.8220\n",
      "Epoch [4/5], Step [244/10336], Loss: 0.0151\n",
      "Epoch [4/5], Step [246/10336], Loss: 0.9057\n",
      "Epoch [4/5], Step [248/10336], Loss: 0.1083\n",
      "Epoch [4/5], Step [250/10336], Loss: 0.3084\n",
      "Epoch [4/5], Step [252/10336], Loss: 0.2265\n",
      "Epoch [4/5], Step [254/10336], Loss: 0.1646\n",
      "Epoch [4/5], Step [256/10336], Loss: 1.5545\n",
      "Epoch [4/5], Step [258/10336], Loss: 0.3160\n",
      "Epoch [4/5], Step [260/10336], Loss: 1.5724\n",
      "Epoch [4/5], Step [262/10336], Loss: 0.1053\n",
      "Epoch [4/5], Step [264/10336], Loss: 1.0733\n",
      "Epoch [4/5], Step [266/10336], Loss: 0.0064\n",
      "Epoch [4/5], Step [268/10336], Loss: 0.0097\n",
      "Epoch [4/5], Step [270/10336], Loss: 0.0035\n",
      "Epoch [4/5], Step [272/10336], Loss: 0.4391\n",
      "Epoch [4/5], Step [274/10336], Loss: 0.0500\n",
      "Epoch [4/5], Step [276/10336], Loss: 0.0217\n",
      "Epoch [4/5], Step [278/10336], Loss: 0.0250\n",
      "Epoch [4/5], Step [280/10336], Loss: 1.1157\n",
      "Epoch [4/5], Step [282/10336], Loss: 0.1941\n",
      "Epoch [4/5], Step [284/10336], Loss: 0.6263\n",
      "Epoch [4/5], Step [286/10336], Loss: 0.0505\n",
      "Epoch [4/5], Step [288/10336], Loss: 0.1799\n",
      "Epoch [4/5], Step [290/10336], Loss: 1.8494\n",
      "Epoch [4/5], Step [292/10336], Loss: 0.2321\n",
      "Epoch [4/5], Step [294/10336], Loss: 2.0324\n",
      "Epoch [4/5], Step [296/10336], Loss: 0.3604\n",
      "Epoch [4/5], Step [298/10336], Loss: 0.0336\n",
      "Epoch [4/5], Step [300/10336], Loss: 3.0130\n",
      "Epoch [4/5], Step [302/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [304/10336], Loss: 0.2840\n",
      "Epoch [4/5], Step [306/10336], Loss: 0.0158\n",
      "Epoch [4/5], Step [308/10336], Loss: 0.1007\n",
      "Epoch [4/5], Step [310/10336], Loss: 0.4321\n",
      "Epoch [4/5], Step [312/10336], Loss: 3.8614\n",
      "Epoch [4/5], Step [314/10336], Loss: 0.1355\n",
      "Epoch [4/5], Step [316/10336], Loss: 0.1241\n",
      "Epoch [4/5], Step [318/10336], Loss: 0.1830\n",
      "Epoch [4/5], Step [320/10336], Loss: 0.0323\n",
      "Epoch [4/5], Step [322/10336], Loss: 1.3880\n",
      "Epoch [4/5], Step [324/10336], Loss: 0.0672\n",
      "Epoch [4/5], Step [326/10336], Loss: 1.7835\n",
      "Epoch [4/5], Step [328/10336], Loss: 2.0587\n",
      "Epoch [4/5], Step [330/10336], Loss: 1.2505\n",
      "Epoch [4/5], Step [332/10336], Loss: 0.3578\n",
      "Epoch [4/5], Step [334/10336], Loss: 0.4301\n",
      "Epoch [4/5], Step [336/10336], Loss: 1.2871\n",
      "Epoch [4/5], Step [338/10336], Loss: 0.2368\n",
      "Epoch [4/5], Step [340/10336], Loss: 1.3207\n",
      "Epoch [4/5], Step [342/10336], Loss: 1.0463\n",
      "Epoch [4/5], Step [344/10336], Loss: 0.7511\n",
      "Epoch [4/5], Step [346/10336], Loss: 0.1035\n",
      "Epoch [4/5], Step [348/10336], Loss: 0.0652\n",
      "Epoch [4/5], Step [350/10336], Loss: 0.1822\n",
      "Epoch [4/5], Step [352/10336], Loss: 0.6918\n",
      "Epoch [4/5], Step [354/10336], Loss: 1.3777\n",
      "Epoch [4/5], Step [356/10336], Loss: 0.0342\n",
      "Epoch [4/5], Step [358/10336], Loss: 0.4534\n",
      "Epoch [4/5], Step [360/10336], Loss: 0.1268\n",
      "Epoch [4/5], Step [362/10336], Loss: 2.0455\n",
      "Epoch [4/5], Step [364/10336], Loss: 0.2187\n",
      "Epoch [4/5], Step [366/10336], Loss: 0.1292\n",
      "Epoch [4/5], Step [368/10336], Loss: 3.3641\n",
      "Epoch [4/5], Step [370/10336], Loss: 1.8025\n",
      "Epoch [4/5], Step [372/10336], Loss: 0.0027\n",
      "Epoch [4/5], Step [374/10336], Loss: 0.4307\n",
      "Epoch [4/5], Step [376/10336], Loss: 0.4225\n",
      "Epoch [4/5], Step [378/10336], Loss: 0.9453\n",
      "Epoch [4/5], Step [380/10336], Loss: 1.1352\n",
      "Epoch [4/5], Step [382/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [384/10336], Loss: 0.0571\n",
      "Epoch [4/5], Step [386/10336], Loss: 0.5354\n",
      "Epoch [4/5], Step [388/10336], Loss: 0.0808\n",
      "Epoch [4/5], Step [390/10336], Loss: 0.3772\n",
      "Epoch [4/5], Step [392/10336], Loss: 0.0094\n",
      "Epoch [4/5], Step [394/10336], Loss: 1.4175\n",
      "Epoch [4/5], Step [396/10336], Loss: 0.2199\n",
      "Epoch [4/5], Step [398/10336], Loss: 1.2945\n",
      "Epoch [4/5], Step [400/10336], Loss: 4.6850\n",
      "Epoch [4/5], Step [402/10336], Loss: 0.7190\n",
      "Epoch [4/5], Step [404/10336], Loss: 0.4014\n",
      "Epoch [4/5], Step [406/10336], Loss: 0.3857\n",
      "Epoch [4/5], Step [408/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [410/10336], Loss: 0.0423\n",
      "Epoch [4/5], Step [412/10336], Loss: 0.9069\n",
      "Epoch [4/5], Step [414/10336], Loss: 0.2079\n",
      "Epoch [4/5], Step [416/10336], Loss: 0.1030\n",
      "Epoch [4/5], Step [418/10336], Loss: 0.1357\n",
      "Epoch [4/5], Step [420/10336], Loss: 1.1637\n",
      "Epoch [4/5], Step [422/10336], Loss: 0.3592\n",
      "Epoch [4/5], Step [424/10336], Loss: 0.0113\n",
      "Epoch [4/5], Step [426/10336], Loss: 0.6719\n",
      "Epoch [4/5], Step [428/10336], Loss: 0.0293\n",
      "Epoch [4/5], Step [430/10336], Loss: 0.0095\n",
      "Epoch [4/5], Step [432/10336], Loss: 0.5879\n",
      "Epoch [4/5], Step [434/10336], Loss: 0.2614\n",
      "Epoch [4/5], Step [436/10336], Loss: 0.0157\n",
      "Epoch [4/5], Step [438/10336], Loss: 0.8739\n",
      "Epoch [4/5], Step [440/10336], Loss: 0.7845\n",
      "Epoch [4/5], Step [442/10336], Loss: 0.9676\n",
      "Epoch [4/5], Step [444/10336], Loss: 0.3394\n",
      "Epoch [4/5], Step [446/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [448/10336], Loss: 0.0633\n",
      "Epoch [4/5], Step [450/10336], Loss: 2.2788\n",
      "Epoch [4/5], Step [452/10336], Loss: 2.2353\n",
      "Epoch [4/5], Step [454/10336], Loss: 0.1736\n",
      "Epoch [4/5], Step [456/10336], Loss: 1.2348\n",
      "Epoch [4/5], Step [458/10336], Loss: 0.1427\n",
      "Epoch [4/5], Step [460/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [462/10336], Loss: 1.1352\n",
      "Epoch [4/5], Step [464/10336], Loss: 0.0365\n",
      "Epoch [4/5], Step [466/10336], Loss: 0.0975\n",
      "Epoch [4/5], Step [468/10336], Loss: 1.7963\n",
      "Epoch [4/5], Step [470/10336], Loss: 0.7205\n",
      "Epoch [4/5], Step [472/10336], Loss: 0.9378\n",
      "Epoch [4/5], Step [474/10336], Loss: 0.9551\n",
      "Epoch [4/5], Step [476/10336], Loss: 0.9210\n",
      "Epoch [4/5], Step [478/10336], Loss: 2.4151\n",
      "Epoch [4/5], Step [480/10336], Loss: 1.2878\n",
      "Epoch [4/5], Step [482/10336], Loss: 0.6592\n",
      "Epoch [4/5], Step [484/10336], Loss: 0.0273\n",
      "Epoch [4/5], Step [486/10336], Loss: 0.2172\n",
      "Epoch [4/5], Step [488/10336], Loss: 0.1189\n",
      "Epoch [4/5], Step [490/10336], Loss: 0.0097\n",
      "Epoch [4/5], Step [492/10336], Loss: 2.2052\n",
      "Epoch [4/5], Step [494/10336], Loss: 0.0221\n",
      "Epoch [4/5], Step [496/10336], Loss: 0.0638\n",
      "Epoch [4/5], Step [498/10336], Loss: 2.2083\n",
      "Epoch [4/5], Step [500/10336], Loss: 1.8430\n",
      "Epoch [4/5], Step [502/10336], Loss: 0.0179\n",
      "Epoch [4/5], Step [504/10336], Loss: 0.2833\n",
      "Epoch [4/5], Step [506/10336], Loss: 0.2536\n",
      "Epoch [4/5], Step [508/10336], Loss: 0.1127\n",
      "Epoch [4/5], Step [510/10336], Loss: 0.8445\n",
      "Epoch [4/5], Step [512/10336], Loss: 3.0320\n",
      "Epoch [4/5], Step [514/10336], Loss: 0.2311\n",
      "Epoch [4/5], Step [516/10336], Loss: 0.0732\n",
      "Epoch [4/5], Step [518/10336], Loss: 0.1168\n",
      "Epoch [4/5], Step [520/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [522/10336], Loss: 0.1332\n",
      "Epoch [4/5], Step [524/10336], Loss: 1.6358\n",
      "Epoch [4/5], Step [526/10336], Loss: 0.3332\n",
      "Epoch [4/5], Step [528/10336], Loss: 0.0081\n",
      "Epoch [4/5], Step [530/10336], Loss: 0.0503\n",
      "Epoch [4/5], Step [532/10336], Loss: 0.1816\n",
      "Epoch [4/5], Step [534/10336], Loss: 0.0371\n",
      "Epoch [4/5], Step [536/10336], Loss: 0.2192\n",
      "Epoch [4/5], Step [538/10336], Loss: 0.0240\n",
      "Epoch [4/5], Step [540/10336], Loss: 0.0548\n",
      "Epoch [4/5], Step [542/10336], Loss: 0.0294\n",
      "Epoch [4/5], Step [544/10336], Loss: 0.8869\n",
      "Epoch [4/5], Step [546/10336], Loss: 2.2537\n",
      "Epoch [4/5], Step [548/10336], Loss: 0.0972\n",
      "Epoch [4/5], Step [550/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [552/10336], Loss: 0.3060\n",
      "Epoch [4/5], Step [554/10336], Loss: 1.5559\n",
      "Epoch [4/5], Step [556/10336], Loss: 4.9706\n",
      "Epoch [4/5], Step [558/10336], Loss: 0.1057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [560/10336], Loss: 0.1972\n",
      "Epoch [4/5], Step [562/10336], Loss: 1.5880\n",
      "Epoch [4/5], Step [564/10336], Loss: 0.0232\n",
      "Epoch [4/5], Step [566/10336], Loss: 0.2409\n",
      "Epoch [4/5], Step [568/10336], Loss: 0.4714\n",
      "Epoch [4/5], Step [570/10336], Loss: 0.0262\n",
      "Epoch [4/5], Step [572/10336], Loss: 0.3498\n",
      "Epoch [4/5], Step [574/10336], Loss: 0.0656\n",
      "Epoch [4/5], Step [576/10336], Loss: 0.6259\n",
      "Epoch [4/5], Step [578/10336], Loss: 0.1411\n",
      "Epoch [4/5], Step [580/10336], Loss: 0.8659\n",
      "Epoch [4/5], Step [582/10336], Loss: 0.5452\n",
      "Epoch [4/5], Step [584/10336], Loss: 2.0653\n",
      "Epoch [4/5], Step [586/10336], Loss: 0.7145\n",
      "Epoch [4/5], Step [588/10336], Loss: 0.1449\n",
      "Epoch [4/5], Step [590/10336], Loss: 0.0351\n",
      "Epoch [4/5], Step [592/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [594/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [596/10336], Loss: 0.5594\n",
      "Epoch [4/5], Step [598/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [600/10336], Loss: 0.0893\n",
      "Epoch [4/5], Step [602/10336], Loss: 2.7586\n",
      "Epoch [4/5], Step [604/10336], Loss: 4.8340\n",
      "Epoch [4/5], Step [606/10336], Loss: 0.3566\n",
      "Epoch [4/5], Step [608/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [610/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [612/10336], Loss: 1.9817\n",
      "Epoch [4/5], Step [614/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [616/10336], Loss: 0.2532\n",
      "Epoch [4/5], Step [618/10336], Loss: 0.6652\n",
      "Epoch [4/5], Step [620/10336], Loss: 1.0772\n",
      "Epoch [4/5], Step [622/10336], Loss: 0.0898\n",
      "Epoch [4/5], Step [624/10336], Loss: 0.1171\n",
      "Epoch [4/5], Step [626/10336], Loss: 0.0143\n",
      "Epoch [4/5], Step [628/10336], Loss: 1.1015\n",
      "Epoch [4/5], Step [630/10336], Loss: 0.0058\n",
      "Epoch [4/5], Step [632/10336], Loss: 0.3039\n",
      "Epoch [4/5], Step [634/10336], Loss: 0.6852\n",
      "Epoch [4/5], Step [636/10336], Loss: 0.0621\n",
      "Epoch [4/5], Step [638/10336], Loss: 2.2901\n",
      "Epoch [4/5], Step [640/10336], Loss: 1.2686\n",
      "Epoch [4/5], Step [642/10336], Loss: 2.4234\n",
      "Epoch [4/5], Step [644/10336], Loss: 0.2677\n",
      "Epoch [4/5], Step [646/10336], Loss: 0.0359\n",
      "Epoch [4/5], Step [648/10336], Loss: 0.1074\n",
      "Epoch [4/5], Step [650/10336], Loss: 0.4878\n",
      "Epoch [4/5], Step [652/10336], Loss: 0.8119\n",
      "Epoch [4/5], Step [654/10336], Loss: 0.0038\n",
      "Epoch [4/5], Step [656/10336], Loss: 0.7552\n",
      "Epoch [4/5], Step [658/10336], Loss: 0.2388\n",
      "Epoch [4/5], Step [660/10336], Loss: 0.1051\n",
      "Epoch [4/5], Step [662/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [664/10336], Loss: 0.1855\n",
      "Epoch [4/5], Step [666/10336], Loss: 0.5597\n",
      "Epoch [4/5], Step [668/10336], Loss: 0.3310\n",
      "Epoch [4/5], Step [670/10336], Loss: 0.9950\n",
      "Epoch [4/5], Step [672/10336], Loss: 0.7939\n",
      "Epoch [4/5], Step [674/10336], Loss: 2.2206\n",
      "Epoch [4/5], Step [676/10336], Loss: 0.5016\n",
      "Epoch [4/5], Step [678/10336], Loss: 0.0133\n",
      "Epoch [4/5], Step [680/10336], Loss: 1.5435\n",
      "Epoch [4/5], Step [682/10336], Loss: 2.1892\n",
      "Epoch [4/5], Step [684/10336], Loss: 0.3714\n",
      "Epoch [4/5], Step [686/10336], Loss: 0.9851\n",
      "Epoch [4/5], Step [688/10336], Loss: 0.5081\n",
      "Epoch [4/5], Step [690/10336], Loss: 0.2401\n",
      "Epoch [4/5], Step [692/10336], Loss: 0.4349\n",
      "Epoch [4/5], Step [694/10336], Loss: 1.7794\n",
      "Epoch [4/5], Step [696/10336], Loss: 0.0120\n",
      "Epoch [4/5], Step [698/10336], Loss: 0.3315\n",
      "Epoch [4/5], Step [700/10336], Loss: 0.0659\n",
      "Epoch [4/5], Step [702/10336], Loss: 0.7097\n",
      "Epoch [4/5], Step [704/10336], Loss: 1.2846\n",
      "Epoch [4/5], Step [706/10336], Loss: 0.0752\n",
      "Epoch [4/5], Step [708/10336], Loss: 0.0541\n",
      "Epoch [4/5], Step [710/10336], Loss: 0.1616\n",
      "Epoch [4/5], Step [712/10336], Loss: 0.2077\n",
      "Epoch [4/5], Step [714/10336], Loss: 2.0029\n",
      "Epoch [4/5], Step [716/10336], Loss: 0.6211\n",
      "Epoch [4/5], Step [718/10336], Loss: 0.3769\n",
      "Epoch [4/5], Step [720/10336], Loss: 0.2282\n",
      "Epoch [4/5], Step [722/10336], Loss: 0.0331\n",
      "Epoch [4/5], Step [724/10336], Loss: 0.6689\n",
      "Epoch [4/5], Step [726/10336], Loss: 0.0343\n",
      "Epoch [4/5], Step [728/10336], Loss: 0.2387\n",
      "Epoch [4/5], Step [730/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [732/10336], Loss: 0.7138\n",
      "Epoch [4/5], Step [734/10336], Loss: 0.3847\n",
      "Epoch [4/5], Step [736/10336], Loss: 0.0219\n",
      "Epoch [4/5], Step [738/10336], Loss: 0.6014\n",
      "Epoch [4/5], Step [740/10336], Loss: 0.4095\n",
      "Epoch [4/5], Step [742/10336], Loss: 0.1413\n",
      "Epoch [4/5], Step [744/10336], Loss: 0.0857\n",
      "Epoch [4/5], Step [746/10336], Loss: 1.8627\n",
      "Epoch [4/5], Step [748/10336], Loss: 0.2364\n",
      "Epoch [4/5], Step [750/10336], Loss: 0.6970\n",
      "Epoch [4/5], Step [752/10336], Loss: 0.3945\n",
      "Epoch [4/5], Step [754/10336], Loss: 0.0912\n",
      "Epoch [4/5], Step [756/10336], Loss: 2.4186\n",
      "Epoch [4/5], Step [758/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [760/10336], Loss: 2.9825\n",
      "Epoch [4/5], Step [762/10336], Loss: 0.8961\n",
      "Epoch [4/5], Step [764/10336], Loss: 0.0250\n",
      "Epoch [4/5], Step [766/10336], Loss: 0.9435\n",
      "Epoch [4/5], Step [768/10336], Loss: 0.1420\n",
      "Epoch [4/5], Step [770/10336], Loss: 2.7774\n",
      "Epoch [4/5], Step [772/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [774/10336], Loss: 0.7330\n",
      "Epoch [4/5], Step [776/10336], Loss: 0.1378\n",
      "Epoch [4/5], Step [778/10336], Loss: 0.5084\n",
      "Epoch [4/5], Step [780/10336], Loss: 0.2445\n",
      "Epoch [4/5], Step [782/10336], Loss: 0.7508\n",
      "Epoch [4/5], Step [784/10336], Loss: 0.0546\n",
      "Epoch [4/5], Step [786/10336], Loss: 0.5298\n",
      "Epoch [4/5], Step [788/10336], Loss: 0.0287\n",
      "Epoch [4/5], Step [790/10336], Loss: 0.5477\n",
      "Epoch [4/5], Step [792/10336], Loss: 0.9592\n",
      "Epoch [4/5], Step [794/10336], Loss: 1.2587\n",
      "Epoch [4/5], Step [796/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [798/10336], Loss: 1.2905\n",
      "Epoch [4/5], Step [800/10336], Loss: 0.0245\n",
      "Epoch [4/5], Step [802/10336], Loss: 0.0414\n",
      "Epoch [4/5], Step [804/10336], Loss: 2.4440\n",
      "Epoch [4/5], Step [806/10336], Loss: 0.5800\n",
      "Epoch [4/5], Step [808/10336], Loss: 0.7256\n",
      "Epoch [4/5], Step [810/10336], Loss: 0.0111\n",
      "Epoch [4/5], Step [812/10336], Loss: 1.4656\n",
      "Epoch [4/5], Step [814/10336], Loss: 0.3106\n",
      "Epoch [4/5], Step [816/10336], Loss: 1.1492\n",
      "Epoch [4/5], Step [818/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [820/10336], Loss: 0.1166\n",
      "Epoch [4/5], Step [822/10336], Loss: 0.4097\n",
      "Epoch [4/5], Step [824/10336], Loss: 0.3516\n",
      "Epoch [4/5], Step [826/10336], Loss: 1.1821\n",
      "Epoch [4/5], Step [828/10336], Loss: 0.0129\n",
      "Epoch [4/5], Step [830/10336], Loss: 0.4985\n",
      "Epoch [4/5], Step [832/10336], Loss: 0.2294\n",
      "Epoch [4/5], Step [834/10336], Loss: 0.0999\n",
      "Epoch [4/5], Step [836/10336], Loss: 0.4384\n",
      "Epoch [4/5], Step [838/10336], Loss: 0.1891\n",
      "Epoch [4/5], Step [840/10336], Loss: 0.4629\n",
      "Epoch [4/5], Step [842/10336], Loss: 1.0479\n",
      "Epoch [4/5], Step [844/10336], Loss: 0.1585\n",
      "Epoch [4/5], Step [846/10336], Loss: 0.2743\n",
      "Epoch [4/5], Step [848/10336], Loss: 0.1447\n",
      "Epoch [4/5], Step [850/10336], Loss: 0.4715\n",
      "Epoch [4/5], Step [852/10336], Loss: 0.1806\n",
      "Epoch [4/5], Step [854/10336], Loss: 0.3486\n",
      "Epoch [4/5], Step [856/10336], Loss: 0.0069\n",
      "Epoch [4/5], Step [858/10336], Loss: 1.6068\n",
      "Epoch [4/5], Step [860/10336], Loss: 0.7787\n",
      "Epoch [4/5], Step [862/10336], Loss: 0.9673\n",
      "Epoch [4/5], Step [864/10336], Loss: 0.4761\n",
      "Epoch [4/5], Step [866/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [868/10336], Loss: 0.3761\n",
      "Epoch [4/5], Step [870/10336], Loss: 0.9664\n",
      "Epoch [4/5], Step [872/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [874/10336], Loss: 1.3378\n",
      "Epoch [4/5], Step [876/10336], Loss: 0.6800\n",
      "Epoch [4/5], Step [878/10336], Loss: 0.0666\n",
      "Epoch [4/5], Step [880/10336], Loss: 0.1054\n",
      "Epoch [4/5], Step [882/10336], Loss: 0.8704\n",
      "Epoch [4/5], Step [884/10336], Loss: 0.6863\n",
      "Epoch [4/5], Step [886/10336], Loss: 0.6896\n",
      "Epoch [4/5], Step [888/10336], Loss: 1.5107\n",
      "Epoch [4/5], Step [890/10336], Loss: 0.9894\n",
      "Epoch [4/5], Step [892/10336], Loss: 0.1079\n",
      "Epoch [4/5], Step [894/10336], Loss: 0.3167\n",
      "Epoch [4/5], Step [896/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [898/10336], Loss: 3.3856\n",
      "Epoch [4/5], Step [900/10336], Loss: 0.8962\n",
      "Epoch [4/5], Step [902/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [904/10336], Loss: 1.6439\n",
      "Epoch [4/5], Step [906/10336], Loss: 2.8907\n",
      "Epoch [4/5], Step [908/10336], Loss: 0.6577\n",
      "Epoch [4/5], Step [910/10336], Loss: 0.5235\n",
      "Epoch [4/5], Step [912/10336], Loss: 0.0299\n",
      "Epoch [4/5], Step [914/10336], Loss: 2.3752\n",
      "Epoch [4/5], Step [916/10336], Loss: 2.0139\n",
      "Epoch [4/5], Step [918/10336], Loss: 0.2063\n",
      "Epoch [4/5], Step [920/10336], Loss: 3.5235\n",
      "Epoch [4/5], Step [922/10336], Loss: 2.1872\n",
      "Epoch [4/5], Step [924/10336], Loss: 0.5651\n",
      "Epoch [4/5], Step [926/10336], Loss: 1.0258\n",
      "Epoch [4/5], Step [928/10336], Loss: 0.1127\n",
      "Epoch [4/5], Step [930/10336], Loss: 2.1606\n",
      "Epoch [4/5], Step [932/10336], Loss: 0.2049\n",
      "Epoch [4/5], Step [934/10336], Loss: 0.9711\n",
      "Epoch [4/5], Step [936/10336], Loss: 1.1122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [938/10336], Loss: 0.0553\n",
      "Epoch [4/5], Step [940/10336], Loss: 0.2852\n",
      "Epoch [4/5], Step [942/10336], Loss: 0.0159\n",
      "Epoch [4/5], Step [944/10336], Loss: 0.9302\n",
      "Epoch [4/5], Step [946/10336], Loss: 0.1116\n",
      "Epoch [4/5], Step [948/10336], Loss: 0.1792\n",
      "Epoch [4/5], Step [950/10336], Loss: 0.9589\n",
      "Epoch [4/5], Step [952/10336], Loss: 2.4283\n",
      "Epoch [4/5], Step [954/10336], Loss: 0.3253\n",
      "Epoch [4/5], Step [956/10336], Loss: 0.3295\n",
      "Epoch [4/5], Step [958/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [960/10336], Loss: 0.1324\n",
      "Epoch [4/5], Step [962/10336], Loss: 0.1563\n",
      "Epoch [4/5], Step [964/10336], Loss: 0.7031\n",
      "Epoch [4/5], Step [966/10336], Loss: 0.2767\n",
      "Epoch [4/5], Step [968/10336], Loss: 1.0638\n",
      "Epoch [4/5], Step [970/10336], Loss: 0.2246\n",
      "Epoch [4/5], Step [972/10336], Loss: 2.7151\n",
      "Epoch [4/5], Step [974/10336], Loss: 2.6532\n",
      "Epoch [4/5], Step [976/10336], Loss: 0.0450\n",
      "Epoch [4/5], Step [978/10336], Loss: 0.0691\n",
      "Epoch [4/5], Step [980/10336], Loss: 0.2649\n",
      "Epoch [4/5], Step [982/10336], Loss: 0.0254\n",
      "Epoch [4/5], Step [984/10336], Loss: 0.0422\n",
      "Epoch [4/5], Step [986/10336], Loss: 0.3136\n",
      "Epoch [4/5], Step [988/10336], Loss: 0.4193\n",
      "Epoch [4/5], Step [990/10336], Loss: 0.0471\n",
      "Epoch [4/5], Step [992/10336], Loss: 0.1498\n",
      "Epoch [4/5], Step [994/10336], Loss: 0.1994\n",
      "Epoch [4/5], Step [996/10336], Loss: 0.2599\n",
      "Epoch [4/5], Step [998/10336], Loss: 0.7141\n",
      "Epoch [4/5], Step [1000/10336], Loss: 0.0309\n",
      "Epoch [4/5], Step [1002/10336], Loss: 0.1291\n",
      "Epoch [4/5], Step [1004/10336], Loss: 2.6420\n",
      "Epoch [4/5], Step [1006/10336], Loss: 0.4699\n",
      "Epoch [4/5], Step [1008/10336], Loss: 1.4610\n",
      "Epoch [4/5], Step [1010/10336], Loss: 0.9563\n",
      "Epoch [4/5], Step [1012/10336], Loss: 2.6756\n",
      "Epoch [4/5], Step [1014/10336], Loss: 0.0738\n",
      "Epoch [4/5], Step [1016/10336], Loss: 1.0299\n",
      "Epoch [4/5], Step [1018/10336], Loss: 0.0250\n",
      "Epoch [4/5], Step [1020/10336], Loss: 0.0052\n",
      "Epoch [4/5], Step [1022/10336], Loss: 1.6390\n",
      "Epoch [4/5], Step [1024/10336], Loss: 0.6311\n",
      "Epoch [4/5], Step [1026/10336], Loss: 1.0134\n",
      "Epoch [4/5], Step [1028/10336], Loss: 2.1589\n",
      "Epoch [4/5], Step [1030/10336], Loss: 0.4013\n",
      "Epoch [4/5], Step [1032/10336], Loss: 0.0032\n",
      "Epoch [4/5], Step [1034/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [1036/10336], Loss: 0.3761\n",
      "Epoch [4/5], Step [1038/10336], Loss: 0.1328\n",
      "Epoch [4/5], Step [1040/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [1042/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [1044/10336], Loss: 0.6872\n",
      "Epoch [4/5], Step [1046/10336], Loss: 0.1244\n",
      "Epoch [4/5], Step [1048/10336], Loss: 0.0499\n",
      "Epoch [4/5], Step [1050/10336], Loss: 1.3622\n",
      "Epoch [4/5], Step [1052/10336], Loss: 1.7070\n",
      "Epoch [4/5], Step [1054/10336], Loss: 1.0021\n",
      "Epoch [4/5], Step [1056/10336], Loss: 2.5236\n",
      "Epoch [4/5], Step [1058/10336], Loss: 0.2352\n",
      "Epoch [4/5], Step [1060/10336], Loss: 0.1743\n",
      "Epoch [4/5], Step [1062/10336], Loss: 2.7846\n",
      "Epoch [4/5], Step [1064/10336], Loss: 0.5164\n",
      "Epoch [4/5], Step [1066/10336], Loss: 0.3842\n",
      "Epoch [4/5], Step [1068/10336], Loss: 0.0684\n",
      "Epoch [4/5], Step [1070/10336], Loss: 0.2303\n",
      "Epoch [4/5], Step [1072/10336], Loss: 0.0237\n",
      "Epoch [4/5], Step [1074/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [1076/10336], Loss: 2.1778\n",
      "Epoch [4/5], Step [1078/10336], Loss: 0.0336\n",
      "Epoch [4/5], Step [1080/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [1082/10336], Loss: 0.6030\n",
      "Epoch [4/5], Step [1084/10336], Loss: 0.8048\n",
      "Epoch [4/5], Step [1086/10336], Loss: 0.0811\n",
      "Epoch [4/5], Step [1088/10336], Loss: 1.2105\n",
      "Epoch [4/5], Step [1090/10336], Loss: 0.0184\n",
      "Epoch [4/5], Step [1092/10336], Loss: 1.1079\n",
      "Epoch [4/5], Step [1094/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [1096/10336], Loss: 0.1134\n",
      "Epoch [4/5], Step [1098/10336], Loss: 1.2532\n",
      "Epoch [4/5], Step [1100/10336], Loss: 0.0030\n",
      "Epoch [4/5], Step [1102/10336], Loss: 0.3603\n",
      "Epoch [4/5], Step [1104/10336], Loss: 0.2707\n",
      "Epoch [4/5], Step [1106/10336], Loss: 0.1931\n",
      "Epoch [4/5], Step [1108/10336], Loss: 0.0111\n",
      "Epoch [4/5], Step [1110/10336], Loss: 0.2605\n",
      "Epoch [4/5], Step [1112/10336], Loss: 0.2909\n",
      "Epoch [4/5], Step [1114/10336], Loss: 0.9179\n",
      "Epoch [4/5], Step [1116/10336], Loss: 7.1529\n",
      "Epoch [4/5], Step [1118/10336], Loss: 0.2392\n",
      "Epoch [4/5], Step [1120/10336], Loss: 2.3308\n",
      "Epoch [4/5], Step [1122/10336], Loss: 3.4222\n",
      "Epoch [4/5], Step [1124/10336], Loss: 0.0371\n",
      "Epoch [4/5], Step [1126/10336], Loss: 2.2304\n",
      "Epoch [4/5], Step [1128/10336], Loss: 0.5073\n",
      "Epoch [4/5], Step [1130/10336], Loss: 1.2663\n",
      "Epoch [4/5], Step [1132/10336], Loss: 0.7042\n",
      "Epoch [4/5], Step [1134/10336], Loss: 0.0176\n",
      "Epoch [4/5], Step [1136/10336], Loss: 1.1710\n",
      "Epoch [4/5], Step [1138/10336], Loss: 0.0259\n",
      "Epoch [4/5], Step [1140/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [1142/10336], Loss: 0.0093\n",
      "Epoch [4/5], Step [1144/10336], Loss: 1.4501\n",
      "Epoch [4/5], Step [1146/10336], Loss: 0.4019\n",
      "Epoch [4/5], Step [1148/10336], Loss: 1.5123\n",
      "Epoch [4/5], Step [1150/10336], Loss: 1.9148\n",
      "Epoch [4/5], Step [1152/10336], Loss: 1.0018\n",
      "Epoch [4/5], Step [1154/10336], Loss: 0.6923\n",
      "Epoch [4/5], Step [1156/10336], Loss: 3.1712\n",
      "Epoch [4/5], Step [1158/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [1160/10336], Loss: 0.0339\n",
      "Epoch [4/5], Step [1162/10336], Loss: 2.4039\n",
      "Epoch [4/5], Step [1164/10336], Loss: 0.2255\n",
      "Epoch [4/5], Step [1166/10336], Loss: 0.8385\n",
      "Epoch [4/5], Step [1168/10336], Loss: 2.2452\n",
      "Epoch [4/5], Step [1170/10336], Loss: 0.8887\n",
      "Epoch [4/5], Step [1172/10336], Loss: 1.6739\n",
      "Epoch [4/5], Step [1174/10336], Loss: 1.1108\n",
      "Epoch [4/5], Step [1176/10336], Loss: 0.3291\n",
      "Epoch [4/5], Step [1178/10336], Loss: 3.3056\n",
      "Epoch [4/5], Step [1180/10336], Loss: 0.4232\n",
      "Epoch [4/5], Step [1182/10336], Loss: 0.0093\n",
      "Epoch [4/5], Step [1184/10336], Loss: 0.0163\n",
      "Epoch [4/5], Step [1186/10336], Loss: 1.8045\n",
      "Epoch [4/5], Step [1188/10336], Loss: 0.6419\n",
      "Epoch [4/5], Step [1190/10336], Loss: 0.4152\n",
      "Epoch [4/5], Step [1192/10336], Loss: 0.4953\n",
      "Epoch [4/5], Step [1194/10336], Loss: 1.2118\n",
      "Epoch [4/5], Step [1196/10336], Loss: 0.5487\n",
      "Epoch [4/5], Step [1198/10336], Loss: 0.1165\n",
      "Epoch [4/5], Step [1200/10336], Loss: 2.8992\n",
      "Epoch [4/5], Step [1202/10336], Loss: 0.1413\n",
      "Epoch [4/5], Step [1204/10336], Loss: 1.2949\n",
      "Epoch [4/5], Step [1206/10336], Loss: 0.5954\n",
      "Epoch [4/5], Step [1208/10336], Loss: 0.1788\n",
      "Epoch [4/5], Step [1210/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [1212/10336], Loss: 1.4406\n",
      "Epoch [4/5], Step [1214/10336], Loss: 0.5098\n",
      "Epoch [4/5], Step [1216/10336], Loss: 0.0220\n",
      "Epoch [4/5], Step [1218/10336], Loss: 0.0576\n",
      "Epoch [4/5], Step [1220/10336], Loss: 0.7346\n",
      "Epoch [4/5], Step [1222/10336], Loss: 1.2010\n",
      "Epoch [4/5], Step [1224/10336], Loss: 0.5323\n",
      "Epoch [4/5], Step [1226/10336], Loss: 0.3936\n",
      "Epoch [4/5], Step [1228/10336], Loss: 0.1793\n",
      "Epoch [4/5], Step [1230/10336], Loss: 0.2020\n",
      "Epoch [4/5], Step [1232/10336], Loss: 0.0135\n",
      "Epoch [4/5], Step [1234/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [1236/10336], Loss: 1.2687\n",
      "Epoch [4/5], Step [1238/10336], Loss: 0.9678\n",
      "Epoch [4/5], Step [1240/10336], Loss: 0.0314\n",
      "Epoch [4/5], Step [1242/10336], Loss: 0.3733\n",
      "Epoch [4/5], Step [1244/10336], Loss: 1.2124\n",
      "Epoch [4/5], Step [1246/10336], Loss: 0.2351\n",
      "Epoch [4/5], Step [1248/10336], Loss: 0.1239\n",
      "Epoch [4/5], Step [1250/10336], Loss: 0.9538\n",
      "Epoch [4/5], Step [1252/10336], Loss: 0.3044\n",
      "Epoch [4/5], Step [1254/10336], Loss: 0.9491\n",
      "Epoch [4/5], Step [1256/10336], Loss: 0.1640\n",
      "Epoch [4/5], Step [1258/10336], Loss: 0.0976\n",
      "Epoch [4/5], Step [1260/10336], Loss: 0.6947\n",
      "Epoch [4/5], Step [1262/10336], Loss: 0.1553\n",
      "Epoch [4/5], Step [1264/10336], Loss: 0.0184\n",
      "Epoch [4/5], Step [1266/10336], Loss: 0.7724\n",
      "Epoch [4/5], Step [1268/10336], Loss: 0.0222\n",
      "Epoch [4/5], Step [1270/10336], Loss: 1.0778\n",
      "Epoch [4/5], Step [1272/10336], Loss: 0.1381\n",
      "Epoch [4/5], Step [1274/10336], Loss: 0.0151\n",
      "Epoch [4/5], Step [1276/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [1278/10336], Loss: 0.3777\n",
      "Epoch [4/5], Step [1280/10336], Loss: 2.8571\n",
      "Epoch [4/5], Step [1282/10336], Loss: 0.1185\n",
      "Epoch [4/5], Step [1284/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [1286/10336], Loss: 0.1221\n",
      "Epoch [4/5], Step [1288/10336], Loss: 0.0322\n",
      "Epoch [4/5], Step [1290/10336], Loss: 2.1946\n",
      "Epoch [4/5], Step [1292/10336], Loss: 0.4978\n",
      "Epoch [4/5], Step [1294/10336], Loss: 0.0550\n",
      "Epoch [4/5], Step [1296/10336], Loss: 0.1363\n",
      "Epoch [4/5], Step [1298/10336], Loss: 0.2634\n",
      "Epoch [4/5], Step [1300/10336], Loss: 1.6271\n",
      "Epoch [4/5], Step [1302/10336], Loss: 0.3370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [1304/10336], Loss: 0.8324\n",
      "Epoch [4/5], Step [1306/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [1308/10336], Loss: 0.4281\n",
      "Epoch [4/5], Step [1310/10336], Loss: 0.0260\n",
      "Epoch [4/5], Step [1312/10336], Loss: 0.2075\n",
      "Epoch [4/5], Step [1314/10336], Loss: 0.0096\n",
      "Epoch [4/5], Step [1316/10336], Loss: 1.4810\n",
      "Epoch [4/5], Step [1318/10336], Loss: 0.0269\n",
      "Epoch [4/5], Step [1320/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [1322/10336], Loss: 0.7248\n",
      "Epoch [4/5], Step [1324/10336], Loss: 0.9214\n",
      "Epoch [4/5], Step [1326/10336], Loss: 0.0509\n",
      "Epoch [4/5], Step [1328/10336], Loss: 0.0095\n",
      "Epoch [4/5], Step [1330/10336], Loss: 0.0333\n",
      "Epoch [4/5], Step [1332/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [1334/10336], Loss: 0.0542\n",
      "Epoch [4/5], Step [1336/10336], Loss: 0.1809\n",
      "Epoch [4/5], Step [1338/10336], Loss: 0.1625\n",
      "Epoch [4/5], Step [1340/10336], Loss: 0.0505\n",
      "Epoch [4/5], Step [1342/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [1344/10336], Loss: 0.5658\n",
      "Epoch [4/5], Step [1346/10336], Loss: 1.9217\n",
      "Epoch [4/5], Step [1348/10336], Loss: 0.0169\n",
      "Epoch [4/5], Step [1350/10336], Loss: 2.1620\n",
      "Epoch [4/5], Step [1352/10336], Loss: 0.0478\n",
      "Epoch [4/5], Step [1354/10336], Loss: 0.7734\n",
      "Epoch [4/5], Step [1356/10336], Loss: 0.1884\n",
      "Epoch [4/5], Step [1358/10336], Loss: 0.1191\n",
      "Epoch [4/5], Step [1360/10336], Loss: 0.0107\n",
      "Epoch [4/5], Step [1362/10336], Loss: 1.2636\n",
      "Epoch [4/5], Step [1364/10336], Loss: 0.3270\n",
      "Epoch [4/5], Step [1366/10336], Loss: 0.0132\n",
      "Epoch [4/5], Step [1368/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [1370/10336], Loss: 0.1328\n",
      "Epoch [4/5], Step [1372/10336], Loss: 0.0560\n",
      "Epoch [4/5], Step [1374/10336], Loss: 0.0104\n",
      "Epoch [4/5], Step [1376/10336], Loss: 3.1253\n",
      "Epoch [4/5], Step [1378/10336], Loss: 0.1433\n",
      "Epoch [4/5], Step [1380/10336], Loss: 0.0104\n",
      "Epoch [4/5], Step [1382/10336], Loss: 0.3500\n",
      "Epoch [4/5], Step [1384/10336], Loss: 0.1353\n",
      "Epoch [4/5], Step [1386/10336], Loss: 0.9558\n",
      "Epoch [4/5], Step [1388/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [1390/10336], Loss: 1.6142\n",
      "Epoch [4/5], Step [1392/10336], Loss: 0.8491\n",
      "Epoch [4/5], Step [1394/10336], Loss: 0.6746\n",
      "Epoch [4/5], Step [1396/10336], Loss: 0.0409\n",
      "Epoch [4/5], Step [1398/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [1400/10336], Loss: 1.0648\n",
      "Epoch [4/5], Step [1402/10336], Loss: 0.0110\n",
      "Epoch [4/5], Step [1404/10336], Loss: 0.7756\n",
      "Epoch [4/5], Step [1406/10336], Loss: 0.9594\n",
      "Epoch [4/5], Step [1408/10336], Loss: 0.0530\n",
      "Epoch [4/5], Step [1410/10336], Loss: 0.3729\n",
      "Epoch [4/5], Step [1412/10336], Loss: 0.1898\n",
      "Epoch [4/5], Step [1414/10336], Loss: 0.0074\n",
      "Epoch [4/5], Step [1416/10336], Loss: 0.8769\n",
      "Epoch [4/5], Step [1418/10336], Loss: 0.1048\n",
      "Epoch [4/5], Step [1420/10336], Loss: 0.2104\n",
      "Epoch [4/5], Step [1422/10336], Loss: 0.0890\n",
      "Epoch [4/5], Step [1424/10336], Loss: 0.2103\n",
      "Epoch [4/5], Step [1426/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [1428/10336], Loss: 0.2269\n",
      "Epoch [4/5], Step [1430/10336], Loss: 2.8989\n",
      "Epoch [4/5], Step [1432/10336], Loss: 1.3512\n",
      "Epoch [4/5], Step [1434/10336], Loss: 0.1179\n",
      "Epoch [4/5], Step [1436/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [1438/10336], Loss: 0.0808\n",
      "Epoch [4/5], Step [1440/10336], Loss: 0.0541\n",
      "Epoch [4/5], Step [1442/10336], Loss: 1.5445\n",
      "Epoch [4/5], Step [1444/10336], Loss: 1.1226\n",
      "Epoch [4/5], Step [1446/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [1448/10336], Loss: 0.1229\n",
      "Epoch [4/5], Step [1450/10336], Loss: 0.0490\n",
      "Epoch [4/5], Step [1452/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [1454/10336], Loss: 1.4606\n",
      "Epoch [4/5], Step [1456/10336], Loss: 1.6421\n",
      "Epoch [4/5], Step [1458/10336], Loss: 1.7054\n",
      "Epoch [4/5], Step [1460/10336], Loss: 1.7829\n",
      "Epoch [4/5], Step [1462/10336], Loss: 0.0756\n",
      "Epoch [4/5], Step [1464/10336], Loss: 0.0247\n",
      "Epoch [4/5], Step [1466/10336], Loss: 0.1186\n",
      "Epoch [4/5], Step [1468/10336], Loss: 0.6156\n",
      "Epoch [4/5], Step [1470/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [1472/10336], Loss: 1.5907\n",
      "Epoch [4/5], Step [1474/10336], Loss: 0.0252\n",
      "Epoch [4/5], Step [1476/10336], Loss: 0.0207\n",
      "Epoch [4/5], Step [1478/10336], Loss: 1.6825\n",
      "Epoch [4/5], Step [1480/10336], Loss: 1.2548\n",
      "Epoch [4/5], Step [1482/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [1484/10336], Loss: 0.5213\n",
      "Epoch [4/5], Step [1486/10336], Loss: 0.1015\n",
      "Epoch [4/5], Step [1488/10336], Loss: 1.5447\n",
      "Epoch [4/5], Step [1490/10336], Loss: 0.0421\n",
      "Epoch [4/5], Step [1492/10336], Loss: 0.0160\n",
      "Epoch [4/5], Step [1494/10336], Loss: 0.6128\n",
      "Epoch [4/5], Step [1496/10336], Loss: 0.0352\n",
      "Epoch [4/5], Step [1498/10336], Loss: 0.0285\n",
      "Epoch [4/5], Step [1500/10336], Loss: 0.0582\n",
      "Epoch [4/5], Step [1502/10336], Loss: 0.5598\n",
      "Epoch [4/5], Step [1504/10336], Loss: 0.1428\n",
      "Epoch [4/5], Step [1506/10336], Loss: 0.0113\n",
      "Epoch [4/5], Step [1508/10336], Loss: 0.2104\n",
      "Epoch [4/5], Step [1510/10336], Loss: 0.2093\n",
      "Epoch [4/5], Step [1512/10336], Loss: 2.2119\n",
      "Epoch [4/5], Step [1514/10336], Loss: 0.9728\n",
      "Epoch [4/5], Step [1516/10336], Loss: 0.0065\n",
      "Epoch [4/5], Step [1518/10336], Loss: 0.5963\n",
      "Epoch [4/5], Step [1520/10336], Loss: 0.0435\n",
      "Epoch [4/5], Step [1522/10336], Loss: 0.2386\n",
      "Epoch [4/5], Step [1524/10336], Loss: 0.9096\n",
      "Epoch [4/5], Step [1526/10336], Loss: 0.7970\n",
      "Epoch [4/5], Step [1528/10336], Loss: 0.0791\n",
      "Epoch [4/5], Step [1530/10336], Loss: 0.0048\n",
      "Epoch [4/5], Step [1532/10336], Loss: 0.4286\n",
      "Epoch [4/5], Step [1534/10336], Loss: 0.0828\n",
      "Epoch [4/5], Step [1536/10336], Loss: 0.0881\n",
      "Epoch [4/5], Step [1538/10336], Loss: 1.8475\n",
      "Epoch [4/5], Step [1540/10336], Loss: 0.3941\n",
      "Epoch [4/5], Step [1542/10336], Loss: 0.0940\n",
      "Epoch [4/5], Step [1544/10336], Loss: 0.0106\n",
      "Epoch [4/5], Step [1546/10336], Loss: 0.0087\n",
      "Epoch [4/5], Step [1548/10336], Loss: 0.0226\n",
      "Epoch [4/5], Step [1550/10336], Loss: 0.1425\n",
      "Epoch [4/5], Step [1552/10336], Loss: 1.7250\n",
      "Epoch [4/5], Step [1554/10336], Loss: 0.6482\n",
      "Epoch [4/5], Step [1556/10336], Loss: 0.0118\n",
      "Epoch [4/5], Step [1558/10336], Loss: 2.4582\n",
      "Epoch [4/5], Step [1560/10336], Loss: 5.6717\n",
      "Epoch [4/5], Step [1562/10336], Loss: 1.3567\n",
      "Epoch [4/5], Step [1564/10336], Loss: 2.9301\n",
      "Epoch [4/5], Step [1566/10336], Loss: 0.8251\n",
      "Epoch [4/5], Step [1568/10336], Loss: 0.0090\n",
      "Epoch [4/5], Step [1570/10336], Loss: 0.2237\n",
      "Epoch [4/5], Step [1572/10336], Loss: 0.1240\n",
      "Epoch [4/5], Step [1574/10336], Loss: 0.7434\n",
      "Epoch [4/5], Step [1576/10336], Loss: 2.9716\n",
      "Epoch [4/5], Step [1578/10336], Loss: 1.7551\n",
      "Epoch [4/5], Step [1580/10336], Loss: 0.0130\n",
      "Epoch [4/5], Step [1582/10336], Loss: 1.9528\n",
      "Epoch [4/5], Step [1584/10336], Loss: 0.4310\n",
      "Epoch [4/5], Step [1586/10336], Loss: 0.5204\n",
      "Epoch [4/5], Step [1588/10336], Loss: 0.4005\n",
      "Epoch [4/5], Step [1590/10336], Loss: 1.9451\n",
      "Epoch [4/5], Step [1592/10336], Loss: 2.8307\n",
      "Epoch [4/5], Step [1594/10336], Loss: 0.1209\n",
      "Epoch [4/5], Step [1596/10336], Loss: 2.7294\n",
      "Epoch [4/5], Step [1598/10336], Loss: 1.5084\n",
      "Epoch [4/5], Step [1600/10336], Loss: 0.4641\n",
      "Epoch [4/5], Step [1602/10336], Loss: 0.3250\n",
      "Epoch [4/5], Step [1604/10336], Loss: 2.4988\n",
      "Epoch [4/5], Step [1606/10336], Loss: 0.0522\n",
      "Epoch [4/5], Step [1608/10336], Loss: 0.6674\n",
      "Epoch [4/5], Step [1610/10336], Loss: 0.4595\n",
      "Epoch [4/5], Step [1612/10336], Loss: 0.2881\n",
      "Epoch [4/5], Step [1614/10336], Loss: 1.7309\n",
      "Epoch [4/5], Step [1616/10336], Loss: 0.0493\n",
      "Epoch [4/5], Step [1618/10336], Loss: 1.0832\n",
      "Epoch [4/5], Step [1620/10336], Loss: 1.3270\n",
      "Epoch [4/5], Step [1622/10336], Loss: 0.4304\n",
      "Epoch [4/5], Step [1624/10336], Loss: 1.5187\n",
      "Epoch [4/5], Step [1626/10336], Loss: 0.0959\n",
      "Epoch [4/5], Step [1628/10336], Loss: 0.2917\n",
      "Epoch [4/5], Step [1630/10336], Loss: 0.4278\n",
      "Epoch [4/5], Step [1632/10336], Loss: 0.0132\n",
      "Epoch [4/5], Step [1634/10336], Loss: 0.9243\n",
      "Epoch [4/5], Step [1636/10336], Loss: 1.7101\n",
      "Epoch [4/5], Step [1638/10336], Loss: 0.8469\n",
      "Epoch [4/5], Step [1640/10336], Loss: 0.8141\n",
      "Epoch [4/5], Step [1642/10336], Loss: 0.2623\n",
      "Epoch [4/5], Step [1644/10336], Loss: 0.9178\n",
      "Epoch [4/5], Step [1646/10336], Loss: 0.1606\n",
      "Epoch [4/5], Step [1648/10336], Loss: 0.0786\n",
      "Epoch [4/5], Step [1650/10336], Loss: 0.0429\n",
      "Epoch [4/5], Step [1652/10336], Loss: 0.3986\n",
      "Epoch [4/5], Step [1654/10336], Loss: 0.3281\n",
      "Epoch [4/5], Step [1656/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [1658/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [1660/10336], Loss: 1.6250\n",
      "Epoch [4/5], Step [1662/10336], Loss: 0.1076\n",
      "Epoch [4/5], Step [1664/10336], Loss: 0.6249\n",
      "Epoch [4/5], Step [1666/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [1668/10336], Loss: 0.1322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [1670/10336], Loss: 0.9510\n",
      "Epoch [4/5], Step [1672/10336], Loss: 0.2986\n",
      "Epoch [4/5], Step [1674/10336], Loss: 0.5645\n",
      "Epoch [4/5], Step [1676/10336], Loss: 0.2032\n",
      "Epoch [4/5], Step [1678/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [1680/10336], Loss: 0.9244\n",
      "Epoch [4/5], Step [1682/10336], Loss: 0.9646\n",
      "Epoch [4/5], Step [1684/10336], Loss: 0.3825\n",
      "Epoch [4/5], Step [1686/10336], Loss: 1.1340\n",
      "Epoch [4/5], Step [1688/10336], Loss: 1.8080\n",
      "Epoch [4/5], Step [1690/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [1692/10336], Loss: 0.5590\n",
      "Epoch [4/5], Step [1694/10336], Loss: 3.0171\n",
      "Epoch [4/5], Step [1696/10336], Loss: 2.0475\n",
      "Epoch [4/5], Step [1698/10336], Loss: 0.7986\n",
      "Epoch [4/5], Step [1700/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [1702/10336], Loss: 0.0470\n",
      "Epoch [4/5], Step [1704/10336], Loss: 0.2376\n",
      "Epoch [4/5], Step [1706/10336], Loss: 0.0031\n",
      "Epoch [4/5], Step [1708/10336], Loss: 0.2462\n",
      "Epoch [4/5], Step [1710/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [1712/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [1714/10336], Loss: 1.3974\n",
      "Epoch [4/5], Step [1716/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [1718/10336], Loss: 0.3582\n",
      "Epoch [4/5], Step [1720/10336], Loss: 0.6032\n",
      "Epoch [4/5], Step [1722/10336], Loss: 1.6479\n",
      "Epoch [4/5], Step [1724/10336], Loss: 0.1623\n",
      "Epoch [4/5], Step [1726/10336], Loss: 0.0066\n",
      "Epoch [4/5], Step [1728/10336], Loss: 1.6073\n",
      "Epoch [4/5], Step [1730/10336], Loss: 1.3882\n",
      "Epoch [4/5], Step [1732/10336], Loss: 0.0164\n",
      "Epoch [4/5], Step [1734/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [1736/10336], Loss: 0.0199\n",
      "Epoch [4/5], Step [1738/10336], Loss: 0.4707\n",
      "Epoch [4/5], Step [1740/10336], Loss: 0.0652\n",
      "Epoch [4/5], Step [1742/10336], Loss: 0.8670\n",
      "Epoch [4/5], Step [1744/10336], Loss: 1.4558\n",
      "Epoch [4/5], Step [1746/10336], Loss: 0.1322\n",
      "Epoch [4/5], Step [1748/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [1750/10336], Loss: 2.5984\n",
      "Epoch [4/5], Step [1752/10336], Loss: 0.0631\n",
      "Epoch [4/5], Step [1754/10336], Loss: 0.3129\n",
      "Epoch [4/5], Step [1756/10336], Loss: 0.4613\n",
      "Epoch [4/5], Step [1758/10336], Loss: 0.2175\n",
      "Epoch [4/5], Step [1760/10336], Loss: 0.4202\n",
      "Epoch [4/5], Step [1762/10336], Loss: 0.2116\n",
      "Epoch [4/5], Step [1764/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [1766/10336], Loss: 2.8690\n",
      "Epoch [4/5], Step [1768/10336], Loss: 0.0571\n",
      "Epoch [4/5], Step [1770/10336], Loss: 1.7891\n",
      "Epoch [4/5], Step [1772/10336], Loss: 3.7030\n",
      "Epoch [4/5], Step [1774/10336], Loss: 0.2475\n",
      "Epoch [4/5], Step [1776/10336], Loss: 0.1468\n",
      "Epoch [4/5], Step [1778/10336], Loss: 0.1703\n",
      "Epoch [4/5], Step [1780/10336], Loss: 0.6688\n",
      "Epoch [4/5], Step [1782/10336], Loss: 0.4101\n",
      "Epoch [4/5], Step [1784/10336], Loss: 1.0379\n",
      "Epoch [4/5], Step [1786/10336], Loss: 0.6207\n",
      "Epoch [4/5], Step [1788/10336], Loss: 0.0712\n",
      "Epoch [4/5], Step [1790/10336], Loss: 1.4378\n",
      "Epoch [4/5], Step [1792/10336], Loss: 3.9950\n",
      "Epoch [4/5], Step [1794/10336], Loss: 0.1034\n",
      "Epoch [4/5], Step [1796/10336], Loss: 2.6116\n",
      "Epoch [4/5], Step [1798/10336], Loss: 1.0258\n",
      "Epoch [4/5], Step [1800/10336], Loss: 3.5860\n",
      "Epoch [4/5], Step [1802/10336], Loss: 0.5970\n",
      "Epoch [4/5], Step [1804/10336], Loss: 0.4041\n",
      "Epoch [4/5], Step [1806/10336], Loss: 0.3797\n",
      "Epoch [4/5], Step [1808/10336], Loss: 0.4553\n",
      "Epoch [4/5], Step [1810/10336], Loss: 0.1047\n",
      "Epoch [4/5], Step [1812/10336], Loss: 0.2123\n",
      "Epoch [4/5], Step [1814/10336], Loss: 0.7250\n",
      "Epoch [4/5], Step [1816/10336], Loss: 0.0488\n",
      "Epoch [4/5], Step [1818/10336], Loss: 0.0064\n",
      "Epoch [4/5], Step [1820/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [1822/10336], Loss: 0.2598\n",
      "Epoch [4/5], Step [1824/10336], Loss: 1.5787\n",
      "Epoch [4/5], Step [1826/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [1828/10336], Loss: 1.5458\n",
      "Epoch [4/5], Step [1830/10336], Loss: 0.0095\n",
      "Epoch [4/5], Step [1832/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [1834/10336], Loss: 0.5368\n",
      "Epoch [4/5], Step [1836/10336], Loss: 0.3142\n",
      "Epoch [4/5], Step [1838/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [1840/10336], Loss: 2.0941\n",
      "Epoch [4/5], Step [1842/10336], Loss: 0.4908\n",
      "Epoch [4/5], Step [1844/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [1846/10336], Loss: 0.0211\n",
      "Epoch [4/5], Step [1848/10336], Loss: 1.4500\n",
      "Epoch [4/5], Step [1850/10336], Loss: 0.0168\n",
      "Epoch [4/5], Step [1852/10336], Loss: 0.9321\n",
      "Epoch [4/5], Step [1854/10336], Loss: 2.8797\n",
      "Epoch [4/5], Step [1856/10336], Loss: 0.1071\n",
      "Epoch [4/5], Step [1858/10336], Loss: 1.4668\n",
      "Epoch [4/5], Step [1860/10336], Loss: 0.1413\n",
      "Epoch [4/5], Step [1862/10336], Loss: 0.0060\n",
      "Epoch [4/5], Step [1864/10336], Loss: 0.0045\n",
      "Epoch [4/5], Step [1866/10336], Loss: 0.0434\n",
      "Epoch [4/5], Step [1868/10336], Loss: 0.0674\n",
      "Epoch [4/5], Step [1870/10336], Loss: 0.0087\n",
      "Epoch [4/5], Step [1872/10336], Loss: 0.0356\n",
      "Epoch [4/5], Step [1874/10336], Loss: 0.3545\n",
      "Epoch [4/5], Step [1876/10336], Loss: 1.0773\n",
      "Epoch [4/5], Step [1878/10336], Loss: 2.8780\n",
      "Epoch [4/5], Step [1880/10336], Loss: 0.2229\n",
      "Epoch [4/5], Step [1882/10336], Loss: 0.0098\n",
      "Epoch [4/5], Step [1884/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [1886/10336], Loss: 0.0608\n",
      "Epoch [4/5], Step [1888/10336], Loss: 0.1759\n",
      "Epoch [4/5], Step [1890/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [1892/10336], Loss: 0.3960\n",
      "Epoch [4/5], Step [1894/10336], Loss: 0.8020\n",
      "Epoch [4/5], Step [1896/10336], Loss: 0.0110\n",
      "Epoch [4/5], Step [1898/10336], Loss: 0.9916\n",
      "Epoch [4/5], Step [1900/10336], Loss: 1.1014\n",
      "Epoch [4/5], Step [1902/10336], Loss: 0.2603\n",
      "Epoch [4/5], Step [1904/10336], Loss: 0.3188\n",
      "Epoch [4/5], Step [1906/10336], Loss: 0.0458\n",
      "Epoch [4/5], Step [1908/10336], Loss: 0.0326\n",
      "Epoch [4/5], Step [1910/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [1912/10336], Loss: 0.0177\n",
      "Epoch [4/5], Step [1914/10336], Loss: 2.4140\n",
      "Epoch [4/5], Step [1916/10336], Loss: 0.0490\n",
      "Epoch [4/5], Step [1918/10336], Loss: 0.0244\n",
      "Epoch [4/5], Step [1920/10336], Loss: 0.2322\n",
      "Epoch [4/5], Step [1922/10336], Loss: 0.6376\n",
      "Epoch [4/5], Step [1924/10336], Loss: 1.4819\n",
      "Epoch [4/5], Step [1926/10336], Loss: 0.0103\n",
      "Epoch [4/5], Step [1928/10336], Loss: 0.4075\n",
      "Epoch [4/5], Step [1930/10336], Loss: 0.0535\n",
      "Epoch [4/5], Step [1932/10336], Loss: 0.0139\n",
      "Epoch [4/5], Step [1934/10336], Loss: 0.1574\n",
      "Epoch [4/5], Step [1936/10336], Loss: 0.0091\n",
      "Epoch [4/5], Step [1938/10336], Loss: 0.8615\n",
      "Epoch [4/5], Step [1940/10336], Loss: 0.0242\n",
      "Epoch [4/5], Step [1942/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [1944/10336], Loss: 0.5238\n",
      "Epoch [4/5], Step [1946/10336], Loss: 0.2269\n",
      "Epoch [4/5], Step [1948/10336], Loss: 0.3038\n",
      "Epoch [4/5], Step [1950/10336], Loss: 1.4850\n",
      "Epoch [4/5], Step [1952/10336], Loss: 1.0369\n",
      "Epoch [4/5], Step [1954/10336], Loss: 0.5342\n",
      "Epoch [4/5], Step [1956/10336], Loss: 0.1745\n",
      "Epoch [4/5], Step [1958/10336], Loss: 0.0447\n",
      "Epoch [4/5], Step [1960/10336], Loss: 0.0255\n",
      "Epoch [4/5], Step [1962/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [1964/10336], Loss: 0.0029\n",
      "Epoch [4/5], Step [1966/10336], Loss: 1.3206\n",
      "Epoch [4/5], Step [1968/10336], Loss: 1.4959\n",
      "Epoch [4/5], Step [1970/10336], Loss: 0.0457\n",
      "Epoch [4/5], Step [1972/10336], Loss: 0.3926\n",
      "Epoch [4/5], Step [1974/10336], Loss: 0.4848\n",
      "Epoch [4/5], Step [1976/10336], Loss: 0.0361\n",
      "Epoch [4/5], Step [1978/10336], Loss: 0.3858\n",
      "Epoch [4/5], Step [1980/10336], Loss: 2.2535\n",
      "Epoch [4/5], Step [1982/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [1984/10336], Loss: 0.1085\n",
      "Epoch [4/5], Step [1986/10336], Loss: 3.8280\n",
      "Epoch [4/5], Step [1988/10336], Loss: 0.4178\n",
      "Epoch [4/5], Step [1990/10336], Loss: 0.0933\n",
      "Epoch [4/5], Step [1992/10336], Loss: 3.8398\n",
      "Epoch [4/5], Step [1994/10336], Loss: 1.1318\n",
      "Epoch [4/5], Step [1996/10336], Loss: 0.5081\n",
      "Epoch [4/5], Step [1998/10336], Loss: 0.6977\n",
      "Epoch [4/5], Step [2000/10336], Loss: 0.0347\n",
      "Epoch [4/5], Step [2002/10336], Loss: 1.6654\n",
      "Epoch [4/5], Step [2004/10336], Loss: 1.5722\n",
      "Epoch [4/5], Step [2006/10336], Loss: 0.6272\n",
      "Epoch [4/5], Step [2008/10336], Loss: 0.0265\n",
      "Epoch [4/5], Step [2010/10336], Loss: 1.3223\n",
      "Epoch [4/5], Step [2012/10336], Loss: 0.1221\n",
      "Epoch [4/5], Step [2014/10336], Loss: 0.7544\n",
      "Epoch [4/5], Step [2016/10336], Loss: 0.2117\n",
      "Epoch [4/5], Step [2018/10336], Loss: 0.0723\n",
      "Epoch [4/5], Step [2020/10336], Loss: 0.2012\n",
      "Epoch [4/5], Step [2022/10336], Loss: 0.2337\n",
      "Epoch [4/5], Step [2024/10336], Loss: 0.1593\n",
      "Epoch [4/5], Step [2026/10336], Loss: 1.3258\n",
      "Epoch [4/5], Step [2028/10336], Loss: 0.2331\n",
      "Epoch [4/5], Step [2030/10336], Loss: 0.4116\n",
      "Epoch [4/5], Step [2032/10336], Loss: 0.1440\n",
      "Epoch [4/5], Step [2034/10336], Loss: 1.9641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [2036/10336], Loss: 0.2290\n",
      "Epoch [4/5], Step [2038/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [2040/10336], Loss: 2.2070\n",
      "Epoch [4/5], Step [2042/10336], Loss: 0.0161\n",
      "Epoch [4/5], Step [2044/10336], Loss: 0.0504\n",
      "Epoch [4/5], Step [2046/10336], Loss: 0.0880\n",
      "Epoch [4/5], Step [2048/10336], Loss: 0.0347\n",
      "Epoch [4/5], Step [2050/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [2052/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [2054/10336], Loss: 0.3075\n",
      "Epoch [4/5], Step [2056/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [2058/10336], Loss: 0.8157\n",
      "Epoch [4/5], Step [2060/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [2062/10336], Loss: 1.8366\n",
      "Epoch [4/5], Step [2064/10336], Loss: 0.4366\n",
      "Epoch [4/5], Step [2066/10336], Loss: 0.0857\n",
      "Epoch [4/5], Step [2068/10336], Loss: 0.0550\n",
      "Epoch [4/5], Step [2070/10336], Loss: 0.8455\n",
      "Epoch [4/5], Step [2072/10336], Loss: 1.4540\n",
      "Epoch [4/5], Step [2074/10336], Loss: 1.8110\n",
      "Epoch [4/5], Step [2076/10336], Loss: 0.5030\n",
      "Epoch [4/5], Step [2078/10336], Loss: 0.3084\n",
      "Epoch [4/5], Step [2080/10336], Loss: 0.2466\n",
      "Epoch [4/5], Step [2082/10336], Loss: 2.4487\n",
      "Epoch [4/5], Step [2084/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [2086/10336], Loss: 1.1763\n",
      "Epoch [4/5], Step [2088/10336], Loss: 0.0428\n",
      "Epoch [4/5], Step [2090/10336], Loss: 0.0552\n",
      "Epoch [4/5], Step [2092/10336], Loss: 1.7031\n",
      "Epoch [4/5], Step [2094/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [2096/10336], Loss: 2.0770\n",
      "Epoch [4/5], Step [2098/10336], Loss: 0.0852\n",
      "Epoch [4/5], Step [2100/10336], Loss: 0.2011\n",
      "Epoch [4/5], Step [2102/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [2104/10336], Loss: 1.1137\n",
      "Epoch [4/5], Step [2106/10336], Loss: 0.6501\n",
      "Epoch [4/5], Step [2108/10336], Loss: 0.1860\n",
      "Epoch [4/5], Step [2110/10336], Loss: 1.8494\n",
      "Epoch [4/5], Step [2112/10336], Loss: 0.0164\n",
      "Epoch [4/5], Step [2114/10336], Loss: 0.6647\n",
      "Epoch [4/5], Step [2116/10336], Loss: 0.0222\n",
      "Epoch [4/5], Step [2118/10336], Loss: 0.6211\n",
      "Epoch [4/5], Step [2120/10336], Loss: 0.4932\n",
      "Epoch [4/5], Step [2122/10336], Loss: 0.0853\n",
      "Epoch [4/5], Step [2124/10336], Loss: 0.0513\n",
      "Epoch [4/5], Step [2126/10336], Loss: 0.2896\n",
      "Epoch [4/5], Step [2128/10336], Loss: 1.0271\n",
      "Epoch [4/5], Step [2130/10336], Loss: 1.7773\n",
      "Epoch [4/5], Step [2132/10336], Loss: 2.5403\n",
      "Epoch [4/5], Step [2134/10336], Loss: 0.5143\n",
      "Epoch [4/5], Step [2136/10336], Loss: 0.0660\n",
      "Epoch [4/5], Step [2138/10336], Loss: 0.2711\n",
      "Epoch [4/5], Step [2140/10336], Loss: 1.2435\n",
      "Epoch [4/5], Step [2142/10336], Loss: 2.4859\n",
      "Epoch [4/5], Step [2144/10336], Loss: 0.6224\n",
      "Epoch [4/5], Step [2146/10336], Loss: 0.2670\n",
      "Epoch [4/5], Step [2148/10336], Loss: 0.0457\n",
      "Epoch [4/5], Step [2150/10336], Loss: 3.8349\n",
      "Epoch [4/5], Step [2152/10336], Loss: 0.1561\n",
      "Epoch [4/5], Step [2154/10336], Loss: 0.5635\n",
      "Epoch [4/5], Step [2156/10336], Loss: 0.0247\n",
      "Epoch [4/5], Step [2158/10336], Loss: 0.2174\n",
      "Epoch [4/5], Step [2160/10336], Loss: 1.4143\n",
      "Epoch [4/5], Step [2162/10336], Loss: 0.6155\n",
      "Epoch [4/5], Step [2164/10336], Loss: 0.0655\n",
      "Epoch [4/5], Step [2166/10336], Loss: 0.3782\n",
      "Epoch [4/5], Step [2168/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [2170/10336], Loss: 0.0287\n",
      "Epoch [4/5], Step [2172/10336], Loss: 0.0151\n",
      "Epoch [4/5], Step [2174/10336], Loss: 2.8962\n",
      "Epoch [4/5], Step [2176/10336], Loss: 1.3926\n",
      "Epoch [4/5], Step [2178/10336], Loss: 0.3224\n",
      "Epoch [4/5], Step [2180/10336], Loss: 0.0455\n",
      "Epoch [4/5], Step [2182/10336], Loss: 0.1713\n",
      "Epoch [4/5], Step [2184/10336], Loss: 0.0801\n",
      "Epoch [4/5], Step [2186/10336], Loss: 0.0329\n",
      "Epoch [4/5], Step [2188/10336], Loss: 1.6588\n",
      "Epoch [4/5], Step [2190/10336], Loss: 0.1147\n",
      "Epoch [4/5], Step [2192/10336], Loss: 0.5935\n",
      "Epoch [4/5], Step [2194/10336], Loss: 0.1100\n",
      "Epoch [4/5], Step [2196/10336], Loss: 0.7352\n",
      "Epoch [4/5], Step [2198/10336], Loss: 0.5660\n",
      "Epoch [4/5], Step [2200/10336], Loss: 0.1755\n",
      "Epoch [4/5], Step [2202/10336], Loss: 0.1044\n",
      "Epoch [4/5], Step [2204/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [2206/10336], Loss: 0.1870\n",
      "Epoch [4/5], Step [2208/10336], Loss: 0.8424\n",
      "Epoch [4/5], Step [2210/10336], Loss: 0.2323\n",
      "Epoch [4/5], Step [2212/10336], Loss: 0.2898\n",
      "Epoch [4/5], Step [2214/10336], Loss: 0.6365\n",
      "Epoch [4/5], Step [2216/10336], Loss: 0.0856\n",
      "Epoch [4/5], Step [2218/10336], Loss: 0.4229\n",
      "Epoch [4/5], Step [2220/10336], Loss: 0.0081\n",
      "Epoch [4/5], Step [2222/10336], Loss: 2.7619\n",
      "Epoch [4/5], Step [2224/10336], Loss: 0.0045\n",
      "Epoch [4/5], Step [2226/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [2228/10336], Loss: 0.2056\n",
      "Epoch [4/5], Step [2230/10336], Loss: 0.0108\n",
      "Epoch [4/5], Step [2232/10336], Loss: 0.0332\n",
      "Epoch [4/5], Step [2234/10336], Loss: 0.0259\n",
      "Epoch [4/5], Step [2236/10336], Loss: 0.0361\n",
      "Epoch [4/5], Step [2238/10336], Loss: 0.1593\n",
      "Epoch [4/5], Step [2240/10336], Loss: 0.8246\n",
      "Epoch [4/5], Step [2242/10336], Loss: 0.8436\n",
      "Epoch [4/5], Step [2244/10336], Loss: 0.1802\n",
      "Epoch [4/5], Step [2246/10336], Loss: 0.0428\n",
      "Epoch [4/5], Step [2248/10336], Loss: 2.0791\n",
      "Epoch [4/5], Step [2250/10336], Loss: 0.6932\n",
      "Epoch [4/5], Step [2252/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [2254/10336], Loss: 0.1132\n",
      "Epoch [4/5], Step [2256/10336], Loss: 0.0205\n",
      "Epoch [4/5], Step [2258/10336], Loss: 0.6233\n",
      "Epoch [4/5], Step [2260/10336], Loss: 0.0307\n",
      "Epoch [4/5], Step [2262/10336], Loss: 1.5623\n",
      "Epoch [4/5], Step [2264/10336], Loss: 0.0143\n",
      "Epoch [4/5], Step [2266/10336], Loss: 0.6329\n",
      "Epoch [4/5], Step [2268/10336], Loss: 0.0514\n",
      "Epoch [4/5], Step [2270/10336], Loss: 2.2382\n",
      "Epoch [4/5], Step [2272/10336], Loss: 0.0157\n",
      "Epoch [4/5], Step [2274/10336], Loss: 1.6835\n",
      "Epoch [4/5], Step [2276/10336], Loss: 0.0369\n",
      "Epoch [4/5], Step [2278/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [2280/10336], Loss: 0.0116\n",
      "Epoch [4/5], Step [2282/10336], Loss: 0.0731\n",
      "Epoch [4/5], Step [2284/10336], Loss: 1.2298\n",
      "Epoch [4/5], Step [2286/10336], Loss: 0.0270\n",
      "Epoch [4/5], Step [2288/10336], Loss: 2.3719\n",
      "Epoch [4/5], Step [2290/10336], Loss: 0.2511\n",
      "Epoch [4/5], Step [2292/10336], Loss: 0.4002\n",
      "Epoch [4/5], Step [2294/10336], Loss: 0.4063\n",
      "Epoch [4/5], Step [2296/10336], Loss: 0.3262\n",
      "Epoch [4/5], Step [2298/10336], Loss: 0.2874\n",
      "Epoch [4/5], Step [2300/10336], Loss: 0.3606\n",
      "Epoch [4/5], Step [2302/10336], Loss: 0.1497\n",
      "Epoch [4/5], Step [2304/10336], Loss: 0.0628\n",
      "Epoch [4/5], Step [2306/10336], Loss: 2.1045\n",
      "Epoch [4/5], Step [2308/10336], Loss: 0.5391\n",
      "Epoch [4/5], Step [2310/10336], Loss: 0.0542\n",
      "Epoch [4/5], Step [2312/10336], Loss: 2.5472\n",
      "Epoch [4/5], Step [2314/10336], Loss: 0.0937\n",
      "Epoch [4/5], Step [2316/10336], Loss: 0.1638\n",
      "Epoch [4/5], Step [2318/10336], Loss: 0.2753\n",
      "Epoch [4/5], Step [2320/10336], Loss: 0.2174\n",
      "Epoch [4/5], Step [2322/10336], Loss: 0.2035\n",
      "Epoch [4/5], Step [2324/10336], Loss: 0.5648\n",
      "Epoch [4/5], Step [2326/10336], Loss: 0.0341\n",
      "Epoch [4/5], Step [2328/10336], Loss: 0.5077\n",
      "Epoch [4/5], Step [2330/10336], Loss: 1.9071\n",
      "Epoch [4/5], Step [2332/10336], Loss: 1.5204\n",
      "Epoch [4/5], Step [2334/10336], Loss: 0.0384\n",
      "Epoch [4/5], Step [2336/10336], Loss: 0.0041\n",
      "Epoch [4/5], Step [2338/10336], Loss: 0.5296\n",
      "Epoch [4/5], Step [2340/10336], Loss: 0.7251\n",
      "Epoch [4/5], Step [2342/10336], Loss: 0.0087\n",
      "Epoch [4/5], Step [2344/10336], Loss: 0.0061\n",
      "Epoch [4/5], Step [2346/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [2348/10336], Loss: 0.1898\n",
      "Epoch [4/5], Step [2350/10336], Loss: 0.6054\n",
      "Epoch [4/5], Step [2352/10336], Loss: 0.2544\n",
      "Epoch [4/5], Step [2354/10336], Loss: 0.0162\n",
      "Epoch [4/5], Step [2356/10336], Loss: 0.0638\n",
      "Epoch [4/5], Step [2358/10336], Loss: 6.5549\n",
      "Epoch [4/5], Step [2360/10336], Loss: 0.0168\n",
      "Epoch [4/5], Step [2362/10336], Loss: 0.5789\n",
      "Epoch [4/5], Step [2364/10336], Loss: 0.1728\n",
      "Epoch [4/5], Step [2366/10336], Loss: 1.7738\n",
      "Epoch [4/5], Step [2368/10336], Loss: 1.0818\n",
      "Epoch [4/5], Step [2370/10336], Loss: 3.4138\n",
      "Epoch [4/5], Step [2372/10336], Loss: 0.1463\n",
      "Epoch [4/5], Step [2374/10336], Loss: 0.4146\n",
      "Epoch [4/5], Step [2376/10336], Loss: 0.0415\n",
      "Epoch [4/5], Step [2378/10336], Loss: 0.2145\n",
      "Epoch [4/5], Step [2380/10336], Loss: 0.2104\n",
      "Epoch [4/5], Step [2382/10336], Loss: 1.1030\n",
      "Epoch [4/5], Step [2384/10336], Loss: 0.3383\n",
      "Epoch [4/5], Step [2386/10336], Loss: 0.0184\n",
      "Epoch [4/5], Step [2388/10336], Loss: 0.0305\n",
      "Epoch [4/5], Step [2390/10336], Loss: 0.0961\n",
      "Epoch [4/5], Step [2392/10336], Loss: 0.0341\n",
      "Epoch [4/5], Step [2394/10336], Loss: 1.3442\n",
      "Epoch [4/5], Step [2396/10336], Loss: 1.7788\n",
      "Epoch [4/5], Step [2398/10336], Loss: 0.2310\n",
      "Epoch [4/5], Step [2400/10336], Loss: 0.8390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [2402/10336], Loss: 1.8071\n",
      "Epoch [4/5], Step [2404/10336], Loss: 0.7086\n",
      "Epoch [4/5], Step [2406/10336], Loss: 0.0497\n",
      "Epoch [4/5], Step [2408/10336], Loss: 0.8494\n",
      "Epoch [4/5], Step [2410/10336], Loss: 0.6322\n",
      "Epoch [4/5], Step [2412/10336], Loss: 0.5954\n",
      "Epoch [4/5], Step [2414/10336], Loss: 0.4002\n",
      "Epoch [4/5], Step [2416/10336], Loss: 0.3486\n",
      "Epoch [4/5], Step [2418/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [2420/10336], Loss: 0.2843\n",
      "Epoch [4/5], Step [2422/10336], Loss: 0.1628\n",
      "Epoch [4/5], Step [2424/10336], Loss: 3.1982\n",
      "Epoch [4/5], Step [2426/10336], Loss: 0.7966\n",
      "Epoch [4/5], Step [2428/10336], Loss: 0.0165\n",
      "Epoch [4/5], Step [2430/10336], Loss: 0.3639\n",
      "Epoch [4/5], Step [2432/10336], Loss: 0.3072\n",
      "Epoch [4/5], Step [2434/10336], Loss: 0.2562\n",
      "Epoch [4/5], Step [2436/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [2438/10336], Loss: 0.1568\n",
      "Epoch [4/5], Step [2440/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [2442/10336], Loss: 1.9756\n",
      "Epoch [4/5], Step [2444/10336], Loss: 0.2343\n",
      "Epoch [4/5], Step [2446/10336], Loss: 0.1336\n",
      "Epoch [4/5], Step [2448/10336], Loss: 0.0900\n",
      "Epoch [4/5], Step [2450/10336], Loss: 0.0079\n",
      "Epoch [4/5], Step [2452/10336], Loss: 1.8180\n",
      "Epoch [4/5], Step [2454/10336], Loss: 1.0905\n",
      "Epoch [4/5], Step [2456/10336], Loss: 0.1089\n",
      "Epoch [4/5], Step [2458/10336], Loss: 0.1463\n",
      "Epoch [4/5], Step [2460/10336], Loss: 1.6000\n",
      "Epoch [4/5], Step [2462/10336], Loss: 1.5770\n",
      "Epoch [4/5], Step [2464/10336], Loss: 1.1342\n",
      "Epoch [4/5], Step [2466/10336], Loss: 0.9283\n",
      "Epoch [4/5], Step [2468/10336], Loss: 2.4492\n",
      "Epoch [4/5], Step [2470/10336], Loss: 0.6500\n",
      "Epoch [4/5], Step [2472/10336], Loss: 2.0524\n",
      "Epoch [4/5], Step [2474/10336], Loss: 4.1488\n",
      "Epoch [4/5], Step [2476/10336], Loss: 0.0088\n",
      "Epoch [4/5], Step [2478/10336], Loss: 2.1172\n",
      "Epoch [4/5], Step [2480/10336], Loss: 0.0703\n",
      "Epoch [4/5], Step [2482/10336], Loss: 0.0309\n",
      "Epoch [4/5], Step [2484/10336], Loss: 0.5277\n",
      "Epoch [4/5], Step [2486/10336], Loss: 0.0032\n",
      "Epoch [4/5], Step [2488/10336], Loss: 1.6960\n",
      "Epoch [4/5], Step [2490/10336], Loss: 1.1796\n",
      "Epoch [4/5], Step [2492/10336], Loss: 0.3094\n",
      "Epoch [4/5], Step [2494/10336], Loss: 3.0738\n",
      "Epoch [4/5], Step [2496/10336], Loss: 0.8264\n",
      "Epoch [4/5], Step [2498/10336], Loss: 0.1810\n",
      "Epoch [4/5], Step [2500/10336], Loss: 0.4717\n",
      "Epoch [4/5], Step [2502/10336], Loss: 1.0593\n",
      "Epoch [4/5], Step [2504/10336], Loss: 0.7973\n",
      "Epoch [4/5], Step [2506/10336], Loss: 0.0184\n",
      "Epoch [4/5], Step [2508/10336], Loss: 0.0088\n",
      "Epoch [4/5], Step [2510/10336], Loss: 0.0468\n",
      "Epoch [4/5], Step [2512/10336], Loss: 0.7497\n",
      "Epoch [4/5], Step [2514/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [2516/10336], Loss: 0.1481\n",
      "Epoch [4/5], Step [2518/10336], Loss: 0.4258\n",
      "Epoch [4/5], Step [2520/10336], Loss: 3.1364\n",
      "Epoch [4/5], Step [2522/10336], Loss: 0.0344\n",
      "Epoch [4/5], Step [2524/10336], Loss: 0.3609\n",
      "Epoch [4/5], Step [2526/10336], Loss: 1.8096\n",
      "Epoch [4/5], Step [2528/10336], Loss: 0.5935\n",
      "Epoch [4/5], Step [2530/10336], Loss: 0.2473\n",
      "Epoch [4/5], Step [2532/10336], Loss: 0.5183\n",
      "Epoch [4/5], Step [2534/10336], Loss: 0.0691\n",
      "Epoch [4/5], Step [2536/10336], Loss: 0.1928\n",
      "Epoch [4/5], Step [2538/10336], Loss: 0.6613\n",
      "Epoch [4/5], Step [2540/10336], Loss: 0.8172\n",
      "Epoch [4/5], Step [2542/10336], Loss: 0.8298\n",
      "Epoch [4/5], Step [2544/10336], Loss: 3.0219\n",
      "Epoch [4/5], Step [2546/10336], Loss: 0.9241\n",
      "Epoch [4/5], Step [2548/10336], Loss: 1.7685\n",
      "Epoch [4/5], Step [2550/10336], Loss: 0.0539\n",
      "Epoch [4/5], Step [2552/10336], Loss: 0.0292\n",
      "Epoch [4/5], Step [2554/10336], Loss: 0.0785\n",
      "Epoch [4/5], Step [2556/10336], Loss: 0.0085\n",
      "Epoch [4/5], Step [2558/10336], Loss: 4.5137\n",
      "Epoch [4/5], Step [2560/10336], Loss: 1.4860\n",
      "Epoch [4/5], Step [2562/10336], Loss: 1.6044\n",
      "Epoch [4/5], Step [2564/10336], Loss: 0.3687\n",
      "Epoch [4/5], Step [2566/10336], Loss: 1.1085\n",
      "Epoch [4/5], Step [2568/10336], Loss: 0.4893\n",
      "Epoch [4/5], Step [2570/10336], Loss: 1.3657\n",
      "Epoch [4/5], Step [2572/10336], Loss: 1.2262\n",
      "Epoch [4/5], Step [2574/10336], Loss: 0.5829\n",
      "Epoch [4/5], Step [2576/10336], Loss: 0.0587\n",
      "Epoch [4/5], Step [2578/10336], Loss: 1.5776\n",
      "Epoch [4/5], Step [2580/10336], Loss: 0.4134\n",
      "Epoch [4/5], Step [2582/10336], Loss: 1.4180\n",
      "Epoch [4/5], Step [2584/10336], Loss: 0.8506\n",
      "Epoch [4/5], Step [2586/10336], Loss: 0.5522\n",
      "Epoch [4/5], Step [2588/10336], Loss: 2.9362\n",
      "Epoch [4/5], Step [2590/10336], Loss: 1.4492\n",
      "Epoch [4/5], Step [2592/10336], Loss: 0.0191\n",
      "Epoch [4/5], Step [2594/10336], Loss: 2.2999\n",
      "Epoch [4/5], Step [2596/10336], Loss: 1.7926\n",
      "Epoch [4/5], Step [2598/10336], Loss: 2.8149\n",
      "Epoch [4/5], Step [2600/10336], Loss: 0.0716\n",
      "Epoch [4/5], Step [2602/10336], Loss: 0.2011\n",
      "Epoch [4/5], Step [2604/10336], Loss: 0.0616\n",
      "Epoch [4/5], Step [2606/10336], Loss: 1.4288\n",
      "Epoch [4/5], Step [2608/10336], Loss: 0.0156\n",
      "Epoch [4/5], Step [2610/10336], Loss: 1.6427\n",
      "Epoch [4/5], Step [2612/10336], Loss: 0.4745\n",
      "Epoch [4/5], Step [2614/10336], Loss: 0.0414\n",
      "Epoch [4/5], Step [2616/10336], Loss: 0.2553\n",
      "Epoch [4/5], Step [2618/10336], Loss: 0.0253\n",
      "Epoch [4/5], Step [2620/10336], Loss: 0.5783\n",
      "Epoch [4/5], Step [2622/10336], Loss: 0.3127\n",
      "Epoch [4/5], Step [2624/10336], Loss: 0.5461\n",
      "Epoch [4/5], Step [2626/10336], Loss: 0.1502\n",
      "Epoch [4/5], Step [2628/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [2630/10336], Loss: 0.2215\n",
      "Epoch [4/5], Step [2632/10336], Loss: 0.0099\n",
      "Epoch [4/5], Step [2634/10336], Loss: 0.0382\n",
      "Epoch [4/5], Step [2636/10336], Loss: 0.2359\n",
      "Epoch [4/5], Step [2638/10336], Loss: 0.2423\n",
      "Epoch [4/5], Step [2640/10336], Loss: 1.5430\n",
      "Epoch [4/5], Step [2642/10336], Loss: 0.4382\n",
      "Epoch [4/5], Step [2644/10336], Loss: 0.3589\n",
      "Epoch [4/5], Step [2646/10336], Loss: 0.1312\n",
      "Epoch [4/5], Step [2648/10336], Loss: 0.2891\n",
      "Epoch [4/5], Step [2650/10336], Loss: 0.6610\n",
      "Epoch [4/5], Step [2652/10336], Loss: 1.4265\n",
      "Epoch [4/5], Step [2654/10336], Loss: 0.3682\n",
      "Epoch [4/5], Step [2656/10336], Loss: 1.9355\n",
      "Epoch [4/5], Step [2658/10336], Loss: 0.3872\n",
      "Epoch [4/5], Step [2660/10336], Loss: 1.4269\n",
      "Epoch [4/5], Step [2662/10336], Loss: 0.2058\n",
      "Epoch [4/5], Step [2664/10336], Loss: 0.0092\n",
      "Epoch [4/5], Step [2666/10336], Loss: 1.0993\n",
      "Epoch [4/5], Step [2668/10336], Loss: 0.6186\n",
      "Epoch [4/5], Step [2670/10336], Loss: 0.7320\n",
      "Epoch [4/5], Step [2672/10336], Loss: 0.1829\n",
      "Epoch [4/5], Step [2674/10336], Loss: 0.3964\n",
      "Epoch [4/5], Step [2676/10336], Loss: 0.4133\n",
      "Epoch [4/5], Step [2678/10336], Loss: 1.7898\n",
      "Epoch [4/5], Step [2680/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [2682/10336], Loss: 0.0060\n",
      "Epoch [4/5], Step [2684/10336], Loss: 2.1307\n",
      "Epoch [4/5], Step [2686/10336], Loss: 0.9402\n",
      "Epoch [4/5], Step [2688/10336], Loss: 0.3764\n",
      "Epoch [4/5], Step [2690/10336], Loss: 2.3440\n",
      "Epoch [4/5], Step [2692/10336], Loss: 0.0389\n",
      "Epoch [4/5], Step [2694/10336], Loss: 0.0095\n",
      "Epoch [4/5], Step [2696/10336], Loss: 0.2533\n",
      "Epoch [4/5], Step [2698/10336], Loss: 0.1137\n",
      "Epoch [4/5], Step [2700/10336], Loss: 2.0934\n",
      "Epoch [4/5], Step [2702/10336], Loss: 0.3837\n",
      "Epoch [4/5], Step [2704/10336], Loss: 0.0572\n",
      "Epoch [4/5], Step [2706/10336], Loss: 0.0103\n",
      "Epoch [4/5], Step [2708/10336], Loss: 0.3172\n",
      "Epoch [4/5], Step [2710/10336], Loss: 0.8017\n",
      "Epoch [4/5], Step [2712/10336], Loss: 0.5354\n",
      "Epoch [4/5], Step [2714/10336], Loss: 0.0474\n",
      "Epoch [4/5], Step [2716/10336], Loss: 2.5515\n",
      "Epoch [4/5], Step [2718/10336], Loss: 0.0888\n",
      "Epoch [4/5], Step [2720/10336], Loss: 0.1697\n",
      "Epoch [4/5], Step [2722/10336], Loss: 3.9316\n",
      "Epoch [4/5], Step [2724/10336], Loss: 5.5432\n",
      "Epoch [4/5], Step [2726/10336], Loss: 0.2926\n",
      "Epoch [4/5], Step [2728/10336], Loss: 1.3180\n",
      "Epoch [4/5], Step [2730/10336], Loss: 0.4843\n",
      "Epoch [4/5], Step [2732/10336], Loss: 0.1690\n",
      "Epoch [4/5], Step [2734/10336], Loss: 0.0744\n",
      "Epoch [4/5], Step [2736/10336], Loss: 0.8311\n",
      "Epoch [4/5], Step [2738/10336], Loss: 0.0339\n",
      "Epoch [4/5], Step [2740/10336], Loss: 1.3407\n",
      "Epoch [4/5], Step [2742/10336], Loss: 0.0240\n",
      "Epoch [4/5], Step [2744/10336], Loss: 0.9123\n",
      "Epoch [4/5], Step [2746/10336], Loss: 0.0204\n",
      "Epoch [4/5], Step [2748/10336], Loss: 0.5700\n",
      "Epoch [4/5], Step [2750/10336], Loss: 0.0171\n",
      "Epoch [4/5], Step [2752/10336], Loss: 0.2337\n",
      "Epoch [4/5], Step [2754/10336], Loss: 1.7803\n",
      "Epoch [4/5], Step [2756/10336], Loss: 2.2259\n",
      "Epoch [4/5], Step [2758/10336], Loss: 0.8177\n",
      "Epoch [4/5], Step [2760/10336], Loss: 1.0494\n",
      "Epoch [4/5], Step [2762/10336], Loss: 0.0300\n",
      "Epoch [4/5], Step [2764/10336], Loss: 0.6087\n",
      "Epoch [4/5], Step [2766/10336], Loss: 0.6999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [2768/10336], Loss: 0.5052\n",
      "Epoch [4/5], Step [2770/10336], Loss: 0.0200\n",
      "Epoch [4/5], Step [2772/10336], Loss: 0.0948\n",
      "Epoch [4/5], Step [2774/10336], Loss: 0.3268\n",
      "Epoch [4/5], Step [2776/10336], Loss: 1.8527\n",
      "Epoch [4/5], Step [2778/10336], Loss: 0.3855\n",
      "Epoch [4/5], Step [2780/10336], Loss: 0.0169\n",
      "Epoch [4/5], Step [2782/10336], Loss: 0.2837\n",
      "Epoch [4/5], Step [2784/10336], Loss: 3.5623\n",
      "Epoch [4/5], Step [2786/10336], Loss: 1.6771\n",
      "Epoch [4/5], Step [2788/10336], Loss: 0.3350\n",
      "Epoch [4/5], Step [2790/10336], Loss: 0.2883\n",
      "Epoch [4/5], Step [2792/10336], Loss: 2.5550\n",
      "Epoch [4/5], Step [2794/10336], Loss: 1.3789\n",
      "Epoch [4/5], Step [2796/10336], Loss: 0.0115\n",
      "Epoch [4/5], Step [2798/10336], Loss: 1.9029\n",
      "Epoch [4/5], Step [2800/10336], Loss: 0.1446\n",
      "Epoch [4/5], Step [2802/10336], Loss: 0.0450\n",
      "Epoch [4/5], Step [2804/10336], Loss: 1.9776\n",
      "Epoch [4/5], Step [2806/10336], Loss: 0.0418\n",
      "Epoch [4/5], Step [2808/10336], Loss: 0.1074\n",
      "Epoch [4/5], Step [2810/10336], Loss: 0.0098\n",
      "Epoch [4/5], Step [2812/10336], Loss: 0.4707\n",
      "Epoch [4/5], Step [2814/10336], Loss: 1.9983\n",
      "Epoch [4/5], Step [2816/10336], Loss: 0.0504\n",
      "Epoch [4/5], Step [2818/10336], Loss: 0.8238\n",
      "Epoch [4/5], Step [2820/10336], Loss: 0.4841\n",
      "Epoch [4/5], Step [2822/10336], Loss: 0.6461\n",
      "Epoch [4/5], Step [2824/10336], Loss: 1.3451\n",
      "Epoch [4/5], Step [2826/10336], Loss: 0.0113\n",
      "Epoch [4/5], Step [2828/10336], Loss: 0.7500\n",
      "Epoch [4/5], Step [2830/10336], Loss: 1.9493\n",
      "Epoch [4/5], Step [2832/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [2834/10336], Loss: 0.0225\n",
      "Epoch [4/5], Step [2836/10336], Loss: 0.2151\n",
      "Epoch [4/5], Step [2838/10336], Loss: 0.0299\n",
      "Epoch [4/5], Step [2840/10336], Loss: 1.1859\n",
      "Epoch [4/5], Step [2842/10336], Loss: 1.4970\n",
      "Epoch [4/5], Step [2844/10336], Loss: 0.5807\n",
      "Epoch [4/5], Step [2846/10336], Loss: 1.7723\n",
      "Epoch [4/5], Step [2848/10336], Loss: 1.6694\n",
      "Epoch [4/5], Step [2850/10336], Loss: 0.8998\n",
      "Epoch [4/5], Step [2852/10336], Loss: 0.2134\n",
      "Epoch [4/5], Step [2854/10336], Loss: 0.6314\n",
      "Epoch [4/5], Step [2856/10336], Loss: 0.1087\n",
      "Epoch [4/5], Step [2858/10336], Loss: 0.0283\n",
      "Epoch [4/5], Step [2860/10336], Loss: 2.4899\n",
      "Epoch [4/5], Step [2862/10336], Loss: 0.2516\n",
      "Epoch [4/5], Step [2864/10336], Loss: 0.0473\n",
      "Epoch [4/5], Step [2866/10336], Loss: 0.7997\n",
      "Epoch [4/5], Step [2868/10336], Loss: 0.8011\n",
      "Epoch [4/5], Step [2870/10336], Loss: 0.3905\n",
      "Epoch [4/5], Step [2872/10336], Loss: 1.2064\n",
      "Epoch [4/5], Step [2874/10336], Loss: 0.0113\n",
      "Epoch [4/5], Step [2876/10336], Loss: 1.2167\n",
      "Epoch [4/5], Step [2878/10336], Loss: 0.5339\n",
      "Epoch [4/5], Step [2880/10336], Loss: 0.2275\n",
      "Epoch [4/5], Step [2882/10336], Loss: 0.7429\n",
      "Epoch [4/5], Step [2884/10336], Loss: 0.0704\n",
      "Epoch [4/5], Step [2886/10336], Loss: 0.0463\n",
      "Epoch [4/5], Step [2888/10336], Loss: 0.0612\n",
      "Epoch [4/5], Step [2890/10336], Loss: 1.7271\n",
      "Epoch [4/5], Step [2892/10336], Loss: 0.1281\n",
      "Epoch [4/5], Step [2894/10336], Loss: 1.7411\n",
      "Epoch [4/5], Step [2896/10336], Loss: 1.1827\n",
      "Epoch [4/5], Step [2898/10336], Loss: 0.1132\n",
      "Epoch [4/5], Step [2900/10336], Loss: 0.0225\n",
      "Epoch [4/5], Step [2902/10336], Loss: 0.0029\n",
      "Epoch [4/5], Step [2904/10336], Loss: 0.3893\n",
      "Epoch [4/5], Step [2906/10336], Loss: 0.0060\n",
      "Epoch [4/5], Step [2908/10336], Loss: 0.0675\n",
      "Epoch [4/5], Step [2910/10336], Loss: 0.0439\n",
      "Epoch [4/5], Step [2912/10336], Loss: 0.5480\n",
      "Epoch [4/5], Step [2914/10336], Loss: 0.0244\n",
      "Epoch [4/5], Step [2916/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [2918/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [2920/10336], Loss: 0.1544\n",
      "Epoch [4/5], Step [2922/10336], Loss: 0.2596\n",
      "Epoch [4/5], Step [2924/10336], Loss: 0.4110\n",
      "Epoch [4/5], Step [2926/10336], Loss: 1.9507\n",
      "Epoch [4/5], Step [2928/10336], Loss: 0.0795\n",
      "Epoch [4/5], Step [2930/10336], Loss: 0.0451\n",
      "Epoch [4/5], Step [2932/10336], Loss: 0.5071\n",
      "Epoch [4/5], Step [2934/10336], Loss: 0.0147\n",
      "Epoch [4/5], Step [2936/10336], Loss: 0.1459\n",
      "Epoch [4/5], Step [2938/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [2940/10336], Loss: 0.4246\n",
      "Epoch [4/5], Step [2942/10336], Loss: 0.0554\n",
      "Epoch [4/5], Step [2944/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [2946/10336], Loss: 0.1272\n",
      "Epoch [4/5], Step [2948/10336], Loss: 1.8760\n",
      "Epoch [4/5], Step [2950/10336], Loss: 1.0944\n",
      "Epoch [4/5], Step [2952/10336], Loss: 0.0554\n",
      "Epoch [4/5], Step [2954/10336], Loss: 0.0074\n",
      "Epoch [4/5], Step [2956/10336], Loss: 0.0141\n",
      "Epoch [4/5], Step [2958/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [2960/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [2962/10336], Loss: 0.1375\n",
      "Epoch [4/5], Step [2964/10336], Loss: 0.2927\n",
      "Epoch [4/5], Step [2966/10336], Loss: 0.0248\n",
      "Epoch [4/5], Step [2968/10336], Loss: 0.1264\n",
      "Epoch [4/5], Step [2970/10336], Loss: 0.0331\n",
      "Epoch [4/5], Step [2972/10336], Loss: 1.0769\n",
      "Epoch [4/5], Step [2974/10336], Loss: 0.1714\n",
      "Epoch [4/5], Step [2976/10336], Loss: 0.1110\n",
      "Epoch [4/5], Step [2978/10336], Loss: 1.3859\n",
      "Epoch [4/5], Step [2980/10336], Loss: 0.1148\n",
      "Epoch [4/5], Step [2982/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [2984/10336], Loss: 0.7314\n",
      "Epoch [4/5], Step [2986/10336], Loss: 0.0206\n",
      "Epoch [4/5], Step [2988/10336], Loss: 2.0423\n",
      "Epoch [4/5], Step [2990/10336], Loss: 1.9110\n",
      "Epoch [4/5], Step [2992/10336], Loss: 0.4155\n",
      "Epoch [4/5], Step [2994/10336], Loss: 0.8193\n",
      "Epoch [4/5], Step [2996/10336], Loss: 0.9573\n",
      "Epoch [4/5], Step [2998/10336], Loss: 0.0700\n",
      "Epoch [4/5], Step [3000/10336], Loss: 0.0522\n",
      "Epoch [4/5], Step [3002/10336], Loss: 2.6088\n",
      "Epoch [4/5], Step [3004/10336], Loss: 0.1901\n",
      "Epoch [4/5], Step [3006/10336], Loss: 0.1425\n",
      "Epoch [4/5], Step [3008/10336], Loss: 0.0664\n",
      "Epoch [4/5], Step [3010/10336], Loss: 0.2670\n",
      "Epoch [4/5], Step [3012/10336], Loss: 0.2878\n",
      "Epoch [4/5], Step [3014/10336], Loss: 0.0045\n",
      "Epoch [4/5], Step [3016/10336], Loss: 0.0422\n",
      "Epoch [4/5], Step [3018/10336], Loss: 1.1883\n",
      "Epoch [4/5], Step [3020/10336], Loss: 0.8587\n",
      "Epoch [4/5], Step [3022/10336], Loss: 0.3725\n",
      "Epoch [4/5], Step [3024/10336], Loss: 0.7054\n",
      "Epoch [4/5], Step [3026/10336], Loss: 1.4968\n",
      "Epoch [4/5], Step [3028/10336], Loss: 0.6003\n",
      "Epoch [4/5], Step [3030/10336], Loss: 0.4290\n",
      "Epoch [4/5], Step [3032/10336], Loss: 0.3458\n",
      "Epoch [4/5], Step [3034/10336], Loss: 0.2935\n",
      "Epoch [4/5], Step [3036/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [3038/10336], Loss: 2.5673\n",
      "Epoch [4/5], Step [3040/10336], Loss: 0.3673\n",
      "Epoch [4/5], Step [3042/10336], Loss: 4.3136\n",
      "Epoch [4/5], Step [3044/10336], Loss: 0.1804\n",
      "Epoch [4/5], Step [3046/10336], Loss: 2.4618\n",
      "Epoch [4/5], Step [3048/10336], Loss: 0.0181\n",
      "Epoch [4/5], Step [3050/10336], Loss: 0.2773\n",
      "Epoch [4/5], Step [3052/10336], Loss: 0.0078\n",
      "Epoch [4/5], Step [3054/10336], Loss: 0.8941\n",
      "Epoch [4/5], Step [3056/10336], Loss: 0.5087\n",
      "Epoch [4/5], Step [3058/10336], Loss: 3.4183\n",
      "Epoch [4/5], Step [3060/10336], Loss: 0.5749\n",
      "Epoch [4/5], Step [3062/10336], Loss: 0.3515\n",
      "Epoch [4/5], Step [3064/10336], Loss: 0.0235\n",
      "Epoch [4/5], Step [3066/10336], Loss: 1.2716\n",
      "Epoch [4/5], Step [3068/10336], Loss: 0.0488\n",
      "Epoch [4/5], Step [3070/10336], Loss: 0.8003\n",
      "Epoch [4/5], Step [3072/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [3074/10336], Loss: 0.0411\n",
      "Epoch [4/5], Step [3076/10336], Loss: 0.1600\n",
      "Epoch [4/5], Step [3078/10336], Loss: 0.6118\n",
      "Epoch [4/5], Step [3080/10336], Loss: 1.1368\n",
      "Epoch [4/5], Step [3082/10336], Loss: 1.1753\n",
      "Epoch [4/5], Step [3084/10336], Loss: 0.1762\n",
      "Epoch [4/5], Step [3086/10336], Loss: 0.0142\n",
      "Epoch [4/5], Step [3088/10336], Loss: 0.0105\n",
      "Epoch [4/5], Step [3090/10336], Loss: 0.0463\n",
      "Epoch [4/5], Step [3092/10336], Loss: 0.5212\n",
      "Epoch [4/5], Step [3094/10336], Loss: 0.0501\n",
      "Epoch [4/5], Step [3096/10336], Loss: 0.5367\n",
      "Epoch [4/5], Step [3098/10336], Loss: 2.3857\n",
      "Epoch [4/5], Step [3100/10336], Loss: 0.5039\n",
      "Epoch [4/5], Step [3102/10336], Loss: 0.0269\n",
      "Epoch [4/5], Step [3104/10336], Loss: 1.2314\n",
      "Epoch [4/5], Step [3106/10336], Loss: 0.7343\n",
      "Epoch [4/5], Step [3108/10336], Loss: 0.0390\n",
      "Epoch [4/5], Step [3110/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [3112/10336], Loss: 1.1406\n",
      "Epoch [4/5], Step [3114/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [3116/10336], Loss: 0.5418\n",
      "Epoch [4/5], Step [3118/10336], Loss: 4.0311\n",
      "Epoch [4/5], Step [3120/10336], Loss: 3.5666\n",
      "Epoch [4/5], Step [3122/10336], Loss: 0.4956\n",
      "Epoch [4/5], Step [3124/10336], Loss: 0.0164\n",
      "Epoch [4/5], Step [3126/10336], Loss: 0.0363\n",
      "Epoch [4/5], Step [3128/10336], Loss: 0.8395\n",
      "Epoch [4/5], Step [3130/10336], Loss: 0.0561\n",
      "Epoch [4/5], Step [3132/10336], Loss: 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [3134/10336], Loss: 2.5547\n",
      "Epoch [4/5], Step [3136/10336], Loss: 0.0136\n",
      "Epoch [4/5], Step [3138/10336], Loss: 0.0190\n",
      "Epoch [4/5], Step [3140/10336], Loss: 0.9137\n",
      "Epoch [4/5], Step [3142/10336], Loss: 0.1723\n",
      "Epoch [4/5], Step [3144/10336], Loss: 0.0342\n",
      "Epoch [4/5], Step [3146/10336], Loss: 0.1904\n",
      "Epoch [4/5], Step [3148/10336], Loss: 2.8294\n",
      "Epoch [4/5], Step [3150/10336], Loss: 0.0343\n",
      "Epoch [4/5], Step [3152/10336], Loss: 0.0283\n",
      "Epoch [4/5], Step [3154/10336], Loss: 0.0501\n",
      "Epoch [4/5], Step [3156/10336], Loss: 0.0321\n",
      "Epoch [4/5], Step [3158/10336], Loss: 1.2877\n",
      "Epoch [4/5], Step [3160/10336], Loss: 0.7986\n",
      "Epoch [4/5], Step [3162/10336], Loss: 0.0347\n",
      "Epoch [4/5], Step [3164/10336], Loss: 2.0722\n",
      "Epoch [4/5], Step [3166/10336], Loss: 0.0510\n",
      "Epoch [4/5], Step [3168/10336], Loss: 0.1182\n",
      "Epoch [4/5], Step [3170/10336], Loss: 1.4769\n",
      "Epoch [4/5], Step [3172/10336], Loss: 1.8622\n",
      "Epoch [4/5], Step [3174/10336], Loss: 0.0097\n",
      "Epoch [4/5], Step [3176/10336], Loss: 0.0683\n",
      "Epoch [4/5], Step [3178/10336], Loss: 1.1417\n",
      "Epoch [4/5], Step [3180/10336], Loss: 0.1848\n",
      "Epoch [4/5], Step [3182/10336], Loss: 0.0121\n",
      "Epoch [4/5], Step [3184/10336], Loss: 0.1173\n",
      "Epoch [4/5], Step [3186/10336], Loss: 1.5022\n",
      "Epoch [4/5], Step [3188/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [3190/10336], Loss: 0.1167\n",
      "Epoch [4/5], Step [3192/10336], Loss: 0.1207\n",
      "Epoch [4/5], Step [3194/10336], Loss: 1.1734\n",
      "Epoch [4/5], Step [3196/10336], Loss: 0.4655\n",
      "Epoch [4/5], Step [3198/10336], Loss: 0.9246\n",
      "Epoch [4/5], Step [3200/10336], Loss: 1.4428\n",
      "Epoch [4/5], Step [3202/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [3204/10336], Loss: 0.0363\n",
      "Epoch [4/5], Step [3206/10336], Loss: 0.5162\n",
      "Epoch [4/5], Step [3208/10336], Loss: 0.6827\n",
      "Epoch [4/5], Step [3210/10336], Loss: 1.7542\n",
      "Epoch [4/5], Step [3212/10336], Loss: 0.3388\n",
      "Epoch [4/5], Step [3214/10336], Loss: 2.4748\n",
      "Epoch [4/5], Step [3216/10336], Loss: 1.5491\n",
      "Epoch [4/5], Step [3218/10336], Loss: 0.1047\n",
      "Epoch [4/5], Step [3220/10336], Loss: 0.0622\n",
      "Epoch [4/5], Step [3222/10336], Loss: 0.2080\n",
      "Epoch [4/5], Step [3224/10336], Loss: 0.0180\n",
      "Epoch [4/5], Step [3226/10336], Loss: 0.1989\n",
      "Epoch [4/5], Step [3228/10336], Loss: 0.5942\n",
      "Epoch [4/5], Step [3230/10336], Loss: 2.1269\n",
      "Epoch [4/5], Step [3232/10336], Loss: 0.0296\n",
      "Epoch [4/5], Step [3234/10336], Loss: 1.2598\n",
      "Epoch [4/5], Step [3236/10336], Loss: 0.2474\n",
      "Epoch [4/5], Step [3238/10336], Loss: 0.1012\n",
      "Epoch [4/5], Step [3240/10336], Loss: 0.8972\n",
      "Epoch [4/5], Step [3242/10336], Loss: 1.3458\n",
      "Epoch [4/5], Step [3244/10336], Loss: 0.2292\n",
      "Epoch [4/5], Step [3246/10336], Loss: 0.0475\n",
      "Epoch [4/5], Step [3248/10336], Loss: 0.5661\n",
      "Epoch [4/5], Step [3250/10336], Loss: 0.0868\n",
      "Epoch [4/5], Step [3252/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [3254/10336], Loss: 0.3377\n",
      "Epoch [4/5], Step [3256/10336], Loss: 4.4452\n",
      "Epoch [4/5], Step [3258/10336], Loss: 0.0367\n",
      "Epoch [4/5], Step [3260/10336], Loss: 1.8937\n",
      "Epoch [4/5], Step [3262/10336], Loss: 1.0008\n",
      "Epoch [4/5], Step [3264/10336], Loss: 0.3422\n",
      "Epoch [4/5], Step [3266/10336], Loss: 3.5130\n",
      "Epoch [4/5], Step [3268/10336], Loss: 0.2506\n",
      "Epoch [4/5], Step [3270/10336], Loss: 0.1834\n",
      "Epoch [4/5], Step [3272/10336], Loss: 0.1103\n",
      "Epoch [4/5], Step [3274/10336], Loss: 0.0759\n",
      "Epoch [4/5], Step [3276/10336], Loss: 0.0077\n",
      "Epoch [4/5], Step [3278/10336], Loss: 0.0569\n",
      "Epoch [4/5], Step [3280/10336], Loss: 0.9308\n",
      "Epoch [4/5], Step [3282/10336], Loss: 1.0318\n",
      "Epoch [4/5], Step [3284/10336], Loss: 0.0794\n",
      "Epoch [4/5], Step [3286/10336], Loss: 0.5977\n",
      "Epoch [4/5], Step [3288/10336], Loss: 2.2957\n",
      "Epoch [4/5], Step [3290/10336], Loss: 0.1619\n",
      "Epoch [4/5], Step [3292/10336], Loss: 0.1696\n",
      "Epoch [4/5], Step [3294/10336], Loss: 0.2678\n",
      "Epoch [4/5], Step [3296/10336], Loss: 0.4428\n",
      "Epoch [4/5], Step [3298/10336], Loss: 1.2122\n",
      "Epoch [4/5], Step [3300/10336], Loss: 0.3080\n",
      "Epoch [4/5], Step [3302/10336], Loss: 0.1327\n",
      "Epoch [4/5], Step [3304/10336], Loss: 0.7653\n",
      "Epoch [4/5], Step [3306/10336], Loss: 1.7630\n",
      "Epoch [4/5], Step [3308/10336], Loss: 0.0474\n",
      "Epoch [4/5], Step [3310/10336], Loss: 3.2549\n",
      "Epoch [4/5], Step [3312/10336], Loss: 0.5405\n",
      "Epoch [4/5], Step [3314/10336], Loss: 0.7868\n",
      "Epoch [4/5], Step [3316/10336], Loss: 0.2598\n",
      "Epoch [4/5], Step [3318/10336], Loss: 0.0484\n",
      "Epoch [4/5], Step [3320/10336], Loss: 0.5966\n",
      "Epoch [4/5], Step [3322/10336], Loss: 0.8014\n",
      "Epoch [4/5], Step [3324/10336], Loss: 0.0304\n",
      "Epoch [4/5], Step [3326/10336], Loss: 0.0113\n",
      "Epoch [4/5], Step [3328/10336], Loss: 0.6342\n",
      "Epoch [4/5], Step [3330/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [3332/10336], Loss: 0.3112\n",
      "Epoch [4/5], Step [3334/10336], Loss: 0.0585\n",
      "Epoch [4/5], Step [3336/10336], Loss: 4.6425\n",
      "Epoch [4/5], Step [3338/10336], Loss: 0.1032\n",
      "Epoch [4/5], Step [3340/10336], Loss: 1.6576\n",
      "Epoch [4/5], Step [3342/10336], Loss: 0.0073\n",
      "Epoch [4/5], Step [3344/10336], Loss: 0.4906\n",
      "Epoch [4/5], Step [3346/10336], Loss: 0.0789\n",
      "Epoch [4/5], Step [3348/10336], Loss: 0.1717\n",
      "Epoch [4/5], Step [3350/10336], Loss: 0.0814\n",
      "Epoch [4/5], Step [3352/10336], Loss: 1.9151\n",
      "Epoch [4/5], Step [3354/10336], Loss: 0.4855\n",
      "Epoch [4/5], Step [3356/10336], Loss: 0.1448\n",
      "Epoch [4/5], Step [3358/10336], Loss: 0.0275\n",
      "Epoch [4/5], Step [3360/10336], Loss: 2.9497\n",
      "Epoch [4/5], Step [3362/10336], Loss: 0.0421\n",
      "Epoch [4/5], Step [3364/10336], Loss: 1.7485\n",
      "Epoch [4/5], Step [3366/10336], Loss: 2.3076\n",
      "Epoch [4/5], Step [3368/10336], Loss: 2.8475\n",
      "Epoch [4/5], Step [3370/10336], Loss: 0.0644\n",
      "Epoch [4/5], Step [3372/10336], Loss: 1.1864\n",
      "Epoch [4/5], Step [3374/10336], Loss: 0.1507\n",
      "Epoch [4/5], Step [3376/10336], Loss: 0.9793\n",
      "Epoch [4/5], Step [3378/10336], Loss: 0.6134\n",
      "Epoch [4/5], Step [3380/10336], Loss: 1.0131\n",
      "Epoch [4/5], Step [3382/10336], Loss: 1.6023\n",
      "Epoch [4/5], Step [3384/10336], Loss: 0.1061\n",
      "Epoch [4/5], Step [3386/10336], Loss: 0.0075\n",
      "Epoch [4/5], Step [3388/10336], Loss: 0.2361\n",
      "Epoch [4/5], Step [3390/10336], Loss: 0.5260\n",
      "Epoch [4/5], Step [3392/10336], Loss: 0.0482\n",
      "Epoch [4/5], Step [3394/10336], Loss: 0.2250\n",
      "Epoch [4/5], Step [3396/10336], Loss: 0.6057\n",
      "Epoch [4/5], Step [3398/10336], Loss: 0.0322\n",
      "Epoch [4/5], Step [3400/10336], Loss: 0.0401\n",
      "Epoch [4/5], Step [3402/10336], Loss: 0.0027\n",
      "Epoch [4/5], Step [3404/10336], Loss: 0.2471\n",
      "Epoch [4/5], Step [3406/10336], Loss: 1.8834\n",
      "Epoch [4/5], Step [3408/10336], Loss: 0.0189\n",
      "Epoch [4/5], Step [3410/10336], Loss: 0.0160\n",
      "Epoch [4/5], Step [3412/10336], Loss: 0.4746\n",
      "Epoch [4/5], Step [3414/10336], Loss: 0.0282\n",
      "Epoch [4/5], Step [3416/10336], Loss: 0.2307\n",
      "Epoch [4/5], Step [3418/10336], Loss: 0.8225\n",
      "Epoch [4/5], Step [3420/10336], Loss: 0.0818\n",
      "Epoch [4/5], Step [3422/10336], Loss: 0.0671\n",
      "Epoch [4/5], Step [3424/10336], Loss: 0.1057\n",
      "Epoch [4/5], Step [3426/10336], Loss: 0.7518\n",
      "Epoch [4/5], Step [3428/10336], Loss: 0.1006\n",
      "Epoch [4/5], Step [3430/10336], Loss: 1.6654\n",
      "Epoch [4/5], Step [3432/10336], Loss: 0.0859\n",
      "Epoch [4/5], Step [3434/10336], Loss: 0.0163\n",
      "Epoch [4/5], Step [3436/10336], Loss: 0.0325\n",
      "Epoch [4/5], Step [3438/10336], Loss: 0.0557\n",
      "Epoch [4/5], Step [3440/10336], Loss: 0.0121\n",
      "Epoch [4/5], Step [3442/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [3444/10336], Loss: 1.3158\n",
      "Epoch [4/5], Step [3446/10336], Loss: 0.0055\n",
      "Epoch [4/5], Step [3448/10336], Loss: 0.0027\n",
      "Epoch [4/5], Step [3450/10336], Loss: 0.0098\n",
      "Epoch [4/5], Step [3452/10336], Loss: 0.0178\n",
      "Epoch [4/5], Step [3454/10336], Loss: 3.5932\n",
      "Epoch [4/5], Step [3456/10336], Loss: 0.1517\n",
      "Epoch [4/5], Step [3458/10336], Loss: 0.5398\n",
      "Epoch [4/5], Step [3460/10336], Loss: 0.3921\n",
      "Epoch [4/5], Step [3462/10336], Loss: 0.0512\n",
      "Epoch [4/5], Step [3464/10336], Loss: 0.0775\n",
      "Epoch [4/5], Step [3466/10336], Loss: 0.2607\n",
      "Epoch [4/5], Step [3468/10336], Loss: 0.7273\n",
      "Epoch [4/5], Step [3470/10336], Loss: 1.7436\n",
      "Epoch [4/5], Step [3472/10336], Loss: 0.1199\n",
      "Epoch [4/5], Step [3474/10336], Loss: 0.2122\n",
      "Epoch [4/5], Step [3476/10336], Loss: 0.2068\n",
      "Epoch [4/5], Step [3478/10336], Loss: 2.7692\n",
      "Epoch [4/5], Step [3480/10336], Loss: 0.6653\n",
      "Epoch [4/5], Step [3482/10336], Loss: 0.1131\n",
      "Epoch [4/5], Step [3484/10336], Loss: 0.0270\n",
      "Epoch [4/5], Step [3486/10336], Loss: 0.0398\n",
      "Epoch [4/5], Step [3488/10336], Loss: 0.7509\n",
      "Epoch [4/5], Step [3490/10336], Loss: 0.0174\n",
      "Epoch [4/5], Step [3492/10336], Loss: 0.0330\n",
      "Epoch [4/5], Step [3494/10336], Loss: 0.4442\n",
      "Epoch [4/5], Step [3496/10336], Loss: 0.8418\n",
      "Epoch [4/5], Step [3498/10336], Loss: 0.0006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [3500/10336], Loss: 0.0343\n",
      "Epoch [4/5], Step [3502/10336], Loss: 0.1711\n",
      "Epoch [4/5], Step [3504/10336], Loss: 0.0230\n",
      "Epoch [4/5], Step [3506/10336], Loss: 0.0382\n",
      "Epoch [4/5], Step [3508/10336], Loss: 0.1743\n",
      "Epoch [4/5], Step [3510/10336], Loss: 0.0561\n",
      "Epoch [4/5], Step [3512/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [3514/10336], Loss: 1.6182\n",
      "Epoch [4/5], Step [3516/10336], Loss: 1.4664\n",
      "Epoch [4/5], Step [3518/10336], Loss: 0.1556\n",
      "Epoch [4/5], Step [3520/10336], Loss: 1.3284\n",
      "Epoch [4/5], Step [3522/10336], Loss: 2.9194\n",
      "Epoch [4/5], Step [3524/10336], Loss: 2.5116\n",
      "Epoch [4/5], Step [3526/10336], Loss: 0.2270\n",
      "Epoch [4/5], Step [3528/10336], Loss: 1.3686\n",
      "Epoch [4/5], Step [3530/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [3532/10336], Loss: 0.0101\n",
      "Epoch [4/5], Step [3534/10336], Loss: 0.5437\n",
      "Epoch [4/5], Step [3536/10336], Loss: 0.0064\n",
      "Epoch [4/5], Step [3538/10336], Loss: 1.7911\n",
      "Epoch [4/5], Step [3540/10336], Loss: 0.5624\n",
      "Epoch [4/5], Step [3542/10336], Loss: 1.4477\n",
      "Epoch [4/5], Step [3544/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [3546/10336], Loss: 0.2130\n",
      "Epoch [4/5], Step [3548/10336], Loss: 0.4611\n",
      "Epoch [4/5], Step [3550/10336], Loss: 2.7301\n",
      "Epoch [4/5], Step [3552/10336], Loss: 0.2005\n",
      "Epoch [4/5], Step [3554/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [3556/10336], Loss: 1.1064\n",
      "Epoch [4/5], Step [3558/10336], Loss: 0.6214\n",
      "Epoch [4/5], Step [3560/10336], Loss: 0.0489\n",
      "Epoch [4/5], Step [3562/10336], Loss: 0.3687\n",
      "Epoch [4/5], Step [3564/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [3566/10336], Loss: 0.6591\n",
      "Epoch [4/5], Step [3568/10336], Loss: 0.2380\n",
      "Epoch [4/5], Step [3570/10336], Loss: 0.1606\n",
      "Epoch [4/5], Step [3572/10336], Loss: 0.0174\n",
      "Epoch [4/5], Step [3574/10336], Loss: 0.0397\n",
      "Epoch [4/5], Step [3576/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [3578/10336], Loss: 0.6472\n",
      "Epoch [4/5], Step [3580/10336], Loss: 0.0372\n",
      "Epoch [4/5], Step [3582/10336], Loss: 2.0966\n",
      "Epoch [4/5], Step [3584/10336], Loss: 0.1144\n",
      "Epoch [4/5], Step [3586/10336], Loss: 0.3335\n",
      "Epoch [4/5], Step [3588/10336], Loss: 0.0083\n",
      "Epoch [4/5], Step [3590/10336], Loss: 0.1502\n",
      "Epoch [4/5], Step [3592/10336], Loss: 1.7879\n",
      "Epoch [4/5], Step [3594/10336], Loss: 0.0964\n",
      "Epoch [4/5], Step [3596/10336], Loss: 1.7733\n",
      "Epoch [4/5], Step [3598/10336], Loss: 1.8953\n",
      "Epoch [4/5], Step [3600/10336], Loss: 4.3438\n",
      "Epoch [4/5], Step [3602/10336], Loss: 0.3848\n",
      "Epoch [4/5], Step [3604/10336], Loss: 2.2639\n",
      "Epoch [4/5], Step [3606/10336], Loss: 0.1004\n",
      "Epoch [4/5], Step [3608/10336], Loss: 0.1130\n",
      "Epoch [4/5], Step [3610/10336], Loss: 0.0105\n",
      "Epoch [4/5], Step [3612/10336], Loss: 1.3563\n",
      "Epoch [4/5], Step [3614/10336], Loss: 0.0561\n",
      "Epoch [4/5], Step [3616/10336], Loss: 0.3501\n",
      "Epoch [4/5], Step [3618/10336], Loss: 1.5751\n",
      "Epoch [4/5], Step [3620/10336], Loss: 0.9673\n",
      "Epoch [4/5], Step [3622/10336], Loss: 0.4556\n",
      "Epoch [4/5], Step [3624/10336], Loss: 0.2590\n",
      "Epoch [4/5], Step [3626/10336], Loss: 0.1198\n",
      "Epoch [4/5], Step [3628/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [3630/10336], Loss: 2.1144\n",
      "Epoch [4/5], Step [3632/10336], Loss: 0.8178\n",
      "Epoch [4/5], Step [3634/10336], Loss: 0.3730\n",
      "Epoch [4/5], Step [3636/10336], Loss: 1.3439\n",
      "Epoch [4/5], Step [3638/10336], Loss: 0.5216\n",
      "Epoch [4/5], Step [3640/10336], Loss: 0.0736\n",
      "Epoch [4/5], Step [3642/10336], Loss: 0.0364\n",
      "Epoch [4/5], Step [3644/10336], Loss: 0.5331\n",
      "Epoch [4/5], Step [3646/10336], Loss: 2.2313\n",
      "Epoch [4/5], Step [3648/10336], Loss: 0.0613\n",
      "Epoch [4/5], Step [3650/10336], Loss: 0.4564\n",
      "Epoch [4/5], Step [3652/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [3654/10336], Loss: 3.1908\n",
      "Epoch [4/5], Step [3656/10336], Loss: 1.1107\n",
      "Epoch [4/5], Step [3658/10336], Loss: 0.3684\n",
      "Epoch [4/5], Step [3660/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [3662/10336], Loss: 0.4290\n",
      "Epoch [4/5], Step [3664/10336], Loss: 0.0245\n",
      "Epoch [4/5], Step [3666/10336], Loss: 0.0947\n",
      "Epoch [4/5], Step [3668/10336], Loss: 0.1151\n",
      "Epoch [4/5], Step [3670/10336], Loss: 0.0241\n",
      "Epoch [4/5], Step [3672/10336], Loss: 2.1055\n",
      "Epoch [4/5], Step [3674/10336], Loss: 1.0774\n",
      "Epoch [4/5], Step [3676/10336], Loss: 1.1050\n",
      "Epoch [4/5], Step [3678/10336], Loss: 0.0091\n",
      "Epoch [4/5], Step [3680/10336], Loss: 3.3671\n",
      "Epoch [4/5], Step [3682/10336], Loss: 0.4699\n",
      "Epoch [4/5], Step [3684/10336], Loss: 0.0066\n",
      "Epoch [4/5], Step [3686/10336], Loss: 0.0848\n",
      "Epoch [4/5], Step [3688/10336], Loss: 0.0592\n",
      "Epoch [4/5], Step [3690/10336], Loss: 0.0576\n",
      "Epoch [4/5], Step [3692/10336], Loss: 0.2416\n",
      "Epoch [4/5], Step [3694/10336], Loss: 0.0756\n",
      "Epoch [4/5], Step [3696/10336], Loss: 0.2072\n",
      "Epoch [4/5], Step [3698/10336], Loss: 0.0639\n",
      "Epoch [4/5], Step [3700/10336], Loss: 1.0123\n",
      "Epoch [4/5], Step [3702/10336], Loss: 0.0462\n",
      "Epoch [4/5], Step [3704/10336], Loss: 1.8909\n",
      "Epoch [4/5], Step [3706/10336], Loss: 1.1568\n",
      "Epoch [4/5], Step [3708/10336], Loss: 0.0258\n",
      "Epoch [4/5], Step [3710/10336], Loss: 0.0101\n",
      "Epoch [4/5], Step [3712/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [3714/10336], Loss: 0.1149\n",
      "Epoch [4/5], Step [3716/10336], Loss: 0.0096\n",
      "Epoch [4/5], Step [3718/10336], Loss: 1.8774\n",
      "Epoch [4/5], Step [3720/10336], Loss: 0.3270\n",
      "Epoch [4/5], Step [3722/10336], Loss: 0.0066\n",
      "Epoch [4/5], Step [3724/10336], Loss: 0.1952\n",
      "Epoch [4/5], Step [3726/10336], Loss: 0.0138\n",
      "Epoch [4/5], Step [3728/10336], Loss: 0.0222\n",
      "Epoch [4/5], Step [3730/10336], Loss: 5.0024\n",
      "Epoch [4/5], Step [3732/10336], Loss: 0.0257\n",
      "Epoch [4/5], Step [3734/10336], Loss: 2.5134\n",
      "Epoch [4/5], Step [3736/10336], Loss: 0.0336\n",
      "Epoch [4/5], Step [3738/10336], Loss: 0.7154\n",
      "Epoch [4/5], Step [3740/10336], Loss: 0.4308\n",
      "Epoch [4/5], Step [3742/10336], Loss: 0.2363\n",
      "Epoch [4/5], Step [3744/10336], Loss: 0.3946\n",
      "Epoch [4/5], Step [3746/10336], Loss: 0.0450\n",
      "Epoch [4/5], Step [3748/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [3750/10336], Loss: 0.1330\n",
      "Epoch [4/5], Step [3752/10336], Loss: 0.0593\n",
      "Epoch [4/5], Step [3754/10336], Loss: 0.1326\n",
      "Epoch [4/5], Step [3756/10336], Loss: 0.0126\n",
      "Epoch [4/5], Step [3758/10336], Loss: 0.4110\n",
      "Epoch [4/5], Step [3760/10336], Loss: 0.3293\n",
      "Epoch [4/5], Step [3762/10336], Loss: 0.3697\n",
      "Epoch [4/5], Step [3764/10336], Loss: 0.0373\n",
      "Epoch [4/5], Step [3766/10336], Loss: 0.0066\n",
      "Epoch [4/5], Step [3768/10336], Loss: 0.9405\n",
      "Epoch [4/5], Step [3770/10336], Loss: 0.1055\n",
      "Epoch [4/5], Step [3772/10336], Loss: 1.8923\n",
      "Epoch [4/5], Step [3774/10336], Loss: 1.5464\n",
      "Epoch [4/5], Step [3776/10336], Loss: 1.3707\n",
      "Epoch [4/5], Step [3778/10336], Loss: 0.1123\n",
      "Epoch [4/5], Step [3780/10336], Loss: 0.6549\n",
      "Epoch [4/5], Step [3782/10336], Loss: 0.3196\n",
      "Epoch [4/5], Step [3784/10336], Loss: 0.2642\n",
      "Epoch [4/5], Step [3786/10336], Loss: 0.0077\n",
      "Epoch [4/5], Step [3788/10336], Loss: 0.3910\n",
      "Epoch [4/5], Step [3790/10336], Loss: 0.3285\n",
      "Epoch [4/5], Step [3792/10336], Loss: 0.0166\n",
      "Epoch [4/5], Step [3794/10336], Loss: 0.7713\n",
      "Epoch [4/5], Step [3796/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [3798/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [3800/10336], Loss: 0.8453\n",
      "Epoch [4/5], Step [3802/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [3804/10336], Loss: 0.6042\n",
      "Epoch [4/5], Step [3806/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [3808/10336], Loss: 1.7571\n",
      "Epoch [4/5], Step [3810/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [3812/10336], Loss: 1.5467\n",
      "Epoch [4/5], Step [3814/10336], Loss: 0.5491\n",
      "Epoch [4/5], Step [3816/10336], Loss: 1.4166\n",
      "Epoch [4/5], Step [3818/10336], Loss: 1.1067\n",
      "Epoch [4/5], Step [3820/10336], Loss: 0.0375\n",
      "Epoch [4/5], Step [3822/10336], Loss: 0.1776\n",
      "Epoch [4/5], Step [3824/10336], Loss: 0.6651\n",
      "Epoch [4/5], Step [3826/10336], Loss: 0.9589\n",
      "Epoch [4/5], Step [3828/10336], Loss: 0.0087\n",
      "Epoch [4/5], Step [3830/10336], Loss: 1.6391\n",
      "Epoch [4/5], Step [3832/10336], Loss: 0.8774\n",
      "Epoch [4/5], Step [3834/10336], Loss: 0.5805\n",
      "Epoch [4/5], Step [3836/10336], Loss: 0.1320\n",
      "Epoch [4/5], Step [3838/10336], Loss: 1.1670\n",
      "Epoch [4/5], Step [3840/10336], Loss: 0.0523\n",
      "Epoch [4/5], Step [3842/10336], Loss: 1.6692\n",
      "Epoch [4/5], Step [3844/10336], Loss: 0.0227\n",
      "Epoch [4/5], Step [3846/10336], Loss: 0.0172\n",
      "Epoch [4/5], Step [3848/10336], Loss: 0.3057\n",
      "Epoch [4/5], Step [3850/10336], Loss: 0.1199\n",
      "Epoch [4/5], Step [3852/10336], Loss: 0.1220\n",
      "Epoch [4/5], Step [3854/10336], Loss: 0.1619\n",
      "Epoch [4/5], Step [3856/10336], Loss: 0.6333\n",
      "Epoch [4/5], Step [3858/10336], Loss: 0.8723\n",
      "Epoch [4/5], Step [3860/10336], Loss: 0.6805\n",
      "Epoch [4/5], Step [3862/10336], Loss: 0.3751\n",
      "Epoch [4/5], Step [3864/10336], Loss: 0.1968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [3866/10336], Loss: 0.1810\n",
      "Epoch [4/5], Step [3868/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [3870/10336], Loss: 3.8741\n",
      "Epoch [4/5], Step [3872/10336], Loss: 0.3527\n",
      "Epoch [4/5], Step [3874/10336], Loss: 0.0241\n",
      "Epoch [4/5], Step [3876/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [3878/10336], Loss: 1.6178\n",
      "Epoch [4/5], Step [3880/10336], Loss: 0.7352\n",
      "Epoch [4/5], Step [3882/10336], Loss: 0.1239\n",
      "Epoch [4/5], Step [3884/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [3886/10336], Loss: 0.3688\n",
      "Epoch [4/5], Step [3888/10336], Loss: 2.4966\n",
      "Epoch [4/5], Step [3890/10336], Loss: 0.0135\n",
      "Epoch [4/5], Step [3892/10336], Loss: 0.6346\n",
      "Epoch [4/5], Step [3894/10336], Loss: 3.5431\n",
      "Epoch [4/5], Step [3896/10336], Loss: 0.0844\n",
      "Epoch [4/5], Step [3898/10336], Loss: 1.7648\n",
      "Epoch [4/5], Step [3900/10336], Loss: 0.0106\n",
      "Epoch [4/5], Step [3902/10336], Loss: 2.3428\n",
      "Epoch [4/5], Step [3904/10336], Loss: 0.5159\n",
      "Epoch [4/5], Step [3906/10336], Loss: 0.1416\n",
      "Epoch [4/5], Step [3908/10336], Loss: 0.0624\n",
      "Epoch [4/5], Step [3910/10336], Loss: 0.0751\n",
      "Epoch [4/5], Step [3912/10336], Loss: 0.1600\n",
      "Epoch [4/5], Step [3914/10336], Loss: 2.8558\n",
      "Epoch [4/5], Step [3916/10336], Loss: 0.3154\n",
      "Epoch [4/5], Step [3918/10336], Loss: 2.1290\n",
      "Epoch [4/5], Step [3920/10336], Loss: 0.0370\n",
      "Epoch [4/5], Step [3922/10336], Loss: 2.2362\n",
      "Epoch [4/5], Step [3924/10336], Loss: 0.1055\n",
      "Epoch [4/5], Step [3926/10336], Loss: 1.2978\n",
      "Epoch [4/5], Step [3928/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [3930/10336], Loss: 0.1333\n",
      "Epoch [4/5], Step [3932/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [3934/10336], Loss: 1.1446\n",
      "Epoch [4/5], Step [3936/10336], Loss: 0.0077\n",
      "Epoch [4/5], Step [3938/10336], Loss: 1.1892\n",
      "Epoch [4/5], Step [3940/10336], Loss: 0.6042\n",
      "Epoch [4/5], Step [3942/10336], Loss: 0.1712\n",
      "Epoch [4/5], Step [3944/10336], Loss: 1.4249\n",
      "Epoch [4/5], Step [3946/10336], Loss: 0.4685\n",
      "Epoch [4/5], Step [3948/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [3950/10336], Loss: 0.2487\n",
      "Epoch [4/5], Step [3952/10336], Loss: 0.2842\n",
      "Epoch [4/5], Step [3954/10336], Loss: 0.0169\n",
      "Epoch [4/5], Step [3956/10336], Loss: 1.2250\n",
      "Epoch [4/5], Step [3958/10336], Loss: 0.7955\n",
      "Epoch [4/5], Step [3960/10336], Loss: 0.3070\n",
      "Epoch [4/5], Step [3962/10336], Loss: 0.2803\n",
      "Epoch [4/5], Step [3964/10336], Loss: 0.4429\n",
      "Epoch [4/5], Step [3966/10336], Loss: 0.4142\n",
      "Epoch [4/5], Step [3968/10336], Loss: 1.4286\n",
      "Epoch [4/5], Step [3970/10336], Loss: 0.8747\n",
      "Epoch [4/5], Step [3972/10336], Loss: 0.2442\n",
      "Epoch [4/5], Step [3974/10336], Loss: 0.5725\n",
      "Epoch [4/5], Step [3976/10336], Loss: 1.6433\n",
      "Epoch [4/5], Step [3978/10336], Loss: 0.5947\n",
      "Epoch [4/5], Step [3980/10336], Loss: 3.1634\n",
      "Epoch [4/5], Step [3982/10336], Loss: 0.3577\n",
      "Epoch [4/5], Step [3984/10336], Loss: 1.0776\n",
      "Epoch [4/5], Step [3986/10336], Loss: 0.3346\n",
      "Epoch [4/5], Step [3988/10336], Loss: 2.5080\n",
      "Epoch [4/5], Step [3990/10336], Loss: 2.1221\n",
      "Epoch [4/5], Step [3992/10336], Loss: 0.0035\n",
      "Epoch [4/5], Step [3994/10336], Loss: 2.0266\n",
      "Epoch [4/5], Step [3996/10336], Loss: 1.5234\n",
      "Epoch [4/5], Step [3998/10336], Loss: 0.0123\n",
      "Epoch [4/5], Step [4000/10336], Loss: 1.4418\n",
      "Epoch [4/5], Step [4002/10336], Loss: 1.3370\n",
      "Epoch [4/5], Step [4004/10336], Loss: 0.0561\n",
      "Epoch [4/5], Step [4006/10336], Loss: 0.0336\n",
      "Epoch [4/5], Step [4008/10336], Loss: 2.0224\n",
      "Epoch [4/5], Step [4010/10336], Loss: 0.0104\n",
      "Epoch [4/5], Step [4012/10336], Loss: 0.8816\n",
      "Epoch [4/5], Step [4014/10336], Loss: 0.4343\n",
      "Epoch [4/5], Step [4016/10336], Loss: 0.0884\n",
      "Epoch [4/5], Step [4018/10336], Loss: 1.1146\n",
      "Epoch [4/5], Step [4020/10336], Loss: 0.0241\n",
      "Epoch [4/5], Step [4022/10336], Loss: 0.0439\n",
      "Epoch [4/5], Step [4024/10336], Loss: 0.1181\n",
      "Epoch [4/5], Step [4026/10336], Loss: 0.1332\n",
      "Epoch [4/5], Step [4028/10336], Loss: 0.9730\n",
      "Epoch [4/5], Step [4030/10336], Loss: 2.1627\n",
      "Epoch [4/5], Step [4032/10336], Loss: 1.9254\n",
      "Epoch [4/5], Step [4034/10336], Loss: 1.0574\n",
      "Epoch [4/5], Step [4036/10336], Loss: 0.6160\n",
      "Epoch [4/5], Step [4038/10336], Loss: 0.3145\n",
      "Epoch [4/5], Step [4040/10336], Loss: 0.0903\n",
      "Epoch [4/5], Step [4042/10336], Loss: 2.5367\n",
      "Epoch [4/5], Step [4044/10336], Loss: 1.1333\n",
      "Epoch [4/5], Step [4046/10336], Loss: 0.9501\n",
      "Epoch [4/5], Step [4048/10336], Loss: 0.4502\n",
      "Epoch [4/5], Step [4050/10336], Loss: 0.0307\n",
      "Epoch [4/5], Step [4052/10336], Loss: 0.2508\n",
      "Epoch [4/5], Step [4054/10336], Loss: 0.0027\n",
      "Epoch [4/5], Step [4056/10336], Loss: 0.0146\n",
      "Epoch [4/5], Step [4058/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [4060/10336], Loss: 0.2690\n",
      "Epoch [4/5], Step [4062/10336], Loss: 0.0384\n",
      "Epoch [4/5], Step [4064/10336], Loss: 0.1979\n",
      "Epoch [4/5], Step [4066/10336], Loss: 1.5448\n",
      "Epoch [4/5], Step [4068/10336], Loss: 0.3534\n",
      "Epoch [4/5], Step [4070/10336], Loss: 0.3246\n",
      "Epoch [4/5], Step [4072/10336], Loss: 1.7203\n",
      "Epoch [4/5], Step [4074/10336], Loss: 0.8923\n",
      "Epoch [4/5], Step [4076/10336], Loss: 0.1748\n",
      "Epoch [4/5], Step [4078/10336], Loss: 0.0410\n",
      "Epoch [4/5], Step [4080/10336], Loss: 0.5204\n",
      "Epoch [4/5], Step [4082/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [4084/10336], Loss: 0.7542\n",
      "Epoch [4/5], Step [4086/10336], Loss: 0.0225\n",
      "Epoch [4/5], Step [4088/10336], Loss: 0.3940\n",
      "Epoch [4/5], Step [4090/10336], Loss: 3.3349\n",
      "Epoch [4/5], Step [4092/10336], Loss: 0.8809\n",
      "Epoch [4/5], Step [4094/10336], Loss: 0.2637\n",
      "Epoch [4/5], Step [4096/10336], Loss: 0.0072\n",
      "Epoch [4/5], Step [4098/10336], Loss: 0.1000\n",
      "Epoch [4/5], Step [4100/10336], Loss: 0.2127\n",
      "Epoch [4/5], Step [4102/10336], Loss: 0.6122\n",
      "Epoch [4/5], Step [4104/10336], Loss: 1.2147\n",
      "Epoch [4/5], Step [4106/10336], Loss: 0.8454\n",
      "Epoch [4/5], Step [4108/10336], Loss: 0.1498\n",
      "Epoch [4/5], Step [4110/10336], Loss: 0.1904\n",
      "Epoch [4/5], Step [4112/10336], Loss: 0.0061\n",
      "Epoch [4/5], Step [4114/10336], Loss: 0.0117\n",
      "Epoch [4/5], Step [4116/10336], Loss: 1.3839\n",
      "Epoch [4/5], Step [4118/10336], Loss: 0.1073\n",
      "Epoch [4/5], Step [4120/10336], Loss: 0.0319\n",
      "Epoch [4/5], Step [4122/10336], Loss: 0.2796\n",
      "Epoch [4/5], Step [4124/10336], Loss: 0.1556\n",
      "Epoch [4/5], Step [4126/10336], Loss: 0.0823\n",
      "Epoch [4/5], Step [4128/10336], Loss: 1.5861\n",
      "Epoch [4/5], Step [4130/10336], Loss: 1.1100\n",
      "Epoch [4/5], Step [4132/10336], Loss: 0.0257\n",
      "Epoch [4/5], Step [4134/10336], Loss: 0.1171\n",
      "Epoch [4/5], Step [4136/10336], Loss: 2.6511\n",
      "Epoch [4/5], Step [4138/10336], Loss: 0.2025\n",
      "Epoch [4/5], Step [4140/10336], Loss: 0.0250\n",
      "Epoch [4/5], Step [4142/10336], Loss: 0.6622\n",
      "Epoch [4/5], Step [4144/10336], Loss: 0.6633\n",
      "Epoch [4/5], Step [4146/10336], Loss: 0.3593\n",
      "Epoch [4/5], Step [4148/10336], Loss: 2.3578\n",
      "Epoch [4/5], Step [4150/10336], Loss: 0.4018\n",
      "Epoch [4/5], Step [4152/10336], Loss: 0.2843\n",
      "Epoch [4/5], Step [4154/10336], Loss: 0.0383\n",
      "Epoch [4/5], Step [4156/10336], Loss: 0.9304\n",
      "Epoch [4/5], Step [4158/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [4160/10336], Loss: 4.1428\n",
      "Epoch [4/5], Step [4162/10336], Loss: 0.0099\n",
      "Epoch [4/5], Step [4164/10336], Loss: 0.1158\n",
      "Epoch [4/5], Step [4166/10336], Loss: 1.1155\n",
      "Epoch [4/5], Step [4168/10336], Loss: 0.6942\n",
      "Epoch [4/5], Step [4170/10336], Loss: 0.3486\n",
      "Epoch [4/5], Step [4172/10336], Loss: 0.9394\n",
      "Epoch [4/5], Step [4174/10336], Loss: 1.1552\n",
      "Epoch [4/5], Step [4176/10336], Loss: 0.1818\n",
      "Epoch [4/5], Step [4178/10336], Loss: 0.0690\n",
      "Epoch [4/5], Step [4180/10336], Loss: 0.0407\n",
      "Epoch [4/5], Step [4182/10336], Loss: 0.3682\n",
      "Epoch [4/5], Step [4184/10336], Loss: 0.1028\n",
      "Epoch [4/5], Step [4186/10336], Loss: 2.5739\n",
      "Epoch [4/5], Step [4188/10336], Loss: 6.0923\n",
      "Epoch [4/5], Step [4190/10336], Loss: 1.8801\n",
      "Epoch [4/5], Step [4192/10336], Loss: 1.1557\n",
      "Epoch [4/5], Step [4194/10336], Loss: 0.0521\n",
      "Epoch [4/5], Step [4196/10336], Loss: 0.5419\n",
      "Epoch [4/5], Step [4198/10336], Loss: 0.9690\n",
      "Epoch [4/5], Step [4200/10336], Loss: 0.0367\n",
      "Epoch [4/5], Step [4202/10336], Loss: 1.5070\n",
      "Epoch [4/5], Step [4204/10336], Loss: 0.1448\n",
      "Epoch [4/5], Step [4206/10336], Loss: 0.3585\n",
      "Epoch [4/5], Step [4208/10336], Loss: 0.0946\n",
      "Epoch [4/5], Step [4210/10336], Loss: 1.5769\n",
      "Epoch [4/5], Step [4212/10336], Loss: 0.1183\n",
      "Epoch [4/5], Step [4214/10336], Loss: 0.0078\n",
      "Epoch [4/5], Step [4216/10336], Loss: 0.0727\n",
      "Epoch [4/5], Step [4218/10336], Loss: 0.5255\n",
      "Epoch [4/5], Step [4220/10336], Loss: 0.6273\n",
      "Epoch [4/5], Step [4222/10336], Loss: 0.0647\n",
      "Epoch [4/5], Step [4224/10336], Loss: 1.4565\n",
      "Epoch [4/5], Step [4226/10336], Loss: 0.7051\n",
      "Epoch [4/5], Step [4228/10336], Loss: 0.0416\n",
      "Epoch [4/5], Step [4230/10336], Loss: 1.9939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [4232/10336], Loss: 2.1154\n",
      "Epoch [4/5], Step [4234/10336], Loss: 0.4902\n",
      "Epoch [4/5], Step [4236/10336], Loss: 0.0069\n",
      "Epoch [4/5], Step [4238/10336], Loss: 0.1603\n",
      "Epoch [4/5], Step [4240/10336], Loss: 0.0120\n",
      "Epoch [4/5], Step [4242/10336], Loss: 0.0410\n",
      "Epoch [4/5], Step [4244/10336], Loss: 0.6791\n",
      "Epoch [4/5], Step [4246/10336], Loss: 0.0113\n",
      "Epoch [4/5], Step [4248/10336], Loss: 0.5935\n",
      "Epoch [4/5], Step [4250/10336], Loss: 0.1219\n",
      "Epoch [4/5], Step [4252/10336], Loss: 1.5216\n",
      "Epoch [4/5], Step [4254/10336], Loss: 0.4497\n",
      "Epoch [4/5], Step [4256/10336], Loss: 2.4866\n",
      "Epoch [4/5], Step [4258/10336], Loss: 0.5565\n",
      "Epoch [4/5], Step [4260/10336], Loss: 1.7444\n",
      "Epoch [4/5], Step [4262/10336], Loss: 0.0104\n",
      "Epoch [4/5], Step [4264/10336], Loss: 0.0281\n",
      "Epoch [4/5], Step [4266/10336], Loss: 1.2070\n",
      "Epoch [4/5], Step [4268/10336], Loss: 0.0031\n",
      "Epoch [4/5], Step [4270/10336], Loss: 1.1769\n",
      "Epoch [4/5], Step [4272/10336], Loss: 0.1177\n",
      "Epoch [4/5], Step [4274/10336], Loss: 2.5136\n",
      "Epoch [4/5], Step [4276/10336], Loss: 0.1283\n",
      "Epoch [4/5], Step [4278/10336], Loss: 0.0177\n",
      "Epoch [4/5], Step [4280/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [4282/10336], Loss: 0.9832\n",
      "Epoch [4/5], Step [4284/10336], Loss: 0.1177\n",
      "Epoch [4/5], Step [4286/10336], Loss: 1.4559\n",
      "Epoch [4/5], Step [4288/10336], Loss: 0.5085\n",
      "Epoch [4/5], Step [4290/10336], Loss: 0.0957\n",
      "Epoch [4/5], Step [4292/10336], Loss: 0.2781\n",
      "Epoch [4/5], Step [4294/10336], Loss: 0.0380\n",
      "Epoch [4/5], Step [4296/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [4298/10336], Loss: 0.8530\n",
      "Epoch [4/5], Step [4300/10336], Loss: 0.5663\n",
      "Epoch [4/5], Step [4302/10336], Loss: 2.0943\n",
      "Epoch [4/5], Step [4304/10336], Loss: 2.5419\n",
      "Epoch [4/5], Step [4306/10336], Loss: 0.7120\n",
      "Epoch [4/5], Step [4308/10336], Loss: 0.5865\n",
      "Epoch [4/5], Step [4310/10336], Loss: 1.6077\n",
      "Epoch [4/5], Step [4312/10336], Loss: 3.0393\n",
      "Epoch [4/5], Step [4314/10336], Loss: 0.0603\n",
      "Epoch [4/5], Step [4316/10336], Loss: 0.7649\n",
      "Epoch [4/5], Step [4318/10336], Loss: 0.4524\n",
      "Epoch [4/5], Step [4320/10336], Loss: 1.1957\n",
      "Epoch [4/5], Step [4322/10336], Loss: 0.8840\n",
      "Epoch [4/5], Step [4324/10336], Loss: 0.0347\n",
      "Epoch [4/5], Step [4326/10336], Loss: 0.0806\n",
      "Epoch [4/5], Step [4328/10336], Loss: 0.6388\n",
      "Epoch [4/5], Step [4330/10336], Loss: 0.0212\n",
      "Epoch [4/5], Step [4332/10336], Loss: 0.0396\n",
      "Epoch [4/5], Step [4334/10336], Loss: 0.0241\n",
      "Epoch [4/5], Step [4336/10336], Loss: 0.4202\n",
      "Epoch [4/5], Step [4338/10336], Loss: 0.5002\n",
      "Epoch [4/5], Step [4340/10336], Loss: 0.1361\n",
      "Epoch [4/5], Step [4342/10336], Loss: 1.9248\n",
      "Epoch [4/5], Step [4344/10336], Loss: 0.1938\n",
      "Epoch [4/5], Step [4346/10336], Loss: 0.0813\n",
      "Epoch [4/5], Step [4348/10336], Loss: 0.0235\n",
      "Epoch [4/5], Step [4350/10336], Loss: 0.0470\n",
      "Epoch [4/5], Step [4352/10336], Loss: 0.1961\n",
      "Epoch [4/5], Step [4354/10336], Loss: 0.0394\n",
      "Epoch [4/5], Step [4356/10336], Loss: 1.9206\n",
      "Epoch [4/5], Step [4358/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [4360/10336], Loss: 0.0209\n",
      "Epoch [4/5], Step [4362/10336], Loss: 1.2626\n",
      "Epoch [4/5], Step [4364/10336], Loss: 0.1996\n",
      "Epoch [4/5], Step [4366/10336], Loss: 2.6429\n",
      "Epoch [4/5], Step [4368/10336], Loss: 0.0900\n",
      "Epoch [4/5], Step [4370/10336], Loss: 0.0134\n",
      "Epoch [4/5], Step [4372/10336], Loss: 0.9467\n",
      "Epoch [4/5], Step [4374/10336], Loss: 0.0255\n",
      "Epoch [4/5], Step [4376/10336], Loss: 0.3738\n",
      "Epoch [4/5], Step [4378/10336], Loss: 3.6189\n",
      "Epoch [4/5], Step [4380/10336], Loss: 0.0381\n",
      "Epoch [4/5], Step [4382/10336], Loss: 0.9341\n",
      "Epoch [4/5], Step [4384/10336], Loss: 0.0381\n",
      "Epoch [4/5], Step [4386/10336], Loss: 3.4494\n",
      "Epoch [4/5], Step [4388/10336], Loss: 0.3480\n",
      "Epoch [4/5], Step [4390/10336], Loss: 0.2145\n",
      "Epoch [4/5], Step [4392/10336], Loss: 1.0040\n",
      "Epoch [4/5], Step [4394/10336], Loss: 2.3008\n",
      "Epoch [4/5], Step [4396/10336], Loss: 0.9721\n",
      "Epoch [4/5], Step [4398/10336], Loss: 0.1262\n",
      "Epoch [4/5], Step [4400/10336], Loss: 0.9803\n",
      "Epoch [4/5], Step [4402/10336], Loss: 0.0754\n",
      "Epoch [4/5], Step [4404/10336], Loss: 2.6602\n",
      "Epoch [4/5], Step [4406/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [4408/10336], Loss: 0.0133\n",
      "Epoch [4/5], Step [4410/10336], Loss: 0.0993\n",
      "Epoch [4/5], Step [4412/10336], Loss: 0.0129\n",
      "Epoch [4/5], Step [4414/10336], Loss: 2.0341\n",
      "Epoch [4/5], Step [4416/10336], Loss: 1.9895\n",
      "Epoch [4/5], Step [4418/10336], Loss: 0.1143\n",
      "Epoch [4/5], Step [4420/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [4422/10336], Loss: 1.6169\n",
      "Epoch [4/5], Step [4424/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [4426/10336], Loss: 0.1794\n",
      "Epoch [4/5], Step [4428/10336], Loss: 0.0244\n",
      "Epoch [4/5], Step [4430/10336], Loss: 1.9638\n",
      "Epoch [4/5], Step [4432/10336], Loss: 0.6476\n",
      "Epoch [4/5], Step [4434/10336], Loss: 3.0213\n",
      "Epoch [4/5], Step [4436/10336], Loss: 0.3981\n",
      "Epoch [4/5], Step [4438/10336], Loss: 0.0217\n",
      "Epoch [4/5], Step [4440/10336], Loss: 1.1614\n",
      "Epoch [4/5], Step [4442/10336], Loss: 0.6139\n",
      "Epoch [4/5], Step [4444/10336], Loss: 1.3028\n",
      "Epoch [4/5], Step [4446/10336], Loss: 1.0489\n",
      "Epoch [4/5], Step [4448/10336], Loss: 0.1169\n",
      "Epoch [4/5], Step [4450/10336], Loss: 0.1251\n",
      "Epoch [4/5], Step [4452/10336], Loss: 0.4255\n",
      "Epoch [4/5], Step [4454/10336], Loss: 0.9591\n",
      "Epoch [4/5], Step [4456/10336], Loss: 0.3692\n",
      "Epoch [4/5], Step [4458/10336], Loss: 0.6586\n",
      "Epoch [4/5], Step [4460/10336], Loss: 0.9024\n",
      "Epoch [4/5], Step [4462/10336], Loss: 1.4941\n",
      "Epoch [4/5], Step [4464/10336], Loss: 0.7559\n",
      "Epoch [4/5], Step [4466/10336], Loss: 0.0741\n",
      "Epoch [4/5], Step [4468/10336], Loss: 3.6639\n",
      "Epoch [4/5], Step [4470/10336], Loss: 0.0543\n",
      "Epoch [4/5], Step [4472/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [4474/10336], Loss: 0.4289\n",
      "Epoch [4/5], Step [4476/10336], Loss: 0.0338\n",
      "Epoch [4/5], Step [4478/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [4480/10336], Loss: 0.3238\n",
      "Epoch [4/5], Step [4482/10336], Loss: 0.0098\n",
      "Epoch [4/5], Step [4484/10336], Loss: 0.0830\n",
      "Epoch [4/5], Step [4486/10336], Loss: 0.4337\n",
      "Epoch [4/5], Step [4488/10336], Loss: 1.2200\n",
      "Epoch [4/5], Step [4490/10336], Loss: 0.0236\n",
      "Epoch [4/5], Step [4492/10336], Loss: 0.1127\n",
      "Epoch [4/5], Step [4494/10336], Loss: 0.9486\n",
      "Epoch [4/5], Step [4496/10336], Loss: 0.3253\n",
      "Epoch [4/5], Step [4498/10336], Loss: 0.0518\n",
      "Epoch [4/5], Step [4500/10336], Loss: 3.1746\n",
      "Epoch [4/5], Step [4502/10336], Loss: 0.7107\n",
      "Epoch [4/5], Step [4504/10336], Loss: 0.0978\n",
      "Epoch [4/5], Step [4506/10336], Loss: 0.4195\n",
      "Epoch [4/5], Step [4508/10336], Loss: 0.1601\n",
      "Epoch [4/5], Step [4510/10336], Loss: 0.9930\n",
      "Epoch [4/5], Step [4512/10336], Loss: 2.8498\n",
      "Epoch [4/5], Step [4514/10336], Loss: 0.7308\n",
      "Epoch [4/5], Step [4516/10336], Loss: 0.0794\n",
      "Epoch [4/5], Step [4518/10336], Loss: 0.1687\n",
      "Epoch [4/5], Step [4520/10336], Loss: 0.1100\n",
      "Epoch [4/5], Step [4522/10336], Loss: 0.0577\n",
      "Epoch [4/5], Step [4524/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [4526/10336], Loss: 1.8630\n",
      "Epoch [4/5], Step [4528/10336], Loss: 0.9778\n",
      "Epoch [4/5], Step [4530/10336], Loss: 0.2786\n",
      "Epoch [4/5], Step [4532/10336], Loss: 1.3552\n",
      "Epoch [4/5], Step [4534/10336], Loss: 0.9142\n",
      "Epoch [4/5], Step [4536/10336], Loss: 0.1070\n",
      "Epoch [4/5], Step [4538/10336], Loss: 0.5848\n",
      "Epoch [4/5], Step [4540/10336], Loss: 0.6440\n",
      "Epoch [4/5], Step [4542/10336], Loss: 0.3135\n",
      "Epoch [4/5], Step [4544/10336], Loss: 0.0027\n",
      "Epoch [4/5], Step [4546/10336], Loss: 0.0198\n",
      "Epoch [4/5], Step [4548/10336], Loss: 0.8319\n",
      "Epoch [4/5], Step [4550/10336], Loss: 0.2680\n",
      "Epoch [4/5], Step [4552/10336], Loss: 0.0281\n",
      "Epoch [4/5], Step [4554/10336], Loss: 0.0038\n",
      "Epoch [4/5], Step [4556/10336], Loss: 0.4729\n",
      "Epoch [4/5], Step [4558/10336], Loss: 0.9638\n",
      "Epoch [4/5], Step [4560/10336], Loss: 0.0176\n",
      "Epoch [4/5], Step [4562/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [4564/10336], Loss: 0.8602\n",
      "Epoch [4/5], Step [4566/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [4568/10336], Loss: 0.1218\n",
      "Epoch [4/5], Step [4570/10336], Loss: 0.0228\n",
      "Epoch [4/5], Step [4572/10336], Loss: 5.0310\n",
      "Epoch [4/5], Step [4574/10336], Loss: 2.5430\n",
      "Epoch [4/5], Step [4576/10336], Loss: 0.0526\n",
      "Epoch [4/5], Step [4578/10336], Loss: 1.6968\n",
      "Epoch [4/5], Step [4580/10336], Loss: 0.1835\n",
      "Epoch [4/5], Step [4582/10336], Loss: 2.1503\n",
      "Epoch [4/5], Step [4584/10336], Loss: 0.2755\n",
      "Epoch [4/5], Step [4586/10336], Loss: 0.6070\n",
      "Epoch [4/5], Step [4588/10336], Loss: 0.0243\n",
      "Epoch [4/5], Step [4590/10336], Loss: 1.4384\n",
      "Epoch [4/5], Step [4592/10336], Loss: 1.4044\n",
      "Epoch [4/5], Step [4594/10336], Loss: 1.7175\n",
      "Epoch [4/5], Step [4596/10336], Loss: 2.4828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [4598/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [4600/10336], Loss: 1.7914\n",
      "Epoch [4/5], Step [4602/10336], Loss: 0.4608\n",
      "Epoch [4/5], Step [4604/10336], Loss: 0.8005\n",
      "Epoch [4/5], Step [4606/10336], Loss: 0.6861\n",
      "Epoch [4/5], Step [4608/10336], Loss: 1.0235\n",
      "Epoch [4/5], Step [4610/10336], Loss: 0.1212\n",
      "Epoch [4/5], Step [4612/10336], Loss: 0.0155\n",
      "Epoch [4/5], Step [4614/10336], Loss: 0.0083\n",
      "Epoch [4/5], Step [4616/10336], Loss: 1.3455\n",
      "Epoch [4/5], Step [4618/10336], Loss: 1.9602\n",
      "Epoch [4/5], Step [4620/10336], Loss: 0.6352\n",
      "Epoch [4/5], Step [4622/10336], Loss: 2.7254\n",
      "Epoch [4/5], Step [4624/10336], Loss: 0.0806\n",
      "Epoch [4/5], Step [4626/10336], Loss: 1.5790\n",
      "Epoch [4/5], Step [4628/10336], Loss: 0.9202\n",
      "Epoch [4/5], Step [4630/10336], Loss: 0.1154\n",
      "Epoch [4/5], Step [4632/10336], Loss: 0.5022\n",
      "Epoch [4/5], Step [4634/10336], Loss: 0.0254\n",
      "Epoch [4/5], Step [4636/10336], Loss: 1.3454\n",
      "Epoch [4/5], Step [4638/10336], Loss: 0.0419\n",
      "Epoch [4/5], Step [4640/10336], Loss: 4.0029\n",
      "Epoch [4/5], Step [4642/10336], Loss: 0.0750\n",
      "Epoch [4/5], Step [4644/10336], Loss: 1.6259\n",
      "Epoch [4/5], Step [4646/10336], Loss: 0.0214\n",
      "Epoch [4/5], Step [4648/10336], Loss: 2.7178\n",
      "Epoch [4/5], Step [4650/10336], Loss: 0.0270\n",
      "Epoch [4/5], Step [4652/10336], Loss: 0.0395\n",
      "Epoch [4/5], Step [4654/10336], Loss: 0.0261\n",
      "Epoch [4/5], Step [4656/10336], Loss: 1.2884\n",
      "Epoch [4/5], Step [4658/10336], Loss: 0.0439\n",
      "Epoch [4/5], Step [4660/10336], Loss: 0.6987\n",
      "Epoch [4/5], Step [4662/10336], Loss: 0.2145\n",
      "Epoch [4/5], Step [4664/10336], Loss: 0.1058\n",
      "Epoch [4/5], Step [4666/10336], Loss: 0.8639\n",
      "Epoch [4/5], Step [4668/10336], Loss: 0.3715\n",
      "Epoch [4/5], Step [4670/10336], Loss: 0.2999\n",
      "Epoch [4/5], Step [4672/10336], Loss: 0.0717\n",
      "Epoch [4/5], Step [4674/10336], Loss: 0.0182\n",
      "Epoch [4/5], Step [4676/10336], Loss: 0.0308\n",
      "Epoch [4/5], Step [4678/10336], Loss: 1.3416\n",
      "Epoch [4/5], Step [4680/10336], Loss: 0.3387\n",
      "Epoch [4/5], Step [4682/10336], Loss: 0.1209\n",
      "Epoch [4/5], Step [4684/10336], Loss: 0.7562\n",
      "Epoch [4/5], Step [4686/10336], Loss: 0.2289\n",
      "Epoch [4/5], Step [4688/10336], Loss: 0.0094\n",
      "Epoch [4/5], Step [4690/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [4692/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [4694/10336], Loss: 0.0569\n",
      "Epoch [4/5], Step [4696/10336], Loss: 0.0398\n",
      "Epoch [4/5], Step [4698/10336], Loss: 0.9648\n",
      "Epoch [4/5], Step [4700/10336], Loss: 0.0727\n",
      "Epoch [4/5], Step [4702/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [4704/10336], Loss: 0.5946\n",
      "Epoch [4/5], Step [4706/10336], Loss: 0.0567\n",
      "Epoch [4/5], Step [4708/10336], Loss: 0.1195\n",
      "Epoch [4/5], Step [4710/10336], Loss: 2.5819\n",
      "Epoch [4/5], Step [4712/10336], Loss: 0.7272\n",
      "Epoch [4/5], Step [4714/10336], Loss: 0.0558\n",
      "Epoch [4/5], Step [4716/10336], Loss: 0.3250\n",
      "Epoch [4/5], Step [4718/10336], Loss: 0.4628\n",
      "Epoch [4/5], Step [4720/10336], Loss: 0.5408\n",
      "Epoch [4/5], Step [4722/10336], Loss: 0.0170\n",
      "Epoch [4/5], Step [4724/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [4726/10336], Loss: 1.9585\n",
      "Epoch [4/5], Step [4728/10336], Loss: 0.1030\n",
      "Epoch [4/5], Step [4730/10336], Loss: 0.2250\n",
      "Epoch [4/5], Step [4732/10336], Loss: 2.4891\n",
      "Epoch [4/5], Step [4734/10336], Loss: 0.2180\n",
      "Epoch [4/5], Step [4736/10336], Loss: 0.4917\n",
      "Epoch [4/5], Step [4738/10336], Loss: 0.0685\n",
      "Epoch [4/5], Step [4740/10336], Loss: 0.0041\n",
      "Epoch [4/5], Step [4742/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [4744/10336], Loss: 0.6785\n",
      "Epoch [4/5], Step [4746/10336], Loss: 0.2075\n",
      "Epoch [4/5], Step [4748/10336], Loss: 0.7943\n",
      "Epoch [4/5], Step [4750/10336], Loss: 0.1006\n",
      "Epoch [4/5], Step [4752/10336], Loss: 0.0630\n",
      "Epoch [4/5], Step [4754/10336], Loss: 0.0789\n",
      "Epoch [4/5], Step [4756/10336], Loss: 1.3545\n",
      "Epoch [4/5], Step [4758/10336], Loss: 0.0980\n",
      "Epoch [4/5], Step [4760/10336], Loss: 0.3242\n",
      "Epoch [4/5], Step [4762/10336], Loss: 0.2261\n",
      "Epoch [4/5], Step [4764/10336], Loss: 0.1967\n",
      "Epoch [4/5], Step [4766/10336], Loss: 0.6911\n",
      "Epoch [4/5], Step [4768/10336], Loss: 0.0701\n",
      "Epoch [4/5], Step [4770/10336], Loss: 0.2835\n",
      "Epoch [4/5], Step [4772/10336], Loss: 0.0102\n",
      "Epoch [4/5], Step [4774/10336], Loss: 0.2823\n",
      "Epoch [4/5], Step [4776/10336], Loss: 0.0858\n",
      "Epoch [4/5], Step [4778/10336], Loss: 0.2072\n",
      "Epoch [4/5], Step [4780/10336], Loss: 1.3457\n",
      "Epoch [4/5], Step [4782/10336], Loss: 0.0402\n",
      "Epoch [4/5], Step [4784/10336], Loss: 0.9785\n",
      "Epoch [4/5], Step [4786/10336], Loss: 2.4794\n",
      "Epoch [4/5], Step [4788/10336], Loss: 0.0276\n",
      "Epoch [4/5], Step [4790/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [4792/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [4794/10336], Loss: 0.4106\n",
      "Epoch [4/5], Step [4796/10336], Loss: 0.1284\n",
      "Epoch [4/5], Step [4798/10336], Loss: 0.0885\n",
      "Epoch [4/5], Step [4800/10336], Loss: 1.6532\n",
      "Epoch [4/5], Step [4802/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [4804/10336], Loss: 1.8116\n",
      "Epoch [4/5], Step [4806/10336], Loss: 0.1892\n",
      "Epoch [4/5], Step [4808/10336], Loss: 0.2176\n",
      "Epoch [4/5], Step [4810/10336], Loss: 2.5021\n",
      "Epoch [4/5], Step [4812/10336], Loss: 0.0496\n",
      "Epoch [4/5], Step [4814/10336], Loss: 1.8656\n",
      "Epoch [4/5], Step [4816/10336], Loss: 0.0379\n",
      "Epoch [4/5], Step [4818/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [4820/10336], Loss: 0.6056\n",
      "Epoch [4/5], Step [4822/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [4824/10336], Loss: 0.0083\n",
      "Epoch [4/5], Step [4826/10336], Loss: 0.9593\n",
      "Epoch [4/5], Step [4828/10336], Loss: 0.0694\n",
      "Epoch [4/5], Step [4830/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [4832/10336], Loss: 0.3542\n",
      "Epoch [4/5], Step [4834/10336], Loss: 1.9699\n",
      "Epoch [4/5], Step [4836/10336], Loss: 0.0083\n",
      "Epoch [4/5], Step [4838/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [4840/10336], Loss: 0.6160\n",
      "Epoch [4/5], Step [4842/10336], Loss: 0.0304\n",
      "Epoch [4/5], Step [4844/10336], Loss: 0.5489\n",
      "Epoch [4/5], Step [4846/10336], Loss: 0.2292\n",
      "Epoch [4/5], Step [4848/10336], Loss: 0.0625\n",
      "Epoch [4/5], Step [4850/10336], Loss: 0.0232\n",
      "Epoch [4/5], Step [4852/10336], Loss: 0.3648\n",
      "Epoch [4/5], Step [4854/10336], Loss: 0.9643\n",
      "Epoch [4/5], Step [4856/10336], Loss: 1.4754\n",
      "Epoch [4/5], Step [4858/10336], Loss: 0.0324\n",
      "Epoch [4/5], Step [4860/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [4862/10336], Loss: 3.6370\n",
      "Epoch [4/5], Step [4864/10336], Loss: 0.7290\n",
      "Epoch [4/5], Step [4866/10336], Loss: 3.6731\n",
      "Epoch [4/5], Step [4868/10336], Loss: 2.1203\n",
      "Epoch [4/5], Step [4870/10336], Loss: 0.5326\n",
      "Epoch [4/5], Step [4872/10336], Loss: 0.0058\n",
      "Epoch [4/5], Step [4874/10336], Loss: 0.0385\n",
      "Epoch [4/5], Step [4876/10336], Loss: 1.0546\n",
      "Epoch [4/5], Step [4878/10336], Loss: 0.4165\n",
      "Epoch [4/5], Step [4880/10336], Loss: 1.2342\n",
      "Epoch [4/5], Step [4882/10336], Loss: 0.2275\n",
      "Epoch [4/5], Step [4884/10336], Loss: 0.1525\n",
      "Epoch [4/5], Step [4886/10336], Loss: 0.1800\n",
      "Epoch [4/5], Step [4888/10336], Loss: 0.0237\n",
      "Epoch [4/5], Step [4890/10336], Loss: 0.0435\n",
      "Epoch [4/5], Step [4892/10336], Loss: 0.0108\n",
      "Epoch [4/5], Step [4894/10336], Loss: 1.4327\n",
      "Epoch [4/5], Step [4896/10336], Loss: 0.5031\n",
      "Epoch [4/5], Step [4898/10336], Loss: 0.1168\n",
      "Epoch [4/5], Step [4900/10336], Loss: 1.2956\n",
      "Epoch [4/5], Step [4902/10336], Loss: 1.0504\n",
      "Epoch [4/5], Step [4904/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [4906/10336], Loss: 0.1718\n",
      "Epoch [4/5], Step [4908/10336], Loss: 0.4169\n",
      "Epoch [4/5], Step [4910/10336], Loss: 1.2175\n",
      "Epoch [4/5], Step [4912/10336], Loss: 2.5089\n",
      "Epoch [4/5], Step [4914/10336], Loss: 1.3232\n",
      "Epoch [4/5], Step [4916/10336], Loss: 1.1779\n",
      "Epoch [4/5], Step [4918/10336], Loss: 1.1004\n",
      "Epoch [4/5], Step [4920/10336], Loss: 1.3655\n",
      "Epoch [4/5], Step [4922/10336], Loss: 1.8394\n",
      "Epoch [4/5], Step [4924/10336], Loss: 1.2212\n",
      "Epoch [4/5], Step [4926/10336], Loss: 0.0651\n",
      "Epoch [4/5], Step [4928/10336], Loss: 0.0845\n",
      "Epoch [4/5], Step [4930/10336], Loss: 1.6016\n",
      "Epoch [4/5], Step [4932/10336], Loss: 0.6697\n",
      "Epoch [4/5], Step [4934/10336], Loss: 0.2747\n",
      "Epoch [4/5], Step [4936/10336], Loss: 1.0069\n",
      "Epoch [4/5], Step [4938/10336], Loss: 0.0620\n",
      "Epoch [4/5], Step [4940/10336], Loss: 0.1746\n",
      "Epoch [4/5], Step [4942/10336], Loss: 0.5065\n",
      "Epoch [4/5], Step [4944/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [4946/10336], Loss: 0.4507\n",
      "Epoch [4/5], Step [4948/10336], Loss: 2.1087\n",
      "Epoch [4/5], Step [4950/10336], Loss: 2.2897\n",
      "Epoch [4/5], Step [4952/10336], Loss: 3.9618\n",
      "Epoch [4/5], Step [4954/10336], Loss: 0.1319\n",
      "Epoch [4/5], Step [4956/10336], Loss: 0.0552\n",
      "Epoch [4/5], Step [4958/10336], Loss: 2.0782\n",
      "Epoch [4/5], Step [4960/10336], Loss: 0.0610\n",
      "Epoch [4/5], Step [4962/10336], Loss: 4.2909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [4964/10336], Loss: 0.6307\n",
      "Epoch [4/5], Step [4966/10336], Loss: 0.1594\n",
      "Epoch [4/5], Step [4968/10336], Loss: 0.3383\n",
      "Epoch [4/5], Step [4970/10336], Loss: 1.1500\n",
      "Epoch [4/5], Step [4972/10336], Loss: 0.0880\n",
      "Epoch [4/5], Step [4974/10336], Loss: 0.0159\n",
      "Epoch [4/5], Step [4976/10336], Loss: 0.0492\n",
      "Epoch [4/5], Step [4978/10336], Loss: 0.6907\n",
      "Epoch [4/5], Step [4980/10336], Loss: 0.0306\n",
      "Epoch [4/5], Step [4982/10336], Loss: 0.0231\n",
      "Epoch [4/5], Step [4984/10336], Loss: 0.2256\n",
      "Epoch [4/5], Step [4986/10336], Loss: 0.0922\n",
      "Epoch [4/5], Step [4988/10336], Loss: 1.0630\n",
      "Epoch [4/5], Step [4990/10336], Loss: 1.7947\n",
      "Epoch [4/5], Step [4992/10336], Loss: 0.1544\n",
      "Epoch [4/5], Step [4994/10336], Loss: 0.6781\n",
      "Epoch [4/5], Step [4996/10336], Loss: 0.0290\n",
      "Epoch [4/5], Step [4998/10336], Loss: 0.1126\n",
      "Epoch [4/5], Step [5000/10336], Loss: 1.0733\n",
      "Epoch [4/5], Step [5002/10336], Loss: 0.0455\n",
      "Epoch [4/5], Step [5004/10336], Loss: 0.0088\n",
      "Epoch [4/5], Step [5006/10336], Loss: 0.0118\n",
      "Epoch [4/5], Step [5008/10336], Loss: 0.7153\n",
      "Epoch [4/5], Step [5010/10336], Loss: 0.0084\n",
      "Epoch [4/5], Step [5012/10336], Loss: 0.0877\n",
      "Epoch [4/5], Step [5014/10336], Loss: 0.8670\n",
      "Epoch [4/5], Step [5016/10336], Loss: 4.4284\n",
      "Epoch [4/5], Step [5018/10336], Loss: 0.0636\n",
      "Epoch [4/5], Step [5020/10336], Loss: 0.3352\n",
      "Epoch [4/5], Step [5022/10336], Loss: 0.4503\n",
      "Epoch [4/5], Step [5024/10336], Loss: 0.5789\n",
      "Epoch [4/5], Step [5026/10336], Loss: 1.2510\n",
      "Epoch [4/5], Step [5028/10336], Loss: 0.7328\n",
      "Epoch [4/5], Step [5030/10336], Loss: 0.0391\n",
      "Epoch [4/5], Step [5032/10336], Loss: 0.0686\n",
      "Epoch [4/5], Step [5034/10336], Loss: 0.6681\n",
      "Epoch [4/5], Step [5036/10336], Loss: 0.6053\n",
      "Epoch [4/5], Step [5038/10336], Loss: 0.5482\n",
      "Epoch [4/5], Step [5040/10336], Loss: 1.6960\n",
      "Epoch [4/5], Step [5042/10336], Loss: 2.4304\n",
      "Epoch [4/5], Step [5044/10336], Loss: 2.1355\n",
      "Epoch [4/5], Step [5046/10336], Loss: 0.0107\n",
      "Epoch [4/5], Step [5048/10336], Loss: 1.1378\n",
      "Epoch [4/5], Step [5050/10336], Loss: 0.5086\n",
      "Epoch [4/5], Step [5052/10336], Loss: 0.0484\n",
      "Epoch [4/5], Step [5054/10336], Loss: 0.0375\n",
      "Epoch [4/5], Step [5056/10336], Loss: 0.0369\n",
      "Epoch [4/5], Step [5058/10336], Loss: 1.4321\n",
      "Epoch [4/5], Step [5060/10336], Loss: 0.1052\n",
      "Epoch [4/5], Step [5062/10336], Loss: 0.0265\n",
      "Epoch [4/5], Step [5064/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [5066/10336], Loss: 0.2164\n",
      "Epoch [4/5], Step [5068/10336], Loss: 0.0523\n",
      "Epoch [4/5], Step [5070/10336], Loss: 0.0052\n",
      "Epoch [4/5], Step [5072/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [5074/10336], Loss: 0.8384\n",
      "Epoch [4/5], Step [5076/10336], Loss: 3.1270\n",
      "Epoch [4/5], Step [5078/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [5080/10336], Loss: 0.0484\n",
      "Epoch [4/5], Step [5082/10336], Loss: 0.0221\n",
      "Epoch [4/5], Step [5084/10336], Loss: 0.5029\n",
      "Epoch [4/5], Step [5086/10336], Loss: 5.0723\n",
      "Epoch [4/5], Step [5088/10336], Loss: 0.1010\n",
      "Epoch [4/5], Step [5090/10336], Loss: 1.9873\n",
      "Epoch [4/5], Step [5092/10336], Loss: 0.7068\n",
      "Epoch [4/5], Step [5094/10336], Loss: 1.3625\n",
      "Epoch [4/5], Step [5096/10336], Loss: 1.9289\n",
      "Epoch [4/5], Step [5098/10336], Loss: 0.8510\n",
      "Epoch [4/5], Step [5100/10336], Loss: 0.1303\n",
      "Epoch [4/5], Step [5102/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [5104/10336], Loss: 0.0838\n",
      "Epoch [4/5], Step [5106/10336], Loss: 0.3543\n",
      "Epoch [4/5], Step [5108/10336], Loss: 0.0963\n",
      "Epoch [4/5], Step [5110/10336], Loss: 0.0273\n",
      "Epoch [4/5], Step [5112/10336], Loss: 1.1567\n",
      "Epoch [4/5], Step [5114/10336], Loss: 0.1146\n",
      "Epoch [4/5], Step [5116/10336], Loss: 0.0043\n",
      "Epoch [4/5], Step [5118/10336], Loss: 0.7658\n",
      "Epoch [4/5], Step [5120/10336], Loss: 0.0141\n",
      "Epoch [4/5], Step [5122/10336], Loss: 0.7319\n",
      "Epoch [4/5], Step [5124/10336], Loss: 1.6827\n",
      "Epoch [4/5], Step [5126/10336], Loss: 2.2143\n",
      "Epoch [4/5], Step [5128/10336], Loss: 0.4042\n",
      "Epoch [4/5], Step [5130/10336], Loss: 2.7312\n",
      "Epoch [4/5], Step [5132/10336], Loss: 0.9518\n",
      "Epoch [4/5], Step [5134/10336], Loss: 0.1834\n",
      "Epoch [4/5], Step [5136/10336], Loss: 0.4504\n",
      "Epoch [4/5], Step [5138/10336], Loss: 0.0795\n",
      "Epoch [4/5], Step [5140/10336], Loss: 0.0487\n",
      "Epoch [4/5], Step [5142/10336], Loss: 0.0116\n",
      "Epoch [4/5], Step [5144/10336], Loss: 0.0651\n",
      "Epoch [4/5], Step [5146/10336], Loss: 3.5929\n",
      "Epoch [4/5], Step [5148/10336], Loss: 1.7506\n",
      "Epoch [4/5], Step [5150/10336], Loss: 1.6379\n",
      "Epoch [4/5], Step [5152/10336], Loss: 0.0140\n",
      "Epoch [4/5], Step [5154/10336], Loss: 0.0065\n",
      "Epoch [4/5], Step [5156/10336], Loss: 0.0313\n",
      "Epoch [4/5], Step [5158/10336], Loss: 0.0064\n",
      "Epoch [4/5], Step [5160/10336], Loss: 1.8956\n",
      "Epoch [4/5], Step [5162/10336], Loss: 0.9187\n",
      "Epoch [4/5], Step [5164/10336], Loss: 1.2361\n",
      "Epoch [4/5], Step [5166/10336], Loss: 0.1508\n",
      "Epoch [4/5], Step [5168/10336], Loss: 0.2437\n",
      "Epoch [4/5], Step [5170/10336], Loss: 1.0755\n",
      "Epoch [4/5], Step [5172/10336], Loss: 0.2809\n",
      "Epoch [4/5], Step [5174/10336], Loss: 0.3777\n",
      "Epoch [4/5], Step [5176/10336], Loss: 0.0324\n",
      "Epoch [4/5], Step [5178/10336], Loss: 0.0088\n",
      "Epoch [4/5], Step [5180/10336], Loss: 0.0240\n",
      "Epoch [4/5], Step [5182/10336], Loss: 0.0189\n",
      "Epoch [4/5], Step [5184/10336], Loss: 0.0643\n",
      "Epoch [4/5], Step [5186/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [5188/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [5190/10336], Loss: 0.0149\n",
      "Epoch [4/5], Step [5192/10336], Loss: 0.0175\n",
      "Epoch [4/5], Step [5194/10336], Loss: 0.4662\n",
      "Epoch [4/5], Step [5196/10336], Loss: 2.3689\n",
      "Epoch [4/5], Step [5198/10336], Loss: 5.6990\n",
      "Epoch [4/5], Step [5200/10336], Loss: 0.2079\n",
      "Epoch [4/5], Step [5202/10336], Loss: 3.0082\n",
      "Epoch [4/5], Step [5204/10336], Loss: 2.2085\n",
      "Epoch [4/5], Step [5206/10336], Loss: 0.7216\n",
      "Epoch [4/5], Step [5208/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [5210/10336], Loss: 1.7078\n",
      "Epoch [4/5], Step [5212/10336], Loss: 1.5673\n",
      "Epoch [4/5], Step [5214/10336], Loss: 0.6739\n",
      "Epoch [4/5], Step [5216/10336], Loss: 1.8750\n",
      "Epoch [4/5], Step [5218/10336], Loss: 1.8073\n",
      "Epoch [4/5], Step [5220/10336], Loss: 0.2737\n",
      "Epoch [4/5], Step [5222/10336], Loss: 1.9504\n",
      "Epoch [4/5], Step [5224/10336], Loss: 0.0319\n",
      "Epoch [4/5], Step [5226/10336], Loss: 0.1179\n",
      "Epoch [4/5], Step [5228/10336], Loss: 1.8146\n",
      "Epoch [4/5], Step [5230/10336], Loss: 0.0633\n",
      "Epoch [4/5], Step [5232/10336], Loss: 0.1198\n",
      "Epoch [4/5], Step [5234/10336], Loss: 0.5256\n",
      "Epoch [4/5], Step [5236/10336], Loss: 1.5368\n",
      "Epoch [4/5], Step [5238/10336], Loss: 1.0935\n",
      "Epoch [4/5], Step [5240/10336], Loss: 1.5272\n",
      "Epoch [4/5], Step [5242/10336], Loss: 1.1674\n",
      "Epoch [4/5], Step [5244/10336], Loss: 1.9181\n",
      "Epoch [4/5], Step [5246/10336], Loss: 1.8651\n",
      "Epoch [4/5], Step [5248/10336], Loss: 0.3348\n",
      "Epoch [4/5], Step [5250/10336], Loss: 1.9774\n",
      "Epoch [4/5], Step [5252/10336], Loss: 0.0555\n",
      "Epoch [4/5], Step [5254/10336], Loss: 0.0314\n",
      "Epoch [4/5], Step [5256/10336], Loss: 0.8658\n",
      "Epoch [4/5], Step [5258/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [5260/10336], Loss: 1.8618\n",
      "Epoch [4/5], Step [5262/10336], Loss: 0.0390\n",
      "Epoch [4/5], Step [5264/10336], Loss: 0.4322\n",
      "Epoch [4/5], Step [5266/10336], Loss: 0.6154\n",
      "Epoch [4/5], Step [5268/10336], Loss: 0.0517\n",
      "Epoch [4/5], Step [5270/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [5272/10336], Loss: 0.3046\n",
      "Epoch [4/5], Step [5274/10336], Loss: 0.0081\n",
      "Epoch [4/5], Step [5276/10336], Loss: 0.1974\n",
      "Epoch [4/5], Step [5278/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [5280/10336], Loss: 0.0254\n",
      "Epoch [4/5], Step [5282/10336], Loss: 0.1975\n",
      "Epoch [4/5], Step [5284/10336], Loss: 0.1429\n",
      "Epoch [4/5], Step [5286/10336], Loss: 1.6814\n",
      "Epoch [4/5], Step [5288/10336], Loss: 1.4101\n",
      "Epoch [4/5], Step [5290/10336], Loss: 0.4884\n",
      "Epoch [4/5], Step [5292/10336], Loss: 0.0566\n",
      "Epoch [4/5], Step [5294/10336], Loss: 0.1542\n",
      "Epoch [4/5], Step [5296/10336], Loss: 0.4889\n",
      "Epoch [4/5], Step [5298/10336], Loss: 0.0159\n",
      "Epoch [4/5], Step [5300/10336], Loss: 2.2638\n",
      "Epoch [4/5], Step [5302/10336], Loss: 0.0697\n",
      "Epoch [4/5], Step [5304/10336], Loss: 0.2745\n",
      "Epoch [4/5], Step [5306/10336], Loss: 0.2137\n",
      "Epoch [4/5], Step [5308/10336], Loss: 0.0119\n",
      "Epoch [4/5], Step [5310/10336], Loss: 0.0146\n",
      "Epoch [4/5], Step [5312/10336], Loss: 0.0731\n",
      "Epoch [4/5], Step [5314/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [5316/10336], Loss: 0.3216\n",
      "Epoch [4/5], Step [5318/10336], Loss: 1.5823\n",
      "Epoch [4/5], Step [5320/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [5322/10336], Loss: 0.0968\n",
      "Epoch [4/5], Step [5324/10336], Loss: 0.8386\n",
      "Epoch [4/5], Step [5326/10336], Loss: 0.1535\n",
      "Epoch [4/5], Step [5328/10336], Loss: 0.6362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [5330/10336], Loss: 0.0716\n",
      "Epoch [4/5], Step [5332/10336], Loss: 0.1242\n",
      "Epoch [4/5], Step [5334/10336], Loss: 1.0203\n",
      "Epoch [4/5], Step [5336/10336], Loss: 0.0778\n",
      "Epoch [4/5], Step [5338/10336], Loss: 5.0255\n",
      "Epoch [4/5], Step [5340/10336], Loss: 0.0082\n",
      "Epoch [4/5], Step [5342/10336], Loss: 1.4223\n",
      "Epoch [4/5], Step [5344/10336], Loss: 0.6073\n",
      "Epoch [4/5], Step [5346/10336], Loss: 0.0879\n",
      "Epoch [4/5], Step [5348/10336], Loss: 0.0678\n",
      "Epoch [4/5], Step [5350/10336], Loss: 0.1198\n",
      "Epoch [4/5], Step [5352/10336], Loss: 4.8332\n",
      "Epoch [4/5], Step [5354/10336], Loss: 0.2644\n",
      "Epoch [4/5], Step [5356/10336], Loss: 0.3720\n",
      "Epoch [4/5], Step [5358/10336], Loss: 1.7946\n",
      "Epoch [4/5], Step [5360/10336], Loss: 0.0730\n",
      "Epoch [4/5], Step [5362/10336], Loss: 1.5097\n",
      "Epoch [4/5], Step [5364/10336], Loss: 1.0451\n",
      "Epoch [4/5], Step [5366/10336], Loss: 0.1735\n",
      "Epoch [4/5], Step [5368/10336], Loss: 0.2506\n",
      "Epoch [4/5], Step [5370/10336], Loss: 0.0438\n",
      "Epoch [4/5], Step [5372/10336], Loss: 0.7546\n",
      "Epoch [4/5], Step [5374/10336], Loss: 2.5826\n",
      "Epoch [4/5], Step [5376/10336], Loss: 0.3760\n",
      "Epoch [4/5], Step [5378/10336], Loss: 0.0475\n",
      "Epoch [4/5], Step [5380/10336], Loss: 0.1393\n",
      "Epoch [4/5], Step [5382/10336], Loss: 0.4612\n",
      "Epoch [4/5], Step [5384/10336], Loss: 1.9321\n",
      "Epoch [4/5], Step [5386/10336], Loss: 0.2009\n",
      "Epoch [4/5], Step [5388/10336], Loss: 0.0111\n",
      "Epoch [4/5], Step [5390/10336], Loss: 1.1512\n",
      "Epoch [4/5], Step [5392/10336], Loss: 0.0680\n",
      "Epoch [4/5], Step [5394/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [5396/10336], Loss: 1.2490\n",
      "Epoch [4/5], Step [5398/10336], Loss: 0.0309\n",
      "Epoch [4/5], Step [5400/10336], Loss: 1.1060\n",
      "Epoch [4/5], Step [5402/10336], Loss: 3.8856\n",
      "Epoch [4/5], Step [5404/10336], Loss: 0.0586\n",
      "Epoch [4/5], Step [5406/10336], Loss: 0.0188\n",
      "Epoch [4/5], Step [5408/10336], Loss: 0.0357\n",
      "Epoch [4/5], Step [5410/10336], Loss: 0.0467\n",
      "Epoch [4/5], Step [5412/10336], Loss: 0.0790\n",
      "Epoch [4/5], Step [5414/10336], Loss: 1.8532\n",
      "Epoch [4/5], Step [5416/10336], Loss: 0.1630\n",
      "Epoch [4/5], Step [5418/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [5420/10336], Loss: 0.0836\n",
      "Epoch [4/5], Step [5422/10336], Loss: 0.9005\n",
      "Epoch [4/5], Step [5424/10336], Loss: 0.4544\n",
      "Epoch [4/5], Step [5426/10336], Loss: 0.4593\n",
      "Epoch [4/5], Step [5428/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [5430/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [5432/10336], Loss: 1.8424\n",
      "Epoch [4/5], Step [5434/10336], Loss: 0.0711\n",
      "Epoch [4/5], Step [5436/10336], Loss: 0.0697\n",
      "Epoch [4/5], Step [5438/10336], Loss: 0.6796\n",
      "Epoch [4/5], Step [5440/10336], Loss: 2.0823\n",
      "Epoch [4/5], Step [5442/10336], Loss: 0.3482\n",
      "Epoch [4/5], Step [5444/10336], Loss: 0.0818\n",
      "Epoch [4/5], Step [5446/10336], Loss: 0.1431\n",
      "Epoch [4/5], Step [5448/10336], Loss: 0.4178\n",
      "Epoch [4/5], Step [5450/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [5452/10336], Loss: 0.0148\n",
      "Epoch [4/5], Step [5454/10336], Loss: 0.0060\n",
      "Epoch [4/5], Step [5456/10336], Loss: 0.9273\n",
      "Epoch [4/5], Step [5458/10336], Loss: 0.0201\n",
      "Epoch [4/5], Step [5460/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [5462/10336], Loss: 0.5160\n",
      "Epoch [4/5], Step [5464/10336], Loss: 0.0920\n",
      "Epoch [4/5], Step [5466/10336], Loss: 0.0774\n",
      "Epoch [4/5], Step [5468/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [5470/10336], Loss: 0.7517\n",
      "Epoch [4/5], Step [5472/10336], Loss: 0.1606\n",
      "Epoch [4/5], Step [5474/10336], Loss: 0.0664\n",
      "Epoch [4/5], Step [5476/10336], Loss: 1.4154\n",
      "Epoch [4/5], Step [5478/10336], Loss: 0.9710\n",
      "Epoch [4/5], Step [5480/10336], Loss: 1.5452\n",
      "Epoch [4/5], Step [5482/10336], Loss: 1.4091\n",
      "Epoch [4/5], Step [5484/10336], Loss: 0.0125\n",
      "Epoch [4/5], Step [5486/10336], Loss: 0.4940\n",
      "Epoch [4/5], Step [5488/10336], Loss: 1.0665\n",
      "Epoch [4/5], Step [5490/10336], Loss: 0.1966\n",
      "Epoch [4/5], Step [5492/10336], Loss: 0.5523\n",
      "Epoch [4/5], Step [5494/10336], Loss: 0.5367\n",
      "Epoch [4/5], Step [5496/10336], Loss: 0.5896\n",
      "Epoch [4/5], Step [5498/10336], Loss: 1.4236\n",
      "Epoch [4/5], Step [5500/10336], Loss: 0.1758\n",
      "Epoch [4/5], Step [5502/10336], Loss: 0.0500\n",
      "Epoch [4/5], Step [5504/10336], Loss: 0.9636\n",
      "Epoch [4/5], Step [5506/10336], Loss: 1.9545\n",
      "Epoch [4/5], Step [5508/10336], Loss: 0.0397\n",
      "Epoch [4/5], Step [5510/10336], Loss: 2.5076\n",
      "Epoch [4/5], Step [5512/10336], Loss: 0.1603\n",
      "Epoch [4/5], Step [5514/10336], Loss: 2.5141\n",
      "Epoch [4/5], Step [5516/10336], Loss: 0.3023\n",
      "Epoch [4/5], Step [5518/10336], Loss: 0.1106\n",
      "Epoch [4/5], Step [5520/10336], Loss: 0.0041\n",
      "Epoch [4/5], Step [5522/10336], Loss: 1.2356\n",
      "Epoch [4/5], Step [5524/10336], Loss: 0.0162\n",
      "Epoch [4/5], Step [5526/10336], Loss: 0.9273\n",
      "Epoch [4/5], Step [5528/10336], Loss: 0.2127\n",
      "Epoch [4/5], Step [5530/10336], Loss: 0.0058\n",
      "Epoch [4/5], Step [5532/10336], Loss: 1.1010\n",
      "Epoch [4/5], Step [5534/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [5536/10336], Loss: 2.2374\n",
      "Epoch [4/5], Step [5538/10336], Loss: 0.3144\n",
      "Epoch [4/5], Step [5540/10336], Loss: 3.1026\n",
      "Epoch [4/5], Step [5542/10336], Loss: 0.0615\n",
      "Epoch [4/5], Step [5544/10336], Loss: 0.6164\n",
      "Epoch [4/5], Step [5546/10336], Loss: 3.1606\n",
      "Epoch [4/5], Step [5548/10336], Loss: 1.6415\n",
      "Epoch [4/5], Step [5550/10336], Loss: 0.7708\n",
      "Epoch [4/5], Step [5552/10336], Loss: 0.1638\n",
      "Epoch [4/5], Step [5554/10336], Loss: 0.4193\n",
      "Epoch [4/5], Step [5556/10336], Loss: 0.0224\n",
      "Epoch [4/5], Step [5558/10336], Loss: 0.0089\n",
      "Epoch [4/5], Step [5560/10336], Loss: 0.7543\n",
      "Epoch [4/5], Step [5562/10336], Loss: 2.0866\n",
      "Epoch [4/5], Step [5564/10336], Loss: 0.0449\n",
      "Epoch [4/5], Step [5566/10336], Loss: 1.0920\n",
      "Epoch [4/5], Step [5568/10336], Loss: 0.4930\n",
      "Epoch [4/5], Step [5570/10336], Loss: 1.0642\n",
      "Epoch [4/5], Step [5572/10336], Loss: 0.5118\n",
      "Epoch [4/5], Step [5574/10336], Loss: 0.6261\n",
      "Epoch [4/5], Step [5576/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [5578/10336], Loss: 0.4362\n",
      "Epoch [4/5], Step [5580/10336], Loss: 0.0830\n",
      "Epoch [4/5], Step [5582/10336], Loss: 1.0531\n",
      "Epoch [4/5], Step [5584/10336], Loss: 0.0742\n",
      "Epoch [4/5], Step [5586/10336], Loss: 0.0335\n",
      "Epoch [4/5], Step [5588/10336], Loss: 0.5461\n",
      "Epoch [4/5], Step [5590/10336], Loss: 5.7609\n",
      "Epoch [4/5], Step [5592/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [5594/10336], Loss: 0.3231\n",
      "Epoch [4/5], Step [5596/10336], Loss: 0.2291\n",
      "Epoch [4/5], Step [5598/10336], Loss: 0.0528\n",
      "Epoch [4/5], Step [5600/10336], Loss: 0.6241\n",
      "Epoch [4/5], Step [5602/10336], Loss: 0.3557\n",
      "Epoch [4/5], Step [5604/10336], Loss: 1.7603\n",
      "Epoch [4/5], Step [5606/10336], Loss: 0.2412\n",
      "Epoch [4/5], Step [5608/10336], Loss: 1.3363\n",
      "Epoch [4/5], Step [5610/10336], Loss: 0.6627\n",
      "Epoch [4/5], Step [5612/10336], Loss: 1.9236\n",
      "Epoch [4/5], Step [5614/10336], Loss: 0.5300\n",
      "Epoch [4/5], Step [5616/10336], Loss: 0.0136\n",
      "Epoch [4/5], Step [5618/10336], Loss: 1.1181\n",
      "Epoch [4/5], Step [5620/10336], Loss: 0.0056\n",
      "Epoch [4/5], Step [5622/10336], Loss: 0.9696\n",
      "Epoch [4/5], Step [5624/10336], Loss: 0.1222\n",
      "Epoch [4/5], Step [5626/10336], Loss: 1.0582\n",
      "Epoch [4/5], Step [5628/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [5630/10336], Loss: 1.0165\n",
      "Epoch [4/5], Step [5632/10336], Loss: 1.1242\n",
      "Epoch [4/5], Step [5634/10336], Loss: 0.2416\n",
      "Epoch [4/5], Step [5636/10336], Loss: 0.2211\n",
      "Epoch [4/5], Step [5638/10336], Loss: 0.1295\n",
      "Epoch [4/5], Step [5640/10336], Loss: 0.0794\n",
      "Epoch [4/5], Step [5642/10336], Loss: 0.1711\n",
      "Epoch [4/5], Step [5644/10336], Loss: 0.0351\n",
      "Epoch [4/5], Step [5646/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [5648/10336], Loss: 1.0646\n",
      "Epoch [4/5], Step [5650/10336], Loss: 0.1654\n",
      "Epoch [4/5], Step [5652/10336], Loss: 0.8600\n",
      "Epoch [4/5], Step [5654/10336], Loss: 2.0069\n",
      "Epoch [4/5], Step [5656/10336], Loss: 2.0753\n",
      "Epoch [4/5], Step [5658/10336], Loss: 0.1371\n",
      "Epoch [4/5], Step [5660/10336], Loss: 0.9238\n",
      "Epoch [4/5], Step [5662/10336], Loss: 0.0181\n",
      "Epoch [4/5], Step [5664/10336], Loss: 0.0329\n",
      "Epoch [4/5], Step [5666/10336], Loss: 0.0115\n",
      "Epoch [4/5], Step [5668/10336], Loss: 2.4285\n",
      "Epoch [4/5], Step [5670/10336], Loss: 0.1486\n",
      "Epoch [4/5], Step [5672/10336], Loss: 0.5442\n",
      "Epoch [4/5], Step [5674/10336], Loss: 0.4901\n",
      "Epoch [4/5], Step [5676/10336], Loss: 0.7780\n",
      "Epoch [4/5], Step [5678/10336], Loss: 0.0434\n",
      "Epoch [4/5], Step [5680/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [5682/10336], Loss: 1.3329\n",
      "Epoch [4/5], Step [5684/10336], Loss: 1.5472\n",
      "Epoch [4/5], Step [5686/10336], Loss: 1.3436\n",
      "Epoch [4/5], Step [5688/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [5690/10336], Loss: 2.8957\n",
      "Epoch [4/5], Step [5692/10336], Loss: 0.2449\n",
      "Epoch [4/5], Step [5694/10336], Loss: 1.0843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [5696/10336], Loss: 1.9898\n",
      "Epoch [4/5], Step [5698/10336], Loss: 0.5057\n",
      "Epoch [4/5], Step [5700/10336], Loss: 0.8887\n",
      "Epoch [4/5], Step [5702/10336], Loss: 0.2377\n",
      "Epoch [4/5], Step [5704/10336], Loss: 0.0762\n",
      "Epoch [4/5], Step [5706/10336], Loss: 1.3223\n",
      "Epoch [4/5], Step [5708/10336], Loss: 0.0183\n",
      "Epoch [4/5], Step [5710/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [5712/10336], Loss: 0.0197\n",
      "Epoch [4/5], Step [5714/10336], Loss: 0.2016\n",
      "Epoch [4/5], Step [5716/10336], Loss: 0.7528\n",
      "Epoch [4/5], Step [5718/10336], Loss: 0.0147\n",
      "Epoch [4/5], Step [5720/10336], Loss: 1.2892\n",
      "Epoch [4/5], Step [5722/10336], Loss: 0.1361\n",
      "Epoch [4/5], Step [5724/10336], Loss: 0.1518\n",
      "Epoch [4/5], Step [5726/10336], Loss: 0.0072\n",
      "Epoch [4/5], Step [5728/10336], Loss: 0.5889\n",
      "Epoch [4/5], Step [5730/10336], Loss: 0.2949\n",
      "Epoch [4/5], Step [5732/10336], Loss: 0.1757\n",
      "Epoch [4/5], Step [5734/10336], Loss: 0.0603\n",
      "Epoch [4/5], Step [5736/10336], Loss: 0.1480\n",
      "Epoch [4/5], Step [5738/10336], Loss: 0.4454\n",
      "Epoch [4/5], Step [5740/10336], Loss: 0.8963\n",
      "Epoch [4/5], Step [5742/10336], Loss: 0.0320\n",
      "Epoch [4/5], Step [5744/10336], Loss: 0.8266\n",
      "Epoch [4/5], Step [5746/10336], Loss: 2.3578\n",
      "Epoch [4/5], Step [5748/10336], Loss: 0.0084\n",
      "Epoch [4/5], Step [5750/10336], Loss: 0.0344\n",
      "Epoch [4/5], Step [5752/10336], Loss: 1.4442\n",
      "Epoch [4/5], Step [5754/10336], Loss: 0.0038\n",
      "Epoch [4/5], Step [5756/10336], Loss: 0.3285\n",
      "Epoch [4/5], Step [5758/10336], Loss: 0.7472\n",
      "Epoch [4/5], Step [5760/10336], Loss: 0.3984\n",
      "Epoch [4/5], Step [5762/10336], Loss: 0.0944\n",
      "Epoch [4/5], Step [5764/10336], Loss: 0.0888\n",
      "Epoch [4/5], Step [5766/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [5768/10336], Loss: 1.7528\n",
      "Epoch [4/5], Step [5770/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [5772/10336], Loss: 0.0571\n",
      "Epoch [4/5], Step [5774/10336], Loss: 0.1934\n",
      "Epoch [4/5], Step [5776/10336], Loss: 0.2771\n",
      "Epoch [4/5], Step [5778/10336], Loss: 0.0653\n",
      "Epoch [4/5], Step [5780/10336], Loss: 0.1263\n",
      "Epoch [4/5], Step [5782/10336], Loss: 0.0116\n",
      "Epoch [4/5], Step [5784/10336], Loss: 1.9602\n",
      "Epoch [4/5], Step [5786/10336], Loss: 0.6512\n",
      "Epoch [4/5], Step [5788/10336], Loss: 0.0513\n",
      "Epoch [4/5], Step [5790/10336], Loss: 0.1365\n",
      "Epoch [4/5], Step [5792/10336], Loss: 0.1008\n",
      "Epoch [4/5], Step [5794/10336], Loss: 1.6775\n",
      "Epoch [4/5], Step [5796/10336], Loss: 2.6401\n",
      "Epoch [4/5], Step [5798/10336], Loss: 1.8741\n",
      "Epoch [4/5], Step [5800/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [5802/10336], Loss: 0.7254\n",
      "Epoch [4/5], Step [5804/10336], Loss: 0.0784\n",
      "Epoch [4/5], Step [5806/10336], Loss: 0.0666\n",
      "Epoch [4/5], Step [5808/10336], Loss: 0.0565\n",
      "Epoch [4/5], Step [5810/10336], Loss: 0.4268\n",
      "Epoch [4/5], Step [5812/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [5814/10336], Loss: 0.0986\n",
      "Epoch [4/5], Step [5816/10336], Loss: 0.2539\n",
      "Epoch [4/5], Step [5818/10336], Loss: 0.0853\n",
      "Epoch [4/5], Step [5820/10336], Loss: 0.4695\n",
      "Epoch [4/5], Step [5822/10336], Loss: 0.0310\n",
      "Epoch [4/5], Step [5824/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [5826/10336], Loss: 3.1374\n",
      "Epoch [4/5], Step [5828/10336], Loss: 2.1802\n",
      "Epoch [4/5], Step [5830/10336], Loss: 3.4397\n",
      "Epoch [4/5], Step [5832/10336], Loss: 1.0406\n",
      "Epoch [4/5], Step [5834/10336], Loss: 0.3087\n",
      "Epoch [4/5], Step [5836/10336], Loss: 0.0218\n",
      "Epoch [4/5], Step [5838/10336], Loss: 0.8896\n",
      "Epoch [4/5], Step [5840/10336], Loss: 0.2124\n",
      "Epoch [4/5], Step [5842/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [5844/10336], Loss: 0.0281\n",
      "Epoch [4/5], Step [5846/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [5848/10336], Loss: 0.1171\n",
      "Epoch [4/5], Step [5850/10336], Loss: 1.0264\n",
      "Epoch [4/5], Step [5852/10336], Loss: 0.0919\n",
      "Epoch [4/5], Step [5854/10336], Loss: 0.2407\n",
      "Epoch [4/5], Step [5856/10336], Loss: 0.2783\n",
      "Epoch [4/5], Step [5858/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [5860/10336], Loss: 0.7311\n",
      "Epoch [4/5], Step [5862/10336], Loss: 0.8385\n",
      "Epoch [4/5], Step [5864/10336], Loss: 0.0220\n",
      "Epoch [4/5], Step [5866/10336], Loss: 0.2551\n",
      "Epoch [4/5], Step [5868/10336], Loss: 0.3410\n",
      "Epoch [4/5], Step [5870/10336], Loss: 1.4718\n",
      "Epoch [4/5], Step [5872/10336], Loss: 0.0045\n",
      "Epoch [4/5], Step [5874/10336], Loss: 2.3196\n",
      "Epoch [4/5], Step [5876/10336], Loss: 1.0485\n",
      "Epoch [4/5], Step [5878/10336], Loss: 0.1058\n",
      "Epoch [4/5], Step [5880/10336], Loss: 0.7562\n",
      "Epoch [4/5], Step [5882/10336], Loss: 0.5286\n",
      "Epoch [4/5], Step [5884/10336], Loss: 1.8821\n",
      "Epoch [4/5], Step [5886/10336], Loss: 3.8908\n",
      "Epoch [4/5], Step [5888/10336], Loss: 1.5582\n",
      "Epoch [4/5], Step [5890/10336], Loss: 0.5138\n",
      "Epoch [4/5], Step [5892/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [5894/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [5896/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [5898/10336], Loss: 0.9503\n",
      "Epoch [4/5], Step [5900/10336], Loss: 2.6712\n",
      "Epoch [4/5], Step [5902/10336], Loss: 0.6073\n",
      "Epoch [4/5], Step [5904/10336], Loss: 2.0580\n",
      "Epoch [4/5], Step [5906/10336], Loss: 1.6208\n",
      "Epoch [4/5], Step [5908/10336], Loss: 0.0850\n",
      "Epoch [4/5], Step [5910/10336], Loss: 1.0893\n",
      "Epoch [4/5], Step [5912/10336], Loss: 5.9756\n",
      "Epoch [4/5], Step [5914/10336], Loss: 1.5493\n",
      "Epoch [4/5], Step [5916/10336], Loss: 0.4255\n",
      "Epoch [4/5], Step [5918/10336], Loss: 0.2652\n",
      "Epoch [4/5], Step [5920/10336], Loss: 1.2928\n",
      "Epoch [4/5], Step [5922/10336], Loss: 0.0939\n",
      "Epoch [4/5], Step [5924/10336], Loss: 0.0560\n",
      "Epoch [4/5], Step [5926/10336], Loss: 0.0563\n",
      "Epoch [4/5], Step [5928/10336], Loss: 0.7162\n",
      "Epoch [4/5], Step [5930/10336], Loss: 0.1087\n",
      "Epoch [4/5], Step [5932/10336], Loss: 0.8729\n",
      "Epoch [4/5], Step [5934/10336], Loss: 0.0529\n",
      "Epoch [4/5], Step [5936/10336], Loss: 0.1206\n",
      "Epoch [4/5], Step [5938/10336], Loss: 0.1874\n",
      "Epoch [4/5], Step [5940/10336], Loss: 0.7231\n",
      "Epoch [4/5], Step [5942/10336], Loss: 1.1298\n",
      "Epoch [4/5], Step [5944/10336], Loss: 0.0110\n",
      "Epoch [4/5], Step [5946/10336], Loss: 0.1029\n",
      "Epoch [4/5], Step [5948/10336], Loss: 0.4798\n",
      "Epoch [4/5], Step [5950/10336], Loss: 0.0603\n",
      "Epoch [4/5], Step [5952/10336], Loss: 1.0341\n",
      "Epoch [4/5], Step [5954/10336], Loss: 0.1947\n",
      "Epoch [4/5], Step [5956/10336], Loss: 0.3409\n",
      "Epoch [4/5], Step [5958/10336], Loss: 0.1380\n",
      "Epoch [4/5], Step [5960/10336], Loss: 0.4556\n",
      "Epoch [4/5], Step [5962/10336], Loss: 0.1723\n",
      "Epoch [4/5], Step [5964/10336], Loss: 1.2029\n",
      "Epoch [4/5], Step [5966/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [5968/10336], Loss: 0.5035\n",
      "Epoch [4/5], Step [5970/10336], Loss: 0.5865\n",
      "Epoch [4/5], Step [5972/10336], Loss: 0.4943\n",
      "Epoch [4/5], Step [5974/10336], Loss: 2.0170\n",
      "Epoch [4/5], Step [5976/10336], Loss: 1.9162\n",
      "Epoch [4/5], Step [5978/10336], Loss: 0.0072\n",
      "Epoch [4/5], Step [5980/10336], Loss: 0.8306\n",
      "Epoch [4/5], Step [5982/10336], Loss: 0.6053\n",
      "Epoch [4/5], Step [5984/10336], Loss: 1.3287\n",
      "Epoch [4/5], Step [5986/10336], Loss: 0.2108\n",
      "Epoch [4/5], Step [5988/10336], Loss: 0.2219\n",
      "Epoch [4/5], Step [5990/10336], Loss: 1.8021\n",
      "Epoch [4/5], Step [5992/10336], Loss: 2.4481\n",
      "Epoch [4/5], Step [5994/10336], Loss: 2.0616\n",
      "Epoch [4/5], Step [5996/10336], Loss: 0.3147\n",
      "Epoch [4/5], Step [5998/10336], Loss: 1.2265\n",
      "Epoch [4/5], Step [6000/10336], Loss: 1.5528\n",
      "Epoch [4/5], Step [6002/10336], Loss: 0.4518\n",
      "Epoch [4/5], Step [6004/10336], Loss: 0.1090\n",
      "Epoch [4/5], Step [6006/10336], Loss: 0.1157\n",
      "Epoch [4/5], Step [6008/10336], Loss: 0.1045\n",
      "Epoch [4/5], Step [6010/10336], Loss: 0.7237\n",
      "Epoch [4/5], Step [6012/10336], Loss: 0.0728\n",
      "Epoch [4/5], Step [6014/10336], Loss: 0.0200\n",
      "Epoch [4/5], Step [6016/10336], Loss: 1.2646\n",
      "Epoch [4/5], Step [6018/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [6020/10336], Loss: 3.4839\n",
      "Epoch [4/5], Step [6022/10336], Loss: 0.6150\n",
      "Epoch [4/5], Step [6024/10336], Loss: 0.0336\n",
      "Epoch [4/5], Step [6026/10336], Loss: 0.7912\n",
      "Epoch [4/5], Step [6028/10336], Loss: 0.0256\n",
      "Epoch [4/5], Step [6030/10336], Loss: 0.2740\n",
      "Epoch [4/5], Step [6032/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [6034/10336], Loss: 2.2270\n",
      "Epoch [4/5], Step [6036/10336], Loss: 3.2245\n",
      "Epoch [4/5], Step [6038/10336], Loss: 0.2406\n",
      "Epoch [4/5], Step [6040/10336], Loss: 0.3675\n",
      "Epoch [4/5], Step [6042/10336], Loss: 0.3839\n",
      "Epoch [4/5], Step [6044/10336], Loss: 0.0136\n",
      "Epoch [4/5], Step [6046/10336], Loss: 0.0602\n",
      "Epoch [4/5], Step [6048/10336], Loss: 0.8902\n",
      "Epoch [4/5], Step [6050/10336], Loss: 0.2304\n",
      "Epoch [4/5], Step [6052/10336], Loss: 0.4958\n",
      "Epoch [4/5], Step [6054/10336], Loss: 0.9313\n",
      "Epoch [4/5], Step [6056/10336], Loss: 0.0978\n",
      "Epoch [4/5], Step [6058/10336], Loss: 0.4749\n",
      "Epoch [4/5], Step [6060/10336], Loss: 0.0043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [6062/10336], Loss: 1.2909\n",
      "Epoch [4/5], Step [6064/10336], Loss: 0.9606\n",
      "Epoch [4/5], Step [6066/10336], Loss: 0.3986\n",
      "Epoch [4/5], Step [6068/10336], Loss: 0.3495\n",
      "Epoch [4/5], Step [6070/10336], Loss: 0.3702\n",
      "Epoch [4/5], Step [6072/10336], Loss: 0.0114\n",
      "Epoch [4/5], Step [6074/10336], Loss: 0.0215\n",
      "Epoch [4/5], Step [6076/10336], Loss: 0.0214\n",
      "Epoch [4/5], Step [6078/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [6080/10336], Loss: 0.8401\n",
      "Epoch [4/5], Step [6082/10336], Loss: 0.0766\n",
      "Epoch [4/5], Step [6084/10336], Loss: 1.3069\n",
      "Epoch [4/5], Step [6086/10336], Loss: 0.5411\n",
      "Epoch [4/5], Step [6088/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [6090/10336], Loss: 0.7945\n",
      "Epoch [4/5], Step [6092/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [6094/10336], Loss: 0.1028\n",
      "Epoch [4/5], Step [6096/10336], Loss: 0.0086\n",
      "Epoch [4/5], Step [6098/10336], Loss: 0.2862\n",
      "Epoch [4/5], Step [6100/10336], Loss: 0.0177\n",
      "Epoch [4/5], Step [6102/10336], Loss: 0.8232\n",
      "Epoch [4/5], Step [6104/10336], Loss: 0.1528\n",
      "Epoch [4/5], Step [6106/10336], Loss: 0.4653\n",
      "Epoch [4/5], Step [6108/10336], Loss: 0.3016\n",
      "Epoch [4/5], Step [6110/10336], Loss: 0.2963\n",
      "Epoch [4/5], Step [6112/10336], Loss: 2.6117\n",
      "Epoch [4/5], Step [6114/10336], Loss: 0.5412\n",
      "Epoch [4/5], Step [6116/10336], Loss: 0.0776\n",
      "Epoch [4/5], Step [6118/10336], Loss: 0.0422\n",
      "Epoch [4/5], Step [6120/10336], Loss: 2.4021\n",
      "Epoch [4/5], Step [6122/10336], Loss: 0.6860\n",
      "Epoch [4/5], Step [6124/10336], Loss: 0.8370\n",
      "Epoch [4/5], Step [6126/10336], Loss: 0.4151\n",
      "Epoch [4/5], Step [6128/10336], Loss: 0.8625\n",
      "Epoch [4/5], Step [6130/10336], Loss: 0.6686\n",
      "Epoch [4/5], Step [6132/10336], Loss: 0.5600\n",
      "Epoch [4/5], Step [6134/10336], Loss: 0.1884\n",
      "Epoch [4/5], Step [6136/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [6138/10336], Loss: 0.0097\n",
      "Epoch [4/5], Step [6140/10336], Loss: 0.4652\n",
      "Epoch [4/5], Step [6142/10336], Loss: 0.0433\n",
      "Epoch [4/5], Step [6144/10336], Loss: 0.2183\n",
      "Epoch [4/5], Step [6146/10336], Loss: 0.0249\n",
      "Epoch [4/5], Step [6148/10336], Loss: 0.0494\n",
      "Epoch [4/5], Step [6150/10336], Loss: 0.0796\n",
      "Epoch [4/5], Step [6152/10336], Loss: 0.9472\n",
      "Epoch [4/5], Step [6154/10336], Loss: 1.3152\n",
      "Epoch [4/5], Step [6156/10336], Loss: 0.0243\n",
      "Epoch [4/5], Step [6158/10336], Loss: 0.5203\n",
      "Epoch [4/5], Step [6160/10336], Loss: 1.5913\n",
      "Epoch [4/5], Step [6162/10336], Loss: 1.3249\n",
      "Epoch [4/5], Step [6164/10336], Loss: 0.2041\n",
      "Epoch [4/5], Step [6166/10336], Loss: 0.0729\n",
      "Epoch [4/5], Step [6168/10336], Loss: 0.2940\n",
      "Epoch [4/5], Step [6170/10336], Loss: 1.3983\n",
      "Epoch [4/5], Step [6172/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [6174/10336], Loss: 0.0705\n",
      "Epoch [4/5], Step [6176/10336], Loss: 0.2753\n",
      "Epoch [4/5], Step [6178/10336], Loss: 0.2988\n",
      "Epoch [4/5], Step [6180/10336], Loss: 0.1884\n",
      "Epoch [4/5], Step [6182/10336], Loss: 0.8261\n",
      "Epoch [4/5], Step [6184/10336], Loss: 0.0352\n",
      "Epoch [4/5], Step [6186/10336], Loss: 4.0042\n",
      "Epoch [4/5], Step [6188/10336], Loss: 0.2146\n",
      "Epoch [4/5], Step [6190/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [6192/10336], Loss: 0.0971\n",
      "Epoch [4/5], Step [6194/10336], Loss: 2.9531\n",
      "Epoch [4/5], Step [6196/10336], Loss: 0.4824\n",
      "Epoch [4/5], Step [6198/10336], Loss: 0.9020\n",
      "Epoch [4/5], Step [6200/10336], Loss: 0.0337\n",
      "Epoch [4/5], Step [6202/10336], Loss: 1.9581\n",
      "Epoch [4/5], Step [6204/10336], Loss: 0.6452\n",
      "Epoch [4/5], Step [6206/10336], Loss: 1.3333\n",
      "Epoch [4/5], Step [6208/10336], Loss: 0.7705\n",
      "Epoch [4/5], Step [6210/10336], Loss: 0.6737\n",
      "Epoch [4/5], Step [6212/10336], Loss: 0.0444\n",
      "Epoch [4/5], Step [6214/10336], Loss: 0.9728\n",
      "Epoch [4/5], Step [6216/10336], Loss: 0.9620\n",
      "Epoch [4/5], Step [6218/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [6220/10336], Loss: 0.0654\n",
      "Epoch [4/5], Step [6222/10336], Loss: 2.3501\n",
      "Epoch [4/5], Step [6224/10336], Loss: 4.4059\n",
      "Epoch [4/5], Step [6226/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [6228/10336], Loss: 0.7461\n",
      "Epoch [4/5], Step [6230/10336], Loss: 0.3827\n",
      "Epoch [4/5], Step [6232/10336], Loss: 0.0938\n",
      "Epoch [4/5], Step [6234/10336], Loss: 0.4078\n",
      "Epoch [4/5], Step [6236/10336], Loss: 1.0774\n",
      "Epoch [4/5], Step [6238/10336], Loss: 0.2664\n",
      "Epoch [4/5], Step [6240/10336], Loss: 0.0553\n",
      "Epoch [4/5], Step [6242/10336], Loss: 0.3453\n",
      "Epoch [4/5], Step [6244/10336], Loss: 0.7862\n",
      "Epoch [4/5], Step [6246/10336], Loss: 0.0857\n",
      "Epoch [4/5], Step [6248/10336], Loss: 0.8271\n",
      "Epoch [4/5], Step [6250/10336], Loss: 0.1311\n",
      "Epoch [4/5], Step [6252/10336], Loss: 0.0433\n",
      "Epoch [4/5], Step [6254/10336], Loss: 1.1007\n",
      "Epoch [4/5], Step [6256/10336], Loss: 0.3474\n",
      "Epoch [4/5], Step [6258/10336], Loss: 1.5386\n",
      "Epoch [4/5], Step [6260/10336], Loss: 1.1910\n",
      "Epoch [4/5], Step [6262/10336], Loss: 3.2107\n",
      "Epoch [4/5], Step [6264/10336], Loss: 0.2145\n",
      "Epoch [4/5], Step [6266/10336], Loss: 0.0045\n",
      "Epoch [4/5], Step [6268/10336], Loss: 0.4716\n",
      "Epoch [4/5], Step [6270/10336], Loss: 0.1465\n",
      "Epoch [4/5], Step [6272/10336], Loss: 0.0226\n",
      "Epoch [4/5], Step [6274/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [6276/10336], Loss: 0.0316\n",
      "Epoch [4/5], Step [6278/10336], Loss: 0.3196\n",
      "Epoch [4/5], Step [6280/10336], Loss: 2.7456\n",
      "Epoch [4/5], Step [6282/10336], Loss: 0.0508\n",
      "Epoch [4/5], Step [6284/10336], Loss: 0.7818\n",
      "Epoch [4/5], Step [6286/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [6288/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [6290/10336], Loss: 1.7720\n",
      "Epoch [4/5], Step [6292/10336], Loss: 0.9102\n",
      "Epoch [4/5], Step [6294/10336], Loss: 0.1199\n",
      "Epoch [4/5], Step [6296/10336], Loss: 1.5069\n",
      "Epoch [4/5], Step [6298/10336], Loss: 0.6693\n",
      "Epoch [4/5], Step [6300/10336], Loss: 0.3434\n",
      "Epoch [4/5], Step [6302/10336], Loss: 0.0077\n",
      "Epoch [4/5], Step [6304/10336], Loss: 0.4644\n",
      "Epoch [4/5], Step [6306/10336], Loss: 0.4181\n",
      "Epoch [4/5], Step [6308/10336], Loss: 1.2951\n",
      "Epoch [4/5], Step [6310/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [6312/10336], Loss: 0.0771\n",
      "Epoch [4/5], Step [6314/10336], Loss: 1.5129\n",
      "Epoch [4/5], Step [6316/10336], Loss: 0.8093\n",
      "Epoch [4/5], Step [6318/10336], Loss: 1.4033\n",
      "Epoch [4/5], Step [6320/10336], Loss: 0.2095\n",
      "Epoch [4/5], Step [6322/10336], Loss: 0.6802\n",
      "Epoch [4/5], Step [6324/10336], Loss: 0.6538\n",
      "Epoch [4/5], Step [6326/10336], Loss: 0.5513\n",
      "Epoch [4/5], Step [6328/10336], Loss: 0.5065\n",
      "Epoch [4/5], Step [6330/10336], Loss: 0.2019\n",
      "Epoch [4/5], Step [6332/10336], Loss: 0.1538\n",
      "Epoch [4/5], Step [6334/10336], Loss: 0.0104\n",
      "Epoch [4/5], Step [6336/10336], Loss: 0.7304\n",
      "Epoch [4/5], Step [6338/10336], Loss: 0.6000\n",
      "Epoch [4/5], Step [6340/10336], Loss: 0.1131\n",
      "Epoch [4/5], Step [6342/10336], Loss: 0.0782\n",
      "Epoch [4/5], Step [6344/10336], Loss: 0.3312\n",
      "Epoch [4/5], Step [6346/10336], Loss: 1.4691\n",
      "Epoch [4/5], Step [6348/10336], Loss: 0.0137\n",
      "Epoch [4/5], Step [6350/10336], Loss: 0.9730\n",
      "Epoch [4/5], Step [6352/10336], Loss: 0.2699\n",
      "Epoch [4/5], Step [6354/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [6356/10336], Loss: 3.6155\n",
      "Epoch [4/5], Step [6358/10336], Loss: 1.6764\n",
      "Epoch [4/5], Step [6360/10336], Loss: 0.0345\n",
      "Epoch [4/5], Step [6362/10336], Loss: 0.3704\n",
      "Epoch [4/5], Step [6364/10336], Loss: 0.0202\n",
      "Epoch [4/5], Step [6366/10336], Loss: 2.1693\n",
      "Epoch [4/5], Step [6368/10336], Loss: 0.4934\n",
      "Epoch [4/5], Step [6370/10336], Loss: 0.2339\n",
      "Epoch [4/5], Step [6372/10336], Loss: 0.0532\n",
      "Epoch [4/5], Step [6374/10336], Loss: 2.8672\n",
      "Epoch [4/5], Step [6376/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [6378/10336], Loss: 0.3762\n",
      "Epoch [4/5], Step [6380/10336], Loss: 0.0035\n",
      "Epoch [4/5], Step [6382/10336], Loss: 0.0899\n",
      "Epoch [4/5], Step [6384/10336], Loss: 2.9408\n",
      "Epoch [4/5], Step [6386/10336], Loss: 1.7725\n",
      "Epoch [4/5], Step [6388/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [6390/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [6392/10336], Loss: 0.7129\n",
      "Epoch [4/5], Step [6394/10336], Loss: 0.1341\n",
      "Epoch [4/5], Step [6396/10336], Loss: 0.6674\n",
      "Epoch [4/5], Step [6398/10336], Loss: 0.2402\n",
      "Epoch [4/5], Step [6400/10336], Loss: 0.0964\n",
      "Epoch [4/5], Step [6402/10336], Loss: 0.4775\n",
      "Epoch [4/5], Step [6404/10336], Loss: 0.9039\n",
      "Epoch [4/5], Step [6406/10336], Loss: 2.1839\n",
      "Epoch [4/5], Step [6408/10336], Loss: 0.4478\n",
      "Epoch [4/5], Step [6410/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [6412/10336], Loss: 0.2819\n",
      "Epoch [4/5], Step [6414/10336], Loss: 1.3836\n",
      "Epoch [4/5], Step [6416/10336], Loss: 1.7971\n",
      "Epoch [4/5], Step [6418/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [6420/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [6422/10336], Loss: 0.2920\n",
      "Epoch [4/5], Step [6424/10336], Loss: 0.2962\n",
      "Epoch [4/5], Step [6426/10336], Loss: 0.4416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [6428/10336], Loss: 0.2421\n",
      "Epoch [4/5], Step [6430/10336], Loss: 0.9537\n",
      "Epoch [4/5], Step [6432/10336], Loss: 0.1304\n",
      "Epoch [4/5], Step [6434/10336], Loss: 0.3310\n",
      "Epoch [4/5], Step [6436/10336], Loss: 0.1768\n",
      "Epoch [4/5], Step [6438/10336], Loss: 0.0220\n",
      "Epoch [4/5], Step [6440/10336], Loss: 0.7907\n",
      "Epoch [4/5], Step [6442/10336], Loss: 0.0034\n",
      "Epoch [4/5], Step [6444/10336], Loss: 0.0117\n",
      "Epoch [4/5], Step [6446/10336], Loss: 0.9769\n",
      "Epoch [4/5], Step [6448/10336], Loss: 0.0293\n",
      "Epoch [4/5], Step [6450/10336], Loss: 0.0373\n",
      "Epoch [4/5], Step [6452/10336], Loss: 0.0048\n",
      "Epoch [4/5], Step [6454/10336], Loss: 1.7240\n",
      "Epoch [4/5], Step [6456/10336], Loss: 0.0721\n",
      "Epoch [4/5], Step [6458/10336], Loss: 0.0377\n",
      "Epoch [4/5], Step [6460/10336], Loss: 1.6318\n",
      "Epoch [4/5], Step [6462/10336], Loss: 0.0185\n",
      "Epoch [4/5], Step [6464/10336], Loss: 0.0544\n",
      "Epoch [4/5], Step [6466/10336], Loss: 0.0163\n",
      "Epoch [4/5], Step [6468/10336], Loss: 1.4031\n",
      "Epoch [4/5], Step [6470/10336], Loss: 0.0254\n",
      "Epoch [4/5], Step [6472/10336], Loss: 0.0454\n",
      "Epoch [4/5], Step [6474/10336], Loss: 0.1400\n",
      "Epoch [4/5], Step [6476/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [6478/10336], Loss: 0.1073\n",
      "Epoch [4/5], Step [6480/10336], Loss: 0.2613\n",
      "Epoch [4/5], Step [6482/10336], Loss: 0.0612\n",
      "Epoch [4/5], Step [6484/10336], Loss: 1.3490\n",
      "Epoch [4/5], Step [6486/10336], Loss: 0.2619\n",
      "Epoch [4/5], Step [6488/10336], Loss: 0.0915\n",
      "Epoch [4/5], Step [6490/10336], Loss: 0.0096\n",
      "Epoch [4/5], Step [6492/10336], Loss: 1.3118\n",
      "Epoch [4/5], Step [6494/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [6496/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [6498/10336], Loss: 0.5378\n",
      "Epoch [4/5], Step [6500/10336], Loss: 1.2736\n",
      "Epoch [4/5], Step [6502/10336], Loss: 2.2371\n",
      "Epoch [4/5], Step [6504/10336], Loss: 2.4197\n",
      "Epoch [4/5], Step [6506/10336], Loss: 0.8409\n",
      "Epoch [4/5], Step [6508/10336], Loss: 1.2016\n",
      "Epoch [4/5], Step [6510/10336], Loss: 0.0309\n",
      "Epoch [4/5], Step [6512/10336], Loss: 0.1181\n",
      "Epoch [4/5], Step [6514/10336], Loss: 3.1809\n",
      "Epoch [4/5], Step [6516/10336], Loss: 0.0240\n",
      "Epoch [4/5], Step [6518/10336], Loss: 0.0154\n",
      "Epoch [4/5], Step [6520/10336], Loss: 0.0356\n",
      "Epoch [4/5], Step [6522/10336], Loss: 0.5252\n",
      "Epoch [4/5], Step [6524/10336], Loss: 1.1102\n",
      "Epoch [4/5], Step [6526/10336], Loss: 0.1833\n",
      "Epoch [4/5], Step [6528/10336], Loss: 1.3417\n",
      "Epoch [4/5], Step [6530/10336], Loss: 0.2374\n",
      "Epoch [4/5], Step [6532/10336], Loss: 0.0676\n",
      "Epoch [4/5], Step [6534/10336], Loss: 0.7948\n",
      "Epoch [4/5], Step [6536/10336], Loss: 1.1049\n",
      "Epoch [4/5], Step [6538/10336], Loss: 0.1170\n",
      "Epoch [4/5], Step [6540/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [6542/10336], Loss: 0.8168\n",
      "Epoch [4/5], Step [6544/10336], Loss: 0.8933\n",
      "Epoch [4/5], Step [6546/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [6548/10336], Loss: 1.5420\n",
      "Epoch [4/5], Step [6550/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [6552/10336], Loss: 0.6348\n",
      "Epoch [4/5], Step [6554/10336], Loss: 2.4766\n",
      "Epoch [4/5], Step [6556/10336], Loss: 0.4051\n",
      "Epoch [4/5], Step [6558/10336], Loss: 2.8366\n",
      "Epoch [4/5], Step [6560/10336], Loss: 0.3428\n",
      "Epoch [4/5], Step [6562/10336], Loss: 0.0140\n",
      "Epoch [4/5], Step [6564/10336], Loss: 0.0192\n",
      "Epoch [4/5], Step [6566/10336], Loss: 0.5805\n",
      "Epoch [4/5], Step [6568/10336], Loss: 1.2669\n",
      "Epoch [4/5], Step [6570/10336], Loss: 0.5301\n",
      "Epoch [4/5], Step [6572/10336], Loss: 1.0843\n",
      "Epoch [4/5], Step [6574/10336], Loss: 0.0838\n",
      "Epoch [4/5], Step [6576/10336], Loss: 0.0260\n",
      "Epoch [4/5], Step [6578/10336], Loss: 0.1623\n",
      "Epoch [4/5], Step [6580/10336], Loss: 0.2500\n",
      "Epoch [4/5], Step [6582/10336], Loss: 0.3506\n",
      "Epoch [4/5], Step [6584/10336], Loss: 2.4980\n",
      "Epoch [4/5], Step [6586/10336], Loss: 0.1433\n",
      "Epoch [4/5], Step [6588/10336], Loss: 0.6813\n",
      "Epoch [4/5], Step [6590/10336], Loss: 0.2178\n",
      "Epoch [4/5], Step [6592/10336], Loss: 0.2846\n",
      "Epoch [4/5], Step [6594/10336], Loss: 0.9009\n",
      "Epoch [4/5], Step [6596/10336], Loss: 0.1022\n",
      "Epoch [4/5], Step [6598/10336], Loss: 0.1918\n",
      "Epoch [4/5], Step [6600/10336], Loss: 0.0173\n",
      "Epoch [4/5], Step [6602/10336], Loss: 0.4435\n",
      "Epoch [4/5], Step [6604/10336], Loss: 0.0731\n",
      "Epoch [4/5], Step [6606/10336], Loss: 0.3649\n",
      "Epoch [4/5], Step [6608/10336], Loss: 0.3428\n",
      "Epoch [4/5], Step [6610/10336], Loss: 1.1131\n",
      "Epoch [4/5], Step [6612/10336], Loss: 0.9958\n",
      "Epoch [4/5], Step [6614/10336], Loss: 0.1120\n",
      "Epoch [4/5], Step [6616/10336], Loss: 0.0522\n",
      "Epoch [4/5], Step [6618/10336], Loss: 0.7925\n",
      "Epoch [4/5], Step [6620/10336], Loss: 2.4481\n",
      "Epoch [4/5], Step [6622/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [6624/10336], Loss: 0.7698\n",
      "Epoch [4/5], Step [6626/10336], Loss: 2.5133\n",
      "Epoch [4/5], Step [6628/10336], Loss: 1.1999\n",
      "Epoch [4/5], Step [6630/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [6632/10336], Loss: 0.1220\n",
      "Epoch [4/5], Step [6634/10336], Loss: 1.6159\n",
      "Epoch [4/5], Step [6636/10336], Loss: 0.2562\n",
      "Epoch [4/5], Step [6638/10336], Loss: 3.0732\n",
      "Epoch [4/5], Step [6640/10336], Loss: 1.8125\n",
      "Epoch [4/5], Step [6642/10336], Loss: 1.5626\n",
      "Epoch [4/5], Step [6644/10336], Loss: 0.8868\n",
      "Epoch [4/5], Step [6646/10336], Loss: 1.9167\n",
      "Epoch [4/5], Step [6648/10336], Loss: 0.2778\n",
      "Epoch [4/5], Step [6650/10336], Loss: 0.0333\n",
      "Epoch [4/5], Step [6652/10336], Loss: 0.0078\n",
      "Epoch [4/5], Step [6654/10336], Loss: 0.1710\n",
      "Epoch [4/5], Step [6656/10336], Loss: 2.6908\n",
      "Epoch [4/5], Step [6658/10336], Loss: 0.1588\n",
      "Epoch [4/5], Step [6660/10336], Loss: 1.4285\n",
      "Epoch [4/5], Step [6662/10336], Loss: 0.5699\n",
      "Epoch [4/5], Step [6664/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [6666/10336], Loss: 0.3619\n",
      "Epoch [4/5], Step [6668/10336], Loss: 0.1896\n",
      "Epoch [4/5], Step [6670/10336], Loss: 1.2059\n",
      "Epoch [4/5], Step [6672/10336], Loss: 0.2890\n",
      "Epoch [4/5], Step [6674/10336], Loss: 0.1670\n",
      "Epoch [4/5], Step [6676/10336], Loss: 0.5036\n",
      "Epoch [4/5], Step [6678/10336], Loss: 0.0072\n",
      "Epoch [4/5], Step [6680/10336], Loss: 1.6167\n",
      "Epoch [4/5], Step [6682/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [6684/10336], Loss: 1.0127\n",
      "Epoch [4/5], Step [6686/10336], Loss: 0.0202\n",
      "Epoch [4/5], Step [6688/10336], Loss: 0.1931\n",
      "Epoch [4/5], Step [6690/10336], Loss: 0.3535\n",
      "Epoch [4/5], Step [6692/10336], Loss: 0.0095\n",
      "Epoch [4/5], Step [6694/10336], Loss: 0.1957\n",
      "Epoch [4/5], Step [6696/10336], Loss: 1.8873\n",
      "Epoch [4/5], Step [6698/10336], Loss: 0.7037\n",
      "Epoch [4/5], Step [6700/10336], Loss: 3.9484\n",
      "Epoch [4/5], Step [6702/10336], Loss: 0.7045\n",
      "Epoch [4/5], Step [6704/10336], Loss: 1.2880\n",
      "Epoch [4/5], Step [6706/10336], Loss: 0.7973\n",
      "Epoch [4/5], Step [6708/10336], Loss: 0.1808\n",
      "Epoch [4/5], Step [6710/10336], Loss: 2.5986\n",
      "Epoch [4/5], Step [6712/10336], Loss: 0.1357\n",
      "Epoch [4/5], Step [6714/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [6716/10336], Loss: 0.0077\n",
      "Epoch [4/5], Step [6718/10336], Loss: 0.1013\n",
      "Epoch [4/5], Step [6720/10336], Loss: 0.4130\n",
      "Epoch [4/5], Step [6722/10336], Loss: 1.3258\n",
      "Epoch [4/5], Step [6724/10336], Loss: 0.1588\n",
      "Epoch [4/5], Step [6726/10336], Loss: 0.3085\n",
      "Epoch [4/5], Step [6728/10336], Loss: 0.0237\n",
      "Epoch [4/5], Step [6730/10336], Loss: 1.0942\n",
      "Epoch [4/5], Step [6732/10336], Loss: 0.4592\n",
      "Epoch [4/5], Step [6734/10336], Loss: 0.4104\n",
      "Epoch [4/5], Step [6736/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [6738/10336], Loss: 0.5156\n",
      "Epoch [4/5], Step [6740/10336], Loss: 1.6603\n",
      "Epoch [4/5], Step [6742/10336], Loss: 0.0156\n",
      "Epoch [4/5], Step [6744/10336], Loss: 0.0161\n",
      "Epoch [4/5], Step [6746/10336], Loss: 0.5996\n",
      "Epoch [4/5], Step [6748/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [6750/10336], Loss: 0.4714\n",
      "Epoch [4/5], Step [6752/10336], Loss: 0.5034\n",
      "Epoch [4/5], Step [6754/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [6756/10336], Loss: 0.9542\n",
      "Epoch [4/5], Step [6758/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [6760/10336], Loss: 0.0253\n",
      "Epoch [4/5], Step [6762/10336], Loss: 0.8379\n",
      "Epoch [4/5], Step [6764/10336], Loss: 0.6178\n",
      "Epoch [4/5], Step [6766/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [6768/10336], Loss: 0.5319\n",
      "Epoch [4/5], Step [6770/10336], Loss: 0.1544\n",
      "Epoch [4/5], Step [6772/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [6774/10336], Loss: 0.7077\n",
      "Epoch [4/5], Step [6776/10336], Loss: 0.2806\n",
      "Epoch [4/5], Step [6778/10336], Loss: 0.0333\n",
      "Epoch [4/5], Step [6780/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [6782/10336], Loss: 3.0853\n",
      "Epoch [4/5], Step [6784/10336], Loss: 0.6143\n",
      "Epoch [4/5], Step [6786/10336], Loss: 0.0207\n",
      "Epoch [4/5], Step [6788/10336], Loss: 0.0344\n",
      "Epoch [4/5], Step [6790/10336], Loss: 1.0390\n",
      "Epoch [4/5], Step [6792/10336], Loss: 0.0219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [6794/10336], Loss: 0.0642\n",
      "Epoch [4/5], Step [6796/10336], Loss: 0.1861\n",
      "Epoch [4/5], Step [6798/10336], Loss: 0.2377\n",
      "Epoch [4/5], Step [6800/10336], Loss: 0.0085\n",
      "Epoch [4/5], Step [6802/10336], Loss: 0.2337\n",
      "Epoch [4/5], Step [6804/10336], Loss: 2.3021\n",
      "Epoch [4/5], Step [6806/10336], Loss: 0.4588\n",
      "Epoch [4/5], Step [6808/10336], Loss: 2.0963\n",
      "Epoch [4/5], Step [6810/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [6812/10336], Loss: 0.0452\n",
      "Epoch [4/5], Step [6814/10336], Loss: 2.1305\n",
      "Epoch [4/5], Step [6816/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [6818/10336], Loss: 0.1546\n",
      "Epoch [4/5], Step [6820/10336], Loss: 0.0375\n",
      "Epoch [4/5], Step [6822/10336], Loss: 0.0419\n",
      "Epoch [4/5], Step [6824/10336], Loss: 0.8170\n",
      "Epoch [4/5], Step [6826/10336], Loss: 1.3290\n",
      "Epoch [4/5], Step [6828/10336], Loss: 0.1204\n",
      "Epoch [4/5], Step [6830/10336], Loss: 0.0004\n",
      "Epoch [4/5], Step [6832/10336], Loss: 0.3190\n",
      "Epoch [4/5], Step [6834/10336], Loss: 1.0606\n",
      "Epoch [4/5], Step [6836/10336], Loss: 0.0091\n",
      "Epoch [4/5], Step [6838/10336], Loss: 0.5783\n",
      "Epoch [4/5], Step [6840/10336], Loss: 0.2990\n",
      "Epoch [4/5], Step [6842/10336], Loss: 0.1038\n",
      "Epoch [4/5], Step [6844/10336], Loss: 0.5021\n",
      "Epoch [4/5], Step [6846/10336], Loss: 0.0157\n",
      "Epoch [4/5], Step [6848/10336], Loss: 1.4928\n",
      "Epoch [4/5], Step [6850/10336], Loss: 2.1891\n",
      "Epoch [4/5], Step [6852/10336], Loss: 0.6438\n",
      "Epoch [4/5], Step [6854/10336], Loss: 0.0144\n",
      "Epoch [4/5], Step [6856/10336], Loss: 1.9285\n",
      "Epoch [4/5], Step [6858/10336], Loss: 0.7475\n",
      "Epoch [4/5], Step [6860/10336], Loss: 1.7445\n",
      "Epoch [4/5], Step [6862/10336], Loss: 0.8137\n",
      "Epoch [4/5], Step [6864/10336], Loss: 1.1363\n",
      "Epoch [4/5], Step [6866/10336], Loss: 1.2894\n",
      "Epoch [4/5], Step [6868/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [6870/10336], Loss: 0.3054\n",
      "Epoch [4/5], Step [6872/10336], Loss: 0.0822\n",
      "Epoch [4/5], Step [6874/10336], Loss: 3.0766\n",
      "Epoch [4/5], Step [6876/10336], Loss: 0.0371\n",
      "Epoch [4/5], Step [6878/10336], Loss: 0.3225\n",
      "Epoch [4/5], Step [6880/10336], Loss: 1.7472\n",
      "Epoch [4/5], Step [6882/10336], Loss: 1.1299\n",
      "Epoch [4/5], Step [6884/10336], Loss: 0.1304\n",
      "Epoch [4/5], Step [6886/10336], Loss: 0.8575\n",
      "Epoch [4/5], Step [6888/10336], Loss: 1.1516\n",
      "Epoch [4/5], Step [6890/10336], Loss: 0.0253\n",
      "Epoch [4/5], Step [6892/10336], Loss: 1.3034\n",
      "Epoch [4/5], Step [6894/10336], Loss: 1.6152\n",
      "Epoch [4/5], Step [6896/10336], Loss: 0.3117\n",
      "Epoch [4/5], Step [6898/10336], Loss: 1.1350\n",
      "Epoch [4/5], Step [6900/10336], Loss: 0.0084\n",
      "Epoch [4/5], Step [6902/10336], Loss: 0.1909\n",
      "Epoch [4/5], Step [6904/10336], Loss: 2.7156\n",
      "Epoch [4/5], Step [6906/10336], Loss: 0.7898\n",
      "Epoch [4/5], Step [6908/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [6910/10336], Loss: 0.6366\n",
      "Epoch [4/5], Step [6912/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [6914/10336], Loss: 2.0625\n",
      "Epoch [4/5], Step [6916/10336], Loss: 0.0264\n",
      "Epoch [4/5], Step [6918/10336], Loss: 0.0271\n",
      "Epoch [4/5], Step [6920/10336], Loss: 0.7409\n",
      "Epoch [4/5], Step [6922/10336], Loss: 0.0116\n",
      "Epoch [4/5], Step [6924/10336], Loss: 0.4176\n",
      "Epoch [4/5], Step [6926/10336], Loss: 2.9074\n",
      "Epoch [4/5], Step [6928/10336], Loss: 0.1156\n",
      "Epoch [4/5], Step [6930/10336], Loss: 0.2435\n",
      "Epoch [4/5], Step [6932/10336], Loss: 0.1082\n",
      "Epoch [4/5], Step [6934/10336], Loss: 0.0506\n",
      "Epoch [4/5], Step [6936/10336], Loss: 2.0961\n",
      "Epoch [4/5], Step [6938/10336], Loss: 0.6348\n",
      "Epoch [4/5], Step [6940/10336], Loss: 0.0124\n",
      "Epoch [4/5], Step [6942/10336], Loss: 1.1630\n",
      "Epoch [4/5], Step [6944/10336], Loss: 0.5494\n",
      "Epoch [4/5], Step [6946/10336], Loss: 2.0894\n",
      "Epoch [4/5], Step [6948/10336], Loss: 0.5049\n",
      "Epoch [4/5], Step [6950/10336], Loss: 0.2339\n",
      "Epoch [4/5], Step [6952/10336], Loss: 0.1431\n",
      "Epoch [4/5], Step [6954/10336], Loss: 0.9026\n",
      "Epoch [4/5], Step [6956/10336], Loss: 0.2214\n",
      "Epoch [4/5], Step [6958/10336], Loss: 0.4237\n",
      "Epoch [4/5], Step [6960/10336], Loss: 0.1178\n",
      "Epoch [4/5], Step [6962/10336], Loss: 0.5158\n",
      "Epoch [4/5], Step [6964/10336], Loss: 2.8631\n",
      "Epoch [4/5], Step [6966/10336], Loss: 0.0379\n",
      "Epoch [4/5], Step [6968/10336], Loss: 0.0608\n",
      "Epoch [4/5], Step [6970/10336], Loss: 0.0734\n",
      "Epoch [4/5], Step [6972/10336], Loss: 3.0607\n",
      "Epoch [4/5], Step [6974/10336], Loss: 0.5485\n",
      "Epoch [4/5], Step [6976/10336], Loss: 0.5927\n",
      "Epoch [4/5], Step [6978/10336], Loss: 2.1523\n",
      "Epoch [4/5], Step [6980/10336], Loss: 0.2502\n",
      "Epoch [4/5], Step [6982/10336], Loss: 1.6757\n",
      "Epoch [4/5], Step [6984/10336], Loss: 0.2032\n",
      "Epoch [4/5], Step [6986/10336], Loss: 0.0072\n",
      "Epoch [4/5], Step [6988/10336], Loss: 0.3558\n",
      "Epoch [4/5], Step [6990/10336], Loss: 0.1092\n",
      "Epoch [4/5], Step [6992/10336], Loss: 0.0032\n",
      "Epoch [4/5], Step [6994/10336], Loss: 0.7504\n",
      "Epoch [4/5], Step [6996/10336], Loss: 1.1809\n",
      "Epoch [4/5], Step [6998/10336], Loss: 0.4837\n",
      "Epoch [4/5], Step [7000/10336], Loss: 4.4036\n",
      "Epoch [4/5], Step [7002/10336], Loss: 1.6769\n",
      "Epoch [4/5], Step [7004/10336], Loss: 0.4586\n",
      "Epoch [4/5], Step [7006/10336], Loss: 2.4382\n",
      "Epoch [4/5], Step [7008/10336], Loss: 0.9600\n",
      "Epoch [4/5], Step [7010/10336], Loss: 0.0329\n",
      "Epoch [4/5], Step [7012/10336], Loss: 0.2337\n",
      "Epoch [4/5], Step [7014/10336], Loss: 0.1918\n",
      "Epoch [4/5], Step [7016/10336], Loss: 0.5054\n",
      "Epoch [4/5], Step [7018/10336], Loss: 0.4532\n",
      "Epoch [4/5], Step [7020/10336], Loss: 1.1733\n",
      "Epoch [4/5], Step [7022/10336], Loss: 0.1055\n",
      "Epoch [4/5], Step [7024/10336], Loss: 0.2645\n",
      "Epoch [4/5], Step [7026/10336], Loss: 0.6023\n",
      "Epoch [4/5], Step [7028/10336], Loss: 0.0180\n",
      "Epoch [4/5], Step [7030/10336], Loss: 0.2862\n",
      "Epoch [4/5], Step [7032/10336], Loss: 0.3769\n",
      "Epoch [4/5], Step [7034/10336], Loss: 1.6635\n",
      "Epoch [4/5], Step [7036/10336], Loss: 0.1091\n",
      "Epoch [4/5], Step [7038/10336], Loss: 0.0337\n",
      "Epoch [4/5], Step [7040/10336], Loss: 0.1116\n",
      "Epoch [4/5], Step [7042/10336], Loss: 0.0145\n",
      "Epoch [4/5], Step [7044/10336], Loss: 1.5427\n",
      "Epoch [4/5], Step [7046/10336], Loss: 2.2797\n",
      "Epoch [4/5], Step [7048/10336], Loss: 2.4373\n",
      "Epoch [4/5], Step [7050/10336], Loss: 0.2076\n",
      "Epoch [4/5], Step [7052/10336], Loss: 1.4421\n",
      "Epoch [4/5], Step [7054/10336], Loss: 0.3962\n",
      "Epoch [4/5], Step [7056/10336], Loss: 1.6208\n",
      "Epoch [4/5], Step [7058/10336], Loss: 0.0237\n",
      "Epoch [4/5], Step [7060/10336], Loss: 0.0889\n",
      "Epoch [4/5], Step [7062/10336], Loss: 0.1775\n",
      "Epoch [4/5], Step [7064/10336], Loss: 0.0814\n",
      "Epoch [4/5], Step [7066/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [7068/10336], Loss: 2.4363\n",
      "Epoch [4/5], Step [7070/10336], Loss: 0.0110\n",
      "Epoch [4/5], Step [7072/10336], Loss: 0.3130\n",
      "Epoch [4/5], Step [7074/10336], Loss: 0.1451\n",
      "Epoch [4/5], Step [7076/10336], Loss: 1.6596\n",
      "Epoch [4/5], Step [7078/10336], Loss: 0.1213\n",
      "Epoch [4/5], Step [7080/10336], Loss: 0.0743\n",
      "Epoch [4/5], Step [7082/10336], Loss: 0.0636\n",
      "Epoch [4/5], Step [7084/10336], Loss: 0.0286\n",
      "Epoch [4/5], Step [7086/10336], Loss: 0.4417\n",
      "Epoch [4/5], Step [7088/10336], Loss: 1.2807\n",
      "Epoch [4/5], Step [7090/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [7092/10336], Loss: 1.2689\n",
      "Epoch [4/5], Step [7094/10336], Loss: 0.2085\n",
      "Epoch [4/5], Step [7096/10336], Loss: 1.0118\n",
      "Epoch [4/5], Step [7098/10336], Loss: 0.0222\n",
      "Epoch [4/5], Step [7100/10336], Loss: 0.0174\n",
      "Epoch [4/5], Step [7102/10336], Loss: 0.0169\n",
      "Epoch [4/5], Step [7104/10336], Loss: 0.1869\n",
      "Epoch [4/5], Step [7106/10336], Loss: 0.0468\n",
      "Epoch [4/5], Step [7108/10336], Loss: 0.3674\n",
      "Epoch [4/5], Step [7110/10336], Loss: 1.1071\n",
      "Epoch [4/5], Step [7112/10336], Loss: 2.9049\n",
      "Epoch [4/5], Step [7114/10336], Loss: 1.8310\n",
      "Epoch [4/5], Step [7116/10336], Loss: 0.1927\n",
      "Epoch [4/5], Step [7118/10336], Loss: 0.2850\n",
      "Epoch [4/5], Step [7120/10336], Loss: 0.3133\n",
      "Epoch [4/5], Step [7122/10336], Loss: 1.0741\n",
      "Epoch [4/5], Step [7124/10336], Loss: 0.5059\n",
      "Epoch [4/5], Step [7126/10336], Loss: 1.6073\n",
      "Epoch [4/5], Step [7128/10336], Loss: 1.5333\n",
      "Epoch [4/5], Step [7130/10336], Loss: 1.1400\n",
      "Epoch [4/5], Step [7132/10336], Loss: 0.0477\n",
      "Epoch [4/5], Step [7134/10336], Loss: 2.2051\n",
      "Epoch [4/5], Step [7136/10336], Loss: 2.5840\n",
      "Epoch [4/5], Step [7138/10336], Loss: 0.4511\n",
      "Epoch [4/5], Step [7140/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [7142/10336], Loss: 0.9381\n",
      "Epoch [4/5], Step [7144/10336], Loss: 0.4642\n",
      "Epoch [4/5], Step [7146/10336], Loss: 0.0959\n",
      "Epoch [4/5], Step [7148/10336], Loss: 2.9139\n",
      "Epoch [4/5], Step [7150/10336], Loss: 0.4488\n",
      "Epoch [4/5], Step [7152/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [7154/10336], Loss: 0.0079\n",
      "Epoch [4/5], Step [7156/10336], Loss: 0.8295\n",
      "Epoch [4/5], Step [7158/10336], Loss: 0.1652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [7160/10336], Loss: 0.2372\n",
      "Epoch [4/5], Step [7162/10336], Loss: 0.8599\n",
      "Epoch [4/5], Step [7164/10336], Loss: 0.1841\n",
      "Epoch [4/5], Step [7166/10336], Loss: 1.0114\n",
      "Epoch [4/5], Step [7168/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [7170/10336], Loss: 0.8959\n",
      "Epoch [4/5], Step [7172/10336], Loss: 1.0324\n",
      "Epoch [4/5], Step [7174/10336], Loss: 1.9694\n",
      "Epoch [4/5], Step [7176/10336], Loss: 0.0571\n",
      "Epoch [4/5], Step [7178/10336], Loss: 0.3785\n",
      "Epoch [4/5], Step [7180/10336], Loss: 3.9155\n",
      "Epoch [4/5], Step [7182/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [7184/10336], Loss: 0.1249\n",
      "Epoch [4/5], Step [7186/10336], Loss: 1.4146\n",
      "Epoch [4/5], Step [7188/10336], Loss: 0.7581\n",
      "Epoch [4/5], Step [7190/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [7192/10336], Loss: 1.1293\n",
      "Epoch [4/5], Step [7194/10336], Loss: 0.6657\n",
      "Epoch [4/5], Step [7196/10336], Loss: 0.0724\n",
      "Epoch [4/5], Step [7198/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [7200/10336], Loss: 1.3815\n",
      "Epoch [4/5], Step [7202/10336], Loss: 2.0763\n",
      "Epoch [4/5], Step [7204/10336], Loss: 0.4566\n",
      "Epoch [4/5], Step [7206/10336], Loss: 1.1020\n",
      "Epoch [4/5], Step [7208/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [7210/10336], Loss: 0.6636\n",
      "Epoch [4/5], Step [7212/10336], Loss: 0.0454\n",
      "Epoch [4/5], Step [7214/10336], Loss: 1.4493\n",
      "Epoch [4/5], Step [7216/10336], Loss: 0.1651\n",
      "Epoch [4/5], Step [7218/10336], Loss: 0.0588\n",
      "Epoch [4/5], Step [7220/10336], Loss: 0.2776\n",
      "Epoch [4/5], Step [7222/10336], Loss: 0.0599\n",
      "Epoch [4/5], Step [7224/10336], Loss: 1.6647\n",
      "Epoch [4/5], Step [7226/10336], Loss: 1.8439\n",
      "Epoch [4/5], Step [7228/10336], Loss: 0.8409\n",
      "Epoch [4/5], Step [7230/10336], Loss: 0.0053\n",
      "Epoch [4/5], Step [7232/10336], Loss: 0.2099\n",
      "Epoch [4/5], Step [7234/10336], Loss: 1.2160\n",
      "Epoch [4/5], Step [7236/10336], Loss: 0.1917\n",
      "Epoch [4/5], Step [7238/10336], Loss: 0.0027\n",
      "Epoch [4/5], Step [7240/10336], Loss: 3.2724\n",
      "Epoch [4/5], Step [7242/10336], Loss: 0.6369\n",
      "Epoch [4/5], Step [7244/10336], Loss: 0.6347\n",
      "Epoch [4/5], Step [7246/10336], Loss: 0.1102\n",
      "Epoch [4/5], Step [7248/10336], Loss: 1.1991\n",
      "Epoch [4/5], Step [7250/10336], Loss: 1.8499\n",
      "Epoch [4/5], Step [7252/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [7254/10336], Loss: 0.8214\n",
      "Epoch [4/5], Step [7256/10336], Loss: 1.3713\n",
      "Epoch [4/5], Step [7258/10336], Loss: 0.1362\n",
      "Epoch [4/5], Step [7260/10336], Loss: 0.0538\n",
      "Epoch [4/5], Step [7262/10336], Loss: 0.0362\n",
      "Epoch [4/5], Step [7264/10336], Loss: 1.5847\n",
      "Epoch [4/5], Step [7266/10336], Loss: 0.5652\n",
      "Epoch [4/5], Step [7268/10336], Loss: 0.1248\n",
      "Epoch [4/5], Step [7270/10336], Loss: 0.1332\n",
      "Epoch [4/5], Step [7272/10336], Loss: 0.0065\n",
      "Epoch [4/5], Step [7274/10336], Loss: 0.1023\n",
      "Epoch [4/5], Step [7276/10336], Loss: 0.1375\n",
      "Epoch [4/5], Step [7278/10336], Loss: 0.6608\n",
      "Epoch [4/5], Step [7280/10336], Loss: 0.3514\n",
      "Epoch [4/5], Step [7282/10336], Loss: 0.5589\n",
      "Epoch [4/5], Step [7284/10336], Loss: 0.6185\n",
      "Epoch [4/5], Step [7286/10336], Loss: 1.1404\n",
      "Epoch [4/5], Step [7288/10336], Loss: 0.1740\n",
      "Epoch [4/5], Step [7290/10336], Loss: 0.0085\n",
      "Epoch [4/5], Step [7292/10336], Loss: 0.0041\n",
      "Epoch [4/5], Step [7294/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [7296/10336], Loss: 0.3384\n",
      "Epoch [4/5], Step [7298/10336], Loss: 0.0212\n",
      "Epoch [4/5], Step [7300/10336], Loss: 0.0981\n",
      "Epoch [4/5], Step [7302/10336], Loss: 0.3531\n",
      "Epoch [4/5], Step [7304/10336], Loss: 0.3319\n",
      "Epoch [4/5], Step [7306/10336], Loss: 1.5683\n",
      "Epoch [4/5], Step [7308/10336], Loss: 0.8007\n",
      "Epoch [4/5], Step [7310/10336], Loss: 0.1056\n",
      "Epoch [4/5], Step [7312/10336], Loss: 0.1296\n",
      "Epoch [4/5], Step [7314/10336], Loss: 1.9592\n",
      "Epoch [4/5], Step [7316/10336], Loss: 0.1510\n",
      "Epoch [4/5], Step [7318/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [7320/10336], Loss: 0.6839\n",
      "Epoch [4/5], Step [7322/10336], Loss: 0.5100\n",
      "Epoch [4/5], Step [7324/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [7326/10336], Loss: 0.8850\n",
      "Epoch [4/5], Step [7328/10336], Loss: 0.6945\n",
      "Epoch [4/5], Step [7330/10336], Loss: 0.3014\n",
      "Epoch [4/5], Step [7332/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [7334/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [7336/10336], Loss: 0.5514\n",
      "Epoch [4/5], Step [7338/10336], Loss: 0.1985\n",
      "Epoch [4/5], Step [7340/10336], Loss: 0.0114\n",
      "Epoch [4/5], Step [7342/10336], Loss: 1.1845\n",
      "Epoch [4/5], Step [7344/10336], Loss: 0.0258\n",
      "Epoch [4/5], Step [7346/10336], Loss: 0.0031\n",
      "Epoch [4/5], Step [7348/10336], Loss: 0.0124\n",
      "Epoch [4/5], Step [7350/10336], Loss: 1.7214\n",
      "Epoch [4/5], Step [7352/10336], Loss: 1.4134\n",
      "Epoch [4/5], Step [7354/10336], Loss: 0.9270\n",
      "Epoch [4/5], Step [7356/10336], Loss: 1.9096\n",
      "Epoch [4/5], Step [7358/10336], Loss: 3.5749\n",
      "Epoch [4/5], Step [7360/10336], Loss: 1.4013\n",
      "Epoch [4/5], Step [7362/10336], Loss: 0.4683\n",
      "Epoch [4/5], Step [7364/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [7366/10336], Loss: 0.4696\n",
      "Epoch [4/5], Step [7368/10336], Loss: 0.0227\n",
      "Epoch [4/5], Step [7370/10336], Loss: 0.0096\n",
      "Epoch [4/5], Step [7372/10336], Loss: 0.0207\n",
      "Epoch [4/5], Step [7374/10336], Loss: 0.1159\n",
      "Epoch [4/5], Step [7376/10336], Loss: 0.0128\n",
      "Epoch [4/5], Step [7378/10336], Loss: 0.0868\n",
      "Epoch [4/5], Step [7380/10336], Loss: 3.7459\n",
      "Epoch [4/5], Step [7382/10336], Loss: 0.1244\n",
      "Epoch [4/5], Step [7384/10336], Loss: 0.6120\n",
      "Epoch [4/5], Step [7386/10336], Loss: 0.3379\n",
      "Epoch [4/5], Step [7388/10336], Loss: 0.1120\n",
      "Epoch [4/5], Step [7390/10336], Loss: 0.3957\n",
      "Epoch [4/5], Step [7392/10336], Loss: 4.3476\n",
      "Epoch [4/5], Step [7394/10336], Loss: 0.0491\n",
      "Epoch [4/5], Step [7396/10336], Loss: 0.0957\n",
      "Epoch [4/5], Step [7398/10336], Loss: 0.0195\n",
      "Epoch [4/5], Step [7400/10336], Loss: 0.0153\n",
      "Epoch [4/5], Step [7402/10336], Loss: 0.6422\n",
      "Epoch [4/5], Step [7404/10336], Loss: 0.0114\n",
      "Epoch [4/5], Step [7406/10336], Loss: 0.0168\n",
      "Epoch [4/5], Step [7408/10336], Loss: 1.5370\n",
      "Epoch [4/5], Step [7410/10336], Loss: 0.3242\n",
      "Epoch [4/5], Step [7412/10336], Loss: 0.1763\n",
      "Epoch [4/5], Step [7414/10336], Loss: 0.0126\n",
      "Epoch [4/5], Step [7416/10336], Loss: 0.0674\n",
      "Epoch [4/5], Step [7418/10336], Loss: 0.1106\n",
      "Epoch [4/5], Step [7420/10336], Loss: 0.0278\n",
      "Epoch [4/5], Step [7422/10336], Loss: 0.0892\n",
      "Epoch [4/5], Step [7424/10336], Loss: 0.0446\n",
      "Epoch [4/5], Step [7426/10336], Loss: 0.2730\n",
      "Epoch [4/5], Step [7428/10336], Loss: 0.1041\n",
      "Epoch [4/5], Step [7430/10336], Loss: 0.1117\n",
      "Epoch [4/5], Step [7432/10336], Loss: 0.0345\n",
      "Epoch [4/5], Step [7434/10336], Loss: 0.0493\n",
      "Epoch [4/5], Step [7436/10336], Loss: 0.0234\n",
      "Epoch [4/5], Step [7438/10336], Loss: 0.0200\n",
      "Epoch [4/5], Step [7440/10336], Loss: 0.1621\n",
      "Epoch [4/5], Step [7442/10336], Loss: 0.3777\n",
      "Epoch [4/5], Step [7444/10336], Loss: 0.0119\n",
      "Epoch [4/5], Step [7446/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [7448/10336], Loss: 0.0046\n",
      "Epoch [4/5], Step [7450/10336], Loss: 0.4701\n",
      "Epoch [4/5], Step [7452/10336], Loss: 0.8439\n",
      "Epoch [4/5], Step [7454/10336], Loss: 0.0870\n",
      "Epoch [4/5], Step [7456/10336], Loss: 0.5708\n",
      "Epoch [4/5], Step [7458/10336], Loss: 0.0694\n",
      "Epoch [4/5], Step [7460/10336], Loss: 2.0106\n",
      "Epoch [4/5], Step [7462/10336], Loss: 0.5057\n",
      "Epoch [4/5], Step [7464/10336], Loss: 0.3241\n",
      "Epoch [4/5], Step [7466/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [7468/10336], Loss: 0.0946\n",
      "Epoch [4/5], Step [7470/10336], Loss: 1.7340\n",
      "Epoch [4/5], Step [7472/10336], Loss: 0.0795\n",
      "Epoch [4/5], Step [7474/10336], Loss: 1.4982\n",
      "Epoch [4/5], Step [7476/10336], Loss: 3.3494\n",
      "Epoch [4/5], Step [7478/10336], Loss: 0.0410\n",
      "Epoch [4/5], Step [7480/10336], Loss: 7.1059\n",
      "Epoch [4/5], Step [7482/10336], Loss: 0.1570\n",
      "Epoch [4/5], Step [7484/10336], Loss: 1.5745\n",
      "Epoch [4/5], Step [7486/10336], Loss: 0.5628\n",
      "Epoch [4/5], Step [7488/10336], Loss: 0.1343\n",
      "Epoch [4/5], Step [7490/10336], Loss: 0.0576\n",
      "Epoch [4/5], Step [7492/10336], Loss: 0.0029\n",
      "Epoch [4/5], Step [7494/10336], Loss: 1.7777\n",
      "Epoch [4/5], Step [7496/10336], Loss: 0.7134\n",
      "Epoch [4/5], Step [7498/10336], Loss: 0.9522\n",
      "Epoch [4/5], Step [7500/10336], Loss: 1.1610\n",
      "Epoch [4/5], Step [7502/10336], Loss: 2.3889\n",
      "Epoch [4/5], Step [7504/10336], Loss: 0.0390\n",
      "Epoch [4/5], Step [7506/10336], Loss: 0.3073\n",
      "Epoch [4/5], Step [7508/10336], Loss: 2.3327\n",
      "Epoch [4/5], Step [7510/10336], Loss: 0.0032\n",
      "Epoch [4/5], Step [7512/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [7514/10336], Loss: 2.9337\n",
      "Epoch [4/5], Step [7516/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [7518/10336], Loss: 3.1026\n",
      "Epoch [4/5], Step [7520/10336], Loss: 0.1703\n",
      "Epoch [4/5], Step [7522/10336], Loss: 2.9049\n",
      "Epoch [4/5], Step [7524/10336], Loss: 0.0045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [7526/10336], Loss: 0.6216\n",
      "Epoch [4/5], Step [7528/10336], Loss: 1.2548\n",
      "Epoch [4/5], Step [7530/10336], Loss: 0.1621\n",
      "Epoch [4/5], Step [7532/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [7534/10336], Loss: 3.4030\n",
      "Epoch [4/5], Step [7536/10336], Loss: 0.5801\n",
      "Epoch [4/5], Step [7538/10336], Loss: 1.1894\n",
      "Epoch [4/5], Step [7540/10336], Loss: 1.8202\n",
      "Epoch [4/5], Step [7542/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [7544/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [7546/10336], Loss: 0.3627\n",
      "Epoch [4/5], Step [7548/10336], Loss: 0.1558\n",
      "Epoch [4/5], Step [7550/10336], Loss: 0.0880\n",
      "Epoch [4/5], Step [7552/10336], Loss: 0.2616\n",
      "Epoch [4/5], Step [7554/10336], Loss: 0.6286\n",
      "Epoch [4/5], Step [7556/10336], Loss: 0.7966\n",
      "Epoch [4/5], Step [7558/10336], Loss: 2.3690\n",
      "Epoch [4/5], Step [7560/10336], Loss: 0.1460\n",
      "Epoch [4/5], Step [7562/10336], Loss: 1.8113\n",
      "Epoch [4/5], Step [7564/10336], Loss: 1.5034\n",
      "Epoch [4/5], Step [7566/10336], Loss: 1.3301\n",
      "Epoch [4/5], Step [7568/10336], Loss: 0.4495\n",
      "Epoch [4/5], Step [7570/10336], Loss: 0.8238\n",
      "Epoch [4/5], Step [7572/10336], Loss: 1.1336\n",
      "Epoch [4/5], Step [7574/10336], Loss: 0.0471\n",
      "Epoch [4/5], Step [7576/10336], Loss: 1.1823\n",
      "Epoch [4/5], Step [7578/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [7580/10336], Loss: 0.0655\n",
      "Epoch [4/5], Step [7582/10336], Loss: 0.0951\n",
      "Epoch [4/5], Step [7584/10336], Loss: 0.4418\n",
      "Epoch [4/5], Step [7586/10336], Loss: 0.0053\n",
      "Epoch [4/5], Step [7588/10336], Loss: 0.0391\n",
      "Epoch [4/5], Step [7590/10336], Loss: 0.0205\n",
      "Epoch [4/5], Step [7592/10336], Loss: 0.2158\n",
      "Epoch [4/5], Step [7594/10336], Loss: 0.0474\n",
      "Epoch [4/5], Step [7596/10336], Loss: 0.1387\n",
      "Epoch [4/5], Step [7598/10336], Loss: 3.7374\n",
      "Epoch [4/5], Step [7600/10336], Loss: 0.0162\n",
      "Epoch [4/5], Step [7602/10336], Loss: 0.0128\n",
      "Epoch [4/5], Step [7604/10336], Loss: 0.1901\n",
      "Epoch [4/5], Step [7606/10336], Loss: 1.2726\n",
      "Epoch [4/5], Step [7608/10336], Loss: 0.0377\n",
      "Epoch [4/5], Step [7610/10336], Loss: 0.6101\n",
      "Epoch [4/5], Step [7612/10336], Loss: 0.7616\n",
      "Epoch [4/5], Step [7614/10336], Loss: 0.7432\n",
      "Epoch [4/5], Step [7616/10336], Loss: 0.3936\n",
      "Epoch [4/5], Step [7618/10336], Loss: 0.4450\n",
      "Epoch [4/5], Step [7620/10336], Loss: 0.0361\n",
      "Epoch [4/5], Step [7622/10336], Loss: 1.0649\n",
      "Epoch [4/5], Step [7624/10336], Loss: 1.5032\n",
      "Epoch [4/5], Step [7626/10336], Loss: 3.3398\n",
      "Epoch [4/5], Step [7628/10336], Loss: 2.5812\n",
      "Epoch [4/5], Step [7630/10336], Loss: 0.0461\n",
      "Epoch [4/5], Step [7632/10336], Loss: 0.4926\n",
      "Epoch [4/5], Step [7634/10336], Loss: 0.0601\n",
      "Epoch [4/5], Step [7636/10336], Loss: 0.8299\n",
      "Epoch [4/5], Step [7638/10336], Loss: 0.2760\n",
      "Epoch [4/5], Step [7640/10336], Loss: 2.0511\n",
      "Epoch [4/5], Step [7642/10336], Loss: 0.2056\n",
      "Epoch [4/5], Step [7644/10336], Loss: 1.9101\n",
      "Epoch [4/5], Step [7646/10336], Loss: 0.0101\n",
      "Epoch [4/5], Step [7648/10336], Loss: 0.0045\n",
      "Epoch [4/5], Step [7650/10336], Loss: 1.3785\n",
      "Epoch [4/5], Step [7652/10336], Loss: 2.1911\n",
      "Epoch [4/5], Step [7654/10336], Loss: 0.1528\n",
      "Epoch [4/5], Step [7656/10336], Loss: 0.0039\n",
      "Epoch [4/5], Step [7658/10336], Loss: 0.1660\n",
      "Epoch [4/5], Step [7660/10336], Loss: 0.1612\n",
      "Epoch [4/5], Step [7662/10336], Loss: 0.1309\n",
      "Epoch [4/5], Step [7664/10336], Loss: 0.8716\n",
      "Epoch [4/5], Step [7666/10336], Loss: 0.5424\n",
      "Epoch [4/5], Step [7668/10336], Loss: 0.0728\n",
      "Epoch [4/5], Step [7670/10336], Loss: 1.2326\n",
      "Epoch [4/5], Step [7672/10336], Loss: 2.3318\n",
      "Epoch [4/5], Step [7674/10336], Loss: 0.1637\n",
      "Epoch [4/5], Step [7676/10336], Loss: 0.0848\n",
      "Epoch [4/5], Step [7678/10336], Loss: 3.2963\n",
      "Epoch [4/5], Step [7680/10336], Loss: 0.1855\n",
      "Epoch [4/5], Step [7682/10336], Loss: 0.3999\n",
      "Epoch [4/5], Step [7684/10336], Loss: 0.1502\n",
      "Epoch [4/5], Step [7686/10336], Loss: 0.1955\n",
      "Epoch [4/5], Step [7688/10336], Loss: 0.2613\n",
      "Epoch [4/5], Step [7690/10336], Loss: 0.7386\n",
      "Epoch [4/5], Step [7692/10336], Loss: 0.7241\n",
      "Epoch [4/5], Step [7694/10336], Loss: 0.7397\n",
      "Epoch [4/5], Step [7696/10336], Loss: 0.9418\n",
      "Epoch [4/5], Step [7698/10336], Loss: 0.2373\n",
      "Epoch [4/5], Step [7700/10336], Loss: 0.4151\n",
      "Epoch [4/5], Step [7702/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [7704/10336], Loss: 0.1214\n",
      "Epoch [4/5], Step [7706/10336], Loss: 1.5789\n",
      "Epoch [4/5], Step [7708/10336], Loss: 0.0026\n",
      "Epoch [4/5], Step [7710/10336], Loss: 1.1165\n",
      "Epoch [4/5], Step [7712/10336], Loss: 0.0011\n",
      "Epoch [4/5], Step [7714/10336], Loss: 1.2335\n",
      "Epoch [4/5], Step [7716/10336], Loss: 0.0075\n",
      "Epoch [4/5], Step [7718/10336], Loss: 0.2578\n",
      "Epoch [4/5], Step [7720/10336], Loss: 2.1553\n",
      "Epoch [4/5], Step [7722/10336], Loss: 0.8910\n",
      "Epoch [4/5], Step [7724/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [7726/10336], Loss: 0.3313\n",
      "Epoch [4/5], Step [7728/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [7730/10336], Loss: 0.2126\n",
      "Epoch [4/5], Step [7732/10336], Loss: 0.0035\n",
      "Epoch [4/5], Step [7734/10336], Loss: 0.4149\n",
      "Epoch [4/5], Step [7736/10336], Loss: 0.0572\n",
      "Epoch [4/5], Step [7738/10336], Loss: 0.0430\n",
      "Epoch [4/5], Step [7740/10336], Loss: 0.0413\n",
      "Epoch [4/5], Step [7742/10336], Loss: 0.4470\n",
      "Epoch [4/5], Step [7744/10336], Loss: 0.2244\n",
      "Epoch [4/5], Step [7746/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [7748/10336], Loss: 1.4552\n",
      "Epoch [4/5], Step [7750/10336], Loss: 0.0468\n",
      "Epoch [4/5], Step [7752/10336], Loss: 0.3020\n",
      "Epoch [4/5], Step [7754/10336], Loss: 4.1013\n",
      "Epoch [4/5], Step [7756/10336], Loss: 1.1881\n",
      "Epoch [4/5], Step [7758/10336], Loss: 0.7519\n",
      "Epoch [4/5], Step [7760/10336], Loss: 1.0230\n",
      "Epoch [4/5], Step [7762/10336], Loss: 1.5194\n",
      "Epoch [4/5], Step [7764/10336], Loss: 0.8560\n",
      "Epoch [4/5], Step [7766/10336], Loss: 2.9880\n",
      "Epoch [4/5], Step [7768/10336], Loss: 2.5043\n",
      "Epoch [4/5], Step [7770/10336], Loss: 1.0085\n",
      "Epoch [4/5], Step [7772/10336], Loss: 3.8608\n",
      "Epoch [4/5], Step [7774/10336], Loss: 0.5280\n",
      "Epoch [4/5], Step [7776/10336], Loss: 0.0411\n",
      "Epoch [4/5], Step [7778/10336], Loss: 0.3836\n",
      "Epoch [4/5], Step [7780/10336], Loss: 1.4954\n",
      "Epoch [4/5], Step [7782/10336], Loss: 1.2104\n",
      "Epoch [4/5], Step [7784/10336], Loss: 0.0354\n",
      "Epoch [4/5], Step [7786/10336], Loss: 1.1444\n",
      "Epoch [4/5], Step [7788/10336], Loss: 0.1505\n",
      "Epoch [4/5], Step [7790/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [7792/10336], Loss: 0.0307\n",
      "Epoch [4/5], Step [7794/10336], Loss: 0.1263\n",
      "Epoch [4/5], Step [7796/10336], Loss: 0.2089\n",
      "Epoch [4/5], Step [7798/10336], Loss: 0.0659\n",
      "Epoch [4/5], Step [7800/10336], Loss: 1.4508\n",
      "Epoch [4/5], Step [7802/10336], Loss: 0.0128\n",
      "Epoch [4/5], Step [7804/10336], Loss: 0.5366\n",
      "Epoch [4/5], Step [7806/10336], Loss: 0.7661\n",
      "Epoch [4/5], Step [7808/10336], Loss: 0.2612\n",
      "Epoch [4/5], Step [7810/10336], Loss: 0.0886\n",
      "Epoch [4/5], Step [7812/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [7814/10336], Loss: 0.1295\n",
      "Epoch [4/5], Step [7816/10336], Loss: 2.7364\n",
      "Epoch [4/5], Step [7818/10336], Loss: 0.2018\n",
      "Epoch [4/5], Step [7820/10336], Loss: 0.0068\n",
      "Epoch [4/5], Step [7822/10336], Loss: 0.0163\n",
      "Epoch [4/5], Step [7824/10336], Loss: 0.4215\n",
      "Epoch [4/5], Step [7826/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [7828/10336], Loss: 0.0094\n",
      "Epoch [4/5], Step [7830/10336], Loss: 3.4267\n",
      "Epoch [4/5], Step [7832/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [7834/10336], Loss: 0.1677\n",
      "Epoch [4/5], Step [7836/10336], Loss: 0.0071\n",
      "Epoch [4/5], Step [7838/10336], Loss: 0.0772\n",
      "Epoch [4/5], Step [7840/10336], Loss: 1.7126\n",
      "Epoch [4/5], Step [7842/10336], Loss: 0.0087\n",
      "Epoch [4/5], Step [7844/10336], Loss: 0.2026\n",
      "Epoch [4/5], Step [7846/10336], Loss: 0.0038\n",
      "Epoch [4/5], Step [7848/10336], Loss: 0.1782\n",
      "Epoch [4/5], Step [7850/10336], Loss: 0.2047\n",
      "Epoch [4/5], Step [7852/10336], Loss: 0.5514\n",
      "Epoch [4/5], Step [7854/10336], Loss: 0.1272\n",
      "Epoch [4/5], Step [7856/10336], Loss: 0.0316\n",
      "Epoch [4/5], Step [7858/10336], Loss: 1.6470\n",
      "Epoch [4/5], Step [7860/10336], Loss: 0.4800\n",
      "Epoch [4/5], Step [7862/10336], Loss: 0.7425\n",
      "Epoch [4/5], Step [7864/10336], Loss: 1.0023\n",
      "Epoch [4/5], Step [7866/10336], Loss: 0.5663\n",
      "Epoch [4/5], Step [7868/10336], Loss: 0.0102\n",
      "Epoch [4/5], Step [7870/10336], Loss: 3.0212\n",
      "Epoch [4/5], Step [7872/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [7874/10336], Loss: 0.8130\n",
      "Epoch [4/5], Step [7876/10336], Loss: 0.0675\n",
      "Epoch [4/5], Step [7878/10336], Loss: 0.8684\n",
      "Epoch [4/5], Step [7880/10336], Loss: 2.2171\n",
      "Epoch [4/5], Step [7882/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [7884/10336], Loss: 0.0070\n",
      "Epoch [4/5], Step [7886/10336], Loss: 0.3114\n",
      "Epoch [4/5], Step [7888/10336], Loss: 0.0509\n",
      "Epoch [4/5], Step [7890/10336], Loss: 0.0030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [7892/10336], Loss: 0.1347\n",
      "Epoch [4/5], Step [7894/10336], Loss: 0.2471\n",
      "Epoch [4/5], Step [7896/10336], Loss: 2.6313\n",
      "Epoch [4/5], Step [7898/10336], Loss: 0.5364\n",
      "Epoch [4/5], Step [7900/10336], Loss: 0.0328\n",
      "Epoch [4/5], Step [7902/10336], Loss: 0.0900\n",
      "Epoch [4/5], Step [7904/10336], Loss: 0.2713\n",
      "Epoch [4/5], Step [7906/10336], Loss: 0.2235\n",
      "Epoch [4/5], Step [7908/10336], Loss: 0.5308\n",
      "Epoch [4/5], Step [7910/10336], Loss: 0.8891\n",
      "Epoch [4/5], Step [7912/10336], Loss: 0.5555\n",
      "Epoch [4/5], Step [7914/10336], Loss: 0.0497\n",
      "Epoch [4/5], Step [7916/10336], Loss: 0.0452\n",
      "Epoch [4/5], Step [7918/10336], Loss: 1.1234\n",
      "Epoch [4/5], Step [7920/10336], Loss: 0.1650\n",
      "Epoch [4/5], Step [7922/10336], Loss: 1.1769\n",
      "Epoch [4/5], Step [7924/10336], Loss: 0.1422\n",
      "Epoch [4/5], Step [7926/10336], Loss: 0.3612\n",
      "Epoch [4/5], Step [7928/10336], Loss: 2.6085\n",
      "Epoch [4/5], Step [7930/10336], Loss: 0.4184\n",
      "Epoch [4/5], Step [7932/10336], Loss: 0.0146\n",
      "Epoch [4/5], Step [7934/10336], Loss: 0.3706\n",
      "Epoch [4/5], Step [7936/10336], Loss: 0.7043\n",
      "Epoch [4/5], Step [7938/10336], Loss: 2.1484\n",
      "Epoch [4/5], Step [7940/10336], Loss: 0.0128\n",
      "Epoch [4/5], Step [7942/10336], Loss: 0.0120\n",
      "Epoch [4/5], Step [7944/10336], Loss: 1.1756\n",
      "Epoch [4/5], Step [7946/10336], Loss: 0.7534\n",
      "Epoch [4/5], Step [7948/10336], Loss: 0.3661\n",
      "Epoch [4/5], Step [7950/10336], Loss: 1.8283\n",
      "Epoch [4/5], Step [7952/10336], Loss: 0.1247\n",
      "Epoch [4/5], Step [7954/10336], Loss: 0.0890\n",
      "Epoch [4/5], Step [7956/10336], Loss: 0.2025\n",
      "Epoch [4/5], Step [7958/10336], Loss: 0.0527\n",
      "Epoch [4/5], Step [7960/10336], Loss: 0.8836\n",
      "Epoch [4/5], Step [7962/10336], Loss: 0.0067\n",
      "Epoch [4/5], Step [7964/10336], Loss: 0.4281\n",
      "Epoch [4/5], Step [7966/10336], Loss: 0.3620\n",
      "Epoch [4/5], Step [7968/10336], Loss: 0.9474\n",
      "Epoch [4/5], Step [7970/10336], Loss: 1.4923\n",
      "Epoch [4/5], Step [7972/10336], Loss: 0.0810\n",
      "Epoch [4/5], Step [7974/10336], Loss: 1.8402\n",
      "Epoch [4/5], Step [7976/10336], Loss: 0.4357\n",
      "Epoch [4/5], Step [7978/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [7980/10336], Loss: 0.1847\n",
      "Epoch [4/5], Step [7982/10336], Loss: 1.4821\n",
      "Epoch [4/5], Step [7984/10336], Loss: 4.3076\n",
      "Epoch [4/5], Step [7986/10336], Loss: 0.6395\n",
      "Epoch [4/5], Step [7988/10336], Loss: 1.9642\n",
      "Epoch [4/5], Step [7990/10336], Loss: 0.4982\n",
      "Epoch [4/5], Step [7992/10336], Loss: 1.4031\n",
      "Epoch [4/5], Step [7994/10336], Loss: 0.6401\n",
      "Epoch [4/5], Step [7996/10336], Loss: 2.3104\n",
      "Epoch [4/5], Step [7998/10336], Loss: 0.0044\n",
      "Epoch [4/5], Step [8000/10336], Loss: 0.1933\n",
      "Epoch [4/5], Step [8002/10336], Loss: 0.3194\n",
      "Epoch [4/5], Step [8004/10336], Loss: 0.5705\n",
      "Epoch [4/5], Step [8006/10336], Loss: 0.0147\n",
      "Epoch [4/5], Step [8008/10336], Loss: 0.0282\n",
      "Epoch [4/5], Step [8010/10336], Loss: 1.1714\n",
      "Epoch [4/5], Step [8012/10336], Loss: 0.8657\n",
      "Epoch [4/5], Step [8014/10336], Loss: 0.0483\n",
      "Epoch [4/5], Step [8016/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [8018/10336], Loss: 0.3360\n",
      "Epoch [4/5], Step [8020/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [8022/10336], Loss: 0.0314\n",
      "Epoch [4/5], Step [8024/10336], Loss: 0.0322\n",
      "Epoch [4/5], Step [8026/10336], Loss: 0.4632\n",
      "Epoch [4/5], Step [8028/10336], Loss: 0.0624\n",
      "Epoch [4/5], Step [8030/10336], Loss: 0.0685\n",
      "Epoch [4/5], Step [8032/10336], Loss: 0.0708\n",
      "Epoch [4/5], Step [8034/10336], Loss: 0.2270\n",
      "Epoch [4/5], Step [8036/10336], Loss: 0.0146\n",
      "Epoch [4/5], Step [8038/10336], Loss: 0.4327\n",
      "Epoch [4/5], Step [8040/10336], Loss: 0.7038\n",
      "Epoch [4/5], Step [8042/10336], Loss: 0.0138\n",
      "Epoch [4/5], Step [8044/10336], Loss: 0.0369\n",
      "Epoch [4/5], Step [8046/10336], Loss: 1.3171\n",
      "Epoch [4/5], Step [8048/10336], Loss: 1.4908\n",
      "Epoch [4/5], Step [8050/10336], Loss: 2.9659\n",
      "Epoch [4/5], Step [8052/10336], Loss: 0.0104\n",
      "Epoch [4/5], Step [8054/10336], Loss: 0.0932\n",
      "Epoch [4/5], Step [8056/10336], Loss: 0.3305\n",
      "Epoch [4/5], Step [8058/10336], Loss: 1.4322\n",
      "Epoch [4/5], Step [8060/10336], Loss: 1.8206\n",
      "Epoch [4/5], Step [8062/10336], Loss: 0.0077\n",
      "Epoch [4/5], Step [8064/10336], Loss: 0.0352\n",
      "Epoch [4/5], Step [8066/10336], Loss: 2.5405\n",
      "Epoch [4/5], Step [8068/10336], Loss: 0.0181\n",
      "Epoch [4/5], Step [8070/10336], Loss: 1.8190\n",
      "Epoch [4/5], Step [8072/10336], Loss: 0.0337\n",
      "Epoch [4/5], Step [8074/10336], Loss: 0.5404\n",
      "Epoch [4/5], Step [8076/10336], Loss: 0.3247\n",
      "Epoch [4/5], Step [8078/10336], Loss: 0.4306\n",
      "Epoch [4/5], Step [8080/10336], Loss: 0.5369\n",
      "Epoch [4/5], Step [8082/10336], Loss: 0.0981\n",
      "Epoch [4/5], Step [8084/10336], Loss: 0.0092\n",
      "Epoch [4/5], Step [8086/10336], Loss: 0.0264\n",
      "Epoch [4/5], Step [8088/10336], Loss: 0.6607\n",
      "Epoch [4/5], Step [8090/10336], Loss: 2.3917\n",
      "Epoch [4/5], Step [8092/10336], Loss: 0.8118\n",
      "Epoch [4/5], Step [8094/10336], Loss: 0.6228\n",
      "Epoch [4/5], Step [8096/10336], Loss: 2.4703\n",
      "Epoch [4/5], Step [8098/10336], Loss: 0.0105\n",
      "Epoch [4/5], Step [8100/10336], Loss: 0.0171\n",
      "Epoch [4/5], Step [8102/10336], Loss: 1.0145\n",
      "Epoch [4/5], Step [8104/10336], Loss: 0.0518\n",
      "Epoch [4/5], Step [8106/10336], Loss: 0.0121\n",
      "Epoch [4/5], Step [8108/10336], Loss: 0.0594\n",
      "Epoch [4/5], Step [8110/10336], Loss: 0.0154\n",
      "Epoch [4/5], Step [8112/10336], Loss: 0.0479\n",
      "Epoch [4/5], Step [8114/10336], Loss: 0.4364\n",
      "Epoch [4/5], Step [8116/10336], Loss: 0.0555\n",
      "Epoch [4/5], Step [8118/10336], Loss: 0.4758\n",
      "Epoch [4/5], Step [8120/10336], Loss: 4.0216\n",
      "Epoch [4/5], Step [8122/10336], Loss: 0.3189\n",
      "Epoch [4/5], Step [8124/10336], Loss: 0.1219\n",
      "Epoch [4/5], Step [8126/10336], Loss: 1.0977\n",
      "Epoch [4/5], Step [8128/10336], Loss: 2.0537\n",
      "Epoch [4/5], Step [8130/10336], Loss: 0.0807\n",
      "Epoch [4/5], Step [8132/10336], Loss: 0.0182\n",
      "Epoch [4/5], Step [8134/10336], Loss: 1.8359\n",
      "Epoch [4/5], Step [8136/10336], Loss: 0.1114\n",
      "Epoch [4/5], Step [8138/10336], Loss: 0.0051\n",
      "Epoch [4/5], Step [8140/10336], Loss: 0.3024\n",
      "Epoch [4/5], Step [8142/10336], Loss: 0.1364\n",
      "Epoch [4/5], Step [8144/10336], Loss: 1.3023\n",
      "Epoch [4/5], Step [8146/10336], Loss: 0.4389\n",
      "Epoch [4/5], Step [8148/10336], Loss: 0.1439\n",
      "Epoch [4/5], Step [8150/10336], Loss: 1.4242\n",
      "Epoch [4/5], Step [8152/10336], Loss: 1.4325\n",
      "Epoch [4/5], Step [8154/10336], Loss: 0.3819\n",
      "Epoch [4/5], Step [8156/10336], Loss: 0.0565\n",
      "Epoch [4/5], Step [8158/10336], Loss: 0.0091\n",
      "Epoch [4/5], Step [8160/10336], Loss: 2.2104\n",
      "Epoch [4/5], Step [8162/10336], Loss: 0.1368\n",
      "Epoch [4/5], Step [8164/10336], Loss: 0.0132\n",
      "Epoch [4/5], Step [8166/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [8168/10336], Loss: 0.2357\n",
      "Epoch [4/5], Step [8170/10336], Loss: 0.0586\n",
      "Epoch [4/5], Step [8172/10336], Loss: 1.4287\n",
      "Epoch [4/5], Step [8174/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [8176/10336], Loss: 0.1132\n",
      "Epoch [4/5], Step [8178/10336], Loss: 2.7733\n",
      "Epoch [4/5], Step [8180/10336], Loss: 0.0823\n",
      "Epoch [4/5], Step [8182/10336], Loss: 0.0169\n",
      "Epoch [4/5], Step [8184/10336], Loss: 0.0347\n",
      "Epoch [4/5], Step [8186/10336], Loss: 0.0435\n",
      "Epoch [4/5], Step [8188/10336], Loss: 0.0202\n",
      "Epoch [4/5], Step [8190/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [8192/10336], Loss: 1.3006\n",
      "Epoch [4/5], Step [8194/10336], Loss: 1.3774\n",
      "Epoch [4/5], Step [8196/10336], Loss: 0.2405\n",
      "Epoch [4/5], Step [8198/10336], Loss: 0.3667\n",
      "Epoch [4/5], Step [8200/10336], Loss: 3.2509\n",
      "Epoch [4/5], Step [8202/10336], Loss: 0.0287\n",
      "Epoch [4/5], Step [8204/10336], Loss: 0.4042\n",
      "Epoch [4/5], Step [8206/10336], Loss: 0.5117\n",
      "Epoch [4/5], Step [8208/10336], Loss: 0.4399\n",
      "Epoch [4/5], Step [8210/10336], Loss: 0.8567\n",
      "Epoch [4/5], Step [8212/10336], Loss: 0.3041\n",
      "Epoch [4/5], Step [8214/10336], Loss: 0.2619\n",
      "Epoch [4/5], Step [8216/10336], Loss: 0.1027\n",
      "Epoch [4/5], Step [8218/10336], Loss: 2.2175\n",
      "Epoch [4/5], Step [8220/10336], Loss: 1.3045\n",
      "Epoch [4/5], Step [8222/10336], Loss: 0.3354\n",
      "Epoch [4/5], Step [8224/10336], Loss: 0.6736\n",
      "Epoch [4/5], Step [8226/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [8228/10336], Loss: 2.1361\n",
      "Epoch [4/5], Step [8230/10336], Loss: 0.0078\n",
      "Epoch [4/5], Step [8232/10336], Loss: 0.0137\n",
      "Epoch [4/5], Step [8234/10336], Loss: 1.2372\n",
      "Epoch [4/5], Step [8236/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [8238/10336], Loss: 0.0055\n",
      "Epoch [4/5], Step [8240/10336], Loss: 1.0475\n",
      "Epoch [4/5], Step [8242/10336], Loss: 0.1147\n",
      "Epoch [4/5], Step [8244/10336], Loss: 0.3053\n",
      "Epoch [4/5], Step [8246/10336], Loss: 1.6258\n",
      "Epoch [4/5], Step [8248/10336], Loss: 0.5174\n",
      "Epoch [4/5], Step [8250/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [8252/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [8254/10336], Loss: 0.2795\n",
      "Epoch [4/5], Step [8256/10336], Loss: 2.9314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [8258/10336], Loss: 0.0347\n",
      "Epoch [4/5], Step [8260/10336], Loss: 0.8693\n",
      "Epoch [4/5], Step [8262/10336], Loss: 0.2058\n",
      "Epoch [4/5], Step [8264/10336], Loss: 0.0053\n",
      "Epoch [4/5], Step [8266/10336], Loss: 0.7274\n",
      "Epoch [4/5], Step [8268/10336], Loss: 1.1236\n",
      "Epoch [4/5], Step [8270/10336], Loss: 0.0190\n",
      "Epoch [4/5], Step [8272/10336], Loss: 1.6792\n",
      "Epoch [4/5], Step [8274/10336], Loss: 0.4621\n",
      "Epoch [4/5], Step [8276/10336], Loss: 0.3360\n",
      "Epoch [4/5], Step [8278/10336], Loss: 0.4327\n",
      "Epoch [4/5], Step [8280/10336], Loss: 0.7269\n",
      "Epoch [4/5], Step [8282/10336], Loss: 3.9663\n",
      "Epoch [4/5], Step [8284/10336], Loss: 0.0191\n",
      "Epoch [4/5], Step [8286/10336], Loss: 0.0738\n",
      "Epoch [4/5], Step [8288/10336], Loss: 4.3608\n",
      "Epoch [4/5], Step [8290/10336], Loss: 0.0619\n",
      "Epoch [4/5], Step [8292/10336], Loss: 1.2398\n",
      "Epoch [4/5], Step [8294/10336], Loss: 0.2515\n",
      "Epoch [4/5], Step [8296/10336], Loss: 0.0374\n",
      "Epoch [4/5], Step [8298/10336], Loss: 0.1852\n",
      "Epoch [4/5], Step [8300/10336], Loss: 1.4639\n",
      "Epoch [4/5], Step [8302/10336], Loss: 2.6820\n",
      "Epoch [4/5], Step [8304/10336], Loss: 0.0273\n",
      "Epoch [4/5], Step [8306/10336], Loss: 0.5867\n",
      "Epoch [4/5], Step [8308/10336], Loss: 0.9459\n",
      "Epoch [4/5], Step [8310/10336], Loss: 1.6659\n",
      "Epoch [4/5], Step [8312/10336], Loss: 0.7500\n",
      "Epoch [4/5], Step [8314/10336], Loss: 2.6121\n",
      "Epoch [4/5], Step [8316/10336], Loss: 0.0218\n",
      "Epoch [4/5], Step [8318/10336], Loss: 0.0334\n",
      "Epoch [4/5], Step [8320/10336], Loss: 0.0685\n",
      "Epoch [4/5], Step [8322/10336], Loss: 2.5591\n",
      "Epoch [4/5], Step [8324/10336], Loss: 0.1389\n",
      "Epoch [4/5], Step [8326/10336], Loss: 0.0959\n",
      "Epoch [4/5], Step [8328/10336], Loss: 0.0074\n",
      "Epoch [4/5], Step [8330/10336], Loss: 3.1371\n",
      "Epoch [4/5], Step [8332/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [8334/10336], Loss: 0.8140\n",
      "Epoch [4/5], Step [8336/10336], Loss: 0.2458\n",
      "Epoch [4/5], Step [8338/10336], Loss: 0.0776\n",
      "Epoch [4/5], Step [8340/10336], Loss: 0.8585\n",
      "Epoch [4/5], Step [8342/10336], Loss: 0.6168\n",
      "Epoch [4/5], Step [8344/10336], Loss: 0.1377\n",
      "Epoch [4/5], Step [8346/10336], Loss: 1.2218\n",
      "Epoch [4/5], Step [8348/10336], Loss: 0.3373\n",
      "Epoch [4/5], Step [8350/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [8352/10336], Loss: 0.0980\n",
      "Epoch [4/5], Step [8354/10336], Loss: 2.0534\n",
      "Epoch [4/5], Step [8356/10336], Loss: 0.2636\n",
      "Epoch [4/5], Step [8358/10336], Loss: 0.0057\n",
      "Epoch [4/5], Step [8360/10336], Loss: 0.4510\n",
      "Epoch [4/5], Step [8362/10336], Loss: 1.7334\n",
      "Epoch [4/5], Step [8364/10336], Loss: 4.2719\n",
      "Epoch [4/5], Step [8366/10336], Loss: 0.1219\n",
      "Epoch [4/5], Step [8368/10336], Loss: 0.0279\n",
      "Epoch [4/5], Step [8370/10336], Loss: 0.0361\n",
      "Epoch [4/5], Step [8372/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [8374/10336], Loss: 0.6000\n",
      "Epoch [4/5], Step [8376/10336], Loss: 0.3066\n",
      "Epoch [4/5], Step [8378/10336], Loss: 0.8254\n",
      "Epoch [4/5], Step [8380/10336], Loss: 3.0964\n",
      "Epoch [4/5], Step [8382/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [8384/10336], Loss: 0.4461\n",
      "Epoch [4/5], Step [8386/10336], Loss: 2.1215\n",
      "Epoch [4/5], Step [8388/10336], Loss: 0.0305\n",
      "Epoch [4/5], Step [8390/10336], Loss: 1.5117\n",
      "Epoch [4/5], Step [8392/10336], Loss: 1.1337\n",
      "Epoch [4/5], Step [8394/10336], Loss: 0.6136\n",
      "Epoch [4/5], Step [8396/10336], Loss: 0.7458\n",
      "Epoch [4/5], Step [8398/10336], Loss: 3.3931\n",
      "Epoch [4/5], Step [8400/10336], Loss: 0.2559\n",
      "Epoch [4/5], Step [8402/10336], Loss: 0.4234\n",
      "Epoch [4/5], Step [8404/10336], Loss: 5.7535\n",
      "Epoch [4/5], Step [8406/10336], Loss: 0.8565\n",
      "Epoch [4/5], Step [8408/10336], Loss: 3.8781\n",
      "Epoch [4/5], Step [8410/10336], Loss: 0.3385\n",
      "Epoch [4/5], Step [8412/10336], Loss: 0.0448\n",
      "Epoch [4/5], Step [8414/10336], Loss: 0.0432\n",
      "Epoch [4/5], Step [8416/10336], Loss: 0.5254\n",
      "Epoch [4/5], Step [8418/10336], Loss: 1.5151\n",
      "Epoch [4/5], Step [8420/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [8422/10336], Loss: 0.0899\n",
      "Epoch [4/5], Step [8424/10336], Loss: 0.0135\n",
      "Epoch [4/5], Step [8426/10336], Loss: 0.2662\n",
      "Epoch [4/5], Step [8428/10336], Loss: 0.1589\n",
      "Epoch [4/5], Step [8430/10336], Loss: 0.7835\n",
      "Epoch [4/5], Step [8432/10336], Loss: 0.2170\n",
      "Epoch [4/5], Step [8434/10336], Loss: 1.2245\n",
      "Epoch [4/5], Step [8436/10336], Loss: 1.1204\n",
      "Epoch [4/5], Step [8438/10336], Loss: 0.3370\n",
      "Epoch [4/5], Step [8440/10336], Loss: 0.6319\n",
      "Epoch [4/5], Step [8442/10336], Loss: 0.0125\n",
      "Epoch [4/5], Step [8444/10336], Loss: 0.8090\n",
      "Epoch [4/5], Step [8446/10336], Loss: 0.0865\n",
      "Epoch [4/5], Step [8448/10336], Loss: 1.8178\n",
      "Epoch [4/5], Step [8450/10336], Loss: 2.4669\n",
      "Epoch [4/5], Step [8452/10336], Loss: 1.3579\n",
      "Epoch [4/5], Step [8454/10336], Loss: 1.3889\n",
      "Epoch [4/5], Step [8456/10336], Loss: 0.2557\n",
      "Epoch [4/5], Step [8458/10336], Loss: 0.8158\n",
      "Epoch [4/5], Step [8460/10336], Loss: 1.5210\n",
      "Epoch [4/5], Step [8462/10336], Loss: 0.0151\n",
      "Epoch [4/5], Step [8464/10336], Loss: 0.3581\n",
      "Epoch [4/5], Step [8466/10336], Loss: 0.0599\n",
      "Epoch [4/5], Step [8468/10336], Loss: 0.8900\n",
      "Epoch [4/5], Step [8470/10336], Loss: 3.8015\n",
      "Epoch [4/5], Step [8472/10336], Loss: 0.6065\n",
      "Epoch [4/5], Step [8474/10336], Loss: 0.3097\n",
      "Epoch [4/5], Step [8476/10336], Loss: 0.4654\n",
      "Epoch [4/5], Step [8478/10336], Loss: 0.5872\n",
      "Epoch [4/5], Step [8480/10336], Loss: 0.7289\n",
      "Epoch [4/5], Step [8482/10336], Loss: 0.4135\n",
      "Epoch [4/5], Step [8484/10336], Loss: 0.0630\n",
      "Epoch [4/5], Step [8486/10336], Loss: 2.1466\n",
      "Epoch [4/5], Step [8488/10336], Loss: 0.3070\n",
      "Epoch [4/5], Step [8490/10336], Loss: 0.0065\n",
      "Epoch [4/5], Step [8492/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [8494/10336], Loss: 2.3256\n",
      "Epoch [4/5], Step [8496/10336], Loss: 1.3600\n",
      "Epoch [4/5], Step [8498/10336], Loss: 2.0157\n",
      "Epoch [4/5], Step [8500/10336], Loss: 0.2772\n",
      "Epoch [4/5], Step [8502/10336], Loss: 0.0116\n",
      "Epoch [4/5], Step [8504/10336], Loss: 1.0159\n",
      "Epoch [4/5], Step [8506/10336], Loss: 0.2481\n",
      "Epoch [4/5], Step [8508/10336], Loss: 1.0618\n",
      "Epoch [4/5], Step [8510/10336], Loss: 0.0456\n",
      "Epoch [4/5], Step [8512/10336], Loss: 2.4607\n",
      "Epoch [4/5], Step [8514/10336], Loss: 0.0330\n",
      "Epoch [4/5], Step [8516/10336], Loss: 1.7993\n",
      "Epoch [4/5], Step [8518/10336], Loss: 0.2677\n",
      "Epoch [4/5], Step [8520/10336], Loss: 0.1082\n",
      "Epoch [4/5], Step [8522/10336], Loss: 0.2556\n",
      "Epoch [4/5], Step [8524/10336], Loss: 0.0855\n",
      "Epoch [4/5], Step [8526/10336], Loss: 0.1070\n",
      "Epoch [4/5], Step [8528/10336], Loss: 0.0987\n",
      "Epoch [4/5], Step [8530/10336], Loss: 0.9787\n",
      "Epoch [4/5], Step [8532/10336], Loss: 0.0284\n",
      "Epoch [4/5], Step [8534/10336], Loss: 0.3666\n",
      "Epoch [4/5], Step [8536/10336], Loss: 0.0496\n",
      "Epoch [4/5], Step [8538/10336], Loss: 0.0925\n",
      "Epoch [4/5], Step [8540/10336], Loss: 0.8597\n",
      "Epoch [4/5], Step [8542/10336], Loss: 1.8381\n",
      "Epoch [4/5], Step [8544/10336], Loss: 0.4580\n",
      "Epoch [4/5], Step [8546/10336], Loss: 1.3094\n",
      "Epoch [4/5], Step [8548/10336], Loss: 0.2704\n",
      "Epoch [4/5], Step [8550/10336], Loss: 1.4826\n",
      "Epoch [4/5], Step [8552/10336], Loss: 0.3054\n",
      "Epoch [4/5], Step [8554/10336], Loss: 0.2351\n",
      "Epoch [4/5], Step [8556/10336], Loss: 0.3592\n",
      "Epoch [4/5], Step [8558/10336], Loss: 3.4012\n",
      "Epoch [4/5], Step [8560/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [8562/10336], Loss: 0.0433\n",
      "Epoch [4/5], Step [8564/10336], Loss: 0.3146\n",
      "Epoch [4/5], Step [8566/10336], Loss: 2.0788\n",
      "Epoch [4/5], Step [8568/10336], Loss: 0.5890\n",
      "Epoch [4/5], Step [8570/10336], Loss: 2.2792\n",
      "Epoch [4/5], Step [8572/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [8574/10336], Loss: 2.8306\n",
      "Epoch [4/5], Step [8576/10336], Loss: 0.0400\n",
      "Epoch [4/5], Step [8578/10336], Loss: 0.2465\n",
      "Epoch [4/5], Step [8580/10336], Loss: 0.9287\n",
      "Epoch [4/5], Step [8582/10336], Loss: 0.4077\n",
      "Epoch [4/5], Step [8584/10336], Loss: 1.2937\n",
      "Epoch [4/5], Step [8586/10336], Loss: 0.0225\n",
      "Epoch [4/5], Step [8588/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [8590/10336], Loss: 0.1991\n",
      "Epoch [4/5], Step [8592/10336], Loss: 0.6987\n",
      "Epoch [4/5], Step [8594/10336], Loss: 0.0859\n",
      "Epoch [4/5], Step [8596/10336], Loss: 2.4819\n",
      "Epoch [4/5], Step [8598/10336], Loss: 0.0914\n",
      "Epoch [4/5], Step [8600/10336], Loss: 4.0716\n",
      "Epoch [4/5], Step [8602/10336], Loss: 0.0949\n",
      "Epoch [4/5], Step [8604/10336], Loss: 0.1549\n",
      "Epoch [4/5], Step [8606/10336], Loss: 1.4380\n",
      "Epoch [4/5], Step [8608/10336], Loss: 0.8622\n",
      "Epoch [4/5], Step [8610/10336], Loss: 0.7845\n",
      "Epoch [4/5], Step [8612/10336], Loss: 0.0199\n",
      "Epoch [4/5], Step [8614/10336], Loss: 0.4176\n",
      "Epoch [4/5], Step [8616/10336], Loss: 1.7887\n",
      "Epoch [4/5], Step [8618/10336], Loss: 0.0324\n",
      "Epoch [4/5], Step [8620/10336], Loss: 0.0629\n",
      "Epoch [4/5], Step [8622/10336], Loss: 0.0014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [8624/10336], Loss: 0.4917\n",
      "Epoch [4/5], Step [8626/10336], Loss: 0.2484\n",
      "Epoch [4/5], Step [8628/10336], Loss: 0.5648\n",
      "Epoch [4/5], Step [8630/10336], Loss: 0.0518\n",
      "Epoch [4/5], Step [8632/10336], Loss: 0.1338\n",
      "Epoch [4/5], Step [8634/10336], Loss: 0.0699\n",
      "Epoch [4/5], Step [8636/10336], Loss: 4.4266\n",
      "Epoch [4/5], Step [8638/10336], Loss: 0.0460\n",
      "Epoch [4/5], Step [8640/10336], Loss: 0.9346\n",
      "Epoch [4/5], Step [8642/10336], Loss: 0.0353\n",
      "Epoch [4/5], Step [8644/10336], Loss: 0.3560\n",
      "Epoch [4/5], Step [8646/10336], Loss: 2.7752\n",
      "Epoch [4/5], Step [8648/10336], Loss: 0.0351\n",
      "Epoch [4/5], Step [8650/10336], Loss: 1.1481\n",
      "Epoch [4/5], Step [8652/10336], Loss: 0.1916\n",
      "Epoch [4/5], Step [8654/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [8656/10336], Loss: 0.1049\n",
      "Epoch [4/5], Step [8658/10336], Loss: 0.1954\n",
      "Epoch [4/5], Step [8660/10336], Loss: 1.9254\n",
      "Epoch [4/5], Step [8662/10336], Loss: 0.0159\n",
      "Epoch [4/5], Step [8664/10336], Loss: 0.0085\n",
      "Epoch [4/5], Step [8666/10336], Loss: 0.0454\n",
      "Epoch [4/5], Step [8668/10336], Loss: 0.0290\n",
      "Epoch [4/5], Step [8670/10336], Loss: 0.2206\n",
      "Epoch [4/5], Step [8672/10336], Loss: 0.0030\n",
      "Epoch [4/5], Step [8674/10336], Loss: 0.0249\n",
      "Epoch [4/5], Step [8676/10336], Loss: 0.2977\n",
      "Epoch [4/5], Step [8678/10336], Loss: 0.6497\n",
      "Epoch [4/5], Step [8680/10336], Loss: 0.9772\n",
      "Epoch [4/5], Step [8682/10336], Loss: 0.0454\n",
      "Epoch [4/5], Step [8684/10336], Loss: 0.1878\n",
      "Epoch [4/5], Step [8686/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [8688/10336], Loss: 0.0811\n",
      "Epoch [4/5], Step [8690/10336], Loss: 0.3003\n",
      "Epoch [4/5], Step [8692/10336], Loss: 0.4933\n",
      "Epoch [4/5], Step [8694/10336], Loss: 0.1594\n",
      "Epoch [4/5], Step [8696/10336], Loss: 0.0988\n",
      "Epoch [4/5], Step [8698/10336], Loss: 0.5107\n",
      "Epoch [4/5], Step [8700/10336], Loss: 3.8866\n",
      "Epoch [4/5], Step [8702/10336], Loss: 0.4558\n",
      "Epoch [4/5], Step [8704/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [8706/10336], Loss: 0.0357\n",
      "Epoch [4/5], Step [8708/10336], Loss: 0.0074\n",
      "Epoch [4/5], Step [8710/10336], Loss: 0.3518\n",
      "Epoch [4/5], Step [8712/10336], Loss: 0.6540\n",
      "Epoch [4/5], Step [8714/10336], Loss: 0.0150\n",
      "Epoch [4/5], Step [8716/10336], Loss: 0.0380\n",
      "Epoch [4/5], Step [8718/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [8720/10336], Loss: 4.6512\n",
      "Epoch [4/5], Step [8722/10336], Loss: 2.9778\n",
      "Epoch [4/5], Step [8724/10336], Loss: 0.2935\n",
      "Epoch [4/5], Step [8726/10336], Loss: 1.1881\n",
      "Epoch [4/5], Step [8728/10336], Loss: 2.0238\n",
      "Epoch [4/5], Step [8730/10336], Loss: 0.7300\n",
      "Epoch [4/5], Step [8732/10336], Loss: 0.8072\n",
      "Epoch [4/5], Step [8734/10336], Loss: 0.1390\n",
      "Epoch [4/5], Step [8736/10336], Loss: 0.0800\n",
      "Epoch [4/5], Step [8738/10336], Loss: 0.7640\n",
      "Epoch [4/5], Step [8740/10336], Loss: 0.0032\n",
      "Epoch [4/5], Step [8742/10336], Loss: 0.8290\n",
      "Epoch [4/5], Step [8744/10336], Loss: 0.2256\n",
      "Epoch [4/5], Step [8746/10336], Loss: 0.0006\n",
      "Epoch [4/5], Step [8748/10336], Loss: 0.3771\n",
      "Epoch [4/5], Step [8750/10336], Loss: 0.0629\n",
      "Epoch [4/5], Step [8752/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [8754/10336], Loss: 0.0255\n",
      "Epoch [4/5], Step [8756/10336], Loss: 0.3863\n",
      "Epoch [4/5], Step [8758/10336], Loss: 0.7720\n",
      "Epoch [4/5], Step [8760/10336], Loss: 1.6924\n",
      "Epoch [4/5], Step [8762/10336], Loss: 1.3452\n",
      "Epoch [4/5], Step [8764/10336], Loss: 0.1684\n",
      "Epoch [4/5], Step [8766/10336], Loss: 0.0030\n",
      "Epoch [4/5], Step [8768/10336], Loss: 0.2158\n",
      "Epoch [4/5], Step [8770/10336], Loss: 0.3758\n",
      "Epoch [4/5], Step [8772/10336], Loss: 0.1985\n",
      "Epoch [4/5], Step [8774/10336], Loss: 0.0335\n",
      "Epoch [4/5], Step [8776/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [8778/10336], Loss: 0.2845\n",
      "Epoch [4/5], Step [8780/10336], Loss: 0.5389\n",
      "Epoch [4/5], Step [8782/10336], Loss: 0.8941\n",
      "Epoch [4/5], Step [8784/10336], Loss: 0.0260\n",
      "Epoch [4/5], Step [8786/10336], Loss: 0.1110\n",
      "Epoch [4/5], Step [8788/10336], Loss: 0.0019\n",
      "Epoch [4/5], Step [8790/10336], Loss: 0.7781\n",
      "Epoch [4/5], Step [8792/10336], Loss: 0.4103\n",
      "Epoch [4/5], Step [8794/10336], Loss: 0.1652\n",
      "Epoch [4/5], Step [8796/10336], Loss: 0.0096\n",
      "Epoch [4/5], Step [8798/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [8800/10336], Loss: 0.0283\n",
      "Epoch [4/5], Step [8802/10336], Loss: 0.0530\n",
      "Epoch [4/5], Step [8804/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [8806/10336], Loss: 1.0146\n",
      "Epoch [4/5], Step [8808/10336], Loss: 0.0060\n",
      "Epoch [4/5], Step [8810/10336], Loss: 0.2411\n",
      "Epoch [4/5], Step [8812/10336], Loss: 0.3051\n",
      "Epoch [4/5], Step [8814/10336], Loss: 2.0810\n",
      "Epoch [4/5], Step [8816/10336], Loss: 0.3776\n",
      "Epoch [4/5], Step [8818/10336], Loss: 1.6104\n",
      "Epoch [4/5], Step [8820/10336], Loss: 0.2585\n",
      "Epoch [4/5], Step [8822/10336], Loss: 0.1281\n",
      "Epoch [4/5], Step [8824/10336], Loss: 0.0112\n",
      "Epoch [4/5], Step [8826/10336], Loss: 3.3308\n",
      "Epoch [4/5], Step [8828/10336], Loss: 0.1347\n",
      "Epoch [4/5], Step [8830/10336], Loss: 0.0089\n",
      "Epoch [4/5], Step [8832/10336], Loss: 1.0590\n",
      "Epoch [4/5], Step [8834/10336], Loss: 0.2005\n",
      "Epoch [4/5], Step [8836/10336], Loss: 0.0240\n",
      "Epoch [4/5], Step [8838/10336], Loss: 3.1108\n",
      "Epoch [4/5], Step [8840/10336], Loss: 0.1183\n",
      "Epoch [4/5], Step [8842/10336], Loss: 0.5557\n",
      "Epoch [4/5], Step [8844/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [8846/10336], Loss: 0.1191\n",
      "Epoch [4/5], Step [8848/10336], Loss: 2.5311\n",
      "Epoch [4/5], Step [8850/10336], Loss: 1.7435\n",
      "Epoch [4/5], Step [8852/10336], Loss: 0.1901\n",
      "Epoch [4/5], Step [8854/10336], Loss: 0.1358\n",
      "Epoch [4/5], Step [8856/10336], Loss: 0.8415\n",
      "Epoch [4/5], Step [8858/10336], Loss: 0.0164\n",
      "Epoch [4/5], Step [8860/10336], Loss: 0.0615\n",
      "Epoch [4/5], Step [8862/10336], Loss: 0.0229\n",
      "Epoch [4/5], Step [8864/10336], Loss: 0.1250\n",
      "Epoch [4/5], Step [8866/10336], Loss: 0.4353\n",
      "Epoch [4/5], Step [8868/10336], Loss: 0.5260\n",
      "Epoch [4/5], Step [8870/10336], Loss: 0.0415\n",
      "Epoch [4/5], Step [8872/10336], Loss: 0.6434\n",
      "Epoch [4/5], Step [8874/10336], Loss: 0.0427\n",
      "Epoch [4/5], Step [8876/10336], Loss: 0.3696\n",
      "Epoch [4/5], Step [8878/10336], Loss: 0.4964\n",
      "Epoch [4/5], Step [8880/10336], Loss: 0.0050\n",
      "Epoch [4/5], Step [8882/10336], Loss: 0.0887\n",
      "Epoch [4/5], Step [8884/10336], Loss: 0.4704\n",
      "Epoch [4/5], Step [8886/10336], Loss: 0.4191\n",
      "Epoch [4/5], Step [8888/10336], Loss: 0.0425\n",
      "Epoch [4/5], Step [8890/10336], Loss: 0.0192\n",
      "Epoch [4/5], Step [8892/10336], Loss: 0.2255\n",
      "Epoch [4/5], Step [8894/10336], Loss: 0.0908\n",
      "Epoch [4/5], Step [8896/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [8898/10336], Loss: 0.0794\n",
      "Epoch [4/5], Step [8900/10336], Loss: 0.0064\n",
      "Epoch [4/5], Step [8902/10336], Loss: 0.4440\n",
      "Epoch [4/5], Step [8904/10336], Loss: 0.7805\n",
      "Epoch [4/5], Step [8906/10336], Loss: 0.0184\n",
      "Epoch [4/5], Step [8908/10336], Loss: 0.0200\n",
      "Epoch [4/5], Step [8910/10336], Loss: 1.7457\n",
      "Epoch [4/5], Step [8912/10336], Loss: 0.4649\n",
      "Epoch [4/5], Step [8914/10336], Loss: 0.1055\n",
      "Epoch [4/5], Step [8916/10336], Loss: 1.9630\n",
      "Epoch [4/5], Step [8918/10336], Loss: 0.9610\n",
      "Epoch [4/5], Step [8920/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [8922/10336], Loss: 3.4606\n",
      "Epoch [4/5], Step [8924/10336], Loss: 1.0230\n",
      "Epoch [4/5], Step [8926/10336], Loss: 0.0030\n",
      "Epoch [4/5], Step [8928/10336], Loss: 0.7448\n",
      "Epoch [4/5], Step [8930/10336], Loss: 0.1815\n",
      "Epoch [4/5], Step [8932/10336], Loss: 0.0522\n",
      "Epoch [4/5], Step [8934/10336], Loss: 1.1352\n",
      "Epoch [4/5], Step [8936/10336], Loss: 0.2857\n",
      "Epoch [4/5], Step [8938/10336], Loss: 2.3056\n",
      "Epoch [4/5], Step [8940/10336], Loss: 2.0736\n",
      "Epoch [4/5], Step [8942/10336], Loss: 0.9804\n",
      "Epoch [4/5], Step [8944/10336], Loss: 0.1606\n",
      "Epoch [4/5], Step [8946/10336], Loss: 0.9956\n",
      "Epoch [4/5], Step [8948/10336], Loss: 0.0866\n",
      "Epoch [4/5], Step [8950/10336], Loss: 1.0504\n",
      "Epoch [4/5], Step [8952/10336], Loss: 0.6400\n",
      "Epoch [4/5], Step [8954/10336], Loss: 0.0683\n",
      "Epoch [4/5], Step [8956/10336], Loss: 0.0940\n",
      "Epoch [4/5], Step [8958/10336], Loss: 0.2238\n",
      "Epoch [4/5], Step [8960/10336], Loss: 0.1622\n",
      "Epoch [4/5], Step [8962/10336], Loss: 0.9374\n",
      "Epoch [4/5], Step [8964/10336], Loss: 0.1134\n",
      "Epoch [4/5], Step [8966/10336], Loss: 0.7206\n",
      "Epoch [4/5], Step [8968/10336], Loss: 0.2701\n",
      "Epoch [4/5], Step [8970/10336], Loss: 0.1152\n",
      "Epoch [4/5], Step [8972/10336], Loss: 0.0571\n",
      "Epoch [4/5], Step [8974/10336], Loss: 0.6656\n",
      "Epoch [4/5], Step [8976/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [8978/10336], Loss: 0.0115\n",
      "Epoch [4/5], Step [8980/10336], Loss: 0.0280\n",
      "Epoch [4/5], Step [8982/10336], Loss: 2.9317\n",
      "Epoch [4/5], Step [8984/10336], Loss: 1.0582\n",
      "Epoch [4/5], Step [8986/10336], Loss: 0.0022\n",
      "Epoch [4/5], Step [8988/10336], Loss: 0.2194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [8990/10336], Loss: 0.0054\n",
      "Epoch [4/5], Step [8992/10336], Loss: 0.0730\n",
      "Epoch [4/5], Step [8994/10336], Loss: 0.3173\n",
      "Epoch [4/5], Step [8996/10336], Loss: 0.0502\n",
      "Epoch [4/5], Step [8998/10336], Loss: 0.8372\n",
      "Epoch [4/5], Step [9000/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [9002/10336], Loss: 0.0415\n",
      "Epoch [4/5], Step [9004/10336], Loss: 0.8006\n",
      "Epoch [4/5], Step [9006/10336], Loss: 0.0163\n",
      "Epoch [4/5], Step [9008/10336], Loss: 0.0344\n",
      "Epoch [4/5], Step [9010/10336], Loss: 0.0106\n",
      "Epoch [4/5], Step [9012/10336], Loss: 1.3356\n",
      "Epoch [4/5], Step [9014/10336], Loss: 0.0061\n",
      "Epoch [4/5], Step [9016/10336], Loss: 0.0223\n",
      "Epoch [4/5], Step [9018/10336], Loss: 0.7687\n",
      "Epoch [4/5], Step [9020/10336], Loss: 0.0967\n",
      "Epoch [4/5], Step [9022/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [9024/10336], Loss: 0.1982\n",
      "Epoch [4/5], Step [9026/10336], Loss: 0.1581\n",
      "Epoch [4/5], Step [9028/10336], Loss: 0.5143\n",
      "Epoch [4/5], Step [9030/10336], Loss: 0.4842\n",
      "Epoch [4/5], Step [9032/10336], Loss: 1.1058\n",
      "Epoch [4/5], Step [9034/10336], Loss: 0.4609\n",
      "Epoch [4/5], Step [9036/10336], Loss: 0.0257\n",
      "Epoch [4/5], Step [9038/10336], Loss: 0.7278\n",
      "Epoch [4/5], Step [9040/10336], Loss: 0.0077\n",
      "Epoch [4/5], Step [9042/10336], Loss: 0.8959\n",
      "Epoch [4/5], Step [9044/10336], Loss: 0.2393\n",
      "Epoch [4/5], Step [9046/10336], Loss: 0.1966\n",
      "Epoch [4/5], Step [9048/10336], Loss: 0.6701\n",
      "Epoch [4/5], Step [9050/10336], Loss: 0.0395\n",
      "Epoch [4/5], Step [9052/10336], Loss: 0.1021\n",
      "Epoch [4/5], Step [9054/10336], Loss: 0.0115\n",
      "Epoch [4/5], Step [9056/10336], Loss: 2.5279\n",
      "Epoch [4/5], Step [9058/10336], Loss: 0.9070\n",
      "Epoch [4/5], Step [9060/10336], Loss: 0.0467\n",
      "Epoch [4/5], Step [9062/10336], Loss: 0.8433\n",
      "Epoch [4/5], Step [9064/10336], Loss: 0.0217\n",
      "Epoch [4/5], Step [9066/10336], Loss: 0.1105\n",
      "Epoch [4/5], Step [9068/10336], Loss: 0.4914\n",
      "Epoch [4/5], Step [9070/10336], Loss: 0.3404\n",
      "Epoch [4/5], Step [9072/10336], Loss: 0.0689\n",
      "Epoch [4/5], Step [9074/10336], Loss: 0.9170\n",
      "Epoch [4/5], Step [9076/10336], Loss: 0.0915\n",
      "Epoch [4/5], Step [9078/10336], Loss: 0.1432\n",
      "Epoch [4/5], Step [9080/10336], Loss: 0.0492\n",
      "Epoch [4/5], Step [9082/10336], Loss: 0.0167\n",
      "Epoch [4/5], Step [9084/10336], Loss: 0.2658\n",
      "Epoch [4/5], Step [9086/10336], Loss: 0.0768\n",
      "Epoch [4/5], Step [9088/10336], Loss: 0.3262\n",
      "Epoch [4/5], Step [9090/10336], Loss: 0.2076\n",
      "Epoch [4/5], Step [9092/10336], Loss: 0.0385\n",
      "Epoch [4/5], Step [9094/10336], Loss: 1.0684\n",
      "Epoch [4/5], Step [9096/10336], Loss: 0.1557\n",
      "Epoch [4/5], Step [9098/10336], Loss: 1.9294\n",
      "Epoch [4/5], Step [9100/10336], Loss: 1.3805\n",
      "Epoch [4/5], Step [9102/10336], Loss: 0.7728\n",
      "Epoch [4/5], Step [9104/10336], Loss: 0.5780\n",
      "Epoch [4/5], Step [9106/10336], Loss: 0.0578\n",
      "Epoch [4/5], Step [9108/10336], Loss: 1.3886\n",
      "Epoch [4/5], Step [9110/10336], Loss: 0.4561\n",
      "Epoch [4/5], Step [9112/10336], Loss: 0.0146\n",
      "Epoch [4/5], Step [9114/10336], Loss: 0.5588\n",
      "Epoch [4/5], Step [9116/10336], Loss: 1.4983\n",
      "Epoch [4/5], Step [9118/10336], Loss: 0.7954\n",
      "Epoch [4/5], Step [9120/10336], Loss: 1.7374\n",
      "Epoch [4/5], Step [9122/10336], Loss: 0.7698\n",
      "Epoch [4/5], Step [9124/10336], Loss: 1.0405\n",
      "Epoch [4/5], Step [9126/10336], Loss: 0.0469\n",
      "Epoch [4/5], Step [9128/10336], Loss: 0.1386\n",
      "Epoch [4/5], Step [9130/10336], Loss: 0.1832\n",
      "Epoch [4/5], Step [9132/10336], Loss: 0.6233\n",
      "Epoch [4/5], Step [9134/10336], Loss: 0.8564\n",
      "Epoch [4/5], Step [9136/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [9138/10336], Loss: 0.4483\n",
      "Epoch [4/5], Step [9140/10336], Loss: 0.0908\n",
      "Epoch [4/5], Step [9142/10336], Loss: 0.0096\n",
      "Epoch [4/5], Step [9144/10336], Loss: 0.0015\n",
      "Epoch [4/5], Step [9146/10336], Loss: 0.0666\n",
      "Epoch [4/5], Step [9148/10336], Loss: 1.4392\n",
      "Epoch [4/5], Step [9150/10336], Loss: 0.2228\n",
      "Epoch [4/5], Step [9152/10336], Loss: 0.2781\n",
      "Epoch [4/5], Step [9154/10336], Loss: 0.0021\n",
      "Epoch [4/5], Step [9156/10336], Loss: 2.4776\n",
      "Epoch [4/5], Step [9158/10336], Loss: 1.1456\n",
      "Epoch [4/5], Step [9160/10336], Loss: 0.0981\n",
      "Epoch [4/5], Step [9162/10336], Loss: 2.7712\n",
      "Epoch [4/5], Step [9164/10336], Loss: 0.1215\n",
      "Epoch [4/5], Step [9166/10336], Loss: 1.1565\n",
      "Epoch [4/5], Step [9168/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [9170/10336], Loss: 1.7658\n",
      "Epoch [4/5], Step [9172/10336], Loss: 0.0086\n",
      "Epoch [4/5], Step [9174/10336], Loss: 1.3400\n",
      "Epoch [4/5], Step [9176/10336], Loss: 1.0297\n",
      "Epoch [4/5], Step [9178/10336], Loss: 0.9530\n",
      "Epoch [4/5], Step [9180/10336], Loss: 2.0629\n",
      "Epoch [4/5], Step [9182/10336], Loss: 0.0221\n",
      "Epoch [4/5], Step [9184/10336], Loss: 0.8477\n",
      "Epoch [4/5], Step [9186/10336], Loss: 0.0242\n",
      "Epoch [4/5], Step [9188/10336], Loss: 0.8616\n",
      "Epoch [4/5], Step [9190/10336], Loss: 0.0575\n",
      "Epoch [4/5], Step [9192/10336], Loss: 0.0131\n",
      "Epoch [4/5], Step [9194/10336], Loss: 0.8546\n",
      "Epoch [4/5], Step [9196/10336], Loss: 0.4666\n",
      "Epoch [4/5], Step [9198/10336], Loss: 0.0144\n",
      "Epoch [4/5], Step [9200/10336], Loss: 0.0131\n",
      "Epoch [4/5], Step [9202/10336], Loss: 0.0246\n",
      "Epoch [4/5], Step [9204/10336], Loss: 0.5737\n",
      "Epoch [4/5], Step [9206/10336], Loss: 0.0374\n",
      "Epoch [4/5], Step [9208/10336], Loss: 0.6173\n",
      "Epoch [4/5], Step [9210/10336], Loss: 0.3403\n",
      "Epoch [4/5], Step [9212/10336], Loss: 0.0432\n",
      "Epoch [4/5], Step [9214/10336], Loss: 0.1064\n",
      "Epoch [4/5], Step [9216/10336], Loss: 0.0284\n",
      "Epoch [4/5], Step [9218/10336], Loss: 0.2603\n",
      "Epoch [4/5], Step [9220/10336], Loss: 1.0714\n",
      "Epoch [4/5], Step [9222/10336], Loss: 0.8320\n",
      "Epoch [4/5], Step [9224/10336], Loss: 1.1316\n",
      "Epoch [4/5], Step [9226/10336], Loss: 0.6196\n",
      "Epoch [4/5], Step [9228/10336], Loss: 1.2271\n",
      "Epoch [4/5], Step [9230/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [9232/10336], Loss: 4.4969\n",
      "Epoch [4/5], Step [9234/10336], Loss: 0.0428\n",
      "Epoch [4/5], Step [9236/10336], Loss: 0.0810\n",
      "Epoch [4/5], Step [9238/10336], Loss: 0.5460\n",
      "Epoch [4/5], Step [9240/10336], Loss: 0.7004\n",
      "Epoch [4/5], Step [9242/10336], Loss: 4.5284\n",
      "Epoch [4/5], Step [9244/10336], Loss: 0.0174\n",
      "Epoch [4/5], Step [9246/10336], Loss: 0.0527\n",
      "Epoch [4/5], Step [9248/10336], Loss: 0.4687\n",
      "Epoch [4/5], Step [9250/10336], Loss: 0.2345\n",
      "Epoch [4/5], Step [9252/10336], Loss: 0.9036\n",
      "Epoch [4/5], Step [9254/10336], Loss: 0.3226\n",
      "Epoch [4/5], Step [9256/10336], Loss: 0.3090\n",
      "Epoch [4/5], Step [9258/10336], Loss: 0.1207\n",
      "Epoch [4/5], Step [9260/10336], Loss: 0.0541\n",
      "Epoch [4/5], Step [9262/10336], Loss: 1.4227\n",
      "Epoch [4/5], Step [9264/10336], Loss: 0.6201\n",
      "Epoch [4/5], Step [9266/10336], Loss: 0.0472\n",
      "Epoch [4/5], Step [9268/10336], Loss: 0.0182\n",
      "Epoch [4/5], Step [9270/10336], Loss: 0.1550\n",
      "Epoch [4/5], Step [9272/10336], Loss: 2.4816\n",
      "Epoch [4/5], Step [9274/10336], Loss: 0.0818\n",
      "Epoch [4/5], Step [9276/10336], Loss: 0.5634\n",
      "Epoch [4/5], Step [9278/10336], Loss: 0.0820\n",
      "Epoch [4/5], Step [9280/10336], Loss: 0.0257\n",
      "Epoch [4/5], Step [9282/10336], Loss: 1.6498\n",
      "Epoch [4/5], Step [9284/10336], Loss: 0.0591\n",
      "Epoch [4/5], Step [9286/10336], Loss: 0.0257\n",
      "Epoch [4/5], Step [9288/10336], Loss: 0.0020\n",
      "Epoch [4/5], Step [9290/10336], Loss: 0.9654\n",
      "Epoch [4/5], Step [9292/10336], Loss: 0.4959\n",
      "Epoch [4/5], Step [9294/10336], Loss: 0.7867\n",
      "Epoch [4/5], Step [9296/10336], Loss: 0.1204\n",
      "Epoch [4/5], Step [9298/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [9300/10336], Loss: 0.1975\n",
      "Epoch [4/5], Step [9302/10336], Loss: 0.1830\n",
      "Epoch [4/5], Step [9304/10336], Loss: 2.4289\n",
      "Epoch [4/5], Step [9306/10336], Loss: 0.5622\n",
      "Epoch [4/5], Step [9308/10336], Loss: 0.2176\n",
      "Epoch [4/5], Step [9310/10336], Loss: 0.0139\n",
      "Epoch [4/5], Step [9312/10336], Loss: 2.9722\n",
      "Epoch [4/5], Step [9314/10336], Loss: 0.0099\n",
      "Epoch [4/5], Step [9316/10336], Loss: 1.4487\n",
      "Epoch [4/5], Step [9318/10336], Loss: 0.3318\n",
      "Epoch [4/5], Step [9320/10336], Loss: 0.0946\n",
      "Epoch [4/5], Step [9322/10336], Loss: 0.3624\n",
      "Epoch [4/5], Step [9324/10336], Loss: 2.7595\n",
      "Epoch [4/5], Step [9326/10336], Loss: 0.0003\n",
      "Epoch [4/5], Step [9328/10336], Loss: 0.3682\n",
      "Epoch [4/5], Step [9330/10336], Loss: 2.1557\n",
      "Epoch [4/5], Step [9332/10336], Loss: 1.2681\n",
      "Epoch [4/5], Step [9334/10336], Loss: 0.3053\n",
      "Epoch [4/5], Step [9336/10336], Loss: 0.0198\n",
      "Epoch [4/5], Step [9338/10336], Loss: 0.1827\n",
      "Epoch [4/5], Step [9340/10336], Loss: 0.4766\n",
      "Epoch [4/5], Step [9342/10336], Loss: 0.1195\n",
      "Epoch [4/5], Step [9344/10336], Loss: 0.0902\n",
      "Epoch [4/5], Step [9346/10336], Loss: 5.4233\n",
      "Epoch [4/5], Step [9348/10336], Loss: 1.5132\n",
      "Epoch [4/5], Step [9350/10336], Loss: 2.6084\n",
      "Epoch [4/5], Step [9352/10336], Loss: 0.2883\n",
      "Epoch [4/5], Step [9354/10336], Loss: 0.0906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [9356/10336], Loss: 0.0316\n",
      "Epoch [4/5], Step [9358/10336], Loss: 0.0332\n",
      "Epoch [4/5], Step [9360/10336], Loss: 0.0318\n",
      "Epoch [4/5], Step [9362/10336], Loss: 0.0252\n",
      "Epoch [4/5], Step [9364/10336], Loss: 0.3445\n",
      "Epoch [4/5], Step [9366/10336], Loss: 1.1055\n",
      "Epoch [4/5], Step [9368/10336], Loss: 0.0094\n",
      "Epoch [4/5], Step [9370/10336], Loss: 0.7258\n",
      "Epoch [4/5], Step [9372/10336], Loss: 0.5378\n",
      "Epoch [4/5], Step [9374/10336], Loss: 0.4289\n",
      "Epoch [4/5], Step [9376/10336], Loss: 3.6065\n",
      "Epoch [4/5], Step [9378/10336], Loss: 1.9964\n",
      "Epoch [4/5], Step [9380/10336], Loss: 0.2749\n",
      "Epoch [4/5], Step [9382/10336], Loss: 0.1885\n",
      "Epoch [4/5], Step [9384/10336], Loss: 0.1543\n",
      "Epoch [4/5], Step [9386/10336], Loss: 0.2256\n",
      "Epoch [4/5], Step [9388/10336], Loss: 0.8666\n",
      "Epoch [4/5], Step [9390/10336], Loss: 0.0333\n",
      "Epoch [4/5], Step [9392/10336], Loss: 0.6419\n",
      "Epoch [4/5], Step [9394/10336], Loss: 0.0147\n",
      "Epoch [4/5], Step [9396/10336], Loss: 1.5797\n",
      "Epoch [4/5], Step [9398/10336], Loss: 0.6518\n",
      "Epoch [4/5], Step [9400/10336], Loss: 0.8156\n",
      "Epoch [4/5], Step [9402/10336], Loss: 0.0069\n",
      "Epoch [4/5], Step [9404/10336], Loss: 1.8858\n",
      "Epoch [4/5], Step [9406/10336], Loss: 1.5289\n",
      "Epoch [4/5], Step [9408/10336], Loss: 0.8774\n",
      "Epoch [4/5], Step [9410/10336], Loss: 0.6307\n",
      "Epoch [4/5], Step [9412/10336], Loss: 0.0416\n",
      "Epoch [4/5], Step [9414/10336], Loss: 2.3656\n",
      "Epoch [4/5], Step [9416/10336], Loss: 0.2281\n",
      "Epoch [4/5], Step [9418/10336], Loss: 0.0049\n",
      "Epoch [4/5], Step [9420/10336], Loss: 0.0338\n",
      "Epoch [4/5], Step [9422/10336], Loss: 0.4383\n",
      "Epoch [4/5], Step [9424/10336], Loss: 1.6804\n",
      "Epoch [4/5], Step [9426/10336], Loss: 2.7085\n",
      "Epoch [4/5], Step [9428/10336], Loss: 0.1910\n",
      "Epoch [4/5], Step [9430/10336], Loss: 0.0059\n",
      "Epoch [4/5], Step [9432/10336], Loss: 1.3772\n",
      "Epoch [4/5], Step [9434/10336], Loss: 0.1955\n",
      "Epoch [4/5], Step [9436/10336], Loss: 0.2214\n",
      "Epoch [4/5], Step [9438/10336], Loss: 0.4648\n",
      "Epoch [4/5], Step [9440/10336], Loss: 0.7130\n",
      "Epoch [4/5], Step [9442/10336], Loss: 2.7670\n",
      "Epoch [4/5], Step [9444/10336], Loss: 0.1762\n",
      "Epoch [4/5], Step [9446/10336], Loss: 1.7850\n",
      "Epoch [4/5], Step [9448/10336], Loss: 0.0255\n",
      "Epoch [4/5], Step [9450/10336], Loss: 0.1332\n",
      "Epoch [4/5], Step [9452/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [9454/10336], Loss: 1.0352\n",
      "Epoch [4/5], Step [9456/10336], Loss: 0.1162\n",
      "Epoch [4/5], Step [9458/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [9460/10336], Loss: 0.0073\n",
      "Epoch [4/5], Step [9462/10336], Loss: 0.0412\n",
      "Epoch [4/5], Step [9464/10336], Loss: 0.9599\n",
      "Epoch [4/5], Step [9466/10336], Loss: 0.7039\n",
      "Epoch [4/5], Step [9468/10336], Loss: 0.4352\n",
      "Epoch [4/5], Step [9470/10336], Loss: 0.2092\n",
      "Epoch [4/5], Step [9472/10336], Loss: 0.0014\n",
      "Epoch [4/5], Step [9474/10336], Loss: 0.1351\n",
      "Epoch [4/5], Step [9476/10336], Loss: 0.1262\n",
      "Epoch [4/5], Step [9478/10336], Loss: 0.0618\n",
      "Epoch [4/5], Step [9480/10336], Loss: 0.0023\n",
      "Epoch [4/5], Step [9482/10336], Loss: 0.0040\n",
      "Epoch [4/5], Step [9484/10336], Loss: 0.0072\n",
      "Epoch [4/5], Step [9486/10336], Loss: 0.2776\n",
      "Epoch [4/5], Step [9488/10336], Loss: 0.0797\n",
      "Epoch [4/5], Step [9490/10336], Loss: 0.3705\n",
      "Epoch [4/5], Step [9492/10336], Loss: 0.0818\n",
      "Epoch [4/5], Step [9494/10336], Loss: 0.1069\n",
      "Epoch [4/5], Step [9496/10336], Loss: 0.0080\n",
      "Epoch [4/5], Step [9498/10336], Loss: 0.2871\n",
      "Epoch [4/5], Step [9500/10336], Loss: 2.1221\n",
      "Epoch [4/5], Step [9502/10336], Loss: 0.9172\n",
      "Epoch [4/5], Step [9504/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [9506/10336], Loss: 1.6946\n",
      "Epoch [4/5], Step [9508/10336], Loss: 0.1097\n",
      "Epoch [4/5], Step [9510/10336], Loss: 0.0508\n",
      "Epoch [4/5], Step [9512/10336], Loss: 0.2716\n",
      "Epoch [4/5], Step [9514/10336], Loss: 1.6059\n",
      "Epoch [4/5], Step [9516/10336], Loss: 1.5311\n",
      "Epoch [4/5], Step [9518/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [9520/10336], Loss: 0.0066\n",
      "Epoch [4/5], Step [9522/10336], Loss: 3.2267\n",
      "Epoch [4/5], Step [9524/10336], Loss: 0.0839\n",
      "Epoch [4/5], Step [9526/10336], Loss: 0.1267\n",
      "Epoch [4/5], Step [9528/10336], Loss: 0.6279\n",
      "Epoch [4/5], Step [9530/10336], Loss: 0.0293\n",
      "Epoch [4/5], Step [9532/10336], Loss: 0.0339\n",
      "Epoch [4/5], Step [9534/10336], Loss: 0.0407\n",
      "Epoch [4/5], Step [9536/10336], Loss: 1.5343\n",
      "Epoch [4/5], Step [9538/10336], Loss: 0.0009\n",
      "Epoch [4/5], Step [9540/10336], Loss: 0.2212\n",
      "Epoch [4/5], Step [9542/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [9544/10336], Loss: 0.0077\n",
      "Epoch [4/5], Step [9546/10336], Loss: 0.0081\n",
      "Epoch [4/5], Step [9548/10336], Loss: 0.0055\n",
      "Epoch [4/5], Step [9550/10336], Loss: 0.0285\n",
      "Epoch [4/5], Step [9552/10336], Loss: 2.9069\n",
      "Epoch [4/5], Step [9554/10336], Loss: 0.4259\n",
      "Epoch [4/5], Step [9556/10336], Loss: 0.1530\n",
      "Epoch [4/5], Step [9558/10336], Loss: 0.1005\n",
      "Epoch [4/5], Step [9560/10336], Loss: 0.6156\n",
      "Epoch [4/5], Step [9562/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [9564/10336], Loss: 0.3935\n",
      "Epoch [4/5], Step [9566/10336], Loss: 0.6434\n",
      "Epoch [4/5], Step [9568/10336], Loss: 1.1067\n",
      "Epoch [4/5], Step [9570/10336], Loss: 2.4086\n",
      "Epoch [4/5], Step [9572/10336], Loss: 5.0369\n",
      "Epoch [4/5], Step [9574/10336], Loss: 0.0221\n",
      "Epoch [4/5], Step [9576/10336], Loss: 1.4895\n",
      "Epoch [4/5], Step [9578/10336], Loss: 0.0397\n",
      "Epoch [4/5], Step [9580/10336], Loss: 0.1716\n",
      "Epoch [4/5], Step [9582/10336], Loss: 0.1456\n",
      "Epoch [4/5], Step [9584/10336], Loss: 0.1150\n",
      "Epoch [4/5], Step [9586/10336], Loss: 0.2134\n",
      "Epoch [4/5], Step [9588/10336], Loss: 1.3917\n",
      "Epoch [4/5], Step [9590/10336], Loss: 0.5064\n",
      "Epoch [4/5], Step [9592/10336], Loss: 0.0517\n",
      "Epoch [4/5], Step [9594/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [9596/10336], Loss: 0.1982\n",
      "Epoch [4/5], Step [9598/10336], Loss: 2.3068\n",
      "Epoch [4/5], Step [9600/10336], Loss: 0.1268\n",
      "Epoch [4/5], Step [9602/10336], Loss: 0.0152\n",
      "Epoch [4/5], Step [9604/10336], Loss: 0.0269\n",
      "Epoch [4/5], Step [9606/10336], Loss: 0.0082\n",
      "Epoch [4/5], Step [9608/10336], Loss: 0.3724\n",
      "Epoch [4/5], Step [9610/10336], Loss: 2.9352\n",
      "Epoch [4/5], Step [9612/10336], Loss: 0.0091\n",
      "Epoch [4/5], Step [9614/10336], Loss: 0.1600\n",
      "Epoch [4/5], Step [9616/10336], Loss: 0.0109\n",
      "Epoch [4/5], Step [9618/10336], Loss: 0.5925\n",
      "Epoch [4/5], Step [9620/10336], Loss: 0.0104\n",
      "Epoch [4/5], Step [9622/10336], Loss: 0.0172\n",
      "Epoch [4/5], Step [9624/10336], Loss: 0.0230\n",
      "Epoch [4/5], Step [9626/10336], Loss: 0.8715\n",
      "Epoch [4/5], Step [9628/10336], Loss: 0.0204\n",
      "Epoch [4/5], Step [9630/10336], Loss: 0.0129\n",
      "Epoch [4/5], Step [9632/10336], Loss: 0.1695\n",
      "Epoch [4/5], Step [9634/10336], Loss: 0.0835\n",
      "Epoch [4/5], Step [9636/10336], Loss: 1.0912\n",
      "Epoch [4/5], Step [9638/10336], Loss: 0.0017\n",
      "Epoch [4/5], Step [9640/10336], Loss: 0.2691\n",
      "Epoch [4/5], Step [9642/10336], Loss: 0.9577\n",
      "Epoch [4/5], Step [9644/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [9646/10336], Loss: 2.1491\n",
      "Epoch [4/5], Step [9648/10336], Loss: 0.0557\n",
      "Epoch [4/5], Step [9650/10336], Loss: 0.1052\n",
      "Epoch [4/5], Step [9652/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [9654/10336], Loss: 2.2894\n",
      "Epoch [4/5], Step [9656/10336], Loss: 2.6774\n",
      "Epoch [4/5], Step [9658/10336], Loss: 0.8967\n",
      "Epoch [4/5], Step [9660/10336], Loss: 0.0884\n",
      "Epoch [4/5], Step [9662/10336], Loss: 0.0061\n",
      "Epoch [4/5], Step [9664/10336], Loss: 0.7183\n",
      "Epoch [4/5], Step [9666/10336], Loss: 1.1394\n",
      "Epoch [4/5], Step [9668/10336], Loss: 2.2361\n",
      "Epoch [4/5], Step [9670/10336], Loss: 0.6002\n",
      "Epoch [4/5], Step [9672/10336], Loss: 0.0013\n",
      "Epoch [4/5], Step [9674/10336], Loss: 3.2769\n",
      "Epoch [4/5], Step [9676/10336], Loss: 0.0782\n",
      "Epoch [4/5], Step [9678/10336], Loss: 0.0346\n",
      "Epoch [4/5], Step [9680/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [9682/10336], Loss: 0.7085\n",
      "Epoch [4/5], Step [9684/10336], Loss: 0.6154\n",
      "Epoch [4/5], Step [9686/10336], Loss: 0.7586\n",
      "Epoch [4/5], Step [9688/10336], Loss: 3.5392\n",
      "Epoch [4/5], Step [9690/10336], Loss: 0.0643\n",
      "Epoch [4/5], Step [9692/10336], Loss: 0.2183\n",
      "Epoch [4/5], Step [9694/10336], Loss: 0.0914\n",
      "Epoch [4/5], Step [9696/10336], Loss: 0.1623\n",
      "Epoch [4/5], Step [9698/10336], Loss: 0.0130\n",
      "Epoch [4/5], Step [9700/10336], Loss: 0.6395\n",
      "Epoch [4/5], Step [9702/10336], Loss: 2.3725\n",
      "Epoch [4/5], Step [9704/10336], Loss: 0.5414\n",
      "Epoch [4/5], Step [9706/10336], Loss: 0.0757\n",
      "Epoch [4/5], Step [9708/10336], Loss: 0.6666\n",
      "Epoch [4/5], Step [9710/10336], Loss: 0.0043\n",
      "Epoch [4/5], Step [9712/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [9714/10336], Loss: 2.5916\n",
      "Epoch [4/5], Step [9716/10336], Loss: 0.0097\n",
      "Epoch [4/5], Step [9718/10336], Loss: 0.7613\n",
      "Epoch [4/5], Step [9720/10336], Loss: 0.0016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [9722/10336], Loss: 0.9818\n",
      "Epoch [4/5], Step [9724/10336], Loss: 0.0765\n",
      "Epoch [4/5], Step [9726/10336], Loss: 0.8379\n",
      "Epoch [4/5], Step [9728/10336], Loss: 0.1972\n",
      "Epoch [4/5], Step [9730/10336], Loss: 0.0202\n",
      "Epoch [4/5], Step [9732/10336], Loss: 2.2309\n",
      "Epoch [4/5], Step [9734/10336], Loss: 0.0186\n",
      "Epoch [4/5], Step [9736/10336], Loss: 0.0047\n",
      "Epoch [4/5], Step [9738/10336], Loss: 1.8365\n",
      "Epoch [4/5], Step [9740/10336], Loss: 0.1245\n",
      "Epoch [4/5], Step [9742/10336], Loss: 0.1727\n",
      "Epoch [4/5], Step [9744/10336], Loss: 1.4651\n",
      "Epoch [4/5], Step [9746/10336], Loss: 0.7430\n",
      "Epoch [4/5], Step [9748/10336], Loss: 0.2599\n",
      "Epoch [4/5], Step [9750/10336], Loss: 0.0124\n",
      "Epoch [4/5], Step [9752/10336], Loss: 0.5224\n",
      "Epoch [4/5], Step [9754/10336], Loss: 0.0008\n",
      "Epoch [4/5], Step [9756/10336], Loss: 0.2254\n",
      "Epoch [4/5], Step [9758/10336], Loss: 0.6289\n",
      "Epoch [4/5], Step [9760/10336], Loss: 0.0099\n",
      "Epoch [4/5], Step [9762/10336], Loss: 0.5263\n",
      "Epoch [4/5], Step [9764/10336], Loss: 0.0340\n",
      "Epoch [4/5], Step [9766/10336], Loss: 1.0380\n",
      "Epoch [4/5], Step [9768/10336], Loss: 0.6558\n",
      "Epoch [4/5], Step [9770/10336], Loss: 1.0125\n",
      "Epoch [4/5], Step [9772/10336], Loss: 0.1601\n",
      "Epoch [4/5], Step [9774/10336], Loss: 0.0121\n",
      "Epoch [4/5], Step [9776/10336], Loss: 1.9498\n",
      "Epoch [4/5], Step [9778/10336], Loss: 0.5468\n",
      "Epoch [4/5], Step [9780/10336], Loss: 0.9357\n",
      "Epoch [4/5], Step [9782/10336], Loss: 3.0337\n",
      "Epoch [4/5], Step [9784/10336], Loss: 0.2815\n",
      "Epoch [4/5], Step [9786/10336], Loss: 1.6674\n",
      "Epoch [4/5], Step [9788/10336], Loss: 2.1394\n",
      "Epoch [4/5], Step [9790/10336], Loss: 0.1723\n",
      "Epoch [4/5], Step [9792/10336], Loss: 0.1037\n",
      "Epoch [4/5], Step [9794/10336], Loss: 0.4729\n",
      "Epoch [4/5], Step [9796/10336], Loss: 0.0086\n",
      "Epoch [4/5], Step [9798/10336], Loss: 0.0117\n",
      "Epoch [4/5], Step [9800/10336], Loss: 0.0095\n",
      "Epoch [4/5], Step [9802/10336], Loss: 0.0203\n",
      "Epoch [4/5], Step [9804/10336], Loss: 1.4539\n",
      "Epoch [4/5], Step [9806/10336], Loss: 0.3732\n",
      "Epoch [4/5], Step [9808/10336], Loss: 0.1050\n",
      "Epoch [4/5], Step [9810/10336], Loss: 2.4625\n",
      "Epoch [4/5], Step [9812/10336], Loss: 0.0101\n",
      "Epoch [4/5], Step [9814/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [9816/10336], Loss: 0.0857\n",
      "Epoch [4/5], Step [9818/10336], Loss: 0.0532\n",
      "Epoch [4/5], Step [9820/10336], Loss: 0.5646\n",
      "Epoch [4/5], Step [9822/10336], Loss: 0.6173\n",
      "Epoch [4/5], Step [9824/10336], Loss: 0.4385\n",
      "Epoch [4/5], Step [9826/10336], Loss: 2.2188\n",
      "Epoch [4/5], Step [9828/10336], Loss: 0.1247\n",
      "Epoch [4/5], Step [9830/10336], Loss: 1.6243\n",
      "Epoch [4/5], Step [9832/10336], Loss: 0.0397\n",
      "Epoch [4/5], Step [9834/10336], Loss: 0.2131\n",
      "Epoch [4/5], Step [9836/10336], Loss: 2.9465\n",
      "Epoch [4/5], Step [9838/10336], Loss: 2.0481\n",
      "Epoch [4/5], Step [9840/10336], Loss: 0.3660\n",
      "Epoch [4/5], Step [9842/10336], Loss: 0.7119\n",
      "Epoch [4/5], Step [9844/10336], Loss: 0.1107\n",
      "Epoch [4/5], Step [9846/10336], Loss: 0.0749\n",
      "Epoch [4/5], Step [9848/10336], Loss: 0.5027\n",
      "Epoch [4/5], Step [9850/10336], Loss: 1.5999\n",
      "Epoch [4/5], Step [9852/10336], Loss: 0.1544\n",
      "Epoch [4/5], Step [9854/10336], Loss: 0.1129\n",
      "Epoch [4/5], Step [9856/10336], Loss: 1.6916\n",
      "Epoch [4/5], Step [9858/10336], Loss: 0.1055\n",
      "Epoch [4/5], Step [9860/10336], Loss: 0.0092\n",
      "Epoch [4/5], Step [9862/10336], Loss: 0.3151\n",
      "Epoch [4/5], Step [9864/10336], Loss: 0.4303\n",
      "Epoch [4/5], Step [9866/10336], Loss: 0.0043\n",
      "Epoch [4/5], Step [9868/10336], Loss: 0.0713\n",
      "Epoch [4/5], Step [9870/10336], Loss: 2.5239\n",
      "Epoch [4/5], Step [9872/10336], Loss: 0.0157\n",
      "Epoch [4/5], Step [9874/10336], Loss: 2.2177\n",
      "Epoch [4/5], Step [9876/10336], Loss: 0.5282\n",
      "Epoch [4/5], Step [9878/10336], Loss: 0.0937\n",
      "Epoch [4/5], Step [9880/10336], Loss: 0.5941\n",
      "Epoch [4/5], Step [9882/10336], Loss: 1.3653\n",
      "Epoch [4/5], Step [9884/10336], Loss: 0.8536\n",
      "Epoch [4/5], Step [9886/10336], Loss: 0.3017\n",
      "Epoch [4/5], Step [9888/10336], Loss: 1.4716\n",
      "Epoch [4/5], Step [9890/10336], Loss: 0.0702\n",
      "Epoch [4/5], Step [9892/10336], Loss: 0.7868\n",
      "Epoch [4/5], Step [9894/10336], Loss: 0.0053\n",
      "Epoch [4/5], Step [9896/10336], Loss: 0.0010\n",
      "Epoch [4/5], Step [9898/10336], Loss: 0.0223\n",
      "Epoch [4/5], Step [9900/10336], Loss: 1.0942\n",
      "Epoch [4/5], Step [9902/10336], Loss: 1.0360\n",
      "Epoch [4/5], Step [9904/10336], Loss: 1.7742\n",
      "Epoch [4/5], Step [9906/10336], Loss: 0.0025\n",
      "Epoch [4/5], Step [9908/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [9910/10336], Loss: 1.9494\n",
      "Epoch [4/5], Step [9912/10336], Loss: 1.2394\n",
      "Epoch [4/5], Step [9914/10336], Loss: 1.6109\n",
      "Epoch [4/5], Step [9916/10336], Loss: 2.2720\n",
      "Epoch [4/5], Step [9918/10336], Loss: 2.1293\n",
      "Epoch [4/5], Step [9920/10336], Loss: 0.0897\n",
      "Epoch [4/5], Step [9922/10336], Loss: 1.5572\n",
      "Epoch [4/5], Step [9924/10336], Loss: 0.0168\n",
      "Epoch [4/5], Step [9926/10336], Loss: 0.1128\n",
      "Epoch [4/5], Step [9928/10336], Loss: 1.6687\n",
      "Epoch [4/5], Step [9930/10336], Loss: 1.6280\n",
      "Epoch [4/5], Step [9932/10336], Loss: 0.1791\n",
      "Epoch [4/5], Step [9934/10336], Loss: 1.2185\n",
      "Epoch [4/5], Step [9936/10336], Loss: 0.4347\n",
      "Epoch [4/5], Step [9938/10336], Loss: 0.5544\n",
      "Epoch [4/5], Step [9940/10336], Loss: 3.2391\n",
      "Epoch [4/5], Step [9942/10336], Loss: 0.0041\n",
      "Epoch [4/5], Step [9944/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [9946/10336], Loss: 0.0012\n",
      "Epoch [4/5], Step [9948/10336], Loss: 1.6270\n",
      "Epoch [4/5], Step [9950/10336], Loss: 2.0125\n",
      "Epoch [4/5], Step [9952/10336], Loss: 0.3022\n",
      "Epoch [4/5], Step [9954/10336], Loss: 0.1704\n",
      "Epoch [4/5], Step [9956/10336], Loss: 0.0304\n",
      "Epoch [4/5], Step [9958/10336], Loss: 0.0789\n",
      "Epoch [4/5], Step [9960/10336], Loss: 0.3271\n",
      "Epoch [4/5], Step [9962/10336], Loss: 0.1565\n",
      "Epoch [4/5], Step [9964/10336], Loss: 0.3411\n",
      "Epoch [4/5], Step [9966/10336], Loss: 0.1239\n",
      "Epoch [4/5], Step [9968/10336], Loss: 0.2570\n",
      "Epoch [4/5], Step [9970/10336], Loss: 0.2530\n",
      "Epoch [4/5], Step [9972/10336], Loss: 0.5761\n",
      "Epoch [4/5], Step [9974/10336], Loss: 0.7192\n",
      "Epoch [4/5], Step [9976/10336], Loss: 1.1941\n",
      "Epoch [4/5], Step [9978/10336], Loss: 0.2905\n",
      "Epoch [4/5], Step [9980/10336], Loss: 0.0291\n",
      "Epoch [4/5], Step [9982/10336], Loss: 0.0101\n",
      "Epoch [4/5], Step [9984/10336], Loss: 0.9839\n",
      "Epoch [4/5], Step [9986/10336], Loss: 1.1954\n",
      "Epoch [4/5], Step [9988/10336], Loss: 0.0685\n",
      "Epoch [4/5], Step [9990/10336], Loss: 1.2847\n",
      "Epoch [4/5], Step [9992/10336], Loss: 0.6701\n",
      "Epoch [4/5], Step [9994/10336], Loss: 0.6501\n",
      "Epoch [4/5], Step [9996/10336], Loss: 0.5010\n",
      "Epoch [4/5], Step [9998/10336], Loss: 0.0248\n",
      "Epoch [4/5], Step [10000/10336], Loss: 0.0937\n",
      "Epoch [4/5], Step [10002/10336], Loss: 0.4081\n",
      "Epoch [4/5], Step [10004/10336], Loss: 0.0193\n",
      "Epoch [4/5], Step [10006/10336], Loss: 1.3044\n",
      "Epoch [4/5], Step [10008/10336], Loss: 0.0062\n",
      "Epoch [4/5], Step [10010/10336], Loss: 0.0253\n",
      "Epoch [4/5], Step [10012/10336], Loss: 0.0555\n",
      "Epoch [4/5], Step [10014/10336], Loss: 0.3328\n",
      "Epoch [4/5], Step [10016/10336], Loss: 0.0466\n",
      "Epoch [4/5], Step [10018/10336], Loss: 0.0100\n",
      "Epoch [4/5], Step [10020/10336], Loss: 1.2301\n",
      "Epoch [4/5], Step [10022/10336], Loss: 0.3088\n",
      "Epoch [4/5], Step [10024/10336], Loss: 0.5721\n",
      "Epoch [4/5], Step [10026/10336], Loss: 0.2808\n",
      "Epoch [4/5], Step [10028/10336], Loss: 0.0319\n",
      "Epoch [4/5], Step [10030/10336], Loss: 0.0502\n",
      "Epoch [4/5], Step [10032/10336], Loss: 0.0373\n",
      "Epoch [4/5], Step [10034/10336], Loss: 0.0048\n",
      "Epoch [4/5], Step [10036/10336], Loss: 0.0033\n",
      "Epoch [4/5], Step [10038/10336], Loss: 1.6678\n",
      "Epoch [4/5], Step [10040/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [10042/10336], Loss: 0.0242\n",
      "Epoch [4/5], Step [10044/10336], Loss: 0.0972\n",
      "Epoch [4/5], Step [10046/10336], Loss: 0.2128\n",
      "Epoch [4/5], Step [10048/10336], Loss: 0.1550\n",
      "Epoch [4/5], Step [10050/10336], Loss: 0.6453\n",
      "Epoch [4/5], Step [10052/10336], Loss: 0.1776\n",
      "Epoch [4/5], Step [10054/10336], Loss: 2.7000\n",
      "Epoch [4/5], Step [10056/10336], Loss: 1.6111\n",
      "Epoch [4/5], Step [10058/10336], Loss: 0.7366\n",
      "Epoch [4/5], Step [10060/10336], Loss: 0.1162\n",
      "Epoch [4/5], Step [10062/10336], Loss: 0.1717\n",
      "Epoch [4/5], Step [10064/10336], Loss: 0.0861\n",
      "Epoch [4/5], Step [10066/10336], Loss: 0.2928\n",
      "Epoch [4/5], Step [10068/10336], Loss: 0.2313\n",
      "Epoch [4/5], Step [10070/10336], Loss: 0.0332\n",
      "Epoch [4/5], Step [10072/10336], Loss: 0.9373\n",
      "Epoch [4/5], Step [10074/10336], Loss: 0.5818\n",
      "Epoch [4/5], Step [10076/10336], Loss: 0.1743\n",
      "Epoch [4/5], Step [10078/10336], Loss: 1.2728\n",
      "Epoch [4/5], Step [10080/10336], Loss: 0.0828\n",
      "Epoch [4/5], Step [10082/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [10084/10336], Loss: 0.1349\n",
      "Epoch [4/5], Step [10086/10336], Loss: 0.0226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [10088/10336], Loss: 0.1483\n",
      "Epoch [4/5], Step [10090/10336], Loss: 0.1774\n",
      "Epoch [4/5], Step [10092/10336], Loss: 1.1550\n",
      "Epoch [4/5], Step [10094/10336], Loss: 0.3888\n",
      "Epoch [4/5], Step [10096/10336], Loss: 0.8587\n",
      "Epoch [4/5], Step [10098/10336], Loss: 0.1037\n",
      "Epoch [4/5], Step [10100/10336], Loss: 1.0097\n",
      "Epoch [4/5], Step [10102/10336], Loss: 0.0075\n",
      "Epoch [4/5], Step [10104/10336], Loss: 0.4608\n",
      "Epoch [4/5], Step [10106/10336], Loss: 0.0404\n",
      "Epoch [4/5], Step [10108/10336], Loss: 0.0979\n",
      "Epoch [4/5], Step [10110/10336], Loss: 0.0861\n",
      "Epoch [4/5], Step [10112/10336], Loss: 0.0993\n",
      "Epoch [4/5], Step [10114/10336], Loss: 0.6496\n",
      "Epoch [4/5], Step [10116/10336], Loss: 1.6731\n",
      "Epoch [4/5], Step [10118/10336], Loss: 4.4126\n",
      "Epoch [4/5], Step [10120/10336], Loss: 0.2889\n",
      "Epoch [4/5], Step [10122/10336], Loss: 1.5187\n",
      "Epoch [4/5], Step [10124/10336], Loss: 1.2706\n",
      "Epoch [4/5], Step [10126/10336], Loss: 0.2468\n",
      "Epoch [4/5], Step [10128/10336], Loss: 1.1891\n",
      "Epoch [4/5], Step [10130/10336], Loss: 1.2527\n",
      "Epoch [4/5], Step [10132/10336], Loss: 0.0807\n",
      "Epoch [4/5], Step [10134/10336], Loss: 0.0005\n",
      "Epoch [4/5], Step [10136/10336], Loss: 0.0116\n",
      "Epoch [4/5], Step [10138/10336], Loss: 0.8618\n",
      "Epoch [4/5], Step [10140/10336], Loss: 0.0086\n",
      "Epoch [4/5], Step [10142/10336], Loss: 0.0728\n",
      "Epoch [4/5], Step [10144/10336], Loss: 0.1151\n",
      "Epoch [4/5], Step [10146/10336], Loss: 1.0570\n",
      "Epoch [4/5], Step [10148/10336], Loss: 0.2220\n",
      "Epoch [4/5], Step [10150/10336], Loss: 0.4759\n",
      "Epoch [4/5], Step [10152/10336], Loss: 0.3974\n",
      "Epoch [4/5], Step [10154/10336], Loss: 0.0542\n",
      "Epoch [4/5], Step [10156/10336], Loss: 0.1511\n",
      "Epoch [4/5], Step [10158/10336], Loss: 0.8355\n",
      "Epoch [4/5], Step [10160/10336], Loss: 0.0048\n",
      "Epoch [4/5], Step [10162/10336], Loss: 0.5750\n",
      "Epoch [4/5], Step [10164/10336], Loss: 0.0262\n",
      "Epoch [4/5], Step [10166/10336], Loss: 1.5195\n",
      "Epoch [4/5], Step [10168/10336], Loss: 0.8691\n",
      "Epoch [4/5], Step [10170/10336], Loss: 2.7572\n",
      "Epoch [4/5], Step [10172/10336], Loss: 2.0204\n",
      "Epoch [4/5], Step [10174/10336], Loss: 0.0581\n",
      "Epoch [4/5], Step [10176/10336], Loss: 0.5688\n",
      "Epoch [4/5], Step [10178/10336], Loss: 0.0037\n",
      "Epoch [4/5], Step [10180/10336], Loss: 0.4147\n",
      "Epoch [4/5], Step [10182/10336], Loss: 0.7975\n",
      "Epoch [4/5], Step [10184/10336], Loss: 0.1758\n",
      "Epoch [4/5], Step [10186/10336], Loss: 0.9535\n",
      "Epoch [4/5], Step [10188/10336], Loss: 0.0232\n",
      "Epoch [4/5], Step [10190/10336], Loss: 0.4779\n",
      "Epoch [4/5], Step [10192/10336], Loss: 0.1724\n",
      "Epoch [4/5], Step [10194/10336], Loss: 0.4419\n",
      "Epoch [4/5], Step [10196/10336], Loss: 0.0657\n",
      "Epoch [4/5], Step [10198/10336], Loss: 0.4760\n",
      "Epoch [4/5], Step [10200/10336], Loss: 0.0154\n",
      "Epoch [4/5], Step [10202/10336], Loss: 0.0002\n",
      "Epoch [4/5], Step [10204/10336], Loss: 1.0885\n",
      "Epoch [4/5], Step [10206/10336], Loss: 0.0228\n",
      "Epoch [4/5], Step [10208/10336], Loss: 0.0018\n",
      "Epoch [4/5], Step [10210/10336], Loss: 0.0007\n",
      "Epoch [4/5], Step [10212/10336], Loss: 0.9369\n",
      "Epoch [4/5], Step [10214/10336], Loss: 0.0185\n",
      "Epoch [4/5], Step [10216/10336], Loss: 0.5508\n",
      "Epoch [4/5], Step [10218/10336], Loss: 0.0563\n",
      "Epoch [4/5], Step [10220/10336], Loss: 0.0823\n",
      "Epoch [4/5], Step [10222/10336], Loss: 2.4141\n",
      "Epoch [4/5], Step [10224/10336], Loss: 0.1124\n",
      "Epoch [4/5], Step [10226/10336], Loss: 0.1744\n",
      "Epoch [4/5], Step [10228/10336], Loss: 0.0705\n",
      "Epoch [4/5], Step [10230/10336], Loss: 1.4894\n",
      "Epoch [4/5], Step [10232/10336], Loss: 2.4354\n",
      "Epoch [4/5], Step [10234/10336], Loss: 0.0292\n",
      "Epoch [4/5], Step [10236/10336], Loss: 0.0340\n",
      "Epoch [4/5], Step [10238/10336], Loss: 0.0567\n",
      "Epoch [4/5], Step [10240/10336], Loss: 0.0028\n",
      "Epoch [4/5], Step [10242/10336], Loss: 0.8940\n",
      "Epoch [4/5], Step [10244/10336], Loss: 0.0198\n",
      "Epoch [4/5], Step [10246/10336], Loss: 0.0122\n",
      "Epoch [4/5], Step [10248/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [10250/10336], Loss: 1.3018\n",
      "Epoch [4/5], Step [10252/10336], Loss: 0.0001\n",
      "Epoch [4/5], Step [10254/10336], Loss: 0.9137\n",
      "Epoch [4/5], Step [10256/10336], Loss: 0.0142\n",
      "Epoch [4/5], Step [10258/10336], Loss: 0.2348\n",
      "Epoch [4/5], Step [10260/10336], Loss: 0.0042\n",
      "Epoch [4/5], Step [10262/10336], Loss: 0.1369\n",
      "Epoch [4/5], Step [10264/10336], Loss: 0.3092\n",
      "Epoch [4/5], Step [10266/10336], Loss: 0.7438\n",
      "Epoch [4/5], Step [10268/10336], Loss: 1.8026\n",
      "Epoch [4/5], Step [10270/10336], Loss: 0.0024\n",
      "Epoch [4/5], Step [10272/10336], Loss: 2.7811\n",
      "Epoch [4/5], Step [10274/10336], Loss: 2.1597\n",
      "Epoch [4/5], Step [10276/10336], Loss: 1.3912\n",
      "Epoch [4/5], Step [10278/10336], Loss: 0.0407\n",
      "Epoch [4/5], Step [10280/10336], Loss: 0.1977\n",
      "Epoch [4/5], Step [10282/10336], Loss: 0.0036\n",
      "Epoch [4/5], Step [10284/10336], Loss: 0.3789\n",
      "Epoch [4/5], Step [10286/10336], Loss: 0.7722\n",
      "Epoch [4/5], Step [10288/10336], Loss: 0.6646\n",
      "Epoch [4/5], Step [10290/10336], Loss: 0.3047\n",
      "Epoch [4/5], Step [10292/10336], Loss: 0.0442\n",
      "Epoch [4/5], Step [10294/10336], Loss: 1.0165\n",
      "Epoch [4/5], Step [10296/10336], Loss: 0.2263\n",
      "Epoch [4/5], Step [10298/10336], Loss: 0.0928\n",
      "Epoch [4/5], Step [10300/10336], Loss: 2.1278\n",
      "Epoch [4/5], Step [10302/10336], Loss: 0.0634\n",
      "Epoch [4/5], Step [10304/10336], Loss: 3.4439\n",
      "Epoch [4/5], Step [10306/10336], Loss: 0.0785\n",
      "Epoch [4/5], Step [10308/10336], Loss: 0.7561\n",
      "Epoch [4/5], Step [10310/10336], Loss: 2.1359\n",
      "Epoch [4/5], Step [10312/10336], Loss: 1.5406\n",
      "Epoch [4/5], Step [10314/10336], Loss: 0.1083\n",
      "Epoch [4/5], Step [10316/10336], Loss: 1.0336\n",
      "Epoch [4/5], Step [10318/10336], Loss: 2.1953\n",
      "Epoch [4/5], Step [10320/10336], Loss: 0.7481\n",
      "Epoch [4/5], Step [10322/10336], Loss: 0.1284\n",
      "Epoch [4/5], Step [10324/10336], Loss: 0.0088\n",
      "Epoch [4/5], Step [10326/10336], Loss: 0.0076\n",
      "Epoch [4/5], Step [10328/10336], Loss: 0.1000\n",
      "Epoch [4/5], Step [10330/10336], Loss: 0.0884\n",
      "Epoch [4/5], Step [10332/10336], Loss: 0.0369\n",
      "Epoch [4/5], Step [10334/10336], Loss: 0.0016\n",
      "Epoch [4/5], Step [10336/10336], Loss: 0.0683\n",
      "Epoch [5/5], Step [2/10336], Loss: 0.2721\n",
      "Epoch [5/5], Step [4/10336], Loss: 0.3121\n",
      "Epoch [5/5], Step [6/10336], Loss: 0.0394\n",
      "Epoch [5/5], Step [8/10336], Loss: 2.6528\n",
      "Epoch [5/5], Step [10/10336], Loss: 0.0322\n",
      "Epoch [5/5], Step [12/10336], Loss: 0.1670\n",
      "Epoch [5/5], Step [14/10336], Loss: 0.6512\n",
      "Epoch [5/5], Step [16/10336], Loss: 0.5576\n",
      "Epoch [5/5], Step [18/10336], Loss: 0.0090\n",
      "Epoch [5/5], Step [20/10336], Loss: 0.0138\n",
      "Epoch [5/5], Step [22/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [24/10336], Loss: 0.7510\n",
      "Epoch [5/5], Step [26/10336], Loss: 0.0203\n",
      "Epoch [5/5], Step [28/10336], Loss: 0.0193\n",
      "Epoch [5/5], Step [30/10336], Loss: 0.0659\n",
      "Epoch [5/5], Step [32/10336], Loss: 1.6877\n",
      "Epoch [5/5], Step [34/10336], Loss: 2.8294\n",
      "Epoch [5/5], Step [36/10336], Loss: 0.0363\n",
      "Epoch [5/5], Step [38/10336], Loss: 0.0305\n",
      "Epoch [5/5], Step [40/10336], Loss: 0.0428\n",
      "Epoch [5/5], Step [42/10336], Loss: 0.2475\n",
      "Epoch [5/5], Step [44/10336], Loss: 0.0122\n",
      "Epoch [5/5], Step [46/10336], Loss: 2.0673\n",
      "Epoch [5/5], Step [48/10336], Loss: 0.1510\n",
      "Epoch [5/5], Step [50/10336], Loss: 0.0151\n",
      "Epoch [5/5], Step [52/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [54/10336], Loss: 0.1660\n",
      "Epoch [5/5], Step [56/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [58/10336], Loss: 1.3261\n",
      "Epoch [5/5], Step [60/10336], Loss: 0.3999\n",
      "Epoch [5/5], Step [62/10336], Loss: 0.0319\n",
      "Epoch [5/5], Step [64/10336], Loss: 0.8279\n",
      "Epoch [5/5], Step [66/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [68/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [70/10336], Loss: 0.0730\n",
      "Epoch [5/5], Step [72/10336], Loss: 0.0780\n",
      "Epoch [5/5], Step [74/10336], Loss: 0.1126\n",
      "Epoch [5/5], Step [76/10336], Loss: 0.3999\n",
      "Epoch [5/5], Step [78/10336], Loss: 2.7039\n",
      "Epoch [5/5], Step [80/10336], Loss: 0.7174\n",
      "Epoch [5/5], Step [82/10336], Loss: 1.8240\n",
      "Epoch [5/5], Step [84/10336], Loss: 0.0246\n",
      "Epoch [5/5], Step [86/10336], Loss: 1.7459\n",
      "Epoch [5/5], Step [88/10336], Loss: 0.3727\n",
      "Epoch [5/5], Step [90/10336], Loss: 0.0753\n",
      "Epoch [5/5], Step [92/10336], Loss: 0.4072\n",
      "Epoch [5/5], Step [94/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [96/10336], Loss: 0.0211\n",
      "Epoch [5/5], Step [98/10336], Loss: 0.3621\n",
      "Epoch [5/5], Step [100/10336], Loss: 1.7993\n",
      "Epoch [5/5], Step [102/10336], Loss: 1.0514\n",
      "Epoch [5/5], Step [104/10336], Loss: 0.0930\n",
      "Epoch [5/5], Step [106/10336], Loss: 0.6191\n",
      "Epoch [5/5], Step [108/10336], Loss: 1.9435\n",
      "Epoch [5/5], Step [110/10336], Loss: 0.3417\n",
      "Epoch [5/5], Step [112/10336], Loss: 0.4452\n",
      "Epoch [5/5], Step [114/10336], Loss: 0.1157\n",
      "Epoch [5/5], Step [116/10336], Loss: 1.8845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [118/10336], Loss: 0.5638\n",
      "Epoch [5/5], Step [120/10336], Loss: 0.4553\n",
      "Epoch [5/5], Step [122/10336], Loss: 0.2002\n",
      "Epoch [5/5], Step [124/10336], Loss: 1.7254\n",
      "Epoch [5/5], Step [126/10336], Loss: 0.6925\n",
      "Epoch [5/5], Step [128/10336], Loss: 0.9833\n",
      "Epoch [5/5], Step [130/10336], Loss: 0.1246\n",
      "Epoch [5/5], Step [132/10336], Loss: 0.1719\n",
      "Epoch [5/5], Step [134/10336], Loss: 0.1212\n",
      "Epoch [5/5], Step [136/10336], Loss: 3.8858\n",
      "Epoch [5/5], Step [138/10336], Loss: 0.6208\n",
      "Epoch [5/5], Step [140/10336], Loss: 0.2535\n",
      "Epoch [5/5], Step [142/10336], Loss: 1.3686\n",
      "Epoch [5/5], Step [144/10336], Loss: 0.6098\n",
      "Epoch [5/5], Step [146/10336], Loss: 0.0169\n",
      "Epoch [5/5], Step [148/10336], Loss: 0.1491\n",
      "Epoch [5/5], Step [150/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [152/10336], Loss: 0.1721\n",
      "Epoch [5/5], Step [154/10336], Loss: 1.7443\n",
      "Epoch [5/5], Step [156/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [158/10336], Loss: 0.1343\n",
      "Epoch [5/5], Step [160/10336], Loss: 0.3547\n",
      "Epoch [5/5], Step [162/10336], Loss: 0.7459\n",
      "Epoch [5/5], Step [164/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [166/10336], Loss: 0.6225\n",
      "Epoch [5/5], Step [168/10336], Loss: 0.0086\n",
      "Epoch [5/5], Step [170/10336], Loss: 1.4610\n",
      "Epoch [5/5], Step [172/10336], Loss: 0.0099\n",
      "Epoch [5/5], Step [174/10336], Loss: 1.0155\n",
      "Epoch [5/5], Step [176/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [178/10336], Loss: 0.0741\n",
      "Epoch [5/5], Step [180/10336], Loss: 0.3783\n",
      "Epoch [5/5], Step [182/10336], Loss: 0.0654\n",
      "Epoch [5/5], Step [184/10336], Loss: 0.0764\n",
      "Epoch [5/5], Step [186/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [188/10336], Loss: 0.7262\n",
      "Epoch [5/5], Step [190/10336], Loss: 1.6965\n",
      "Epoch [5/5], Step [192/10336], Loss: 0.8772\n",
      "Epoch [5/5], Step [194/10336], Loss: 0.0374\n",
      "Epoch [5/5], Step [196/10336], Loss: 0.0587\n",
      "Epoch [5/5], Step [198/10336], Loss: 0.1378\n",
      "Epoch [5/5], Step [200/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [202/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [204/10336], Loss: 0.9789\n",
      "Epoch [5/5], Step [206/10336], Loss: 0.2843\n",
      "Epoch [5/5], Step [208/10336], Loss: 0.6070\n",
      "Epoch [5/5], Step [210/10336], Loss: 0.3994\n",
      "Epoch [5/5], Step [212/10336], Loss: 0.0157\n",
      "Epoch [5/5], Step [214/10336], Loss: 0.0601\n",
      "Epoch [5/5], Step [216/10336], Loss: 1.4059\n",
      "Epoch [5/5], Step [218/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [220/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [222/10336], Loss: 0.4818\n",
      "Epoch [5/5], Step [224/10336], Loss: 0.0739\n",
      "Epoch [5/5], Step [226/10336], Loss: 0.5468\n",
      "Epoch [5/5], Step [228/10336], Loss: 2.4927\n",
      "Epoch [5/5], Step [230/10336], Loss: 0.0305\n",
      "Epoch [5/5], Step [232/10336], Loss: 0.4419\n",
      "Epoch [5/5], Step [234/10336], Loss: 0.9827\n",
      "Epoch [5/5], Step [236/10336], Loss: 0.0092\n",
      "Epoch [5/5], Step [238/10336], Loss: 0.4005\n",
      "Epoch [5/5], Step [240/10336], Loss: 1.2723\n",
      "Epoch [5/5], Step [242/10336], Loss: 1.8228\n",
      "Epoch [5/5], Step [244/10336], Loss: 1.2905\n",
      "Epoch [5/5], Step [246/10336], Loss: 2.0110\n",
      "Epoch [5/5], Step [248/10336], Loss: 0.0580\n",
      "Epoch [5/5], Step [250/10336], Loss: 1.3170\n",
      "Epoch [5/5], Step [252/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [254/10336], Loss: 0.0113\n",
      "Epoch [5/5], Step [256/10336], Loss: 0.4921\n",
      "Epoch [5/5], Step [258/10336], Loss: 0.4477\n",
      "Epoch [5/5], Step [260/10336], Loss: 1.4192\n",
      "Epoch [5/5], Step [262/10336], Loss: 1.2091\n",
      "Epoch [5/5], Step [264/10336], Loss: 0.1013\n",
      "Epoch [5/5], Step [266/10336], Loss: 1.4476\n",
      "Epoch [5/5], Step [268/10336], Loss: 0.2677\n",
      "Epoch [5/5], Step [270/10336], Loss: 0.0503\n",
      "Epoch [5/5], Step [272/10336], Loss: 0.0164\n",
      "Epoch [5/5], Step [274/10336], Loss: 1.6047\n",
      "Epoch [5/5], Step [276/10336], Loss: 0.0126\n",
      "Epoch [5/5], Step [278/10336], Loss: 0.0416\n",
      "Epoch [5/5], Step [280/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [282/10336], Loss: 2.0934\n",
      "Epoch [5/5], Step [284/10336], Loss: 0.0384\n",
      "Epoch [5/5], Step [286/10336], Loss: 0.0909\n",
      "Epoch [5/5], Step [288/10336], Loss: 0.1675\n",
      "Epoch [5/5], Step [290/10336], Loss: 0.7578\n",
      "Epoch [5/5], Step [292/10336], Loss: 0.9633\n",
      "Epoch [5/5], Step [294/10336], Loss: 0.1879\n",
      "Epoch [5/5], Step [296/10336], Loss: 1.3326\n",
      "Epoch [5/5], Step [298/10336], Loss: 0.0093\n",
      "Epoch [5/5], Step [300/10336], Loss: 0.1062\n",
      "Epoch [5/5], Step [302/10336], Loss: 0.6580\n",
      "Epoch [5/5], Step [304/10336], Loss: 0.0109\n",
      "Epoch [5/5], Step [306/10336], Loss: 0.1008\n",
      "Epoch [5/5], Step [308/10336], Loss: 1.6742\n",
      "Epoch [5/5], Step [310/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [312/10336], Loss: 0.9528\n",
      "Epoch [5/5], Step [314/10336], Loss: 2.0386\n",
      "Epoch [5/5], Step [316/10336], Loss: 0.6576\n",
      "Epoch [5/5], Step [318/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [320/10336], Loss: 0.1052\n",
      "Epoch [5/5], Step [322/10336], Loss: 2.6920\n",
      "Epoch [5/5], Step [324/10336], Loss: 0.9561\n",
      "Epoch [5/5], Step [326/10336], Loss: 0.4934\n",
      "Epoch [5/5], Step [328/10336], Loss: 1.4416\n",
      "Epoch [5/5], Step [330/10336], Loss: 0.4511\n",
      "Epoch [5/5], Step [332/10336], Loss: 2.3432\n",
      "Epoch [5/5], Step [334/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [336/10336], Loss: 0.1225\n",
      "Epoch [5/5], Step [338/10336], Loss: 1.3777\n",
      "Epoch [5/5], Step [340/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [342/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [344/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [346/10336], Loss: 0.0748\n",
      "Epoch [5/5], Step [348/10336], Loss: 1.3981\n",
      "Epoch [5/5], Step [350/10336], Loss: 0.3669\n",
      "Epoch [5/5], Step [352/10336], Loss: 0.2057\n",
      "Epoch [5/5], Step [354/10336], Loss: 0.1042\n",
      "Epoch [5/5], Step [356/10336], Loss: 1.2638\n",
      "Epoch [5/5], Step [358/10336], Loss: 0.0188\n",
      "Epoch [5/5], Step [360/10336], Loss: 0.0170\n",
      "Epoch [5/5], Step [362/10336], Loss: 0.0146\n",
      "Epoch [5/5], Step [364/10336], Loss: 0.0569\n",
      "Epoch [5/5], Step [366/10336], Loss: 0.0429\n",
      "Epoch [5/5], Step [368/10336], Loss: 0.5451\n",
      "Epoch [5/5], Step [370/10336], Loss: 0.7096\n",
      "Epoch [5/5], Step [372/10336], Loss: 0.2118\n",
      "Epoch [5/5], Step [374/10336], Loss: 1.7611\n",
      "Epoch [5/5], Step [376/10336], Loss: 0.2787\n",
      "Epoch [5/5], Step [378/10336], Loss: 0.0111\n",
      "Epoch [5/5], Step [380/10336], Loss: 0.9890\n",
      "Epoch [5/5], Step [382/10336], Loss: 0.5309\n",
      "Epoch [5/5], Step [384/10336], Loss: 0.0187\n",
      "Epoch [5/5], Step [386/10336], Loss: 0.6473\n",
      "Epoch [5/5], Step [388/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [390/10336], Loss: 0.1691\n",
      "Epoch [5/5], Step [392/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [394/10336], Loss: 0.6482\n",
      "Epoch [5/5], Step [396/10336], Loss: 0.0657\n",
      "Epoch [5/5], Step [398/10336], Loss: 0.0106\n",
      "Epoch [5/5], Step [400/10336], Loss: 0.6299\n",
      "Epoch [5/5], Step [402/10336], Loss: 0.0324\n",
      "Epoch [5/5], Step [404/10336], Loss: 0.8579\n",
      "Epoch [5/5], Step [406/10336], Loss: 1.2922\n",
      "Epoch [5/5], Step [408/10336], Loss: 0.0269\n",
      "Epoch [5/5], Step [410/10336], Loss: 0.0165\n",
      "Epoch [5/5], Step [412/10336], Loss: 0.1352\n",
      "Epoch [5/5], Step [414/10336], Loss: 0.3698\n",
      "Epoch [5/5], Step [416/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [418/10336], Loss: 1.3866\n",
      "Epoch [5/5], Step [420/10336], Loss: 0.1743\n",
      "Epoch [5/5], Step [422/10336], Loss: 0.2554\n",
      "Epoch [5/5], Step [424/10336], Loss: 0.0233\n",
      "Epoch [5/5], Step [426/10336], Loss: 0.4449\n",
      "Epoch [5/5], Step [428/10336], Loss: 0.2008\n",
      "Epoch [5/5], Step [430/10336], Loss: 0.4333\n",
      "Epoch [5/5], Step [432/10336], Loss: 0.3844\n",
      "Epoch [5/5], Step [434/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [436/10336], Loss: 0.0499\n",
      "Epoch [5/5], Step [438/10336], Loss: 0.4282\n",
      "Epoch [5/5], Step [440/10336], Loss: 1.7422\n",
      "Epoch [5/5], Step [442/10336], Loss: 0.3746\n",
      "Epoch [5/5], Step [444/10336], Loss: 0.0575\n",
      "Epoch [5/5], Step [446/10336], Loss: 0.6233\n",
      "Epoch [5/5], Step [448/10336], Loss: 0.2290\n",
      "Epoch [5/5], Step [450/10336], Loss: 0.0655\n",
      "Epoch [5/5], Step [452/10336], Loss: 0.0951\n",
      "Epoch [5/5], Step [454/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [456/10336], Loss: 0.0274\n",
      "Epoch [5/5], Step [458/10336], Loss: 0.1021\n",
      "Epoch [5/5], Step [460/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [462/10336], Loss: 0.1170\n",
      "Epoch [5/5], Step [464/10336], Loss: 0.1942\n",
      "Epoch [5/5], Step [466/10336], Loss: 1.4063\n",
      "Epoch [5/5], Step [468/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [470/10336], Loss: 1.4901\n",
      "Epoch [5/5], Step [472/10336], Loss: 0.4319\n",
      "Epoch [5/5], Step [474/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [476/10336], Loss: 0.0929\n",
      "Epoch [5/5], Step [478/10336], Loss: 0.8029\n",
      "Epoch [5/5], Step [480/10336], Loss: 1.8232\n",
      "Epoch [5/5], Step [482/10336], Loss: 0.0314\n",
      "Epoch [5/5], Step [484/10336], Loss: 0.0498\n",
      "Epoch [5/5], Step [486/10336], Loss: 0.1706\n",
      "Epoch [5/5], Step [488/10336], Loss: 0.2221\n",
      "Epoch [5/5], Step [490/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [492/10336], Loss: 0.6030\n",
      "Epoch [5/5], Step [494/10336], Loss: 0.1292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [496/10336], Loss: 0.0039\n",
      "Epoch [5/5], Step [498/10336], Loss: 0.5385\n",
      "Epoch [5/5], Step [500/10336], Loss: 0.1446\n",
      "Epoch [5/5], Step [502/10336], Loss: 1.9819\n",
      "Epoch [5/5], Step [504/10336], Loss: 0.0375\n",
      "Epoch [5/5], Step [506/10336], Loss: 0.2718\n",
      "Epoch [5/5], Step [508/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [510/10336], Loss: 0.0298\n",
      "Epoch [5/5], Step [512/10336], Loss: 0.0831\n",
      "Epoch [5/5], Step [514/10336], Loss: 0.0445\n",
      "Epoch [5/5], Step [516/10336], Loss: 0.1840\n",
      "Epoch [5/5], Step [518/10336], Loss: 0.0114\n",
      "Epoch [5/5], Step [520/10336], Loss: 1.2713\n",
      "Epoch [5/5], Step [522/10336], Loss: 0.7876\n",
      "Epoch [5/5], Step [524/10336], Loss: 0.0136\n",
      "Epoch [5/5], Step [526/10336], Loss: 0.5125\n",
      "Epoch [5/5], Step [528/10336], Loss: 1.0423\n",
      "Epoch [5/5], Step [530/10336], Loss: 0.1088\n",
      "Epoch [5/5], Step [532/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [534/10336], Loss: 0.0654\n",
      "Epoch [5/5], Step [536/10336], Loss: 0.0133\n",
      "Epoch [5/5], Step [538/10336], Loss: 0.4674\n",
      "Epoch [5/5], Step [540/10336], Loss: 0.0994\n",
      "Epoch [5/5], Step [542/10336], Loss: 0.3447\n",
      "Epoch [5/5], Step [544/10336], Loss: 2.3491\n",
      "Epoch [5/5], Step [546/10336], Loss: 1.4227\n",
      "Epoch [5/5], Step [548/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [550/10336], Loss: 0.0343\n",
      "Epoch [5/5], Step [552/10336], Loss: 0.0186\n",
      "Epoch [5/5], Step [554/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [556/10336], Loss: 0.2193\n",
      "Epoch [5/5], Step [558/10336], Loss: 0.0686\n",
      "Epoch [5/5], Step [560/10336], Loss: 0.4951\n",
      "Epoch [5/5], Step [562/10336], Loss: 0.2191\n",
      "Epoch [5/5], Step [564/10336], Loss: 0.1015\n",
      "Epoch [5/5], Step [566/10336], Loss: 0.0431\n",
      "Epoch [5/5], Step [568/10336], Loss: 0.2149\n",
      "Epoch [5/5], Step [570/10336], Loss: 0.1956\n",
      "Epoch [5/5], Step [572/10336], Loss: 0.3166\n",
      "Epoch [5/5], Step [574/10336], Loss: 0.6454\n",
      "Epoch [5/5], Step [576/10336], Loss: 2.0284\n",
      "Epoch [5/5], Step [578/10336], Loss: 0.7514\n",
      "Epoch [5/5], Step [580/10336], Loss: 0.0739\n",
      "Epoch [5/5], Step [582/10336], Loss: 0.1187\n",
      "Epoch [5/5], Step [584/10336], Loss: 2.3535\n",
      "Epoch [5/5], Step [586/10336], Loss: 0.0237\n",
      "Epoch [5/5], Step [588/10336], Loss: 1.0616\n",
      "Epoch [5/5], Step [590/10336], Loss: 0.0948\n",
      "Epoch [5/5], Step [592/10336], Loss: 0.2908\n",
      "Epoch [5/5], Step [594/10336], Loss: 0.8833\n",
      "Epoch [5/5], Step [596/10336], Loss: 0.0341\n",
      "Epoch [5/5], Step [598/10336], Loss: 0.5559\n",
      "Epoch [5/5], Step [600/10336], Loss: 0.7353\n",
      "Epoch [5/5], Step [602/10336], Loss: 0.1104\n",
      "Epoch [5/5], Step [604/10336], Loss: 0.1670\n",
      "Epoch [5/5], Step [606/10336], Loss: 0.2413\n",
      "Epoch [5/5], Step [608/10336], Loss: 1.0301\n",
      "Epoch [5/5], Step [610/10336], Loss: 2.0050\n",
      "Epoch [5/5], Step [612/10336], Loss: 0.0393\n",
      "Epoch [5/5], Step [614/10336], Loss: 2.6228\n",
      "Epoch [5/5], Step [616/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [618/10336], Loss: 0.2416\n",
      "Epoch [5/5], Step [620/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [622/10336], Loss: 0.3358\n",
      "Epoch [5/5], Step [624/10336], Loss: 0.0102\n",
      "Epoch [5/5], Step [626/10336], Loss: 0.3069\n",
      "Epoch [5/5], Step [628/10336], Loss: 0.3647\n",
      "Epoch [5/5], Step [630/10336], Loss: 0.0115\n",
      "Epoch [5/5], Step [632/10336], Loss: 0.3240\n",
      "Epoch [5/5], Step [634/10336], Loss: 0.0110\n",
      "Epoch [5/5], Step [636/10336], Loss: 0.0240\n",
      "Epoch [5/5], Step [638/10336], Loss: 0.7008\n",
      "Epoch [5/5], Step [640/10336], Loss: 0.0610\n",
      "Epoch [5/5], Step [642/10336], Loss: 0.0449\n",
      "Epoch [5/5], Step [644/10336], Loss: 0.2126\n",
      "Epoch [5/5], Step [646/10336], Loss: 0.9034\n",
      "Epoch [5/5], Step [648/10336], Loss: 1.1232\n",
      "Epoch [5/5], Step [650/10336], Loss: 0.0128\n",
      "Epoch [5/5], Step [652/10336], Loss: 1.6998\n",
      "Epoch [5/5], Step [654/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [656/10336], Loss: 0.5145\n",
      "Epoch [5/5], Step [658/10336], Loss: 2.0630\n",
      "Epoch [5/5], Step [660/10336], Loss: 0.5706\n",
      "Epoch [5/5], Step [662/10336], Loss: 0.0566\n",
      "Epoch [5/5], Step [664/10336], Loss: 0.1917\n",
      "Epoch [5/5], Step [666/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [668/10336], Loss: 0.1145\n",
      "Epoch [5/5], Step [670/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [672/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [674/10336], Loss: 0.7231\n",
      "Epoch [5/5], Step [676/10336], Loss: 0.1633\n",
      "Epoch [5/5], Step [678/10336], Loss: 0.2766\n",
      "Epoch [5/5], Step [680/10336], Loss: 1.4033\n",
      "Epoch [5/5], Step [682/10336], Loss: 0.4648\n",
      "Epoch [5/5], Step [684/10336], Loss: 0.1323\n",
      "Epoch [5/5], Step [686/10336], Loss: 0.0225\n",
      "Epoch [5/5], Step [688/10336], Loss: 0.5861\n",
      "Epoch [5/5], Step [690/10336], Loss: 0.4956\n",
      "Epoch [5/5], Step [692/10336], Loss: 0.3596\n",
      "Epoch [5/5], Step [694/10336], Loss: 0.0104\n",
      "Epoch [5/5], Step [696/10336], Loss: 0.1253\n",
      "Epoch [5/5], Step [698/10336], Loss: 0.3026\n",
      "Epoch [5/5], Step [700/10336], Loss: 0.5888\n",
      "Epoch [5/5], Step [702/10336], Loss: 0.0655\n",
      "Epoch [5/5], Step [704/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [706/10336], Loss: 0.0793\n",
      "Epoch [5/5], Step [708/10336], Loss: 0.0150\n",
      "Epoch [5/5], Step [710/10336], Loss: 1.1942\n",
      "Epoch [5/5], Step [712/10336], Loss: 0.0680\n",
      "Epoch [5/5], Step [714/10336], Loss: 1.9681\n",
      "Epoch [5/5], Step [716/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [718/10336], Loss: 0.1461\n",
      "Epoch [5/5], Step [720/10336], Loss: 0.0137\n",
      "Epoch [5/5], Step [722/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [724/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [726/10336], Loss: 0.2570\n",
      "Epoch [5/5], Step [728/10336], Loss: 0.7130\n",
      "Epoch [5/5], Step [730/10336], Loss: 0.6399\n",
      "Epoch [5/5], Step [732/10336], Loss: 0.2040\n",
      "Epoch [5/5], Step [734/10336], Loss: 0.0344\n",
      "Epoch [5/5], Step [736/10336], Loss: 3.3974\n",
      "Epoch [5/5], Step [738/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [740/10336], Loss: 2.5516\n",
      "Epoch [5/5], Step [742/10336], Loss: 2.5811\n",
      "Epoch [5/5], Step [744/10336], Loss: 0.0239\n",
      "Epoch [5/5], Step [746/10336], Loss: 0.8827\n",
      "Epoch [5/5], Step [748/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [750/10336], Loss: 0.0299\n",
      "Epoch [5/5], Step [752/10336], Loss: 0.0174\n",
      "Epoch [5/5], Step [754/10336], Loss: 1.0876\n",
      "Epoch [5/5], Step [756/10336], Loss: 0.0375\n",
      "Epoch [5/5], Step [758/10336], Loss: 0.4784\n",
      "Epoch [5/5], Step [760/10336], Loss: 0.0171\n",
      "Epoch [5/5], Step [762/10336], Loss: 0.4910\n",
      "Epoch [5/5], Step [764/10336], Loss: 0.0792\n",
      "Epoch [5/5], Step [766/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [768/10336], Loss: 0.0620\n",
      "Epoch [5/5], Step [770/10336], Loss: 0.8752\n",
      "Epoch [5/5], Step [772/10336], Loss: 0.5045\n",
      "Epoch [5/5], Step [774/10336], Loss: 0.0250\n",
      "Epoch [5/5], Step [776/10336], Loss: 0.0439\n",
      "Epoch [5/5], Step [778/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [780/10336], Loss: 0.7036\n",
      "Epoch [5/5], Step [782/10336], Loss: 0.8353\n",
      "Epoch [5/5], Step [784/10336], Loss: 0.0387\n",
      "Epoch [5/5], Step [786/10336], Loss: 0.3575\n",
      "Epoch [5/5], Step [788/10336], Loss: 0.0866\n",
      "Epoch [5/5], Step [790/10336], Loss: 1.7560\n",
      "Epoch [5/5], Step [792/10336], Loss: 0.0967\n",
      "Epoch [5/5], Step [794/10336], Loss: 0.7962\n",
      "Epoch [5/5], Step [796/10336], Loss: 0.1196\n",
      "Epoch [5/5], Step [798/10336], Loss: 0.3854\n",
      "Epoch [5/5], Step [800/10336], Loss: 0.6873\n",
      "Epoch [5/5], Step [802/10336], Loss: 0.1528\n",
      "Epoch [5/5], Step [804/10336], Loss: 0.4876\n",
      "Epoch [5/5], Step [806/10336], Loss: 0.1656\n",
      "Epoch [5/5], Step [808/10336], Loss: 0.0482\n",
      "Epoch [5/5], Step [810/10336], Loss: 1.0573\n",
      "Epoch [5/5], Step [812/10336], Loss: 0.0721\n",
      "Epoch [5/5], Step [814/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [816/10336], Loss: 0.0374\n",
      "Epoch [5/5], Step [818/10336], Loss: 0.4105\n",
      "Epoch [5/5], Step [820/10336], Loss: 0.8853\n",
      "Epoch [5/5], Step [822/10336], Loss: 0.4213\n",
      "Epoch [5/5], Step [824/10336], Loss: 0.2483\n",
      "Epoch [5/5], Step [826/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [828/10336], Loss: 3.3414\n",
      "Epoch [5/5], Step [830/10336], Loss: 0.0903\n",
      "Epoch [5/5], Step [832/10336], Loss: 0.0393\n",
      "Epoch [5/5], Step [834/10336], Loss: 0.0138\n",
      "Epoch [5/5], Step [836/10336], Loss: 0.1486\n",
      "Epoch [5/5], Step [838/10336], Loss: 2.6458\n",
      "Epoch [5/5], Step [840/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [842/10336], Loss: 0.0177\n",
      "Epoch [5/5], Step [844/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [846/10336], Loss: 0.3179\n",
      "Epoch [5/5], Step [848/10336], Loss: 3.4905\n",
      "Epoch [5/5], Step [850/10336], Loss: 0.3160\n",
      "Epoch [5/5], Step [852/10336], Loss: 4.8366\n",
      "Epoch [5/5], Step [854/10336], Loss: 0.0924\n",
      "Epoch [5/5], Step [856/10336], Loss: 0.0784\n",
      "Epoch [5/5], Step [858/10336], Loss: 0.9881\n",
      "Epoch [5/5], Step [860/10336], Loss: 0.2595\n",
      "Epoch [5/5], Step [862/10336], Loss: 0.2442\n",
      "Epoch [5/5], Step [864/10336], Loss: 0.7210\n",
      "Epoch [5/5], Step [866/10336], Loss: 0.9614\n",
      "Epoch [5/5], Step [868/10336], Loss: 0.5970\n",
      "Epoch [5/5], Step [870/10336], Loss: 0.0113\n",
      "Epoch [5/5], Step [872/10336], Loss: 0.4436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [874/10336], Loss: 0.1076\n",
      "Epoch [5/5], Step [876/10336], Loss: 0.0428\n",
      "Epoch [5/5], Step [878/10336], Loss: 1.9044\n",
      "Epoch [5/5], Step [880/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [882/10336], Loss: 2.2744\n",
      "Epoch [5/5], Step [884/10336], Loss: 0.0708\n",
      "Epoch [5/5], Step [886/10336], Loss: 0.1649\n",
      "Epoch [5/5], Step [888/10336], Loss: 1.6059\n",
      "Epoch [5/5], Step [890/10336], Loss: 1.0417\n",
      "Epoch [5/5], Step [892/10336], Loss: 1.0911\n",
      "Epoch [5/5], Step [894/10336], Loss: 0.0236\n",
      "Epoch [5/5], Step [896/10336], Loss: 1.0030\n",
      "Epoch [5/5], Step [898/10336], Loss: 0.0613\n",
      "Epoch [5/5], Step [900/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [902/10336], Loss: 0.0575\n",
      "Epoch [5/5], Step [904/10336], Loss: 1.5910\n",
      "Epoch [5/5], Step [906/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [908/10336], Loss: 0.1061\n",
      "Epoch [5/5], Step [910/10336], Loss: 0.8075\n",
      "Epoch [5/5], Step [912/10336], Loss: 0.0707\n",
      "Epoch [5/5], Step [914/10336], Loss: 0.4314\n",
      "Epoch [5/5], Step [916/10336], Loss: 0.0827\n",
      "Epoch [5/5], Step [918/10336], Loss: 0.0124\n",
      "Epoch [5/5], Step [920/10336], Loss: 1.6008\n",
      "Epoch [5/5], Step [922/10336], Loss: 0.2920\n",
      "Epoch [5/5], Step [924/10336], Loss: 0.4683\n",
      "Epoch [5/5], Step [926/10336], Loss: 3.6191\n",
      "Epoch [5/5], Step [928/10336], Loss: 1.9561\n",
      "Epoch [5/5], Step [930/10336], Loss: 0.5067\n",
      "Epoch [5/5], Step [932/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [934/10336], Loss: 0.5550\n",
      "Epoch [5/5], Step [936/10336], Loss: 0.1856\n",
      "Epoch [5/5], Step [938/10336], Loss: 0.9017\n",
      "Epoch [5/5], Step [940/10336], Loss: 0.2723\n",
      "Epoch [5/5], Step [942/10336], Loss: 0.8693\n",
      "Epoch [5/5], Step [944/10336], Loss: 0.1469\n",
      "Epoch [5/5], Step [946/10336], Loss: 0.2920\n",
      "Epoch [5/5], Step [948/10336], Loss: 0.2480\n",
      "Epoch [5/5], Step [950/10336], Loss: 0.0625\n",
      "Epoch [5/5], Step [952/10336], Loss: 0.8808\n",
      "Epoch [5/5], Step [954/10336], Loss: 0.0169\n",
      "Epoch [5/5], Step [956/10336], Loss: 1.1749\n",
      "Epoch [5/5], Step [958/10336], Loss: 0.0134\n",
      "Epoch [5/5], Step [960/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [962/10336], Loss: 0.1633\n",
      "Epoch [5/5], Step [964/10336], Loss: 0.8235\n",
      "Epoch [5/5], Step [966/10336], Loss: 0.2036\n",
      "Epoch [5/5], Step [968/10336], Loss: 0.0685\n",
      "Epoch [5/5], Step [970/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [972/10336], Loss: 1.1620\n",
      "Epoch [5/5], Step [974/10336], Loss: 0.0431\n",
      "Epoch [5/5], Step [976/10336], Loss: 0.4683\n",
      "Epoch [5/5], Step [978/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [980/10336], Loss: 0.1401\n",
      "Epoch [5/5], Step [982/10336], Loss: 3.2728\n",
      "Epoch [5/5], Step [984/10336], Loss: 0.4583\n",
      "Epoch [5/5], Step [986/10336], Loss: 0.1997\n",
      "Epoch [5/5], Step [988/10336], Loss: 0.8132\n",
      "Epoch [5/5], Step [990/10336], Loss: 0.1984\n",
      "Epoch [5/5], Step [992/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [994/10336], Loss: 0.3048\n",
      "Epoch [5/5], Step [996/10336], Loss: 0.1557\n",
      "Epoch [5/5], Step [998/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [1000/10336], Loss: 1.0970\n",
      "Epoch [5/5], Step [1002/10336], Loss: 0.1592\n",
      "Epoch [5/5], Step [1004/10336], Loss: 0.1720\n",
      "Epoch [5/5], Step [1006/10336], Loss: 0.5312\n",
      "Epoch [5/5], Step [1008/10336], Loss: 1.0771\n",
      "Epoch [5/5], Step [1010/10336], Loss: 1.8372\n",
      "Epoch [5/5], Step [1012/10336], Loss: 0.0214\n",
      "Epoch [5/5], Step [1014/10336], Loss: 0.1335\n",
      "Epoch [5/5], Step [1016/10336], Loss: 2.1406\n",
      "Epoch [5/5], Step [1018/10336], Loss: 0.0787\n",
      "Epoch [5/5], Step [1020/10336], Loss: 0.0523\n",
      "Epoch [5/5], Step [1022/10336], Loss: 0.1364\n",
      "Epoch [5/5], Step [1024/10336], Loss: 0.1215\n",
      "Epoch [5/5], Step [1026/10336], Loss: 0.0986\n",
      "Epoch [5/5], Step [1028/10336], Loss: 4.8175\n",
      "Epoch [5/5], Step [1030/10336], Loss: 0.0193\n",
      "Epoch [5/5], Step [1032/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [1034/10336], Loss: 0.4561\n",
      "Epoch [5/5], Step [1036/10336], Loss: 0.0681\n",
      "Epoch [5/5], Step [1038/10336], Loss: 0.0158\n",
      "Epoch [5/5], Step [1040/10336], Loss: 0.7895\n",
      "Epoch [5/5], Step [1042/10336], Loss: 0.8125\n",
      "Epoch [5/5], Step [1044/10336], Loss: 2.5209\n",
      "Epoch [5/5], Step [1046/10336], Loss: 0.8585\n",
      "Epoch [5/5], Step [1048/10336], Loss: 0.2199\n",
      "Epoch [5/5], Step [1050/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [1052/10336], Loss: 0.5794\n",
      "Epoch [5/5], Step [1054/10336], Loss: 0.4442\n",
      "Epoch [5/5], Step [1056/10336], Loss: 0.1418\n",
      "Epoch [5/5], Step [1058/10336], Loss: 0.0780\n",
      "Epoch [5/5], Step [1060/10336], Loss: 0.0674\n",
      "Epoch [5/5], Step [1062/10336], Loss: 0.0055\n",
      "Epoch [5/5], Step [1064/10336], Loss: 3.1164\n",
      "Epoch [5/5], Step [1066/10336], Loss: 2.6140\n",
      "Epoch [5/5], Step [1068/10336], Loss: 0.3260\n",
      "Epoch [5/5], Step [1070/10336], Loss: 0.2353\n",
      "Epoch [5/5], Step [1072/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [1074/10336], Loss: 1.7949\n",
      "Epoch [5/5], Step [1076/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [1078/10336], Loss: 1.8818\n",
      "Epoch [5/5], Step [1080/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [1082/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [1084/10336], Loss: 1.1891\n",
      "Epoch [5/5], Step [1086/10336], Loss: 3.9560\n",
      "Epoch [5/5], Step [1088/10336], Loss: 0.0880\n",
      "Epoch [5/5], Step [1090/10336], Loss: 0.5854\n",
      "Epoch [5/5], Step [1092/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [1094/10336], Loss: 0.2243\n",
      "Epoch [5/5], Step [1096/10336], Loss: 4.8055\n",
      "Epoch [5/5], Step [1098/10336], Loss: 0.0668\n",
      "Epoch [5/5], Step [1100/10336], Loss: 2.0148\n",
      "Epoch [5/5], Step [1102/10336], Loss: 3.6713\n",
      "Epoch [5/5], Step [1104/10336], Loss: 0.5750\n",
      "Epoch [5/5], Step [1106/10336], Loss: 1.2003\n",
      "Epoch [5/5], Step [1108/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [1110/10336], Loss: 1.1594\n",
      "Epoch [5/5], Step [1112/10336], Loss: 0.0409\n",
      "Epoch [5/5], Step [1114/10336], Loss: 0.5436\n",
      "Epoch [5/5], Step [1116/10336], Loss: 0.0109\n",
      "Epoch [5/5], Step [1118/10336], Loss: 0.0460\n",
      "Epoch [5/5], Step [1120/10336], Loss: 0.2532\n",
      "Epoch [5/5], Step [1122/10336], Loss: 0.4221\n",
      "Epoch [5/5], Step [1124/10336], Loss: 0.5077\n",
      "Epoch [5/5], Step [1126/10336], Loss: 0.4046\n",
      "Epoch [5/5], Step [1128/10336], Loss: 0.9091\n",
      "Epoch [5/5], Step [1130/10336], Loss: 0.2220\n",
      "Epoch [5/5], Step [1132/10336], Loss: 0.0053\n",
      "Epoch [5/5], Step [1134/10336], Loss: 3.0116\n",
      "Epoch [5/5], Step [1136/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [1138/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [1140/10336], Loss: 0.1994\n",
      "Epoch [5/5], Step [1142/10336], Loss: 1.3298\n",
      "Epoch [5/5], Step [1144/10336], Loss: 1.4208\n",
      "Epoch [5/5], Step [1146/10336], Loss: 1.8483\n",
      "Epoch [5/5], Step [1148/10336], Loss: 0.1290\n",
      "Epoch [5/5], Step [1150/10336], Loss: 1.1300\n",
      "Epoch [5/5], Step [1152/10336], Loss: 0.5307\n",
      "Epoch [5/5], Step [1154/10336], Loss: 0.9244\n",
      "Epoch [5/5], Step [1156/10336], Loss: 0.0142\n",
      "Epoch [5/5], Step [1158/10336], Loss: 0.1309\n",
      "Epoch [5/5], Step [1160/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [1162/10336], Loss: 0.6731\n",
      "Epoch [5/5], Step [1164/10336], Loss: 0.0720\n",
      "Epoch [5/5], Step [1166/10336], Loss: 0.0093\n",
      "Epoch [5/5], Step [1168/10336], Loss: 1.4883\n",
      "Epoch [5/5], Step [1170/10336], Loss: 0.2986\n",
      "Epoch [5/5], Step [1172/10336], Loss: 0.6339\n",
      "Epoch [5/5], Step [1174/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [1176/10336], Loss: 0.2308\n",
      "Epoch [5/5], Step [1178/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [1180/10336], Loss: 1.4039\n",
      "Epoch [5/5], Step [1182/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [1184/10336], Loss: 1.2729\n",
      "Epoch [5/5], Step [1186/10336], Loss: 0.1733\n",
      "Epoch [5/5], Step [1188/10336], Loss: 0.0644\n",
      "Epoch [5/5], Step [1190/10336], Loss: 1.5266\n",
      "Epoch [5/5], Step [1192/10336], Loss: 0.2884\n",
      "Epoch [5/5], Step [1194/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [1196/10336], Loss: 0.0681\n",
      "Epoch [5/5], Step [1198/10336], Loss: 0.7301\n",
      "Epoch [5/5], Step [1200/10336], Loss: 0.3093\n",
      "Epoch [5/5], Step [1202/10336], Loss: 0.2803\n",
      "Epoch [5/5], Step [1204/10336], Loss: 1.3186\n",
      "Epoch [5/5], Step [1206/10336], Loss: 0.2826\n",
      "Epoch [5/5], Step [1208/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [1210/10336], Loss: 0.7442\n",
      "Epoch [5/5], Step [1212/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [1214/10336], Loss: 0.0199\n",
      "Epoch [5/5], Step [1216/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [1218/10336], Loss: 0.0326\n",
      "Epoch [5/5], Step [1220/10336], Loss: 0.0107\n",
      "Epoch [5/5], Step [1222/10336], Loss: 0.0397\n",
      "Epoch [5/5], Step [1224/10336], Loss: 1.5152\n",
      "Epoch [5/5], Step [1226/10336], Loss: 0.0907\n",
      "Epoch [5/5], Step [1228/10336], Loss: 1.0377\n",
      "Epoch [5/5], Step [1230/10336], Loss: 0.0719\n",
      "Epoch [5/5], Step [1232/10336], Loss: 1.2382\n",
      "Epoch [5/5], Step [1234/10336], Loss: 2.6554\n",
      "Epoch [5/5], Step [1236/10336], Loss: 0.1188\n",
      "Epoch [5/5], Step [1238/10336], Loss: 0.1741\n",
      "Epoch [5/5], Step [1240/10336], Loss: 0.0325\n",
      "Epoch [5/5], Step [1242/10336], Loss: 1.0099\n",
      "Epoch [5/5], Step [1244/10336], Loss: 1.9253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [1246/10336], Loss: 0.0128\n",
      "Epoch [5/5], Step [1248/10336], Loss: 0.1068\n",
      "Epoch [5/5], Step [1250/10336], Loss: 0.2075\n",
      "Epoch [5/5], Step [1252/10336], Loss: 0.9888\n",
      "Epoch [5/5], Step [1254/10336], Loss: 0.3408\n",
      "Epoch [5/5], Step [1256/10336], Loss: 4.0761\n",
      "Epoch [5/5], Step [1258/10336], Loss: 0.5677\n",
      "Epoch [5/5], Step [1260/10336], Loss: 1.0104\n",
      "Epoch [5/5], Step [1262/10336], Loss: 0.8362\n",
      "Epoch [5/5], Step [1264/10336], Loss: 1.0254\n",
      "Epoch [5/5], Step [1266/10336], Loss: 0.0082\n",
      "Epoch [5/5], Step [1268/10336], Loss: 0.1271\n",
      "Epoch [5/5], Step [1270/10336], Loss: 0.6711\n",
      "Epoch [5/5], Step [1272/10336], Loss: 1.8708\n",
      "Epoch [5/5], Step [1274/10336], Loss: 1.8481\n",
      "Epoch [5/5], Step [1276/10336], Loss: 0.0584\n",
      "Epoch [5/5], Step [1278/10336], Loss: 1.5279\n",
      "Epoch [5/5], Step [1280/10336], Loss: 0.7003\n",
      "Epoch [5/5], Step [1282/10336], Loss: 0.1059\n",
      "Epoch [5/5], Step [1284/10336], Loss: 0.2600\n",
      "Epoch [5/5], Step [1286/10336], Loss: 0.6733\n",
      "Epoch [5/5], Step [1288/10336], Loss: 1.2331\n",
      "Epoch [5/5], Step [1290/10336], Loss: 0.0185\n",
      "Epoch [5/5], Step [1292/10336], Loss: 0.0687\n",
      "Epoch [5/5], Step [1294/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [1296/10336], Loss: 1.1110\n",
      "Epoch [5/5], Step [1298/10336], Loss: 0.3679\n",
      "Epoch [5/5], Step [1300/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [1302/10336], Loss: 1.3172\n",
      "Epoch [5/5], Step [1304/10336], Loss: 0.0792\n",
      "Epoch [5/5], Step [1306/10336], Loss: 0.3414\n",
      "Epoch [5/5], Step [1308/10336], Loss: 0.8318\n",
      "Epoch [5/5], Step [1310/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [1312/10336], Loss: 0.0364\n",
      "Epoch [5/5], Step [1314/10336], Loss: 0.0342\n",
      "Epoch [5/5], Step [1316/10336], Loss: 0.0228\n",
      "Epoch [5/5], Step [1318/10336], Loss: 0.7011\n",
      "Epoch [5/5], Step [1320/10336], Loss: 0.0472\n",
      "Epoch [5/5], Step [1322/10336], Loss: 0.8339\n",
      "Epoch [5/5], Step [1324/10336], Loss: 1.6470\n",
      "Epoch [5/5], Step [1326/10336], Loss: 1.7565\n",
      "Epoch [5/5], Step [1328/10336], Loss: 0.4517\n",
      "Epoch [5/5], Step [1330/10336], Loss: 1.0098\n",
      "Epoch [5/5], Step [1332/10336], Loss: 0.0990\n",
      "Epoch [5/5], Step [1334/10336], Loss: 0.0118\n",
      "Epoch [5/5], Step [1336/10336], Loss: 0.0184\n",
      "Epoch [5/5], Step [1338/10336], Loss: 0.0805\n",
      "Epoch [5/5], Step [1340/10336], Loss: 0.2173\n",
      "Epoch [5/5], Step [1342/10336], Loss: 0.0132\n",
      "Epoch [5/5], Step [1344/10336], Loss: 0.0567\n",
      "Epoch [5/5], Step [1346/10336], Loss: 1.0128\n",
      "Epoch [5/5], Step [1348/10336], Loss: 0.1168\n",
      "Epoch [5/5], Step [1350/10336], Loss: 0.0981\n",
      "Epoch [5/5], Step [1352/10336], Loss: 0.0166\n",
      "Epoch [5/5], Step [1354/10336], Loss: 1.3930\n",
      "Epoch [5/5], Step [1356/10336], Loss: 1.8133\n",
      "Epoch [5/5], Step [1358/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [1360/10336], Loss: 0.0278\n",
      "Epoch [5/5], Step [1362/10336], Loss: 0.3233\n",
      "Epoch [5/5], Step [1364/10336], Loss: 0.6348\n",
      "Epoch [5/5], Step [1366/10336], Loss: 0.1932\n",
      "Epoch [5/5], Step [1368/10336], Loss: 0.4323\n",
      "Epoch [5/5], Step [1370/10336], Loss: 0.5614\n",
      "Epoch [5/5], Step [1372/10336], Loss: 0.1166\n",
      "Epoch [5/5], Step [1374/10336], Loss: 0.8703\n",
      "Epoch [5/5], Step [1376/10336], Loss: 0.3077\n",
      "Epoch [5/5], Step [1378/10336], Loss: 1.9607\n",
      "Epoch [5/5], Step [1380/10336], Loss: 0.0454\n",
      "Epoch [5/5], Step [1382/10336], Loss: 1.7648\n",
      "Epoch [5/5], Step [1384/10336], Loss: 0.0887\n",
      "Epoch [5/5], Step [1386/10336], Loss: 0.3320\n",
      "Epoch [5/5], Step [1388/10336], Loss: 1.1067\n",
      "Epoch [5/5], Step [1390/10336], Loss: 2.2851\n",
      "Epoch [5/5], Step [1392/10336], Loss: 1.1743\n",
      "Epoch [5/5], Step [1394/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [1396/10336], Loss: 0.9974\n",
      "Epoch [5/5], Step [1398/10336], Loss: 3.3357\n",
      "Epoch [5/5], Step [1400/10336], Loss: 0.4687\n",
      "Epoch [5/5], Step [1402/10336], Loss: 0.4984\n",
      "Epoch [5/5], Step [1404/10336], Loss: 2.1015\n",
      "Epoch [5/5], Step [1406/10336], Loss: 0.6253\n",
      "Epoch [5/5], Step [1408/10336], Loss: 0.3354\n",
      "Epoch [5/5], Step [1410/10336], Loss: 0.0396\n",
      "Epoch [5/5], Step [1412/10336], Loss: 0.0438\n",
      "Epoch [5/5], Step [1414/10336], Loss: 0.3925\n",
      "Epoch [5/5], Step [1416/10336], Loss: 0.8474\n",
      "Epoch [5/5], Step [1418/10336], Loss: 2.3325\n",
      "Epoch [5/5], Step [1420/10336], Loss: 0.0865\n",
      "Epoch [5/5], Step [1422/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [1424/10336], Loss: 5.1218\n",
      "Epoch [5/5], Step [1426/10336], Loss: 0.0109\n",
      "Epoch [5/5], Step [1428/10336], Loss: 1.1336\n",
      "Epoch [5/5], Step [1430/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [1432/10336], Loss: 0.0328\n",
      "Epoch [5/5], Step [1434/10336], Loss: 0.2810\n",
      "Epoch [5/5], Step [1436/10336], Loss: 0.7287\n",
      "Epoch [5/5], Step [1438/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [1440/10336], Loss: 0.0981\n",
      "Epoch [5/5], Step [1442/10336], Loss: 0.9908\n",
      "Epoch [5/5], Step [1444/10336], Loss: 0.3550\n",
      "Epoch [5/5], Step [1446/10336], Loss: 0.5470\n",
      "Epoch [5/5], Step [1448/10336], Loss: 0.0160\n",
      "Epoch [5/5], Step [1450/10336], Loss: 0.6382\n",
      "Epoch [5/5], Step [1452/10336], Loss: 0.1487\n",
      "Epoch [5/5], Step [1454/10336], Loss: 0.5715\n",
      "Epoch [5/5], Step [1456/10336], Loss: 0.0398\n",
      "Epoch [5/5], Step [1458/10336], Loss: 1.3548\n",
      "Epoch [5/5], Step [1460/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [1462/10336], Loss: 0.8969\n",
      "Epoch [5/5], Step [1464/10336], Loss: 0.5506\n",
      "Epoch [5/5], Step [1466/10336], Loss: 0.0615\n",
      "Epoch [5/5], Step [1468/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [1470/10336], Loss: 0.0237\n",
      "Epoch [5/5], Step [1472/10336], Loss: 1.5171\n",
      "Epoch [5/5], Step [1474/10336], Loss: 0.0299\n",
      "Epoch [5/5], Step [1476/10336], Loss: 0.2647\n",
      "Epoch [5/5], Step [1478/10336], Loss: 0.0931\n",
      "Epoch [5/5], Step [1480/10336], Loss: 0.0979\n",
      "Epoch [5/5], Step [1482/10336], Loss: 0.0048\n",
      "Epoch [5/5], Step [1484/10336], Loss: 0.0538\n",
      "Epoch [5/5], Step [1486/10336], Loss: 0.8091\n",
      "Epoch [5/5], Step [1488/10336], Loss: 0.0145\n",
      "Epoch [5/5], Step [1490/10336], Loss: 0.0642\n",
      "Epoch [5/5], Step [1492/10336], Loss: 0.0063\n",
      "Epoch [5/5], Step [1494/10336], Loss: 1.3637\n",
      "Epoch [5/5], Step [1496/10336], Loss: 2.5989\n",
      "Epoch [5/5], Step [1498/10336], Loss: 0.2058\n",
      "Epoch [5/5], Step [1500/10336], Loss: 0.0807\n",
      "Epoch [5/5], Step [1502/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [1504/10336], Loss: 1.0623\n",
      "Epoch [5/5], Step [1506/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [1508/10336], Loss: 0.6032\n",
      "Epoch [5/5], Step [1510/10336], Loss: 0.1857\n",
      "Epoch [5/5], Step [1512/10336], Loss: 0.4718\n",
      "Epoch [5/5], Step [1514/10336], Loss: 0.3932\n",
      "Epoch [5/5], Step [1516/10336], Loss: 0.1694\n",
      "Epoch [5/5], Step [1518/10336], Loss: 1.9029\n",
      "Epoch [5/5], Step [1520/10336], Loss: 0.2045\n",
      "Epoch [5/5], Step [1522/10336], Loss: 0.5116\n",
      "Epoch [5/5], Step [1524/10336], Loss: 0.0659\n",
      "Epoch [5/5], Step [1526/10336], Loss: 0.0515\n",
      "Epoch [5/5], Step [1528/10336], Loss: 0.0236\n",
      "Epoch [5/5], Step [1530/10336], Loss: 1.4813\n",
      "Epoch [5/5], Step [1532/10336], Loss: 1.0034\n",
      "Epoch [5/5], Step [1534/10336], Loss: 0.0832\n",
      "Epoch [5/5], Step [1536/10336], Loss: 0.3439\n",
      "Epoch [5/5], Step [1538/10336], Loss: 0.0360\n",
      "Epoch [5/5], Step [1540/10336], Loss: 0.0723\n",
      "Epoch [5/5], Step [1542/10336], Loss: 0.2050\n",
      "Epoch [5/5], Step [1544/10336], Loss: 0.0673\n",
      "Epoch [5/5], Step [1546/10336], Loss: 1.5513\n",
      "Epoch [5/5], Step [1548/10336], Loss: 0.1297\n",
      "Epoch [5/5], Step [1550/10336], Loss: 0.4128\n",
      "Epoch [5/5], Step [1552/10336], Loss: 1.9089\n",
      "Epoch [5/5], Step [1554/10336], Loss: 0.5664\n",
      "Epoch [5/5], Step [1556/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [1558/10336], Loss: 2.0460\n",
      "Epoch [5/5], Step [1560/10336], Loss: 0.7279\n",
      "Epoch [5/5], Step [1562/10336], Loss: 0.0705\n",
      "Epoch [5/5], Step [1564/10336], Loss: 2.2463\n",
      "Epoch [5/5], Step [1566/10336], Loss: 0.1180\n",
      "Epoch [5/5], Step [1568/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [1570/10336], Loss: 0.3436\n",
      "Epoch [5/5], Step [1572/10336], Loss: 0.4330\n",
      "Epoch [5/5], Step [1574/10336], Loss: 0.7921\n",
      "Epoch [5/5], Step [1576/10336], Loss: 1.3181\n",
      "Epoch [5/5], Step [1578/10336], Loss: 1.0886\n",
      "Epoch [5/5], Step [1580/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [1582/10336], Loss: 0.2915\n",
      "Epoch [5/5], Step [1584/10336], Loss: 0.3290\n",
      "Epoch [5/5], Step [1586/10336], Loss: 2.8666\n",
      "Epoch [5/5], Step [1588/10336], Loss: 0.0983\n",
      "Epoch [5/5], Step [1590/10336], Loss: 0.0055\n",
      "Epoch [5/5], Step [1592/10336], Loss: 2.7653\n",
      "Epoch [5/5], Step [1594/10336], Loss: 0.3983\n",
      "Epoch [5/5], Step [1596/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [1598/10336], Loss: 1.4889\n",
      "Epoch [5/5], Step [1600/10336], Loss: 1.0070\n",
      "Epoch [5/5], Step [1602/10336], Loss: 0.9515\n",
      "Epoch [5/5], Step [1604/10336], Loss: 0.3219\n",
      "Epoch [5/5], Step [1606/10336], Loss: 0.1954\n",
      "Epoch [5/5], Step [1608/10336], Loss: 0.0065\n",
      "Epoch [5/5], Step [1610/10336], Loss: 0.1086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [1612/10336], Loss: 0.3619\n",
      "Epoch [5/5], Step [1614/10336], Loss: 2.3817\n",
      "Epoch [5/5], Step [1616/10336], Loss: 1.4202\n",
      "Epoch [5/5], Step [1618/10336], Loss: 0.5696\n",
      "Epoch [5/5], Step [1620/10336], Loss: 0.0958\n",
      "Epoch [5/5], Step [1622/10336], Loss: 0.0621\n",
      "Epoch [5/5], Step [1624/10336], Loss: 0.0403\n",
      "Epoch [5/5], Step [1626/10336], Loss: 0.1351\n",
      "Epoch [5/5], Step [1628/10336], Loss: 0.6276\n",
      "Epoch [5/5], Step [1630/10336], Loss: 0.4160\n",
      "Epoch [5/5], Step [1632/10336], Loss: 0.7177\n",
      "Epoch [5/5], Step [1634/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [1636/10336], Loss: 0.0939\n",
      "Epoch [5/5], Step [1638/10336], Loss: 0.0900\n",
      "Epoch [5/5], Step [1640/10336], Loss: 0.0332\n",
      "Epoch [5/5], Step [1642/10336], Loss: 0.0348\n",
      "Epoch [5/5], Step [1644/10336], Loss: 0.5656\n",
      "Epoch [5/5], Step [1646/10336], Loss: 1.2613\n",
      "Epoch [5/5], Step [1648/10336], Loss: 0.0675\n",
      "Epoch [5/5], Step [1650/10336], Loss: 1.5327\n",
      "Epoch [5/5], Step [1652/10336], Loss: 0.1156\n",
      "Epoch [5/5], Step [1654/10336], Loss: 0.6558\n",
      "Epoch [5/5], Step [1656/10336], Loss: 0.3132\n",
      "Epoch [5/5], Step [1658/10336], Loss: 0.0735\n",
      "Epoch [5/5], Step [1660/10336], Loss: 0.4306\n",
      "Epoch [5/5], Step [1662/10336], Loss: 1.6450\n",
      "Epoch [5/5], Step [1664/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [1666/10336], Loss: 0.0096\n",
      "Epoch [5/5], Step [1668/10336], Loss: 0.3526\n",
      "Epoch [5/5], Step [1670/10336], Loss: 0.0299\n",
      "Epoch [5/5], Step [1672/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [1674/10336], Loss: 0.1275\n",
      "Epoch [5/5], Step [1676/10336], Loss: 0.0836\n",
      "Epoch [5/5], Step [1678/10336], Loss: 1.7756\n",
      "Epoch [5/5], Step [1680/10336], Loss: 0.0347\n",
      "Epoch [5/5], Step [1682/10336], Loss: 0.1075\n",
      "Epoch [5/5], Step [1684/10336], Loss: 0.0095\n",
      "Epoch [5/5], Step [1686/10336], Loss: 0.4999\n",
      "Epoch [5/5], Step [1688/10336], Loss: 0.0143\n",
      "Epoch [5/5], Step [1690/10336], Loss: 1.3704\n",
      "Epoch [5/5], Step [1692/10336], Loss: 0.3902\n",
      "Epoch [5/5], Step [1694/10336], Loss: 1.0601\n",
      "Epoch [5/5], Step [1696/10336], Loss: 0.2274\n",
      "Epoch [5/5], Step [1698/10336], Loss: 1.1314\n",
      "Epoch [5/5], Step [1700/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [1702/10336], Loss: 0.0122\n",
      "Epoch [5/5], Step [1704/10336], Loss: 0.0730\n",
      "Epoch [5/5], Step [1706/10336], Loss: 0.2989\n",
      "Epoch [5/5], Step [1708/10336], Loss: 1.3382\n",
      "Epoch [5/5], Step [1710/10336], Loss: 0.0089\n",
      "Epoch [5/5], Step [1712/10336], Loss: 0.4792\n",
      "Epoch [5/5], Step [1714/10336], Loss: 2.2950\n",
      "Epoch [5/5], Step [1716/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [1718/10336], Loss: 0.7909\n",
      "Epoch [5/5], Step [1720/10336], Loss: 0.4295\n",
      "Epoch [5/5], Step [1722/10336], Loss: 0.0151\n",
      "Epoch [5/5], Step [1724/10336], Loss: 0.2515\n",
      "Epoch [5/5], Step [1726/10336], Loss: 0.0935\n",
      "Epoch [5/5], Step [1728/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [1730/10336], Loss: 0.6764\n",
      "Epoch [5/5], Step [1732/10336], Loss: 0.9696\n",
      "Epoch [5/5], Step [1734/10336], Loss: 0.8563\n",
      "Epoch [5/5], Step [1736/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [1738/10336], Loss: 0.4444\n",
      "Epoch [5/5], Step [1740/10336], Loss: 0.1689\n",
      "Epoch [5/5], Step [1742/10336], Loss: 0.0108\n",
      "Epoch [5/5], Step [1744/10336], Loss: 0.6525\n",
      "Epoch [5/5], Step [1746/10336], Loss: 0.0866\n",
      "Epoch [5/5], Step [1748/10336], Loss: 0.0083\n",
      "Epoch [5/5], Step [1750/10336], Loss: 0.9315\n",
      "Epoch [5/5], Step [1752/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [1754/10336], Loss: 0.2249\n",
      "Epoch [5/5], Step [1756/10336], Loss: 0.2148\n",
      "Epoch [5/5], Step [1758/10336], Loss: 0.9700\n",
      "Epoch [5/5], Step [1760/10336], Loss: 0.9452\n",
      "Epoch [5/5], Step [1762/10336], Loss: 0.0242\n",
      "Epoch [5/5], Step [1764/10336], Loss: 0.0143\n",
      "Epoch [5/5], Step [1766/10336], Loss: 0.1521\n",
      "Epoch [5/5], Step [1768/10336], Loss: 1.7806\n",
      "Epoch [5/5], Step [1770/10336], Loss: 1.3691\n",
      "Epoch [5/5], Step [1772/10336], Loss: 0.0285\n",
      "Epoch [5/5], Step [1774/10336], Loss: 0.6087\n",
      "Epoch [5/5], Step [1776/10336], Loss: 0.6336\n",
      "Epoch [5/5], Step [1778/10336], Loss: 1.1388\n",
      "Epoch [5/5], Step [1780/10336], Loss: 1.0512\n",
      "Epoch [5/5], Step [1782/10336], Loss: 1.3768\n",
      "Epoch [5/5], Step [1784/10336], Loss: 0.0665\n",
      "Epoch [5/5], Step [1786/10336], Loss: 0.0277\n",
      "Epoch [5/5], Step [1788/10336], Loss: 0.4236\n",
      "Epoch [5/5], Step [1790/10336], Loss: 0.5022\n",
      "Epoch [5/5], Step [1792/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [1794/10336], Loss: 0.8823\n",
      "Epoch [5/5], Step [1796/10336], Loss: 0.2933\n",
      "Epoch [5/5], Step [1798/10336], Loss: 0.5391\n",
      "Epoch [5/5], Step [1800/10336], Loss: 0.3813\n",
      "Epoch [5/5], Step [1802/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [1804/10336], Loss: 2.7626\n",
      "Epoch [5/5], Step [1806/10336], Loss: 2.4299\n",
      "Epoch [5/5], Step [1808/10336], Loss: 0.4732\n",
      "Epoch [5/5], Step [1810/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [1812/10336], Loss: 0.4615\n",
      "Epoch [5/5], Step [1814/10336], Loss: 0.9064\n",
      "Epoch [5/5], Step [1816/10336], Loss: 0.0206\n",
      "Epoch [5/5], Step [1818/10336], Loss: 0.2069\n",
      "Epoch [5/5], Step [1820/10336], Loss: 0.0681\n",
      "Epoch [5/5], Step [1822/10336], Loss: 1.3834\n",
      "Epoch [5/5], Step [1824/10336], Loss: 4.4899\n",
      "Epoch [5/5], Step [1826/10336], Loss: 0.0056\n",
      "Epoch [5/5], Step [1828/10336], Loss: 0.2380\n",
      "Epoch [5/5], Step [1830/10336], Loss: 1.9002\n",
      "Epoch [5/5], Step [1832/10336], Loss: 0.0304\n",
      "Epoch [5/5], Step [1834/10336], Loss: 0.7845\n",
      "Epoch [5/5], Step [1836/10336], Loss: 0.3605\n",
      "Epoch [5/5], Step [1838/10336], Loss: 0.0990\n",
      "Epoch [5/5], Step [1840/10336], Loss: 0.0219\n",
      "Epoch [5/5], Step [1842/10336], Loss: 0.0055\n",
      "Epoch [5/5], Step [1844/10336], Loss: 0.0202\n",
      "Epoch [5/5], Step [1846/10336], Loss: 0.0476\n",
      "Epoch [5/5], Step [1848/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [1850/10336], Loss: 1.7943\n",
      "Epoch [5/5], Step [1852/10336], Loss: 0.1170\n",
      "Epoch [5/5], Step [1854/10336], Loss: 0.4414\n",
      "Epoch [5/5], Step [1856/10336], Loss: 0.2132\n",
      "Epoch [5/5], Step [1858/10336], Loss: 0.0237\n",
      "Epoch [5/5], Step [1860/10336], Loss: 0.6768\n",
      "Epoch [5/5], Step [1862/10336], Loss: 2.1726\n",
      "Epoch [5/5], Step [1864/10336], Loss: 0.9636\n",
      "Epoch [5/5], Step [1866/10336], Loss: 0.0429\n",
      "Epoch [5/5], Step [1868/10336], Loss: 0.4994\n",
      "Epoch [5/5], Step [1870/10336], Loss: 0.3224\n",
      "Epoch [5/5], Step [1872/10336], Loss: 0.1010\n",
      "Epoch [5/5], Step [1874/10336], Loss: 0.1667\n",
      "Epoch [5/5], Step [1876/10336], Loss: 0.0229\n",
      "Epoch [5/5], Step [1878/10336], Loss: 2.4588\n",
      "Epoch [5/5], Step [1880/10336], Loss: 1.7280\n",
      "Epoch [5/5], Step [1882/10336], Loss: 0.1221\n",
      "Epoch [5/5], Step [1884/10336], Loss: 1.7022\n",
      "Epoch [5/5], Step [1886/10336], Loss: 0.0656\n",
      "Epoch [5/5], Step [1888/10336], Loss: 2.5468\n",
      "Epoch [5/5], Step [1890/10336], Loss: 0.0426\n",
      "Epoch [5/5], Step [1892/10336], Loss: 1.2618\n",
      "Epoch [5/5], Step [1894/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [1896/10336], Loss: 3.0315\n",
      "Epoch [5/5], Step [1898/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [1900/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [1902/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [1904/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [1906/10336], Loss: 0.0140\n",
      "Epoch [5/5], Step [1908/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [1910/10336], Loss: 0.3442\n",
      "Epoch [5/5], Step [1912/10336], Loss: 0.8278\n",
      "Epoch [5/5], Step [1914/10336], Loss: 1.0288\n",
      "Epoch [5/5], Step [1916/10336], Loss: 0.4651\n",
      "Epoch [5/5], Step [1918/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [1920/10336], Loss: 0.0159\n",
      "Epoch [5/5], Step [1922/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [1924/10336], Loss: 0.0222\n",
      "Epoch [5/5], Step [1926/10336], Loss: 0.4274\n",
      "Epoch [5/5], Step [1928/10336], Loss: 1.4183\n",
      "Epoch [5/5], Step [1930/10336], Loss: 0.2645\n",
      "Epoch [5/5], Step [1932/10336], Loss: 0.0312\n",
      "Epoch [5/5], Step [1934/10336], Loss: 0.4185\n",
      "Epoch [5/5], Step [1936/10336], Loss: 0.0312\n",
      "Epoch [5/5], Step [1938/10336], Loss: 0.3276\n",
      "Epoch [5/5], Step [1940/10336], Loss: 0.1420\n",
      "Epoch [5/5], Step [1942/10336], Loss: 1.9982\n",
      "Epoch [5/5], Step [1944/10336], Loss: 0.0414\n",
      "Epoch [5/5], Step [1946/10336], Loss: 0.1205\n",
      "Epoch [5/5], Step [1948/10336], Loss: 0.0093\n",
      "Epoch [5/5], Step [1950/10336], Loss: 0.0575\n",
      "Epoch [5/5], Step [1952/10336], Loss: 1.4931\n",
      "Epoch [5/5], Step [1954/10336], Loss: 0.4834\n",
      "Epoch [5/5], Step [1956/10336], Loss: 1.3041\n",
      "Epoch [5/5], Step [1958/10336], Loss: 0.0868\n",
      "Epoch [5/5], Step [1960/10336], Loss: 0.0532\n",
      "Epoch [5/5], Step [1962/10336], Loss: 0.1253\n",
      "Epoch [5/5], Step [1964/10336], Loss: 1.1493\n",
      "Epoch [5/5], Step [1966/10336], Loss: 0.0063\n",
      "Epoch [5/5], Step [1968/10336], Loss: 3.5440\n",
      "Epoch [5/5], Step [1970/10336], Loss: 0.1879\n",
      "Epoch [5/5], Step [1972/10336], Loss: 0.0194\n",
      "Epoch [5/5], Step [1974/10336], Loss: 0.3448\n",
      "Epoch [5/5], Step [1976/10336], Loss: 3.6412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [1978/10336], Loss: 0.2771\n",
      "Epoch [5/5], Step [1980/10336], Loss: 0.0257\n",
      "Epoch [5/5], Step [1982/10336], Loss: 0.0849\n",
      "Epoch [5/5], Step [1984/10336], Loss: 0.0622\n",
      "Epoch [5/5], Step [1986/10336], Loss: 0.1609\n",
      "Epoch [5/5], Step [1988/10336], Loss: 0.7220\n",
      "Epoch [5/5], Step [1990/10336], Loss: 0.5672\n",
      "Epoch [5/5], Step [1992/10336], Loss: 0.0065\n",
      "Epoch [5/5], Step [1994/10336], Loss: 0.1386\n",
      "Epoch [5/5], Step [1996/10336], Loss: 2.9128\n",
      "Epoch [5/5], Step [1998/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [2000/10336], Loss: 0.1003\n",
      "Epoch [5/5], Step [2002/10336], Loss: 0.1989\n",
      "Epoch [5/5], Step [2004/10336], Loss: 2.4308\n",
      "Epoch [5/5], Step [2006/10336], Loss: 0.1127\n",
      "Epoch [5/5], Step [2008/10336], Loss: 0.0217\n",
      "Epoch [5/5], Step [2010/10336], Loss: 0.2213\n",
      "Epoch [5/5], Step [2012/10336], Loss: 0.4549\n",
      "Epoch [5/5], Step [2014/10336], Loss: 0.0472\n",
      "Epoch [5/5], Step [2016/10336], Loss: 0.1898\n",
      "Epoch [5/5], Step [2018/10336], Loss: 0.6823\n",
      "Epoch [5/5], Step [2020/10336], Loss: 1.4374\n",
      "Epoch [5/5], Step [2022/10336], Loss: 0.0172\n",
      "Epoch [5/5], Step [2024/10336], Loss: 1.0066\n",
      "Epoch [5/5], Step [2026/10336], Loss: 0.7342\n",
      "Epoch [5/5], Step [2028/10336], Loss: 0.0914\n",
      "Epoch [5/5], Step [2030/10336], Loss: 0.0375\n",
      "Epoch [5/5], Step [2032/10336], Loss: 0.0181\n",
      "Epoch [5/5], Step [2034/10336], Loss: 0.0089\n",
      "Epoch [5/5], Step [2036/10336], Loss: 0.6172\n",
      "Epoch [5/5], Step [2038/10336], Loss: 2.6300\n",
      "Epoch [5/5], Step [2040/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [2042/10336], Loss: 0.0173\n",
      "Epoch [5/5], Step [2044/10336], Loss: 0.0660\n",
      "Epoch [5/5], Step [2046/10336], Loss: 2.6719\n",
      "Epoch [5/5], Step [2048/10336], Loss: 0.4721\n",
      "Epoch [5/5], Step [2050/10336], Loss: 1.0550\n",
      "Epoch [5/5], Step [2052/10336], Loss: 0.0484\n",
      "Epoch [5/5], Step [2054/10336], Loss: 0.0332\n",
      "Epoch [5/5], Step [2056/10336], Loss: 0.1155\n",
      "Epoch [5/5], Step [2058/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [2060/10336], Loss: 2.4283\n",
      "Epoch [5/5], Step [2062/10336], Loss: 0.6237\n",
      "Epoch [5/5], Step [2064/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [2066/10336], Loss: 1.3771\n",
      "Epoch [5/5], Step [2068/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [2070/10336], Loss: 0.0180\n",
      "Epoch [5/5], Step [2072/10336], Loss: 0.1512\n",
      "Epoch [5/5], Step [2074/10336], Loss: 0.1474\n",
      "Epoch [5/5], Step [2076/10336], Loss: 0.0804\n",
      "Epoch [5/5], Step [2078/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [2080/10336], Loss: 1.3039\n",
      "Epoch [5/5], Step [2082/10336], Loss: 0.0235\n",
      "Epoch [5/5], Step [2084/10336], Loss: 0.0263\n",
      "Epoch [5/5], Step [2086/10336], Loss: 0.0151\n",
      "Epoch [5/5], Step [2088/10336], Loss: 0.0559\n",
      "Epoch [5/5], Step [2090/10336], Loss: 0.0132\n",
      "Epoch [5/5], Step [2092/10336], Loss: 0.1582\n",
      "Epoch [5/5], Step [2094/10336], Loss: 0.0704\n",
      "Epoch [5/5], Step [2096/10336], Loss: 0.5369\n",
      "Epoch [5/5], Step [2098/10336], Loss: 0.2998\n",
      "Epoch [5/5], Step [2100/10336], Loss: 1.0180\n",
      "Epoch [5/5], Step [2102/10336], Loss: 0.5168\n",
      "Epoch [5/5], Step [2104/10336], Loss: 0.4011\n",
      "Epoch [5/5], Step [2106/10336], Loss: 0.0166\n",
      "Epoch [5/5], Step [2108/10336], Loss: 0.2327\n",
      "Epoch [5/5], Step [2110/10336], Loss: 0.0374\n",
      "Epoch [5/5], Step [2112/10336], Loss: 0.0210\n",
      "Epoch [5/5], Step [2114/10336], Loss: 0.3823\n",
      "Epoch [5/5], Step [2116/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [2118/10336], Loss: 0.8825\n",
      "Epoch [5/5], Step [2120/10336], Loss: 0.1258\n",
      "Epoch [5/5], Step [2122/10336], Loss: 0.7765\n",
      "Epoch [5/5], Step [2124/10336], Loss: 3.5178\n",
      "Epoch [5/5], Step [2126/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [2128/10336], Loss: 0.6382\n",
      "Epoch [5/5], Step [2130/10336], Loss: 0.5039\n",
      "Epoch [5/5], Step [2132/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [2134/10336], Loss: 2.1658\n",
      "Epoch [5/5], Step [2136/10336], Loss: 0.6126\n",
      "Epoch [5/5], Step [2138/10336], Loss: 0.2366\n",
      "Epoch [5/5], Step [2140/10336], Loss: 0.9599\n",
      "Epoch [5/5], Step [2142/10336], Loss: 0.2867\n",
      "Epoch [5/5], Step [2144/10336], Loss: 1.3198\n",
      "Epoch [5/5], Step [2146/10336], Loss: 0.0116\n",
      "Epoch [5/5], Step [2148/10336], Loss: 1.0928\n",
      "Epoch [5/5], Step [2150/10336], Loss: 0.1694\n",
      "Epoch [5/5], Step [2152/10336], Loss: 0.3134\n",
      "Epoch [5/5], Step [2154/10336], Loss: 0.0292\n",
      "Epoch [5/5], Step [2156/10336], Loss: 5.9940\n",
      "Epoch [5/5], Step [2158/10336], Loss: 1.4969\n",
      "Epoch [5/5], Step [2160/10336], Loss: 0.7367\n",
      "Epoch [5/5], Step [2162/10336], Loss: 2.5422\n",
      "Epoch [5/5], Step [2164/10336], Loss: 0.0302\n",
      "Epoch [5/5], Step [2166/10336], Loss: 0.0521\n",
      "Epoch [5/5], Step [2168/10336], Loss: 0.0092\n",
      "Epoch [5/5], Step [2170/10336], Loss: 0.5313\n",
      "Epoch [5/5], Step [2172/10336], Loss: 3.2687\n",
      "Epoch [5/5], Step [2174/10336], Loss: 0.3582\n",
      "Epoch [5/5], Step [2176/10336], Loss: 0.0804\n",
      "Epoch [5/5], Step [2178/10336], Loss: 0.0135\n",
      "Epoch [5/5], Step [2180/10336], Loss: 0.3090\n",
      "Epoch [5/5], Step [2182/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [2184/10336], Loss: 0.4076\n",
      "Epoch [5/5], Step [2186/10336], Loss: 0.4017\n",
      "Epoch [5/5], Step [2188/10336], Loss: 1.0355\n",
      "Epoch [5/5], Step [2190/10336], Loss: 0.5153\n",
      "Epoch [5/5], Step [2192/10336], Loss: 0.2011\n",
      "Epoch [5/5], Step [2194/10336], Loss: 0.0236\n",
      "Epoch [5/5], Step [2196/10336], Loss: 0.6243\n",
      "Epoch [5/5], Step [2198/10336], Loss: 0.0176\n",
      "Epoch [5/5], Step [2200/10336], Loss: 0.0341\n",
      "Epoch [5/5], Step [2202/10336], Loss: 2.3636\n",
      "Epoch [5/5], Step [2204/10336], Loss: 0.4420\n",
      "Epoch [5/5], Step [2206/10336], Loss: 0.0306\n",
      "Epoch [5/5], Step [2208/10336], Loss: 0.5845\n",
      "Epoch [5/5], Step [2210/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [2212/10336], Loss: 0.0075\n",
      "Epoch [5/5], Step [2214/10336], Loss: 0.0124\n",
      "Epoch [5/5], Step [2216/10336], Loss: 0.1848\n",
      "Epoch [5/5], Step [2218/10336], Loss: 0.4798\n",
      "Epoch [5/5], Step [2220/10336], Loss: 0.4504\n",
      "Epoch [5/5], Step [2222/10336], Loss: 0.0235\n",
      "Epoch [5/5], Step [2224/10336], Loss: 0.8723\n",
      "Epoch [5/5], Step [2226/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [2228/10336], Loss: 2.3841\n",
      "Epoch [5/5], Step [2230/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [2232/10336], Loss: 1.2611\n",
      "Epoch [5/5], Step [2234/10336], Loss: 0.1593\n",
      "Epoch [5/5], Step [2236/10336], Loss: 2.5342\n",
      "Epoch [5/5], Step [2238/10336], Loss: 0.2248\n",
      "Epoch [5/5], Step [2240/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [2242/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [2244/10336], Loss: 0.1846\n",
      "Epoch [5/5], Step [2246/10336], Loss: 0.2341\n",
      "Epoch [5/5], Step [2248/10336], Loss: 0.0298\n",
      "Epoch [5/5], Step [2250/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [2252/10336], Loss: 2.2240\n",
      "Epoch [5/5], Step [2254/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [2256/10336], Loss: 0.4168\n",
      "Epoch [5/5], Step [2258/10336], Loss: 0.4672\n",
      "Epoch [5/5], Step [2260/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [2262/10336], Loss: 0.3453\n",
      "Epoch [5/5], Step [2264/10336], Loss: 0.2946\n",
      "Epoch [5/5], Step [2266/10336], Loss: 0.8165\n",
      "Epoch [5/5], Step [2268/10336], Loss: 0.0226\n",
      "Epoch [5/5], Step [2270/10336], Loss: 0.1899\n",
      "Epoch [5/5], Step [2272/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [2274/10336], Loss: 2.1451\n",
      "Epoch [5/5], Step [2276/10336], Loss: 0.0167\n",
      "Epoch [5/5], Step [2278/10336], Loss: 1.1326\n",
      "Epoch [5/5], Step [2280/10336], Loss: 0.0176\n",
      "Epoch [5/5], Step [2282/10336], Loss: 1.9386\n",
      "Epoch [5/5], Step [2284/10336], Loss: 0.0280\n",
      "Epoch [5/5], Step [2286/10336], Loss: 0.0083\n",
      "Epoch [5/5], Step [2288/10336], Loss: 0.2736\n",
      "Epoch [5/5], Step [2290/10336], Loss: 0.4159\n",
      "Epoch [5/5], Step [2292/10336], Loss: 0.6108\n",
      "Epoch [5/5], Step [2294/10336], Loss: 0.2172\n",
      "Epoch [5/5], Step [2296/10336], Loss: 0.4703\n",
      "Epoch [5/5], Step [2298/10336], Loss: 1.4833\n",
      "Epoch [5/5], Step [2300/10336], Loss: 1.1230\n",
      "Epoch [5/5], Step [2302/10336], Loss: 0.0395\n",
      "Epoch [5/5], Step [2304/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [2306/10336], Loss: 0.5080\n",
      "Epoch [5/5], Step [2308/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [2310/10336], Loss: 1.8205\n",
      "Epoch [5/5], Step [2312/10336], Loss: 0.4443\n",
      "Epoch [5/5], Step [2314/10336], Loss: 1.5640\n",
      "Epoch [5/5], Step [2316/10336], Loss: 0.0491\n",
      "Epoch [5/5], Step [2318/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [2320/10336], Loss: 0.0034\n",
      "Epoch [5/5], Step [2322/10336], Loss: 0.1730\n",
      "Epoch [5/5], Step [2324/10336], Loss: 0.1627\n",
      "Epoch [5/5], Step [2326/10336], Loss: 0.0862\n",
      "Epoch [5/5], Step [2328/10336], Loss: 0.1322\n",
      "Epoch [5/5], Step [2330/10336], Loss: 2.0771\n",
      "Epoch [5/5], Step [2332/10336], Loss: 0.0215\n",
      "Epoch [5/5], Step [2334/10336], Loss: 1.3085\n",
      "Epoch [5/5], Step [2336/10336], Loss: 1.6866\n",
      "Epoch [5/5], Step [2338/10336], Loss: 0.3157\n",
      "Epoch [5/5], Step [2340/10336], Loss: 0.0217\n",
      "Epoch [5/5], Step [2342/10336], Loss: 0.0924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [2344/10336], Loss: 0.3924\n",
      "Epoch [5/5], Step [2346/10336], Loss: 0.9374\n",
      "Epoch [5/5], Step [2348/10336], Loss: 2.2931\n",
      "Epoch [5/5], Step [2350/10336], Loss: 0.4150\n",
      "Epoch [5/5], Step [2352/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [2354/10336], Loss: 1.0800\n",
      "Epoch [5/5], Step [2356/10336], Loss: 0.4872\n",
      "Epoch [5/5], Step [2358/10336], Loss: 0.0465\n",
      "Epoch [5/5], Step [2360/10336], Loss: 0.0076\n",
      "Epoch [5/5], Step [2362/10336], Loss: 1.4911\n",
      "Epoch [5/5], Step [2364/10336], Loss: 0.4276\n",
      "Epoch [5/5], Step [2366/10336], Loss: 0.0151\n",
      "Epoch [5/5], Step [2368/10336], Loss: 0.7555\n",
      "Epoch [5/5], Step [2370/10336], Loss: 0.3079\n",
      "Epoch [5/5], Step [2372/10336], Loss: 0.1160\n",
      "Epoch [5/5], Step [2374/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [2376/10336], Loss: 0.0504\n",
      "Epoch [5/5], Step [2378/10336], Loss: 0.0152\n",
      "Epoch [5/5], Step [2380/10336], Loss: 0.0910\n",
      "Epoch [5/5], Step [2382/10336], Loss: 0.6640\n",
      "Epoch [5/5], Step [2384/10336], Loss: 0.3842\n",
      "Epoch [5/5], Step [2386/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [2388/10336], Loss: 0.0745\n",
      "Epoch [5/5], Step [2390/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [2392/10336], Loss: 0.5177\n",
      "Epoch [5/5], Step [2394/10336], Loss: 0.8519\n",
      "Epoch [5/5], Step [2396/10336], Loss: 0.1154\n",
      "Epoch [5/5], Step [2398/10336], Loss: 0.4947\n",
      "Epoch [5/5], Step [2400/10336], Loss: 0.1108\n",
      "Epoch [5/5], Step [2402/10336], Loss: 1.2918\n",
      "Epoch [5/5], Step [2404/10336], Loss: 0.0285\n",
      "Epoch [5/5], Step [2406/10336], Loss: 0.7115\n",
      "Epoch [5/5], Step [2408/10336], Loss: 1.7129\n",
      "Epoch [5/5], Step [2410/10336], Loss: 0.1029\n",
      "Epoch [5/5], Step [2412/10336], Loss: 0.0557\n",
      "Epoch [5/5], Step [2414/10336], Loss: 0.0613\n",
      "Epoch [5/5], Step [2416/10336], Loss: 0.0641\n",
      "Epoch [5/5], Step [2418/10336], Loss: 1.3171\n",
      "Epoch [5/5], Step [2420/10336], Loss: 0.0603\n",
      "Epoch [5/5], Step [2422/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [2424/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [2426/10336], Loss: 0.0433\n",
      "Epoch [5/5], Step [2428/10336], Loss: 0.0412\n",
      "Epoch [5/5], Step [2430/10336], Loss: 0.4775\n",
      "Epoch [5/5], Step [2432/10336], Loss: 1.5949\n",
      "Epoch [5/5], Step [2434/10336], Loss: 0.1683\n",
      "Epoch [5/5], Step [2436/10336], Loss: 2.3911\n",
      "Epoch [5/5], Step [2438/10336], Loss: 0.0108\n",
      "Epoch [5/5], Step [2440/10336], Loss: 0.4883\n",
      "Epoch [5/5], Step [2442/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [2444/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [2446/10336], Loss: 1.1235\n",
      "Epoch [5/5], Step [2448/10336], Loss: 0.7189\n",
      "Epoch [5/5], Step [2450/10336], Loss: 0.0807\n",
      "Epoch [5/5], Step [2452/10336], Loss: 0.0488\n",
      "Epoch [5/5], Step [2454/10336], Loss: 0.1002\n",
      "Epoch [5/5], Step [2456/10336], Loss: 0.2173\n",
      "Epoch [5/5], Step [2458/10336], Loss: 0.7003\n",
      "Epoch [5/5], Step [2460/10336], Loss: 0.1717\n",
      "Epoch [5/5], Step [2462/10336], Loss: 0.1312\n",
      "Epoch [5/5], Step [2464/10336], Loss: 0.0310\n",
      "Epoch [5/5], Step [2466/10336], Loss: 0.1765\n",
      "Epoch [5/5], Step [2468/10336], Loss: 0.4638\n",
      "Epoch [5/5], Step [2470/10336], Loss: 0.0579\n",
      "Epoch [5/5], Step [2472/10336], Loss: 0.1628\n",
      "Epoch [5/5], Step [2474/10336], Loss: 0.2080\n",
      "Epoch [5/5], Step [2476/10336], Loss: 0.0720\n",
      "Epoch [5/5], Step [2478/10336], Loss: 0.1334\n",
      "Epoch [5/5], Step [2480/10336], Loss: 0.3162\n",
      "Epoch [5/5], Step [2482/10336], Loss: 2.4543\n",
      "Epoch [5/5], Step [2484/10336], Loss: 0.0107\n",
      "Epoch [5/5], Step [2486/10336], Loss: 0.0096\n",
      "Epoch [5/5], Step [2488/10336], Loss: 0.0501\n",
      "Epoch [5/5], Step [2490/10336], Loss: 1.0264\n",
      "Epoch [5/5], Step [2492/10336], Loss: 0.1687\n",
      "Epoch [5/5], Step [2494/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [2496/10336], Loss: 0.4424\n",
      "Epoch [5/5], Step [2498/10336], Loss: 0.1735\n",
      "Epoch [5/5], Step [2500/10336], Loss: 0.0181\n",
      "Epoch [5/5], Step [2502/10336], Loss: 0.0871\n",
      "Epoch [5/5], Step [2504/10336], Loss: 4.2601\n",
      "Epoch [5/5], Step [2506/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [2508/10336], Loss: 0.7211\n",
      "Epoch [5/5], Step [2510/10336], Loss: 0.4312\n",
      "Epoch [5/5], Step [2512/10336], Loss: 0.2906\n",
      "Epoch [5/5], Step [2514/10336], Loss: 0.0195\n",
      "Epoch [5/5], Step [2516/10336], Loss: 0.2581\n",
      "Epoch [5/5], Step [2518/10336], Loss: 0.0149\n",
      "Epoch [5/5], Step [2520/10336], Loss: 1.1400\n",
      "Epoch [5/5], Step [2522/10336], Loss: 0.3263\n",
      "Epoch [5/5], Step [2524/10336], Loss: 0.6070\n",
      "Epoch [5/5], Step [2526/10336], Loss: 1.3359\n",
      "Epoch [5/5], Step [2528/10336], Loss: 0.0171\n",
      "Epoch [5/5], Step [2530/10336], Loss: 0.0401\n",
      "Epoch [5/5], Step [2532/10336], Loss: 0.0988\n",
      "Epoch [5/5], Step [2534/10336], Loss: 0.2776\n",
      "Epoch [5/5], Step [2536/10336], Loss: 0.1510\n",
      "Epoch [5/5], Step [2538/10336], Loss: 1.3468\n",
      "Epoch [5/5], Step [2540/10336], Loss: 0.4530\n",
      "Epoch [5/5], Step [2542/10336], Loss: 0.2944\n",
      "Epoch [5/5], Step [2544/10336], Loss: 0.1100\n",
      "Epoch [5/5], Step [2546/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [2548/10336], Loss: 0.1834\n",
      "Epoch [5/5], Step [2550/10336], Loss: 0.1056\n",
      "Epoch [5/5], Step [2552/10336], Loss: 0.0512\n",
      "Epoch [5/5], Step [2554/10336], Loss: 0.6051\n",
      "Epoch [5/5], Step [2556/10336], Loss: 0.7343\n",
      "Epoch [5/5], Step [2558/10336], Loss: 0.0197\n",
      "Epoch [5/5], Step [2560/10336], Loss: 0.0338\n",
      "Epoch [5/5], Step [2562/10336], Loss: 0.1547\n",
      "Epoch [5/5], Step [2564/10336], Loss: 0.0180\n",
      "Epoch [5/5], Step [2566/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [2568/10336], Loss: 0.7532\n",
      "Epoch [5/5], Step [2570/10336], Loss: 0.0244\n",
      "Epoch [5/5], Step [2572/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [2574/10336], Loss: 0.0485\n",
      "Epoch [5/5], Step [2576/10336], Loss: 2.7172\n",
      "Epoch [5/5], Step [2578/10336], Loss: 0.3832\n",
      "Epoch [5/5], Step [2580/10336], Loss: 0.6644\n",
      "Epoch [5/5], Step [2582/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [2584/10336], Loss: 3.5052\n",
      "Epoch [5/5], Step [2586/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [2588/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [2590/10336], Loss: 0.4353\n",
      "Epoch [5/5], Step [2592/10336], Loss: 1.1057\n",
      "Epoch [5/5], Step [2594/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [2596/10336], Loss: 0.0277\n",
      "Epoch [5/5], Step [2598/10336], Loss: 0.1288\n",
      "Epoch [5/5], Step [2600/10336], Loss: 0.0277\n",
      "Epoch [5/5], Step [2602/10336], Loss: 0.0155\n",
      "Epoch [5/5], Step [2604/10336], Loss: 0.1299\n",
      "Epoch [5/5], Step [2606/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [2608/10336], Loss: 1.5890\n",
      "Epoch [5/5], Step [2610/10336], Loss: 0.0219\n",
      "Epoch [5/5], Step [2612/10336], Loss: 0.9063\n",
      "Epoch [5/5], Step [2614/10336], Loss: 0.8599\n",
      "Epoch [5/5], Step [2616/10336], Loss: 2.4114\n",
      "Epoch [5/5], Step [2618/10336], Loss: 0.0998\n",
      "Epoch [5/5], Step [2620/10336], Loss: 0.0522\n",
      "Epoch [5/5], Step [2622/10336], Loss: 0.2188\n",
      "Epoch [5/5], Step [2624/10336], Loss: 1.8391\n",
      "Epoch [5/5], Step [2626/10336], Loss: 0.5033\n",
      "Epoch [5/5], Step [2628/10336], Loss: 0.1070\n",
      "Epoch [5/5], Step [2630/10336], Loss: 0.8455\n",
      "Epoch [5/5], Step [2632/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [2634/10336], Loss: 0.0796\n",
      "Epoch [5/5], Step [2636/10336], Loss: 1.0527\n",
      "Epoch [5/5], Step [2638/10336], Loss: 2.1122\n",
      "Epoch [5/5], Step [2640/10336], Loss: 1.4221\n",
      "Epoch [5/5], Step [2642/10336], Loss: 0.0772\n",
      "Epoch [5/5], Step [2644/10336], Loss: 0.3587\n",
      "Epoch [5/5], Step [2646/10336], Loss: 0.3016\n",
      "Epoch [5/5], Step [2648/10336], Loss: 0.0230\n",
      "Epoch [5/5], Step [2650/10336], Loss: 0.1095\n",
      "Epoch [5/5], Step [2652/10336], Loss: 0.0429\n",
      "Epoch [5/5], Step [2654/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [2656/10336], Loss: 1.7029\n",
      "Epoch [5/5], Step [2658/10336], Loss: 0.0627\n",
      "Epoch [5/5], Step [2660/10336], Loss: 0.0114\n",
      "Epoch [5/5], Step [2662/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [2664/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [2666/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [2668/10336], Loss: 0.0684\n",
      "Epoch [5/5], Step [2670/10336], Loss: 0.0482\n",
      "Epoch [5/5], Step [2672/10336], Loss: 2.0971\n",
      "Epoch [5/5], Step [2674/10336], Loss: 0.7792\n",
      "Epoch [5/5], Step [2676/10336], Loss: 1.5086\n",
      "Epoch [5/5], Step [2678/10336], Loss: 0.0248\n",
      "Epoch [5/5], Step [2680/10336], Loss: 3.3250\n",
      "Epoch [5/5], Step [2682/10336], Loss: 0.1352\n",
      "Epoch [5/5], Step [2684/10336], Loss: 0.7277\n",
      "Epoch [5/5], Step [2686/10336], Loss: 0.0222\n",
      "Epoch [5/5], Step [2688/10336], Loss: 0.6944\n",
      "Epoch [5/5], Step [2690/10336], Loss: 1.3527\n",
      "Epoch [5/5], Step [2692/10336], Loss: 0.0884\n",
      "Epoch [5/5], Step [2694/10336], Loss: 0.6049\n",
      "Epoch [5/5], Step [2696/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [2698/10336], Loss: 0.1819\n",
      "Epoch [5/5], Step [2700/10336], Loss: 3.0133\n",
      "Epoch [5/5], Step [2702/10336], Loss: 0.2774\n",
      "Epoch [5/5], Step [2704/10336], Loss: 0.0208\n",
      "Epoch [5/5], Step [2706/10336], Loss: 0.0074\n",
      "Epoch [5/5], Step [2708/10336], Loss: 0.0070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [2710/10336], Loss: 3.0895\n",
      "Epoch [5/5], Step [2712/10336], Loss: 0.1793\n",
      "Epoch [5/5], Step [2714/10336], Loss: 1.1701\n",
      "Epoch [5/5], Step [2716/10336], Loss: 0.0159\n",
      "Epoch [5/5], Step [2718/10336], Loss: 2.2339\n",
      "Epoch [5/5], Step [2720/10336], Loss: 0.1408\n",
      "Epoch [5/5], Step [2722/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [2724/10336], Loss: 0.1382\n",
      "Epoch [5/5], Step [2726/10336], Loss: 0.0172\n",
      "Epoch [5/5], Step [2728/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [2730/10336], Loss: 0.1085\n",
      "Epoch [5/5], Step [2732/10336], Loss: 0.7828\n",
      "Epoch [5/5], Step [2734/10336], Loss: 0.4163\n",
      "Epoch [5/5], Step [2736/10336], Loss: 0.2057\n",
      "Epoch [5/5], Step [2738/10336], Loss: 0.0039\n",
      "Epoch [5/5], Step [2740/10336], Loss: 0.0466\n",
      "Epoch [5/5], Step [2742/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [2744/10336], Loss: 0.0163\n",
      "Epoch [5/5], Step [2746/10336], Loss: 0.0394\n",
      "Epoch [5/5], Step [2748/10336], Loss: 0.5664\n",
      "Epoch [5/5], Step [2750/10336], Loss: 1.0076\n",
      "Epoch [5/5], Step [2752/10336], Loss: 0.8250\n",
      "Epoch [5/5], Step [2754/10336], Loss: 0.0145\n",
      "Epoch [5/5], Step [2756/10336], Loss: 0.0872\n",
      "Epoch [5/5], Step [2758/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [2760/10336], Loss: 0.5572\n",
      "Epoch [5/5], Step [2762/10336], Loss: 0.2973\n",
      "Epoch [5/5], Step [2764/10336], Loss: 0.8853\n",
      "Epoch [5/5], Step [2766/10336], Loss: 0.0694\n",
      "Epoch [5/5], Step [2768/10336], Loss: 0.2539\n",
      "Epoch [5/5], Step [2770/10336], Loss: 1.2744\n",
      "Epoch [5/5], Step [2772/10336], Loss: 0.9171\n",
      "Epoch [5/5], Step [2774/10336], Loss: 1.0798\n",
      "Epoch [5/5], Step [2776/10336], Loss: 0.5687\n",
      "Epoch [5/5], Step [2778/10336], Loss: 0.0454\n",
      "Epoch [5/5], Step [2780/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [2782/10336], Loss: 0.3293\n",
      "Epoch [5/5], Step [2784/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [2786/10336], Loss: 0.1143\n",
      "Epoch [5/5], Step [2788/10336], Loss: 0.1391\n",
      "Epoch [5/5], Step [2790/10336], Loss: 0.2672\n",
      "Epoch [5/5], Step [2792/10336], Loss: 0.2687\n",
      "Epoch [5/5], Step [2794/10336], Loss: 0.0485\n",
      "Epoch [5/5], Step [2796/10336], Loss: 0.4048\n",
      "Epoch [5/5], Step [2798/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [2800/10336], Loss: 0.4733\n",
      "Epoch [5/5], Step [2802/10336], Loss: 0.2790\n",
      "Epoch [5/5], Step [2804/10336], Loss: 0.1965\n",
      "Epoch [5/5], Step [2806/10336], Loss: 1.0478\n",
      "Epoch [5/5], Step [2808/10336], Loss: 1.2548\n",
      "Epoch [5/5], Step [2810/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [2812/10336], Loss: 0.0941\n",
      "Epoch [5/5], Step [2814/10336], Loss: 0.9152\n",
      "Epoch [5/5], Step [2816/10336], Loss: 0.0239\n",
      "Epoch [5/5], Step [2818/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [2820/10336], Loss: 0.9989\n",
      "Epoch [5/5], Step [2822/10336], Loss: 0.0142\n",
      "Epoch [5/5], Step [2824/10336], Loss: 2.1912\n",
      "Epoch [5/5], Step [2826/10336], Loss: 0.1851\n",
      "Epoch [5/5], Step [2828/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [2830/10336], Loss: 2.4372\n",
      "Epoch [5/5], Step [2832/10336], Loss: 1.4653\n",
      "Epoch [5/5], Step [2834/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [2836/10336], Loss: 0.0262\n",
      "Epoch [5/5], Step [2838/10336], Loss: 0.1937\n",
      "Epoch [5/5], Step [2840/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [2842/10336], Loss: 1.3106\n",
      "Epoch [5/5], Step [2844/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [2846/10336], Loss: 0.2795\n",
      "Epoch [5/5], Step [2848/10336], Loss: 0.0308\n",
      "Epoch [5/5], Step [2850/10336], Loss: 1.0259\n",
      "Epoch [5/5], Step [2852/10336], Loss: 0.5868\n",
      "Epoch [5/5], Step [2854/10336], Loss: 0.0354\n",
      "Epoch [5/5], Step [2856/10336], Loss: 0.0127\n",
      "Epoch [5/5], Step [2858/10336], Loss: 0.3124\n",
      "Epoch [5/5], Step [2860/10336], Loss: 0.4313\n",
      "Epoch [5/5], Step [2862/10336], Loss: 0.0039\n",
      "Epoch [5/5], Step [2864/10336], Loss: 0.0094\n",
      "Epoch [5/5], Step [2866/10336], Loss: 0.0332\n",
      "Epoch [5/5], Step [2868/10336], Loss: 0.0291\n",
      "Epoch [5/5], Step [2870/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [2872/10336], Loss: 0.1514\n",
      "Epoch [5/5], Step [2874/10336], Loss: 2.1004\n",
      "Epoch [5/5], Step [2876/10336], Loss: 0.0953\n",
      "Epoch [5/5], Step [2878/10336], Loss: 2.1332\n",
      "Epoch [5/5], Step [2880/10336], Loss: 0.0498\n",
      "Epoch [5/5], Step [2882/10336], Loss: 0.0588\n",
      "Epoch [5/5], Step [2884/10336], Loss: 0.2686\n",
      "Epoch [5/5], Step [2886/10336], Loss: 2.5510\n",
      "Epoch [5/5], Step [2888/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [2890/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [2892/10336], Loss: 0.0646\n",
      "Epoch [5/5], Step [2894/10336], Loss: 0.2870\n",
      "Epoch [5/5], Step [2896/10336], Loss: 0.1401\n",
      "Epoch [5/5], Step [2898/10336], Loss: 0.4803\n",
      "Epoch [5/5], Step [2900/10336], Loss: 0.0218\n",
      "Epoch [5/5], Step [2902/10336], Loss: 0.3873\n",
      "Epoch [5/5], Step [2904/10336], Loss: 0.6082\n",
      "Epoch [5/5], Step [2906/10336], Loss: 0.1129\n",
      "Epoch [5/5], Step [2908/10336], Loss: 0.2284\n",
      "Epoch [5/5], Step [2910/10336], Loss: 1.3719\n",
      "Epoch [5/5], Step [2912/10336], Loss: 0.0098\n",
      "Epoch [5/5], Step [2914/10336], Loss: 0.3733\n",
      "Epoch [5/5], Step [2916/10336], Loss: 0.1483\n",
      "Epoch [5/5], Step [2918/10336], Loss: 0.4535\n",
      "Epoch [5/5], Step [2920/10336], Loss: 0.3309\n",
      "Epoch [5/5], Step [2922/10336], Loss: 0.9082\n",
      "Epoch [5/5], Step [2924/10336], Loss: 0.1286\n",
      "Epoch [5/5], Step [2926/10336], Loss: 0.0269\n",
      "Epoch [5/5], Step [2928/10336], Loss: 1.2348\n",
      "Epoch [5/5], Step [2930/10336], Loss: 0.6829\n",
      "Epoch [5/5], Step [2932/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [2934/10336], Loss: 0.1365\n",
      "Epoch [5/5], Step [2936/10336], Loss: 0.5525\n",
      "Epoch [5/5], Step [2938/10336], Loss: 0.0268\n",
      "Epoch [5/5], Step [2940/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [2942/10336], Loss: 0.0138\n",
      "Epoch [5/5], Step [2944/10336], Loss: 1.1443\n",
      "Epoch [5/5], Step [2946/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [2948/10336], Loss: 0.4774\n",
      "Epoch [5/5], Step [2950/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [2952/10336], Loss: 3.2015\n",
      "Epoch [5/5], Step [2954/10336], Loss: 0.7591\n",
      "Epoch [5/5], Step [2956/10336], Loss: 0.0481\n",
      "Epoch [5/5], Step [2958/10336], Loss: 0.5411\n",
      "Epoch [5/5], Step [2960/10336], Loss: 0.3826\n",
      "Epoch [5/5], Step [2962/10336], Loss: 0.0775\n",
      "Epoch [5/5], Step [2964/10336], Loss: 2.9064\n",
      "Epoch [5/5], Step [2966/10336], Loss: 3.1331\n",
      "Epoch [5/5], Step [2968/10336], Loss: 0.4386\n",
      "Epoch [5/5], Step [2970/10336], Loss: 0.0181\n",
      "Epoch [5/5], Step [2972/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [2974/10336], Loss: 2.0315\n",
      "Epoch [5/5], Step [2976/10336], Loss: 0.9225\n",
      "Epoch [5/5], Step [2978/10336], Loss: 0.0417\n",
      "Epoch [5/5], Step [2980/10336], Loss: 0.0128\n",
      "Epoch [5/5], Step [2982/10336], Loss: 1.0181\n",
      "Epoch [5/5], Step [2984/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [2986/10336], Loss: 0.0249\n",
      "Epoch [5/5], Step [2988/10336], Loss: 1.9215\n",
      "Epoch [5/5], Step [2990/10336], Loss: 0.0575\n",
      "Epoch [5/5], Step [2992/10336], Loss: 0.0350\n",
      "Epoch [5/5], Step [2994/10336], Loss: 0.4030\n",
      "Epoch [5/5], Step [2996/10336], Loss: 0.0215\n",
      "Epoch [5/5], Step [2998/10336], Loss: 1.9842\n",
      "Epoch [5/5], Step [3000/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [3002/10336], Loss: 0.2748\n",
      "Epoch [5/5], Step [3004/10336], Loss: 0.7453\n",
      "Epoch [5/5], Step [3006/10336], Loss: 0.0127\n",
      "Epoch [5/5], Step [3008/10336], Loss: 1.3937\n",
      "Epoch [5/5], Step [3010/10336], Loss: 0.0055\n",
      "Epoch [5/5], Step [3012/10336], Loss: 2.2587\n",
      "Epoch [5/5], Step [3014/10336], Loss: 0.0331\n",
      "Epoch [5/5], Step [3016/10336], Loss: 0.0111\n",
      "Epoch [5/5], Step [3018/10336], Loss: 1.0104\n",
      "Epoch [5/5], Step [3020/10336], Loss: 1.5191\n",
      "Epoch [5/5], Step [3022/10336], Loss: 0.2119\n",
      "Epoch [5/5], Step [3024/10336], Loss: 1.7646\n",
      "Epoch [5/5], Step [3026/10336], Loss: 0.1108\n",
      "Epoch [5/5], Step [3028/10336], Loss: 0.2971\n",
      "Epoch [5/5], Step [3030/10336], Loss: 2.3604\n",
      "Epoch [5/5], Step [3032/10336], Loss: 0.2608\n",
      "Epoch [5/5], Step [3034/10336], Loss: 0.0095\n",
      "Epoch [5/5], Step [3036/10336], Loss: 0.8423\n",
      "Epoch [5/5], Step [3038/10336], Loss: 0.0399\n",
      "Epoch [5/5], Step [3040/10336], Loss: 0.3162\n",
      "Epoch [5/5], Step [3042/10336], Loss: 4.3071\n",
      "Epoch [5/5], Step [3044/10336], Loss: 0.6599\n",
      "Epoch [5/5], Step [3046/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [3048/10336], Loss: 0.6724\n",
      "Epoch [5/5], Step [3050/10336], Loss: 0.5250\n",
      "Epoch [5/5], Step [3052/10336], Loss: 0.5132\n",
      "Epoch [5/5], Step [3054/10336], Loss: 0.1410\n",
      "Epoch [5/5], Step [3056/10336], Loss: 2.1042\n",
      "Epoch [5/5], Step [3058/10336], Loss: 0.2488\n",
      "Epoch [5/5], Step [3060/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [3062/10336], Loss: 0.3630\n",
      "Epoch [5/5], Step [3064/10336], Loss: 0.5466\n",
      "Epoch [5/5], Step [3066/10336], Loss: 0.0118\n",
      "Epoch [5/5], Step [3068/10336], Loss: 1.8985\n",
      "Epoch [5/5], Step [3070/10336], Loss: 0.0206\n",
      "Epoch [5/5], Step [3072/10336], Loss: 0.0225\n",
      "Epoch [5/5], Step [3074/10336], Loss: 0.0612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [3076/10336], Loss: 0.0216\n",
      "Epoch [5/5], Step [3078/10336], Loss: 0.0524\n",
      "Epoch [5/5], Step [3080/10336], Loss: 0.0113\n",
      "Epoch [5/5], Step [3082/10336], Loss: 1.3271\n",
      "Epoch [5/5], Step [3084/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [3086/10336], Loss: 0.2778\n",
      "Epoch [5/5], Step [3088/10336], Loss: 0.1374\n",
      "Epoch [5/5], Step [3090/10336], Loss: 0.5307\n",
      "Epoch [5/5], Step [3092/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [3094/10336], Loss: 0.9422\n",
      "Epoch [5/5], Step [3096/10336], Loss: 0.2812\n",
      "Epoch [5/5], Step [3098/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [3100/10336], Loss: 2.2584\n",
      "Epoch [5/5], Step [3102/10336], Loss: 0.2934\n",
      "Epoch [5/5], Step [3104/10336], Loss: 0.5993\n",
      "Epoch [5/5], Step [3106/10336], Loss: 0.2272\n",
      "Epoch [5/5], Step [3108/10336], Loss: 1.2555\n",
      "Epoch [5/5], Step [3110/10336], Loss: 0.0437\n",
      "Epoch [5/5], Step [3112/10336], Loss: 0.0879\n",
      "Epoch [5/5], Step [3114/10336], Loss: 0.0090\n",
      "Epoch [5/5], Step [3116/10336], Loss: 0.2338\n",
      "Epoch [5/5], Step [3118/10336], Loss: 0.4783\n",
      "Epoch [5/5], Step [3120/10336], Loss: 0.0140\n",
      "Epoch [5/5], Step [3122/10336], Loss: 0.0096\n",
      "Epoch [5/5], Step [3124/10336], Loss: 0.8244\n",
      "Epoch [5/5], Step [3126/10336], Loss: 0.3870\n",
      "Epoch [5/5], Step [3128/10336], Loss: 0.0362\n",
      "Epoch [5/5], Step [3130/10336], Loss: 1.1213\n",
      "Epoch [5/5], Step [3132/10336], Loss: 0.2313\n",
      "Epoch [5/5], Step [3134/10336], Loss: 0.3915\n",
      "Epoch [5/5], Step [3136/10336], Loss: 2.8900\n",
      "Epoch [5/5], Step [3138/10336], Loss: 2.4893\n",
      "Epoch [5/5], Step [3140/10336], Loss: 0.3005\n",
      "Epoch [5/5], Step [3142/10336], Loss: 0.0162\n",
      "Epoch [5/5], Step [3144/10336], Loss: 0.1159\n",
      "Epoch [5/5], Step [3146/10336], Loss: 0.0201\n",
      "Epoch [5/5], Step [3148/10336], Loss: 0.0151\n",
      "Epoch [5/5], Step [3150/10336], Loss: 0.0104\n",
      "Epoch [5/5], Step [3152/10336], Loss: 1.0837\n",
      "Epoch [5/5], Step [3154/10336], Loss: 0.0531\n",
      "Epoch [5/5], Step [3156/10336], Loss: 0.1315\n",
      "Epoch [5/5], Step [3158/10336], Loss: 0.1695\n",
      "Epoch [5/5], Step [3160/10336], Loss: 0.0178\n",
      "Epoch [5/5], Step [3162/10336], Loss: 0.0571\n",
      "Epoch [5/5], Step [3164/10336], Loss: 0.1188\n",
      "Epoch [5/5], Step [3166/10336], Loss: 0.0626\n",
      "Epoch [5/5], Step [3168/10336], Loss: 1.5825\n",
      "Epoch [5/5], Step [3170/10336], Loss: 0.1213\n",
      "Epoch [5/5], Step [3172/10336], Loss: 2.3359\n",
      "Epoch [5/5], Step [3174/10336], Loss: 1.5837\n",
      "Epoch [5/5], Step [3176/10336], Loss: 1.2015\n",
      "Epoch [5/5], Step [3178/10336], Loss: 0.5062\n",
      "Epoch [5/5], Step [3180/10336], Loss: 0.3726\n",
      "Epoch [5/5], Step [3182/10336], Loss: 0.0391\n",
      "Epoch [5/5], Step [3184/10336], Loss: 0.0055\n",
      "Epoch [5/5], Step [3186/10336], Loss: 0.1104\n",
      "Epoch [5/5], Step [3188/10336], Loss: 2.2577\n",
      "Epoch [5/5], Step [3190/10336], Loss: 0.3040\n",
      "Epoch [5/5], Step [3192/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [3194/10336], Loss: 0.2232\n",
      "Epoch [5/5], Step [3196/10336], Loss: 1.4983\n",
      "Epoch [5/5], Step [3198/10336], Loss: 0.1287\n",
      "Epoch [5/5], Step [3200/10336], Loss: 0.2293\n",
      "Epoch [5/5], Step [3202/10336], Loss: 0.8789\n",
      "Epoch [5/5], Step [3204/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [3206/10336], Loss: 0.0917\n",
      "Epoch [5/5], Step [3208/10336], Loss: 1.1310\n",
      "Epoch [5/5], Step [3210/10336], Loss: 1.0179\n",
      "Epoch [5/5], Step [3212/10336], Loss: 0.1358\n",
      "Epoch [5/5], Step [3214/10336], Loss: 0.2813\n",
      "Epoch [5/5], Step [3216/10336], Loss: 3.4008\n",
      "Epoch [5/5], Step [3218/10336], Loss: 0.8694\n",
      "Epoch [5/5], Step [3220/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [3222/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [3224/10336], Loss: 1.2404\n",
      "Epoch [5/5], Step [3226/10336], Loss: 0.1254\n",
      "Epoch [5/5], Step [3228/10336], Loss: 0.9870\n",
      "Epoch [5/5], Step [3230/10336], Loss: 0.2016\n",
      "Epoch [5/5], Step [3232/10336], Loss: 0.1144\n",
      "Epoch [5/5], Step [3234/10336], Loss: 0.5093\n",
      "Epoch [5/5], Step [3236/10336], Loss: 0.7046\n",
      "Epoch [5/5], Step [3238/10336], Loss: 0.1165\n",
      "Epoch [5/5], Step [3240/10336], Loss: 0.0141\n",
      "Epoch [5/5], Step [3242/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [3244/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [3246/10336], Loss: 1.4319\n",
      "Epoch [5/5], Step [3248/10336], Loss: 0.0818\n",
      "Epoch [5/5], Step [3250/10336], Loss: 1.2499\n",
      "Epoch [5/5], Step [3252/10336], Loss: 0.7684\n",
      "Epoch [5/5], Step [3254/10336], Loss: 0.0802\n",
      "Epoch [5/5], Step [3256/10336], Loss: 0.0138\n",
      "Epoch [5/5], Step [3258/10336], Loss: 0.0430\n",
      "Epoch [5/5], Step [3260/10336], Loss: 0.0885\n",
      "Epoch [5/5], Step [3262/10336], Loss: 0.8730\n",
      "Epoch [5/5], Step [3264/10336], Loss: 1.6914\n",
      "Epoch [5/5], Step [3266/10336], Loss: 0.1171\n",
      "Epoch [5/5], Step [3268/10336], Loss: 1.5778\n",
      "Epoch [5/5], Step [3270/10336], Loss: 3.1129\n",
      "Epoch [5/5], Step [3272/10336], Loss: 0.1487\n",
      "Epoch [5/5], Step [3274/10336], Loss: 0.2109\n",
      "Epoch [5/5], Step [3276/10336], Loss: 0.6962\n",
      "Epoch [5/5], Step [3278/10336], Loss: 0.1557\n",
      "Epoch [5/5], Step [3280/10336], Loss: 2.9581\n",
      "Epoch [5/5], Step [3282/10336], Loss: 0.0158\n",
      "Epoch [5/5], Step [3284/10336], Loss: 0.0190\n",
      "Epoch [5/5], Step [3286/10336], Loss: 0.9380\n",
      "Epoch [5/5], Step [3288/10336], Loss: 4.0548\n",
      "Epoch [5/5], Step [3290/10336], Loss: 0.0295\n",
      "Epoch [5/5], Step [3292/10336], Loss: 0.0095\n",
      "Epoch [5/5], Step [3294/10336], Loss: 0.5954\n",
      "Epoch [5/5], Step [3296/10336], Loss: 0.1916\n",
      "Epoch [5/5], Step [3298/10336], Loss: 0.0326\n",
      "Epoch [5/5], Step [3300/10336], Loss: 0.4834\n",
      "Epoch [5/5], Step [3302/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [3304/10336], Loss: 0.0699\n",
      "Epoch [5/5], Step [3306/10336], Loss: 1.5166\n",
      "Epoch [5/5], Step [3308/10336], Loss: 0.2523\n",
      "Epoch [5/5], Step [3310/10336], Loss: 0.2203\n",
      "Epoch [5/5], Step [3312/10336], Loss: 1.0817\n",
      "Epoch [5/5], Step [3314/10336], Loss: 0.0544\n",
      "Epoch [5/5], Step [3316/10336], Loss: 2.0637\n",
      "Epoch [5/5], Step [3318/10336], Loss: 0.7208\n",
      "Epoch [5/5], Step [3320/10336], Loss: 0.0167\n",
      "Epoch [5/5], Step [3322/10336], Loss: 0.1893\n",
      "Epoch [5/5], Step [3324/10336], Loss: 0.9503\n",
      "Epoch [5/5], Step [3326/10336], Loss: 0.7684\n",
      "Epoch [5/5], Step [3328/10336], Loss: 0.7229\n",
      "Epoch [5/5], Step [3330/10336], Loss: 0.1586\n",
      "Epoch [5/5], Step [3332/10336], Loss: 1.0097\n",
      "Epoch [5/5], Step [3334/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [3336/10336], Loss: 3.2604\n",
      "Epoch [5/5], Step [3338/10336], Loss: 0.0241\n",
      "Epoch [5/5], Step [3340/10336], Loss: 0.3318\n",
      "Epoch [5/5], Step [3342/10336], Loss: 0.0616\n",
      "Epoch [5/5], Step [3344/10336], Loss: 0.1656\n",
      "Epoch [5/5], Step [3346/10336], Loss: 0.3427\n",
      "Epoch [5/5], Step [3348/10336], Loss: 1.3067\n",
      "Epoch [5/5], Step [3350/10336], Loss: 0.0383\n",
      "Epoch [5/5], Step [3352/10336], Loss: 0.2202\n",
      "Epoch [5/5], Step [3354/10336], Loss: 0.4763\n",
      "Epoch [5/5], Step [3356/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [3358/10336], Loss: 0.1095\n",
      "Epoch [5/5], Step [3360/10336], Loss: 0.1217\n",
      "Epoch [5/5], Step [3362/10336], Loss: 0.0283\n",
      "Epoch [5/5], Step [3364/10336], Loss: 1.7992\n",
      "Epoch [5/5], Step [3366/10336], Loss: 0.0731\n",
      "Epoch [5/5], Step [3368/10336], Loss: 0.7342\n",
      "Epoch [5/5], Step [3370/10336], Loss: 0.9326\n",
      "Epoch [5/5], Step [3372/10336], Loss: 1.0173\n",
      "Epoch [5/5], Step [3374/10336], Loss: 1.6709\n",
      "Epoch [5/5], Step [3376/10336], Loss: 0.3945\n",
      "Epoch [5/5], Step [3378/10336], Loss: 1.4380\n",
      "Epoch [5/5], Step [3380/10336], Loss: 1.3330\n",
      "Epoch [5/5], Step [3382/10336], Loss: 1.3084\n",
      "Epoch [5/5], Step [3384/10336], Loss: 0.1114\n",
      "Epoch [5/5], Step [3386/10336], Loss: 0.9361\n",
      "Epoch [5/5], Step [3388/10336], Loss: 1.3737\n",
      "Epoch [5/5], Step [3390/10336], Loss: 0.1318\n",
      "Epoch [5/5], Step [3392/10336], Loss: 0.0060\n",
      "Epoch [5/5], Step [3394/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [3396/10336], Loss: 0.1541\n",
      "Epoch [5/5], Step [3398/10336], Loss: 0.0096\n",
      "Epoch [5/5], Step [3400/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [3402/10336], Loss: 0.1881\n",
      "Epoch [5/5], Step [3404/10336], Loss: 0.4853\n",
      "Epoch [5/5], Step [3406/10336], Loss: 0.3401\n",
      "Epoch [5/5], Step [3408/10336], Loss: 0.5375\n",
      "Epoch [5/5], Step [3410/10336], Loss: 0.0171\n",
      "Epoch [5/5], Step [3412/10336], Loss: 0.0362\n",
      "Epoch [5/5], Step [3414/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [3416/10336], Loss: 1.3089\n",
      "Epoch [5/5], Step [3418/10336], Loss: 0.2436\n",
      "Epoch [5/5], Step [3420/10336], Loss: 1.2571\n",
      "Epoch [5/5], Step [3422/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [3424/10336], Loss: 0.2052\n",
      "Epoch [5/5], Step [3426/10336], Loss: 0.4991\n",
      "Epoch [5/5], Step [3428/10336], Loss: 0.0229\n",
      "Epoch [5/5], Step [3430/10336], Loss: 1.2343\n",
      "Epoch [5/5], Step [3432/10336], Loss: 0.5463\n",
      "Epoch [5/5], Step [3434/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [3436/10336], Loss: 0.1372\n",
      "Epoch [5/5], Step [3438/10336], Loss: 0.0063\n",
      "Epoch [5/5], Step [3440/10336], Loss: 0.0038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [3442/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [3444/10336], Loss: 0.0243\n",
      "Epoch [5/5], Step [3446/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [3448/10336], Loss: 0.8511\n",
      "Epoch [5/5], Step [3450/10336], Loss: 1.2082\n",
      "Epoch [5/5], Step [3452/10336], Loss: 1.0368\n",
      "Epoch [5/5], Step [3454/10336], Loss: 0.0108\n",
      "Epoch [5/5], Step [3456/10336], Loss: 0.4973\n",
      "Epoch [5/5], Step [3458/10336], Loss: 0.0440\n",
      "Epoch [5/5], Step [3460/10336], Loss: 0.0233\n",
      "Epoch [5/5], Step [3462/10336], Loss: 0.2288\n",
      "Epoch [5/5], Step [3464/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [3466/10336], Loss: 0.0396\n",
      "Epoch [5/5], Step [3468/10336], Loss: 1.4858\n",
      "Epoch [5/5], Step [3470/10336], Loss: 0.1135\n",
      "Epoch [5/5], Step [3472/10336], Loss: 1.1556\n",
      "Epoch [5/5], Step [3474/10336], Loss: 0.2091\n",
      "Epoch [5/5], Step [3476/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [3478/10336], Loss: 0.0083\n",
      "Epoch [5/5], Step [3480/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [3482/10336], Loss: 1.5319\n",
      "Epoch [5/5], Step [3484/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [3486/10336], Loss: 0.3556\n",
      "Epoch [5/5], Step [3488/10336], Loss: 0.0863\n",
      "Epoch [5/5], Step [3490/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [3492/10336], Loss: 0.8284\n",
      "Epoch [5/5], Step [3494/10336], Loss: 1.7350\n",
      "Epoch [5/5], Step [3496/10336], Loss: 0.0886\n",
      "Epoch [5/5], Step [3498/10336], Loss: 0.0136\n",
      "Epoch [5/5], Step [3500/10336], Loss: 0.0078\n",
      "Epoch [5/5], Step [3502/10336], Loss: 1.2480\n",
      "Epoch [5/5], Step [3504/10336], Loss: 0.0266\n",
      "Epoch [5/5], Step [3506/10336], Loss: 0.0634\n",
      "Epoch [5/5], Step [3508/10336], Loss: 0.3232\n",
      "Epoch [5/5], Step [3510/10336], Loss: 0.2767\n",
      "Epoch [5/5], Step [3512/10336], Loss: 0.2980\n",
      "Epoch [5/5], Step [3514/10336], Loss: 0.8840\n",
      "Epoch [5/5], Step [3516/10336], Loss: 0.8323\n",
      "Epoch [5/5], Step [3518/10336], Loss: 1.1901\n",
      "Epoch [5/5], Step [3520/10336], Loss: 0.0220\n",
      "Epoch [5/5], Step [3522/10336], Loss: 0.0700\n",
      "Epoch [5/5], Step [3524/10336], Loss: 0.1216\n",
      "Epoch [5/5], Step [3526/10336], Loss: 0.4330\n",
      "Epoch [5/5], Step [3528/10336], Loss: 0.1984\n",
      "Epoch [5/5], Step [3530/10336], Loss: 0.0808\n",
      "Epoch [5/5], Step [3532/10336], Loss: 1.1533\n",
      "Epoch [5/5], Step [3534/10336], Loss: 0.0074\n",
      "Epoch [5/5], Step [3536/10336], Loss: 0.9313\n",
      "Epoch [5/5], Step [3538/10336], Loss: 0.0216\n",
      "Epoch [5/5], Step [3540/10336], Loss: 0.2885\n",
      "Epoch [5/5], Step [3542/10336], Loss: 0.4669\n",
      "Epoch [5/5], Step [3544/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [3546/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [3548/10336], Loss: 0.2892\n",
      "Epoch [5/5], Step [3550/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [3552/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [3554/10336], Loss: 0.0066\n",
      "Epoch [5/5], Step [3556/10336], Loss: 0.0474\n",
      "Epoch [5/5], Step [3558/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [3560/10336], Loss: 0.2287\n",
      "Epoch [5/5], Step [3562/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [3564/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [3566/10336], Loss: 0.4363\n",
      "Epoch [5/5], Step [3568/10336], Loss: 0.6960\n",
      "Epoch [5/5], Step [3570/10336], Loss: 0.9680\n",
      "Epoch [5/5], Step [3572/10336], Loss: 1.1346\n",
      "Epoch [5/5], Step [3574/10336], Loss: 5.1088\n",
      "Epoch [5/5], Step [3576/10336], Loss: 0.9278\n",
      "Epoch [5/5], Step [3578/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [3580/10336], Loss: 0.0991\n",
      "Epoch [5/5], Step [3582/10336], Loss: 0.9923\n",
      "Epoch [5/5], Step [3584/10336], Loss: 0.1318\n",
      "Epoch [5/5], Step [3586/10336], Loss: 0.7292\n",
      "Epoch [5/5], Step [3588/10336], Loss: 0.0314\n",
      "Epoch [5/5], Step [3590/10336], Loss: 2.2364\n",
      "Epoch [5/5], Step [3592/10336], Loss: 0.0618\n",
      "Epoch [5/5], Step [3594/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [3596/10336], Loss: 0.0104\n",
      "Epoch [5/5], Step [3598/10336], Loss: 0.2888\n",
      "Epoch [5/5], Step [3600/10336], Loss: 2.3677\n",
      "Epoch [5/5], Step [3602/10336], Loss: 0.4624\n",
      "Epoch [5/5], Step [3604/10336], Loss: 0.7247\n",
      "Epoch [5/5], Step [3606/10336], Loss: 0.2704\n",
      "Epoch [5/5], Step [3608/10336], Loss: 1.0302\n",
      "Epoch [5/5], Step [3610/10336], Loss: 0.2695\n",
      "Epoch [5/5], Step [3612/10336], Loss: 0.3362\n",
      "Epoch [5/5], Step [3614/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [3616/10336], Loss: 0.7285\n",
      "Epoch [5/5], Step [3618/10336], Loss: 0.9488\n",
      "Epoch [5/5], Step [3620/10336], Loss: 0.2761\n",
      "Epoch [5/5], Step [3622/10336], Loss: 1.4971\n",
      "Epoch [5/5], Step [3624/10336], Loss: 0.0338\n",
      "Epoch [5/5], Step [3626/10336], Loss: 0.2671\n",
      "Epoch [5/5], Step [3628/10336], Loss: 1.9312\n",
      "Epoch [5/5], Step [3630/10336], Loss: 0.5678\n",
      "Epoch [5/5], Step [3632/10336], Loss: 0.2747\n",
      "Epoch [5/5], Step [3634/10336], Loss: 0.0392\n",
      "Epoch [5/5], Step [3636/10336], Loss: 0.7913\n",
      "Epoch [5/5], Step [3638/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [3640/10336], Loss: 0.0801\n",
      "Epoch [5/5], Step [3642/10336], Loss: 0.0053\n",
      "Epoch [5/5], Step [3644/10336], Loss: 1.1098\n",
      "Epoch [5/5], Step [3646/10336], Loss: 0.0164\n",
      "Epoch [5/5], Step [3648/10336], Loss: 0.0212\n",
      "Epoch [5/5], Step [3650/10336], Loss: 0.0547\n",
      "Epoch [5/5], Step [3652/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [3654/10336], Loss: 0.0219\n",
      "Epoch [5/5], Step [3656/10336], Loss: 0.6950\n",
      "Epoch [5/5], Step [3658/10336], Loss: 0.8242\n",
      "Epoch [5/5], Step [3660/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [3662/10336], Loss: 0.9233\n",
      "Epoch [5/5], Step [3664/10336], Loss: 0.3815\n",
      "Epoch [5/5], Step [3666/10336], Loss: 0.0341\n",
      "Epoch [5/5], Step [3668/10336], Loss: 1.6942\n",
      "Epoch [5/5], Step [3670/10336], Loss: 0.0803\n",
      "Epoch [5/5], Step [3672/10336], Loss: 0.6172\n",
      "Epoch [5/5], Step [3674/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [3676/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [3678/10336], Loss: 0.0744\n",
      "Epoch [5/5], Step [3680/10336], Loss: 0.0682\n",
      "Epoch [5/5], Step [3682/10336], Loss: 0.4303\n",
      "Epoch [5/5], Step [3684/10336], Loss: 1.2232\n",
      "Epoch [5/5], Step [3686/10336], Loss: 0.0150\n",
      "Epoch [5/5], Step [3688/10336], Loss: 0.3518\n",
      "Epoch [5/5], Step [3690/10336], Loss: 0.4517\n",
      "Epoch [5/5], Step [3692/10336], Loss: 0.1131\n",
      "Epoch [5/5], Step [3694/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [3696/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [3698/10336], Loss: 0.1205\n",
      "Epoch [5/5], Step [3700/10336], Loss: 0.9675\n",
      "Epoch [5/5], Step [3702/10336], Loss: 1.5444\n",
      "Epoch [5/5], Step [3704/10336], Loss: 0.9253\n",
      "Epoch [5/5], Step [3706/10336], Loss: 0.0660\n",
      "Epoch [5/5], Step [3708/10336], Loss: 0.1037\n",
      "Epoch [5/5], Step [3710/10336], Loss: 0.0188\n",
      "Epoch [5/5], Step [3712/10336], Loss: 1.3155\n",
      "Epoch [5/5], Step [3714/10336], Loss: 0.0375\n",
      "Epoch [5/5], Step [3716/10336], Loss: 0.0228\n",
      "Epoch [5/5], Step [3718/10336], Loss: 0.0264\n",
      "Epoch [5/5], Step [3720/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [3722/10336], Loss: 0.1302\n",
      "Epoch [5/5], Step [3724/10336], Loss: 0.0917\n",
      "Epoch [5/5], Step [3726/10336], Loss: 0.0157\n",
      "Epoch [5/5], Step [3728/10336], Loss: 0.0482\n",
      "Epoch [5/5], Step [3730/10336], Loss: 1.6727\n",
      "Epoch [5/5], Step [3732/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [3734/10336], Loss: 0.8625\n",
      "Epoch [5/5], Step [3736/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [3738/10336], Loss: 0.0546\n",
      "Epoch [5/5], Step [3740/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [3742/10336], Loss: 0.1553\n",
      "Epoch [5/5], Step [3744/10336], Loss: 0.7265\n",
      "Epoch [5/5], Step [3746/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [3748/10336], Loss: 0.2373\n",
      "Epoch [5/5], Step [3750/10336], Loss: 0.0952\n",
      "Epoch [5/5], Step [3752/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [3754/10336], Loss: 0.0517\n",
      "Epoch [5/5], Step [3756/10336], Loss: 0.0346\n",
      "Epoch [5/5], Step [3758/10336], Loss: 0.8983\n",
      "Epoch [5/5], Step [3760/10336], Loss: 1.1119\n",
      "Epoch [5/5], Step [3762/10336], Loss: 3.5364\n",
      "Epoch [5/5], Step [3764/10336], Loss: 1.1262\n",
      "Epoch [5/5], Step [3766/10336], Loss: 0.1428\n",
      "Epoch [5/5], Step [3768/10336], Loss: 0.0551\n",
      "Epoch [5/5], Step [3770/10336], Loss: 0.0618\n",
      "Epoch [5/5], Step [3772/10336], Loss: 0.1557\n",
      "Epoch [5/5], Step [3774/10336], Loss: 0.9338\n",
      "Epoch [5/5], Step [3776/10336], Loss: 0.1399\n",
      "Epoch [5/5], Step [3778/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [3780/10336], Loss: 0.0121\n",
      "Epoch [5/5], Step [3782/10336], Loss: 0.2925\n",
      "Epoch [5/5], Step [3784/10336], Loss: 0.3947\n",
      "Epoch [5/5], Step [3786/10336], Loss: 0.0178\n",
      "Epoch [5/5], Step [3788/10336], Loss: 0.2065\n",
      "Epoch [5/5], Step [3790/10336], Loss: 0.0145\n",
      "Epoch [5/5], Step [3792/10336], Loss: 0.0379\n",
      "Epoch [5/5], Step [3794/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [3796/10336], Loss: 1.5582\n",
      "Epoch [5/5], Step [3798/10336], Loss: 1.7257\n",
      "Epoch [5/5], Step [3800/10336], Loss: 0.0519\n",
      "Epoch [5/5], Step [3802/10336], Loss: 3.4337\n",
      "Epoch [5/5], Step [3804/10336], Loss: 0.0121\n",
      "Epoch [5/5], Step [3806/10336], Loss: 3.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [3808/10336], Loss: 0.1199\n",
      "Epoch [5/5], Step [3810/10336], Loss: 2.9286\n",
      "Epoch [5/5], Step [3812/10336], Loss: 0.0219\n",
      "Epoch [5/5], Step [3814/10336], Loss: 0.1212\n",
      "Epoch [5/5], Step [3816/10336], Loss: 0.1366\n",
      "Epoch [5/5], Step [3818/10336], Loss: 0.4495\n",
      "Epoch [5/5], Step [3820/10336], Loss: 2.7806\n",
      "Epoch [5/5], Step [3822/10336], Loss: 0.0311\n",
      "Epoch [5/5], Step [3824/10336], Loss: 0.0746\n",
      "Epoch [5/5], Step [3826/10336], Loss: 3.2773\n",
      "Epoch [5/5], Step [3828/10336], Loss: 0.5774\n",
      "Epoch [5/5], Step [3830/10336], Loss: 0.0512\n",
      "Epoch [5/5], Step [3832/10336], Loss: 0.0351\n",
      "Epoch [5/5], Step [3834/10336], Loss: 3.9677\n",
      "Epoch [5/5], Step [3836/10336], Loss: 0.1316\n",
      "Epoch [5/5], Step [3838/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [3840/10336], Loss: 2.5111\n",
      "Epoch [5/5], Step [3842/10336], Loss: 0.0449\n",
      "Epoch [5/5], Step [3844/10336], Loss: 0.2752\n",
      "Epoch [5/5], Step [3846/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [3848/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [3850/10336], Loss: 0.4979\n",
      "Epoch [5/5], Step [3852/10336], Loss: 0.5162\n",
      "Epoch [5/5], Step [3854/10336], Loss: 1.8708\n",
      "Epoch [5/5], Step [3856/10336], Loss: 1.3079\n",
      "Epoch [5/5], Step [3858/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [3860/10336], Loss: 0.0389\n",
      "Epoch [5/5], Step [3862/10336], Loss: 0.7888\n",
      "Epoch [5/5], Step [3864/10336], Loss: 0.0933\n",
      "Epoch [5/5], Step [3866/10336], Loss: 1.5941\n",
      "Epoch [5/5], Step [3868/10336], Loss: 0.4499\n",
      "Epoch [5/5], Step [3870/10336], Loss: 0.0216\n",
      "Epoch [5/5], Step [3872/10336], Loss: 0.0302\n",
      "Epoch [5/5], Step [3874/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [3876/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [3878/10336], Loss: 0.2893\n",
      "Epoch [5/5], Step [3880/10336], Loss: 1.3606\n",
      "Epoch [5/5], Step [3882/10336], Loss: 0.9866\n",
      "Epoch [5/5], Step [3884/10336], Loss: 0.0371\n",
      "Epoch [5/5], Step [3886/10336], Loss: 0.6423\n",
      "Epoch [5/5], Step [3888/10336], Loss: 0.0815\n",
      "Epoch [5/5], Step [3890/10336], Loss: 0.0122\n",
      "Epoch [5/5], Step [3892/10336], Loss: 0.1176\n",
      "Epoch [5/5], Step [3894/10336], Loss: 0.4772\n",
      "Epoch [5/5], Step [3896/10336], Loss: 0.0331\n",
      "Epoch [5/5], Step [3898/10336], Loss: 0.0075\n",
      "Epoch [5/5], Step [3900/10336], Loss: 0.0235\n",
      "Epoch [5/5], Step [3902/10336], Loss: 1.0222\n",
      "Epoch [5/5], Step [3904/10336], Loss: 0.0959\n",
      "Epoch [5/5], Step [3906/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [3908/10336], Loss: 0.9972\n",
      "Epoch [5/5], Step [3910/10336], Loss: 0.0780\n",
      "Epoch [5/5], Step [3912/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [3914/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [3916/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [3918/10336], Loss: 0.4870\n",
      "Epoch [5/5], Step [3920/10336], Loss: 0.0885\n",
      "Epoch [5/5], Step [3922/10336], Loss: 0.0731\n",
      "Epoch [5/5], Step [3924/10336], Loss: 0.7931\n",
      "Epoch [5/5], Step [3926/10336], Loss: 0.0082\n",
      "Epoch [5/5], Step [3928/10336], Loss: 2.3812\n",
      "Epoch [5/5], Step [3930/10336], Loss: 0.6425\n",
      "Epoch [5/5], Step [3932/10336], Loss: 3.1885\n",
      "Epoch [5/5], Step [3934/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [3936/10336], Loss: 0.6901\n",
      "Epoch [5/5], Step [3938/10336], Loss: 1.1517\n",
      "Epoch [5/5], Step [3940/10336], Loss: 1.6096\n",
      "Epoch [5/5], Step [3942/10336], Loss: 0.1591\n",
      "Epoch [5/5], Step [3944/10336], Loss: 0.2346\n",
      "Epoch [5/5], Step [3946/10336], Loss: 0.0218\n",
      "Epoch [5/5], Step [3948/10336], Loss: 0.0109\n",
      "Epoch [5/5], Step [3950/10336], Loss: 0.3062\n",
      "Epoch [5/5], Step [3952/10336], Loss: 0.2575\n",
      "Epoch [5/5], Step [3954/10336], Loss: 0.0346\n",
      "Epoch [5/5], Step [3956/10336], Loss: 0.0627\n",
      "Epoch [5/5], Step [3958/10336], Loss: 0.3292\n",
      "Epoch [5/5], Step [3960/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [3962/10336], Loss: 0.0425\n",
      "Epoch [5/5], Step [3964/10336], Loss: 0.5789\n",
      "Epoch [5/5], Step [3966/10336], Loss: 1.2117\n",
      "Epoch [5/5], Step [3968/10336], Loss: 2.1057\n",
      "Epoch [5/5], Step [3970/10336], Loss: 0.3284\n",
      "Epoch [5/5], Step [3972/10336], Loss: 0.1104\n",
      "Epoch [5/5], Step [3974/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [3976/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [3978/10336], Loss: 0.9773\n",
      "Epoch [5/5], Step [3980/10336], Loss: 0.8257\n",
      "Epoch [5/5], Step [3982/10336], Loss: 0.0158\n",
      "Epoch [5/5], Step [3984/10336], Loss: 1.2760\n",
      "Epoch [5/5], Step [3986/10336], Loss: 2.3395\n",
      "Epoch [5/5], Step [3988/10336], Loss: 0.0176\n",
      "Epoch [5/5], Step [3990/10336], Loss: 2.9686\n",
      "Epoch [5/5], Step [3992/10336], Loss: 1.2240\n",
      "Epoch [5/5], Step [3994/10336], Loss: 1.2745\n",
      "Epoch [5/5], Step [3996/10336], Loss: 2.5527\n",
      "Epoch [5/5], Step [3998/10336], Loss: 0.0369\n",
      "Epoch [5/5], Step [4000/10336], Loss: 0.4785\n",
      "Epoch [5/5], Step [4002/10336], Loss: 1.4199\n",
      "Epoch [5/5], Step [4004/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [4006/10336], Loss: 0.2528\n",
      "Epoch [5/5], Step [4008/10336], Loss: 0.4774\n",
      "Epoch [5/5], Step [4010/10336], Loss: 0.0206\n",
      "Epoch [5/5], Step [4012/10336], Loss: 1.5768\n",
      "Epoch [5/5], Step [4014/10336], Loss: 0.1141\n",
      "Epoch [5/5], Step [4016/10336], Loss: 1.4684\n",
      "Epoch [5/5], Step [4018/10336], Loss: 0.0185\n",
      "Epoch [5/5], Step [4020/10336], Loss: 0.0380\n",
      "Epoch [5/5], Step [4022/10336], Loss: 0.3203\n",
      "Epoch [5/5], Step [4024/10336], Loss: 0.0248\n",
      "Epoch [5/5], Step [4026/10336], Loss: 0.4616\n",
      "Epoch [5/5], Step [4028/10336], Loss: 0.5041\n",
      "Epoch [5/5], Step [4030/10336], Loss: 0.5190\n",
      "Epoch [5/5], Step [4032/10336], Loss: 0.4918\n",
      "Epoch [5/5], Step [4034/10336], Loss: 0.3207\n",
      "Epoch [5/5], Step [4036/10336], Loss: 0.9820\n",
      "Epoch [5/5], Step [4038/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [4040/10336], Loss: 0.0313\n",
      "Epoch [5/5], Step [4042/10336], Loss: 0.5677\n",
      "Epoch [5/5], Step [4044/10336], Loss: 1.0311\n",
      "Epoch [5/5], Step [4046/10336], Loss: 0.6400\n",
      "Epoch [5/5], Step [4048/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [4050/10336], Loss: 1.7819\n",
      "Epoch [5/5], Step [4052/10336], Loss: 0.4084\n",
      "Epoch [5/5], Step [4054/10336], Loss: 2.2969\n",
      "Epoch [5/5], Step [4056/10336], Loss: 3.5453\n",
      "Epoch [5/5], Step [4058/10336], Loss: 0.8997\n",
      "Epoch [5/5], Step [4060/10336], Loss: 3.2970\n",
      "Epoch [5/5], Step [4062/10336], Loss: 0.0872\n",
      "Epoch [5/5], Step [4064/10336], Loss: 2.3935\n",
      "Epoch [5/5], Step [4066/10336], Loss: 2.4685\n",
      "Epoch [5/5], Step [4068/10336], Loss: 1.0173\n",
      "Epoch [5/5], Step [4070/10336], Loss: 0.0242\n",
      "Epoch [5/5], Step [4072/10336], Loss: 0.1324\n",
      "Epoch [5/5], Step [4074/10336], Loss: 2.9277\n",
      "Epoch [5/5], Step [4076/10336], Loss: 1.4079\n",
      "Epoch [5/5], Step [4078/10336], Loss: 0.5292\n",
      "Epoch [5/5], Step [4080/10336], Loss: 0.0287\n",
      "Epoch [5/5], Step [4082/10336], Loss: 0.4170\n",
      "Epoch [5/5], Step [4084/10336], Loss: 1.0667\n",
      "Epoch [5/5], Step [4086/10336], Loss: 0.1845\n",
      "Epoch [5/5], Step [4088/10336], Loss: 0.5260\n",
      "Epoch [5/5], Step [4090/10336], Loss: 0.9130\n",
      "Epoch [5/5], Step [4092/10336], Loss: 1.2390\n",
      "Epoch [5/5], Step [4094/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [4096/10336], Loss: 0.1145\n",
      "Epoch [5/5], Step [4098/10336], Loss: 1.4773\n",
      "Epoch [5/5], Step [4100/10336], Loss: 0.5230\n",
      "Epoch [5/5], Step [4102/10336], Loss: 0.5534\n",
      "Epoch [5/5], Step [4104/10336], Loss: 0.0124\n",
      "Epoch [5/5], Step [4106/10336], Loss: 0.0546\n",
      "Epoch [5/5], Step [4108/10336], Loss: 0.0862\n",
      "Epoch [5/5], Step [4110/10336], Loss: 0.7544\n",
      "Epoch [5/5], Step [4112/10336], Loss: 0.9148\n",
      "Epoch [5/5], Step [4114/10336], Loss: 0.0034\n",
      "Epoch [5/5], Step [4116/10336], Loss: 0.0079\n",
      "Epoch [5/5], Step [4118/10336], Loss: 0.9252\n",
      "Epoch [5/5], Step [4120/10336], Loss: 0.1397\n",
      "Epoch [5/5], Step [4122/10336], Loss: 0.0449\n",
      "Epoch [5/5], Step [4124/10336], Loss: 0.0290\n",
      "Epoch [5/5], Step [4126/10336], Loss: 0.0053\n",
      "Epoch [5/5], Step [4128/10336], Loss: 0.1787\n",
      "Epoch [5/5], Step [4130/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [4132/10336], Loss: 1.2607\n",
      "Epoch [5/5], Step [4134/10336], Loss: 0.0220\n",
      "Epoch [5/5], Step [4136/10336], Loss: 0.9226\n",
      "Epoch [5/5], Step [4138/10336], Loss: 0.0401\n",
      "Epoch [5/5], Step [4140/10336], Loss: 0.0452\n",
      "Epoch [5/5], Step [4142/10336], Loss: 0.0893\n",
      "Epoch [5/5], Step [4144/10336], Loss: 0.5808\n",
      "Epoch [5/5], Step [4146/10336], Loss: 0.0142\n",
      "Epoch [5/5], Step [4148/10336], Loss: 0.6077\n",
      "Epoch [5/5], Step [4150/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [4152/10336], Loss: 0.1756\n",
      "Epoch [5/5], Step [4154/10336], Loss: 1.0408\n",
      "Epoch [5/5], Step [4156/10336], Loss: 0.6829\n",
      "Epoch [5/5], Step [4158/10336], Loss: 0.2528\n",
      "Epoch [5/5], Step [4160/10336], Loss: 1.6857\n",
      "Epoch [5/5], Step [4162/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [4164/10336], Loss: 0.0127\n",
      "Epoch [5/5], Step [4166/10336], Loss: 0.0405\n",
      "Epoch [5/5], Step [4168/10336], Loss: 0.0793\n",
      "Epoch [5/5], Step [4170/10336], Loss: 0.0775\n",
      "Epoch [5/5], Step [4172/10336], Loss: 0.0059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [4174/10336], Loss: 0.0979\n",
      "Epoch [5/5], Step [4176/10336], Loss: 0.0864\n",
      "Epoch [5/5], Step [4178/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [4180/10336], Loss: 0.1358\n",
      "Epoch [5/5], Step [4182/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [4184/10336], Loss: 0.4283\n",
      "Epoch [5/5], Step [4186/10336], Loss: 0.0592\n",
      "Epoch [5/5], Step [4188/10336], Loss: 0.3475\n",
      "Epoch [5/5], Step [4190/10336], Loss: 0.2721\n",
      "Epoch [5/5], Step [4192/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [4194/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [4196/10336], Loss: 0.0618\n",
      "Epoch [5/5], Step [4198/10336], Loss: 0.8812\n",
      "Epoch [5/5], Step [4200/10336], Loss: 1.1031\n",
      "Epoch [5/5], Step [4202/10336], Loss: 0.9127\n",
      "Epoch [5/5], Step [4204/10336], Loss: 1.6103\n",
      "Epoch [5/5], Step [4206/10336], Loss: 0.4795\n",
      "Epoch [5/5], Step [4208/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [4210/10336], Loss: 0.8788\n",
      "Epoch [5/5], Step [4212/10336], Loss: 0.0676\n",
      "Epoch [5/5], Step [4214/10336], Loss: 1.4121\n",
      "Epoch [5/5], Step [4216/10336], Loss: 0.4263\n",
      "Epoch [5/5], Step [4218/10336], Loss: 0.1845\n",
      "Epoch [5/5], Step [4220/10336], Loss: 0.7378\n",
      "Epoch [5/5], Step [4222/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [4224/10336], Loss: 0.0942\n",
      "Epoch [5/5], Step [4226/10336], Loss: 0.1617\n",
      "Epoch [5/5], Step [4228/10336], Loss: 0.1764\n",
      "Epoch [5/5], Step [4230/10336], Loss: 0.8768\n",
      "Epoch [5/5], Step [4232/10336], Loss: 1.1818\n",
      "Epoch [5/5], Step [4234/10336], Loss: 0.1626\n",
      "Epoch [5/5], Step [4236/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [4238/10336], Loss: 0.2210\n",
      "Epoch [5/5], Step [4240/10336], Loss: 0.2348\n",
      "Epoch [5/5], Step [4242/10336], Loss: 0.0228\n",
      "Epoch [5/5], Step [4244/10336], Loss: 0.7341\n",
      "Epoch [5/5], Step [4246/10336], Loss: 3.3917\n",
      "Epoch [5/5], Step [4248/10336], Loss: 0.4803\n",
      "Epoch [5/5], Step [4250/10336], Loss: 0.1370\n",
      "Epoch [5/5], Step [4252/10336], Loss: 0.1181\n",
      "Epoch [5/5], Step [4254/10336], Loss: 0.1458\n",
      "Epoch [5/5], Step [4256/10336], Loss: 0.4623\n",
      "Epoch [5/5], Step [4258/10336], Loss: 2.6033\n",
      "Epoch [5/5], Step [4260/10336], Loss: 0.0670\n",
      "Epoch [5/5], Step [4262/10336], Loss: 0.0158\n",
      "Epoch [5/5], Step [4264/10336], Loss: 0.3002\n",
      "Epoch [5/5], Step [4266/10336], Loss: 0.0773\n",
      "Epoch [5/5], Step [4268/10336], Loss: 0.1253\n",
      "Epoch [5/5], Step [4270/10336], Loss: 0.0280\n",
      "Epoch [5/5], Step [4272/10336], Loss: 0.2060\n",
      "Epoch [5/5], Step [4274/10336], Loss: 0.3631\n",
      "Epoch [5/5], Step [4276/10336], Loss: 1.5120\n",
      "Epoch [5/5], Step [4278/10336], Loss: 0.0218\n",
      "Epoch [5/5], Step [4280/10336], Loss: 0.7672\n",
      "Epoch [5/5], Step [4282/10336], Loss: 0.0066\n",
      "Epoch [5/5], Step [4284/10336], Loss: 0.5407\n",
      "Epoch [5/5], Step [4286/10336], Loss: 0.6053\n",
      "Epoch [5/5], Step [4288/10336], Loss: 2.7720\n",
      "Epoch [5/5], Step [4290/10336], Loss: 0.0361\n",
      "Epoch [5/5], Step [4292/10336], Loss: 0.2492\n",
      "Epoch [5/5], Step [4294/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [4296/10336], Loss: 0.5213\n",
      "Epoch [5/5], Step [4298/10336], Loss: 0.6553\n",
      "Epoch [5/5], Step [4300/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [4302/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [4304/10336], Loss: 1.2821\n",
      "Epoch [5/5], Step [4306/10336], Loss: 0.0862\n",
      "Epoch [5/5], Step [4308/10336], Loss: 0.6814\n",
      "Epoch [5/5], Step [4310/10336], Loss: 0.0439\n",
      "Epoch [5/5], Step [4312/10336], Loss: 0.8189\n",
      "Epoch [5/5], Step [4314/10336], Loss: 0.5687\n",
      "Epoch [5/5], Step [4316/10336], Loss: 0.7515\n",
      "Epoch [5/5], Step [4318/10336], Loss: 1.3331\n",
      "Epoch [5/5], Step [4320/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [4322/10336], Loss: 0.4015\n",
      "Epoch [5/5], Step [4324/10336], Loss: 0.9475\n",
      "Epoch [5/5], Step [4326/10336], Loss: 0.0160\n",
      "Epoch [5/5], Step [4328/10336], Loss: 0.7550\n",
      "Epoch [5/5], Step [4330/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [4332/10336], Loss: 0.1996\n",
      "Epoch [5/5], Step [4334/10336], Loss: 1.3794\n",
      "Epoch [5/5], Step [4336/10336], Loss: 0.2325\n",
      "Epoch [5/5], Step [4338/10336], Loss: 0.0141\n",
      "Epoch [5/5], Step [4340/10336], Loss: 0.6814\n",
      "Epoch [5/5], Step [4342/10336], Loss: 0.3449\n",
      "Epoch [5/5], Step [4344/10336], Loss: 0.7017\n",
      "Epoch [5/5], Step [4346/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [4348/10336], Loss: 0.4819\n",
      "Epoch [5/5], Step [4350/10336], Loss: 0.1565\n",
      "Epoch [5/5], Step [4352/10336], Loss: 0.9601\n",
      "Epoch [5/5], Step [4354/10336], Loss: 0.8454\n",
      "Epoch [5/5], Step [4356/10336], Loss: 3.7942\n",
      "Epoch [5/5], Step [4358/10336], Loss: 0.6289\n",
      "Epoch [5/5], Step [4360/10336], Loss: 0.0315\n",
      "Epoch [5/5], Step [4362/10336], Loss: 0.4738\n",
      "Epoch [5/5], Step [4364/10336], Loss: 0.0629\n",
      "Epoch [5/5], Step [4366/10336], Loss: 0.6559\n",
      "Epoch [5/5], Step [4368/10336], Loss: 1.8560\n",
      "Epoch [5/5], Step [4370/10336], Loss: 2.5992\n",
      "Epoch [5/5], Step [4372/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [4374/10336], Loss: 0.1888\n",
      "Epoch [5/5], Step [4376/10336], Loss: 0.0529\n",
      "Epoch [5/5], Step [4378/10336], Loss: 0.7481\n",
      "Epoch [5/5], Step [4380/10336], Loss: 0.7904\n",
      "Epoch [5/5], Step [4382/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [4384/10336], Loss: 3.0845\n",
      "Epoch [5/5], Step [4386/10336], Loss: 1.3899\n",
      "Epoch [5/5], Step [4388/10336], Loss: 0.6055\n",
      "Epoch [5/5], Step [4390/10336], Loss: 2.9528\n",
      "Epoch [5/5], Step [4392/10336], Loss: 0.6113\n",
      "Epoch [5/5], Step [4394/10336], Loss: 0.1152\n",
      "Epoch [5/5], Step [4396/10336], Loss: 0.0341\n",
      "Epoch [5/5], Step [4398/10336], Loss: 0.0418\n",
      "Epoch [5/5], Step [4400/10336], Loss: 2.5912\n",
      "Epoch [5/5], Step [4402/10336], Loss: 0.2693\n",
      "Epoch [5/5], Step [4404/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [4406/10336], Loss: 0.0924\n",
      "Epoch [5/5], Step [4408/10336], Loss: 0.0161\n",
      "Epoch [5/5], Step [4410/10336], Loss: 0.0236\n",
      "Epoch [5/5], Step [4412/10336], Loss: 0.0416\n",
      "Epoch [5/5], Step [4414/10336], Loss: 0.0534\n",
      "Epoch [5/5], Step [4416/10336], Loss: 0.2488\n",
      "Epoch [5/5], Step [4418/10336], Loss: 0.3853\n",
      "Epoch [5/5], Step [4420/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [4422/10336], Loss: 0.0198\n",
      "Epoch [5/5], Step [4424/10336], Loss: 0.4282\n",
      "Epoch [5/5], Step [4426/10336], Loss: 0.3358\n",
      "Epoch [5/5], Step [4428/10336], Loss: 1.0512\n",
      "Epoch [5/5], Step [4430/10336], Loss: 0.2208\n",
      "Epoch [5/5], Step [4432/10336], Loss: 0.4697\n",
      "Epoch [5/5], Step [4434/10336], Loss: 0.0495\n",
      "Epoch [5/5], Step [4436/10336], Loss: 0.7963\n",
      "Epoch [5/5], Step [4438/10336], Loss: 1.3448\n",
      "Epoch [5/5], Step [4440/10336], Loss: 0.5009\n",
      "Epoch [5/5], Step [4442/10336], Loss: 0.0615\n",
      "Epoch [5/5], Step [4444/10336], Loss: 1.1035\n",
      "Epoch [5/5], Step [4446/10336], Loss: 0.0317\n",
      "Epoch [5/5], Step [4448/10336], Loss: 0.5022\n",
      "Epoch [5/5], Step [4450/10336], Loss: 0.4374\n",
      "Epoch [5/5], Step [4452/10336], Loss: 1.2318\n",
      "Epoch [5/5], Step [4454/10336], Loss: 0.3200\n",
      "Epoch [5/5], Step [4456/10336], Loss: 1.3982\n",
      "Epoch [5/5], Step [4458/10336], Loss: 1.5869\n",
      "Epoch [5/5], Step [4460/10336], Loss: 0.1572\n",
      "Epoch [5/5], Step [4462/10336], Loss: 0.6457\n",
      "Epoch [5/5], Step [4464/10336], Loss: 0.1338\n",
      "Epoch [5/5], Step [4466/10336], Loss: 0.0529\n",
      "Epoch [5/5], Step [4468/10336], Loss: 2.1075\n",
      "Epoch [5/5], Step [4470/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [4472/10336], Loss: 1.2656\n",
      "Epoch [5/5], Step [4474/10336], Loss: 1.4728\n",
      "Epoch [5/5], Step [4476/10336], Loss: 0.2668\n",
      "Epoch [5/5], Step [4478/10336], Loss: 2.4194\n",
      "Epoch [5/5], Step [4480/10336], Loss: 0.8963\n",
      "Epoch [5/5], Step [4482/10336], Loss: 3.2232\n",
      "Epoch [5/5], Step [4484/10336], Loss: 0.1562\n",
      "Epoch [5/5], Step [4486/10336], Loss: 2.5001\n",
      "Epoch [5/5], Step [4488/10336], Loss: 0.4521\n",
      "Epoch [5/5], Step [4490/10336], Loss: 0.4597\n",
      "Epoch [5/5], Step [4492/10336], Loss: 0.3282\n",
      "Epoch [5/5], Step [4494/10336], Loss: 0.0170\n",
      "Epoch [5/5], Step [4496/10336], Loss: 0.0314\n",
      "Epoch [5/5], Step [4498/10336], Loss: 0.2091\n",
      "Epoch [5/5], Step [4500/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [4502/10336], Loss: 0.0261\n",
      "Epoch [5/5], Step [4504/10336], Loss: 0.2390\n",
      "Epoch [5/5], Step [4506/10336], Loss: 0.9954\n",
      "Epoch [5/5], Step [4508/10336], Loss: 3.1812\n",
      "Epoch [5/5], Step [4510/10336], Loss: 0.0192\n",
      "Epoch [5/5], Step [4512/10336], Loss: 0.0453\n",
      "Epoch [5/5], Step [4514/10336], Loss: 1.2390\n",
      "Epoch [5/5], Step [4516/10336], Loss: 1.4822\n",
      "Epoch [5/5], Step [4518/10336], Loss: 0.7786\n",
      "Epoch [5/5], Step [4520/10336], Loss: 1.2259\n",
      "Epoch [5/5], Step [4522/10336], Loss: 1.6936\n",
      "Epoch [5/5], Step [4524/10336], Loss: 0.0572\n",
      "Epoch [5/5], Step [4526/10336], Loss: 0.2234\n",
      "Epoch [5/5], Step [4528/10336], Loss: 0.0800\n",
      "Epoch [5/5], Step [4530/10336], Loss: 2.1248\n",
      "Epoch [5/5], Step [4532/10336], Loss: 1.5794\n",
      "Epoch [5/5], Step [4534/10336], Loss: 1.3220\n",
      "Epoch [5/5], Step [4536/10336], Loss: 0.1615\n",
      "Epoch [5/5], Step [4538/10336], Loss: 0.0267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [4540/10336], Loss: 1.7447\n",
      "Epoch [5/5], Step [4542/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [4544/10336], Loss: 0.4879\n",
      "Epoch [5/5], Step [4546/10336], Loss: 0.9946\n",
      "Epoch [5/5], Step [4548/10336], Loss: 1.6537\n",
      "Epoch [5/5], Step [4550/10336], Loss: 0.1579\n",
      "Epoch [5/5], Step [4552/10336], Loss: 0.1447\n",
      "Epoch [5/5], Step [4554/10336], Loss: 0.1472\n",
      "Epoch [5/5], Step [4556/10336], Loss: 0.1906\n",
      "Epoch [5/5], Step [4558/10336], Loss: 0.2437\n",
      "Epoch [5/5], Step [4560/10336], Loss: 0.0083\n",
      "Epoch [5/5], Step [4562/10336], Loss: 1.5037\n",
      "Epoch [5/5], Step [4564/10336], Loss: 0.0115\n",
      "Epoch [5/5], Step [4566/10336], Loss: 0.0179\n",
      "Epoch [5/5], Step [4568/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [4570/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [4572/10336], Loss: 0.3533\n",
      "Epoch [5/5], Step [4574/10336], Loss: 0.0713\n",
      "Epoch [5/5], Step [4576/10336], Loss: 1.3096\n",
      "Epoch [5/5], Step [4578/10336], Loss: 1.3001\n",
      "Epoch [5/5], Step [4580/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [4582/10336], Loss: 0.0261\n",
      "Epoch [5/5], Step [4584/10336], Loss: 0.0320\n",
      "Epoch [5/5], Step [4586/10336], Loss: 2.7820\n",
      "Epoch [5/5], Step [4588/10336], Loss: 0.0335\n",
      "Epoch [5/5], Step [4590/10336], Loss: 1.6157\n",
      "Epoch [5/5], Step [4592/10336], Loss: 0.2953\n",
      "Epoch [5/5], Step [4594/10336], Loss: 0.3971\n",
      "Epoch [5/5], Step [4596/10336], Loss: 0.2016\n",
      "Epoch [5/5], Step [4598/10336], Loss: 0.1715\n",
      "Epoch [5/5], Step [4600/10336], Loss: 3.1187\n",
      "Epoch [5/5], Step [4602/10336], Loss: 0.5120\n",
      "Epoch [5/5], Step [4604/10336], Loss: 1.7043\n",
      "Epoch [5/5], Step [4606/10336], Loss: 0.4894\n",
      "Epoch [5/5], Step [4608/10336], Loss: 1.8335\n",
      "Epoch [5/5], Step [4610/10336], Loss: 0.3561\n",
      "Epoch [5/5], Step [4612/10336], Loss: 0.0836\n",
      "Epoch [5/5], Step [4614/10336], Loss: 0.1408\n",
      "Epoch [5/5], Step [4616/10336], Loss: 0.3419\n",
      "Epoch [5/5], Step [4618/10336], Loss: 1.8733\n",
      "Epoch [5/5], Step [4620/10336], Loss: 0.7406\n",
      "Epoch [5/5], Step [4622/10336], Loss: 1.1935\n",
      "Epoch [5/5], Step [4624/10336], Loss: 0.4352\n",
      "Epoch [5/5], Step [4626/10336], Loss: 0.0255\n",
      "Epoch [5/5], Step [4628/10336], Loss: 1.4338\n",
      "Epoch [5/5], Step [4630/10336], Loss: 1.1723\n",
      "Epoch [5/5], Step [4632/10336], Loss: 0.2858\n",
      "Epoch [5/5], Step [4634/10336], Loss: 0.1319\n",
      "Epoch [5/5], Step [4636/10336], Loss: 0.4415\n",
      "Epoch [5/5], Step [4638/10336], Loss: 0.6583\n",
      "Epoch [5/5], Step [4640/10336], Loss: 1.2169\n",
      "Epoch [5/5], Step [4642/10336], Loss: 0.0430\n",
      "Epoch [5/5], Step [4644/10336], Loss: 0.2438\n",
      "Epoch [5/5], Step [4646/10336], Loss: 0.0440\n",
      "Epoch [5/5], Step [4648/10336], Loss: 0.5577\n",
      "Epoch [5/5], Step [4650/10336], Loss: 0.2806\n",
      "Epoch [5/5], Step [4652/10336], Loss: 0.1820\n",
      "Epoch [5/5], Step [4654/10336], Loss: 0.0730\n",
      "Epoch [5/5], Step [4656/10336], Loss: 1.0705\n",
      "Epoch [5/5], Step [4658/10336], Loss: 0.4280\n",
      "Epoch [5/5], Step [4660/10336], Loss: 0.0867\n",
      "Epoch [5/5], Step [4662/10336], Loss: 0.5175\n",
      "Epoch [5/5], Step [4664/10336], Loss: 0.3937\n",
      "Epoch [5/5], Step [4666/10336], Loss: 1.1121\n",
      "Epoch [5/5], Step [4668/10336], Loss: 0.0945\n",
      "Epoch [5/5], Step [4670/10336], Loss: 0.4608\n",
      "Epoch [5/5], Step [4672/10336], Loss: 0.0261\n",
      "Epoch [5/5], Step [4674/10336], Loss: 0.4808\n",
      "Epoch [5/5], Step [4676/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [4678/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [4680/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [4682/10336], Loss: 0.2433\n",
      "Epoch [5/5], Step [4684/10336], Loss: 1.7308\n",
      "Epoch [5/5], Step [4686/10336], Loss: 0.0130\n",
      "Epoch [5/5], Step [4688/10336], Loss: 0.1130\n",
      "Epoch [5/5], Step [4690/10336], Loss: 0.3155\n",
      "Epoch [5/5], Step [4692/10336], Loss: 0.1847\n",
      "Epoch [5/5], Step [4694/10336], Loss: 0.4540\n",
      "Epoch [5/5], Step [4696/10336], Loss: 0.2326\n",
      "Epoch [5/5], Step [4698/10336], Loss: 0.0749\n",
      "Epoch [5/5], Step [4700/10336], Loss: 0.0093\n",
      "Epoch [5/5], Step [4702/10336], Loss: 0.6083\n",
      "Epoch [5/5], Step [4704/10336], Loss: 0.3215\n",
      "Epoch [5/5], Step [4706/10336], Loss: 0.0650\n",
      "Epoch [5/5], Step [4708/10336], Loss: 0.9401\n",
      "Epoch [5/5], Step [4710/10336], Loss: 0.3806\n",
      "Epoch [5/5], Step [4712/10336], Loss: 0.0259\n",
      "Epoch [5/5], Step [4714/10336], Loss: 0.8011\n",
      "Epoch [5/5], Step [4716/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [4718/10336], Loss: 0.0670\n",
      "Epoch [5/5], Step [4720/10336], Loss: 0.0803\n",
      "Epoch [5/5], Step [4722/10336], Loss: 0.4959\n",
      "Epoch [5/5], Step [4724/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [4726/10336], Loss: 0.2214\n",
      "Epoch [5/5], Step [4728/10336], Loss: 0.0101\n",
      "Epoch [5/5], Step [4730/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [4732/10336], Loss: 0.0457\n",
      "Epoch [5/5], Step [4734/10336], Loss: 0.0147\n",
      "Epoch [5/5], Step [4736/10336], Loss: 2.3054\n",
      "Epoch [5/5], Step [4738/10336], Loss: 0.3794\n",
      "Epoch [5/5], Step [4740/10336], Loss: 2.6558\n",
      "Epoch [5/5], Step [4742/10336], Loss: 1.4330\n",
      "Epoch [5/5], Step [4744/10336], Loss: 0.0173\n",
      "Epoch [5/5], Step [4746/10336], Loss: 0.0833\n",
      "Epoch [5/5], Step [4748/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [4750/10336], Loss: 1.4511\n",
      "Epoch [5/5], Step [4752/10336], Loss: 0.5913\n",
      "Epoch [5/5], Step [4754/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [4756/10336], Loss: 0.0596\n",
      "Epoch [5/5], Step [4758/10336], Loss: 0.1422\n",
      "Epoch [5/5], Step [4760/10336], Loss: 0.0104\n",
      "Epoch [5/5], Step [4762/10336], Loss: 1.6087\n",
      "Epoch [5/5], Step [4764/10336], Loss: 0.7520\n",
      "Epoch [5/5], Step [4766/10336], Loss: 0.0742\n",
      "Epoch [5/5], Step [4768/10336], Loss: 0.1635\n",
      "Epoch [5/5], Step [4770/10336], Loss: 0.4331\n",
      "Epoch [5/5], Step [4772/10336], Loss: 0.1167\n",
      "Epoch [5/5], Step [4774/10336], Loss: 0.0571\n",
      "Epoch [5/5], Step [4776/10336], Loss: 1.7570\n",
      "Epoch [5/5], Step [4778/10336], Loss: 0.0644\n",
      "Epoch [5/5], Step [4780/10336], Loss: 0.0095\n",
      "Epoch [5/5], Step [4782/10336], Loss: 0.0078\n",
      "Epoch [5/5], Step [4784/10336], Loss: 0.8813\n",
      "Epoch [5/5], Step [4786/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [4788/10336], Loss: 0.0282\n",
      "Epoch [5/5], Step [4790/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [4792/10336], Loss: 4.4874\n",
      "Epoch [5/5], Step [4794/10336], Loss: 0.0809\n",
      "Epoch [5/5], Step [4796/10336], Loss: 0.3864\n",
      "Epoch [5/5], Step [4798/10336], Loss: 0.3289\n",
      "Epoch [5/5], Step [4800/10336], Loss: 0.0298\n",
      "Epoch [5/5], Step [4802/10336], Loss: 1.5241\n",
      "Epoch [5/5], Step [4804/10336], Loss: 0.2089\n",
      "Epoch [5/5], Step [4806/10336], Loss: 1.0411\n",
      "Epoch [5/5], Step [4808/10336], Loss: 0.0744\n",
      "Epoch [5/5], Step [4810/10336], Loss: 0.5728\n",
      "Epoch [5/5], Step [4812/10336], Loss: 2.4891\n",
      "Epoch [5/5], Step [4814/10336], Loss: 0.1917\n",
      "Epoch [5/5], Step [4816/10336], Loss: 0.6917\n",
      "Epoch [5/5], Step [4818/10336], Loss: 2.5364\n",
      "Epoch [5/5], Step [4820/10336], Loss: 0.0560\n",
      "Epoch [5/5], Step [4822/10336], Loss: 0.2097\n",
      "Epoch [5/5], Step [4824/10336], Loss: 0.2282\n",
      "Epoch [5/5], Step [4826/10336], Loss: 0.2075\n",
      "Epoch [5/5], Step [4828/10336], Loss: 0.5100\n",
      "Epoch [5/5], Step [4830/10336], Loss: 3.8770\n",
      "Epoch [5/5], Step [4832/10336], Loss: 0.0572\n",
      "Epoch [5/5], Step [4834/10336], Loss: 0.1362\n",
      "Epoch [5/5], Step [4836/10336], Loss: 0.0251\n",
      "Epoch [5/5], Step [4838/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [4840/10336], Loss: 0.0245\n",
      "Epoch [5/5], Step [4842/10336], Loss: 0.1388\n",
      "Epoch [5/5], Step [4844/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [4846/10336], Loss: 0.3020\n",
      "Epoch [5/5], Step [4848/10336], Loss: 0.3749\n",
      "Epoch [5/5], Step [4850/10336], Loss: 0.0854\n",
      "Epoch [5/5], Step [4852/10336], Loss: 0.0198\n",
      "Epoch [5/5], Step [4854/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [4856/10336], Loss: 0.0652\n",
      "Epoch [5/5], Step [4858/10336], Loss: 0.1154\n",
      "Epoch [5/5], Step [4860/10336], Loss: 0.0599\n",
      "Epoch [5/5], Step [4862/10336], Loss: 0.0177\n",
      "Epoch [5/5], Step [4864/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [4866/10336], Loss: 0.1792\n",
      "Epoch [5/5], Step [4868/10336], Loss: 0.2690\n",
      "Epoch [5/5], Step [4870/10336], Loss: 0.9250\n",
      "Epoch [5/5], Step [4872/10336], Loss: 0.5857\n",
      "Epoch [5/5], Step [4874/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [4876/10336], Loss: 2.2165\n",
      "Epoch [5/5], Step [4878/10336], Loss: 2.1449\n",
      "Epoch [5/5], Step [4880/10336], Loss: 0.1619\n",
      "Epoch [5/5], Step [4882/10336], Loss: 0.4239\n",
      "Epoch [5/5], Step [4884/10336], Loss: 0.0246\n",
      "Epoch [5/5], Step [4886/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [4888/10336], Loss: 0.4295\n",
      "Epoch [5/5], Step [4890/10336], Loss: 0.2868\n",
      "Epoch [5/5], Step [4892/10336], Loss: 3.6311\n",
      "Epoch [5/5], Step [4894/10336], Loss: 0.6902\n",
      "Epoch [5/5], Step [4896/10336], Loss: 0.1866\n",
      "Epoch [5/5], Step [4898/10336], Loss: 0.3093\n",
      "Epoch [5/5], Step [4900/10336], Loss: 0.0968\n",
      "Epoch [5/5], Step [4902/10336], Loss: 0.3347\n",
      "Epoch [5/5], Step [4904/10336], Loss: 0.2494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [4906/10336], Loss: 0.0073\n",
      "Epoch [5/5], Step [4908/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [4910/10336], Loss: 1.1107\n",
      "Epoch [5/5], Step [4912/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [4914/10336], Loss: 0.0177\n",
      "Epoch [5/5], Step [4916/10336], Loss: 0.0271\n",
      "Epoch [5/5], Step [4918/10336], Loss: 0.3537\n",
      "Epoch [5/5], Step [4920/10336], Loss: 0.0202\n",
      "Epoch [5/5], Step [4922/10336], Loss: 0.0060\n",
      "Epoch [5/5], Step [4924/10336], Loss: 1.2096\n",
      "Epoch [5/5], Step [4926/10336], Loss: 0.3531\n",
      "Epoch [5/5], Step [4928/10336], Loss: 0.3815\n",
      "Epoch [5/5], Step [4930/10336], Loss: 1.0209\n",
      "Epoch [5/5], Step [4932/10336], Loss: 0.0316\n",
      "Epoch [5/5], Step [4934/10336], Loss: 1.2075\n",
      "Epoch [5/5], Step [4936/10336], Loss: 1.4540\n",
      "Epoch [5/5], Step [4938/10336], Loss: 1.1211\n",
      "Epoch [5/5], Step [4940/10336], Loss: 1.3903\n",
      "Epoch [5/5], Step [4942/10336], Loss: 1.6364\n",
      "Epoch [5/5], Step [4944/10336], Loss: 0.0105\n",
      "Epoch [5/5], Step [4946/10336], Loss: 0.0373\n",
      "Epoch [5/5], Step [4948/10336], Loss: 2.1083\n",
      "Epoch [5/5], Step [4950/10336], Loss: 0.1280\n",
      "Epoch [5/5], Step [4952/10336], Loss: 1.2204\n",
      "Epoch [5/5], Step [4954/10336], Loss: 0.3021\n",
      "Epoch [5/5], Step [4956/10336], Loss: 0.7877\n",
      "Epoch [5/5], Step [4958/10336], Loss: 0.0578\n",
      "Epoch [5/5], Step [4960/10336], Loss: 0.0123\n",
      "Epoch [5/5], Step [4962/10336], Loss: 0.4332\n",
      "Epoch [5/5], Step [4964/10336], Loss: 0.0346\n",
      "Epoch [5/5], Step [4966/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [4968/10336], Loss: 0.0206\n",
      "Epoch [5/5], Step [4970/10336], Loss: 1.8062\n",
      "Epoch [5/5], Step [4972/10336], Loss: 2.8538\n",
      "Epoch [5/5], Step [4974/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [4976/10336], Loss: 0.3326\n",
      "Epoch [5/5], Step [4978/10336], Loss: 0.0302\n",
      "Epoch [5/5], Step [4980/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [4982/10336], Loss: 0.6154\n",
      "Epoch [5/5], Step [4984/10336], Loss: 0.2840\n",
      "Epoch [5/5], Step [4986/10336], Loss: 0.5323\n",
      "Epoch [5/5], Step [4988/10336], Loss: 2.7706\n",
      "Epoch [5/5], Step [4990/10336], Loss: 1.8972\n",
      "Epoch [5/5], Step [4992/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [4994/10336], Loss: 2.5835\n",
      "Epoch [5/5], Step [4996/10336], Loss: 2.6973\n",
      "Epoch [5/5], Step [4998/10336], Loss: 0.0835\n",
      "Epoch [5/5], Step [5000/10336], Loss: 0.0531\n",
      "Epoch [5/5], Step [5002/10336], Loss: 0.3101\n",
      "Epoch [5/5], Step [5004/10336], Loss: 2.0488\n",
      "Epoch [5/5], Step [5006/10336], Loss: 0.0370\n",
      "Epoch [5/5], Step [5008/10336], Loss: 0.0095\n",
      "Epoch [5/5], Step [5010/10336], Loss: 1.0015\n",
      "Epoch [5/5], Step [5012/10336], Loss: 0.3690\n",
      "Epoch [5/5], Step [5014/10336], Loss: 0.1981\n",
      "Epoch [5/5], Step [5016/10336], Loss: 0.0688\n",
      "Epoch [5/5], Step [5018/10336], Loss: 0.1081\n",
      "Epoch [5/5], Step [5020/10336], Loss: 0.0132\n",
      "Epoch [5/5], Step [5022/10336], Loss: 1.2723\n",
      "Epoch [5/5], Step [5024/10336], Loss: 0.2363\n",
      "Epoch [5/5], Step [5026/10336], Loss: 0.0204\n",
      "Epoch [5/5], Step [5028/10336], Loss: 0.1643\n",
      "Epoch [5/5], Step [5030/10336], Loss: 1.4866\n",
      "Epoch [5/5], Step [5032/10336], Loss: 0.1747\n",
      "Epoch [5/5], Step [5034/10336], Loss: 0.0286\n",
      "Epoch [5/5], Step [5036/10336], Loss: 0.4906\n",
      "Epoch [5/5], Step [5038/10336], Loss: 1.2645\n",
      "Epoch [5/5], Step [5040/10336], Loss: 0.3948\n",
      "Epoch [5/5], Step [5042/10336], Loss: 0.0253\n",
      "Epoch [5/5], Step [5044/10336], Loss: 0.5016\n",
      "Epoch [5/5], Step [5046/10336], Loss: 3.5895\n",
      "Epoch [5/5], Step [5048/10336], Loss: 1.0784\n",
      "Epoch [5/5], Step [5050/10336], Loss: 2.0736\n",
      "Epoch [5/5], Step [5052/10336], Loss: 0.0676\n",
      "Epoch [5/5], Step [5054/10336], Loss: 0.0311\n",
      "Epoch [5/5], Step [5056/10336], Loss: 0.1353\n",
      "Epoch [5/5], Step [5058/10336], Loss: 0.0149\n",
      "Epoch [5/5], Step [5060/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [5062/10336], Loss: 1.0373\n",
      "Epoch [5/5], Step [5064/10336], Loss: 0.6362\n",
      "Epoch [5/5], Step [5066/10336], Loss: 0.1861\n",
      "Epoch [5/5], Step [5068/10336], Loss: 0.4997\n",
      "Epoch [5/5], Step [5070/10336], Loss: 0.2784\n",
      "Epoch [5/5], Step [5072/10336], Loss: 0.1402\n",
      "Epoch [5/5], Step [5074/10336], Loss: 1.9188\n",
      "Epoch [5/5], Step [5076/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [5078/10336], Loss: 0.0318\n",
      "Epoch [5/5], Step [5080/10336], Loss: 0.8800\n",
      "Epoch [5/5], Step [5082/10336], Loss: 0.0079\n",
      "Epoch [5/5], Step [5084/10336], Loss: 0.4967\n",
      "Epoch [5/5], Step [5086/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [5088/10336], Loss: 1.3219\n",
      "Epoch [5/5], Step [5090/10336], Loss: 0.0157\n",
      "Epoch [5/5], Step [5092/10336], Loss: 0.0557\n",
      "Epoch [5/5], Step [5094/10336], Loss: 0.0398\n",
      "Epoch [5/5], Step [5096/10336], Loss: 0.9618\n",
      "Epoch [5/5], Step [5098/10336], Loss: 0.0376\n",
      "Epoch [5/5], Step [5100/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [5102/10336], Loss: 0.0700\n",
      "Epoch [5/5], Step [5104/10336], Loss: 0.0665\n",
      "Epoch [5/5], Step [5106/10336], Loss: 0.2540\n",
      "Epoch [5/5], Step [5108/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [5110/10336], Loss: 0.0682\n",
      "Epoch [5/5], Step [5112/10336], Loss: 0.7253\n",
      "Epoch [5/5], Step [5114/10336], Loss: 0.9559\n",
      "Epoch [5/5], Step [5116/10336], Loss: 0.0049\n",
      "Epoch [5/5], Step [5118/10336], Loss: 2.7003\n",
      "Epoch [5/5], Step [5120/10336], Loss: 0.6792\n",
      "Epoch [5/5], Step [5122/10336], Loss: 0.0496\n",
      "Epoch [5/5], Step [5124/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [5126/10336], Loss: 0.0213\n",
      "Epoch [5/5], Step [5128/10336], Loss: 0.6524\n",
      "Epoch [5/5], Step [5130/10336], Loss: 2.5927\n",
      "Epoch [5/5], Step [5132/10336], Loss: 0.8198\n",
      "Epoch [5/5], Step [5134/10336], Loss: 3.1095\n",
      "Epoch [5/5], Step [5136/10336], Loss: 0.6310\n",
      "Epoch [5/5], Step [5138/10336], Loss: 0.1160\n",
      "Epoch [5/5], Step [5140/10336], Loss: 0.0150\n",
      "Epoch [5/5], Step [5142/10336], Loss: 0.0379\n",
      "Epoch [5/5], Step [5144/10336], Loss: 2.2757\n",
      "Epoch [5/5], Step [5146/10336], Loss: 1.9799\n",
      "Epoch [5/5], Step [5148/10336], Loss: 0.0118\n",
      "Epoch [5/5], Step [5150/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [5152/10336], Loss: 4.1333\n",
      "Epoch [5/5], Step [5154/10336], Loss: 0.0218\n",
      "Epoch [5/5], Step [5156/10336], Loss: 0.0374\n",
      "Epoch [5/5], Step [5158/10336], Loss: 0.8845\n",
      "Epoch [5/5], Step [5160/10336], Loss: 4.5505\n",
      "Epoch [5/5], Step [5162/10336], Loss: 0.0244\n",
      "Epoch [5/5], Step [5164/10336], Loss: 0.0292\n",
      "Epoch [5/5], Step [5166/10336], Loss: 0.1204\n",
      "Epoch [5/5], Step [5168/10336], Loss: 0.4079\n",
      "Epoch [5/5], Step [5170/10336], Loss: 1.4938\n",
      "Epoch [5/5], Step [5172/10336], Loss: 4.0406\n",
      "Epoch [5/5], Step [5174/10336], Loss: 0.8767\n",
      "Epoch [5/5], Step [5176/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [5178/10336], Loss: 0.2665\n",
      "Epoch [5/5], Step [5180/10336], Loss: 0.8743\n",
      "Epoch [5/5], Step [5182/10336], Loss: 1.1840\n",
      "Epoch [5/5], Step [5184/10336], Loss: 0.7208\n",
      "Epoch [5/5], Step [5186/10336], Loss: 0.3402\n",
      "Epoch [5/5], Step [5188/10336], Loss: 0.0311\n",
      "Epoch [5/5], Step [5190/10336], Loss: 0.0742\n",
      "Epoch [5/5], Step [5192/10336], Loss: 0.0139\n",
      "Epoch [5/5], Step [5194/10336], Loss: 1.1805\n",
      "Epoch [5/5], Step [5196/10336], Loss: 2.0640\n",
      "Epoch [5/5], Step [5198/10336], Loss: 0.0578\n",
      "Epoch [5/5], Step [5200/10336], Loss: 1.8162\n",
      "Epoch [5/5], Step [5202/10336], Loss: 1.1351\n",
      "Epoch [5/5], Step [5204/10336], Loss: 2.6022\n",
      "Epoch [5/5], Step [5206/10336], Loss: 0.4003\n",
      "Epoch [5/5], Step [5208/10336], Loss: 1.6205\n",
      "Epoch [5/5], Step [5210/10336], Loss: 0.5637\n",
      "Epoch [5/5], Step [5212/10336], Loss: 0.1819\n",
      "Epoch [5/5], Step [5214/10336], Loss: 0.7123\n",
      "Epoch [5/5], Step [5216/10336], Loss: 0.0080\n",
      "Epoch [5/5], Step [5218/10336], Loss: 0.1241\n",
      "Epoch [5/5], Step [5220/10336], Loss: 1.8184\n",
      "Epoch [5/5], Step [5222/10336], Loss: 0.0348\n",
      "Epoch [5/5], Step [5224/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [5226/10336], Loss: 0.4265\n",
      "Epoch [5/5], Step [5228/10336], Loss: 0.7552\n",
      "Epoch [5/5], Step [5230/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [5232/10336], Loss: 0.4758\n",
      "Epoch [5/5], Step [5234/10336], Loss: 1.7178\n",
      "Epoch [5/5], Step [5236/10336], Loss: 0.2100\n",
      "Epoch [5/5], Step [5238/10336], Loss: 0.4931\n",
      "Epoch [5/5], Step [5240/10336], Loss: 0.0079\n",
      "Epoch [5/5], Step [5242/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [5244/10336], Loss: 0.2066\n",
      "Epoch [5/5], Step [5246/10336], Loss: 0.4066\n",
      "Epoch [5/5], Step [5248/10336], Loss: 1.3100\n",
      "Epoch [5/5], Step [5250/10336], Loss: 0.3306\n",
      "Epoch [5/5], Step [5252/10336], Loss: 0.2736\n",
      "Epoch [5/5], Step [5254/10336], Loss: 0.3241\n",
      "Epoch [5/5], Step [5256/10336], Loss: 1.3472\n",
      "Epoch [5/5], Step [5258/10336], Loss: 0.3499\n",
      "Epoch [5/5], Step [5260/10336], Loss: 1.2603\n",
      "Epoch [5/5], Step [5262/10336], Loss: 0.4352\n",
      "Epoch [5/5], Step [5264/10336], Loss: 0.4475\n",
      "Epoch [5/5], Step [5266/10336], Loss: 2.1185\n",
      "Epoch [5/5], Step [5268/10336], Loss: 0.0970\n",
      "Epoch [5/5], Step [5270/10336], Loss: 2.8741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [5272/10336], Loss: 0.1572\n",
      "Epoch [5/5], Step [5274/10336], Loss: 0.0442\n",
      "Epoch [5/5], Step [5276/10336], Loss: 1.4785\n",
      "Epoch [5/5], Step [5278/10336], Loss: 1.1523\n",
      "Epoch [5/5], Step [5280/10336], Loss: 0.0177\n",
      "Epoch [5/5], Step [5282/10336], Loss: 0.1044\n",
      "Epoch [5/5], Step [5284/10336], Loss: 0.0307\n",
      "Epoch [5/5], Step [5286/10336], Loss: 0.0651\n",
      "Epoch [5/5], Step [5288/10336], Loss: 0.5040\n",
      "Epoch [5/5], Step [5290/10336], Loss: 1.0995\n",
      "Epoch [5/5], Step [5292/10336], Loss: 2.0790\n",
      "Epoch [5/5], Step [5294/10336], Loss: 0.0149\n",
      "Epoch [5/5], Step [5296/10336], Loss: 0.0116\n",
      "Epoch [5/5], Step [5298/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [5300/10336], Loss: 1.2992\n",
      "Epoch [5/5], Step [5302/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [5304/10336], Loss: 1.8342\n",
      "Epoch [5/5], Step [5306/10336], Loss: 0.8589\n",
      "Epoch [5/5], Step [5308/10336], Loss: 0.5797\n",
      "Epoch [5/5], Step [5310/10336], Loss: 0.1318\n",
      "Epoch [5/5], Step [5312/10336], Loss: 1.3942\n",
      "Epoch [5/5], Step [5314/10336], Loss: 0.4788\n",
      "Epoch [5/5], Step [5316/10336], Loss: 0.0123\n",
      "Epoch [5/5], Step [5318/10336], Loss: 0.1715\n",
      "Epoch [5/5], Step [5320/10336], Loss: 0.0731\n",
      "Epoch [5/5], Step [5322/10336], Loss: 0.8264\n",
      "Epoch [5/5], Step [5324/10336], Loss: 0.0692\n",
      "Epoch [5/5], Step [5326/10336], Loss: 0.7157\n",
      "Epoch [5/5], Step [5328/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [5330/10336], Loss: 0.1868\n",
      "Epoch [5/5], Step [5332/10336], Loss: 1.2703\n",
      "Epoch [5/5], Step [5334/10336], Loss: 0.0316\n",
      "Epoch [5/5], Step [5336/10336], Loss: 1.1240\n",
      "Epoch [5/5], Step [5338/10336], Loss: 1.1267\n",
      "Epoch [5/5], Step [5340/10336], Loss: 0.3068\n",
      "Epoch [5/5], Step [5342/10336], Loss: 0.2264\n",
      "Epoch [5/5], Step [5344/10336], Loss: 0.0176\n",
      "Epoch [5/5], Step [5346/10336], Loss: 1.1536\n",
      "Epoch [5/5], Step [5348/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [5350/10336], Loss: 0.1338\n",
      "Epoch [5/5], Step [5352/10336], Loss: 0.9128\n",
      "Epoch [5/5], Step [5354/10336], Loss: 0.1302\n",
      "Epoch [5/5], Step [5356/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [5358/10336], Loss: 0.3928\n",
      "Epoch [5/5], Step [5360/10336], Loss: 0.7633\n",
      "Epoch [5/5], Step [5362/10336], Loss: 0.1483\n",
      "Epoch [5/5], Step [5364/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [5366/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [5368/10336], Loss: 0.2286\n",
      "Epoch [5/5], Step [5370/10336], Loss: 1.3380\n",
      "Epoch [5/5], Step [5372/10336], Loss: 0.1277\n",
      "Epoch [5/5], Step [5374/10336], Loss: 0.0096\n",
      "Epoch [5/5], Step [5376/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [5378/10336], Loss: 0.0226\n",
      "Epoch [5/5], Step [5380/10336], Loss: 0.0685\n",
      "Epoch [5/5], Step [5382/10336], Loss: 1.5527\n",
      "Epoch [5/5], Step [5384/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [5386/10336], Loss: 0.1815\n",
      "Epoch [5/5], Step [5388/10336], Loss: 0.0224\n",
      "Epoch [5/5], Step [5390/10336], Loss: 2.0569\n",
      "Epoch [5/5], Step [5392/10336], Loss: 5.2814\n",
      "Epoch [5/5], Step [5394/10336], Loss: 1.4988\n",
      "Epoch [5/5], Step [5396/10336], Loss: 1.4758\n",
      "Epoch [5/5], Step [5398/10336], Loss: 1.4330\n",
      "Epoch [5/5], Step [5400/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [5402/10336], Loss: 0.0210\n",
      "Epoch [5/5], Step [5404/10336], Loss: 0.1897\n",
      "Epoch [5/5], Step [5406/10336], Loss: 1.4210\n",
      "Epoch [5/5], Step [5408/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [5410/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [5412/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [5414/10336], Loss: 1.3634\n",
      "Epoch [5/5], Step [5416/10336], Loss: 0.1102\n",
      "Epoch [5/5], Step [5418/10336], Loss: 0.2923\n",
      "Epoch [5/5], Step [5420/10336], Loss: 0.4208\n",
      "Epoch [5/5], Step [5422/10336], Loss: 0.1580\n",
      "Epoch [5/5], Step [5424/10336], Loss: 0.0162\n",
      "Epoch [5/5], Step [5426/10336], Loss: 4.1776\n",
      "Epoch [5/5], Step [5428/10336], Loss: 0.1170\n",
      "Epoch [5/5], Step [5430/10336], Loss: 0.3084\n",
      "Epoch [5/5], Step [5432/10336], Loss: 0.0266\n",
      "Epoch [5/5], Step [5434/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [5436/10336], Loss: 0.0806\n",
      "Epoch [5/5], Step [5438/10336], Loss: 0.6290\n",
      "Epoch [5/5], Step [5440/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [5442/10336], Loss: 3.2187\n",
      "Epoch [5/5], Step [5444/10336], Loss: 3.1351\n",
      "Epoch [5/5], Step [5446/10336], Loss: 0.3430\n",
      "Epoch [5/5], Step [5448/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [5450/10336], Loss: 0.1895\n",
      "Epoch [5/5], Step [5452/10336], Loss: 0.1249\n",
      "Epoch [5/5], Step [5454/10336], Loss: 0.3898\n",
      "Epoch [5/5], Step [5456/10336], Loss: 0.0705\n",
      "Epoch [5/5], Step [5458/10336], Loss: 0.6987\n",
      "Epoch [5/5], Step [5460/10336], Loss: 0.8942\n",
      "Epoch [5/5], Step [5462/10336], Loss: 0.9639\n",
      "Epoch [5/5], Step [5464/10336], Loss: 0.0784\n",
      "Epoch [5/5], Step [5466/10336], Loss: 0.3007\n",
      "Epoch [5/5], Step [5468/10336], Loss: 1.2602\n",
      "Epoch [5/5], Step [5470/10336], Loss: 0.4207\n",
      "Epoch [5/5], Step [5472/10336], Loss: 0.7640\n",
      "Epoch [5/5], Step [5474/10336], Loss: 0.0280\n",
      "Epoch [5/5], Step [5476/10336], Loss: 0.1760\n",
      "Epoch [5/5], Step [5478/10336], Loss: 0.4128\n",
      "Epoch [5/5], Step [5480/10336], Loss: 0.0494\n",
      "Epoch [5/5], Step [5482/10336], Loss: 0.0262\n",
      "Epoch [5/5], Step [5484/10336], Loss: 0.5889\n",
      "Epoch [5/5], Step [5486/10336], Loss: 0.8929\n",
      "Epoch [5/5], Step [5488/10336], Loss: 0.4165\n",
      "Epoch [5/5], Step [5490/10336], Loss: 0.0346\n",
      "Epoch [5/5], Step [5492/10336], Loss: 1.0274\n",
      "Epoch [5/5], Step [5494/10336], Loss: 0.0260\n",
      "Epoch [5/5], Step [5496/10336], Loss: 0.0770\n",
      "Epoch [5/5], Step [5498/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [5500/10336], Loss: 0.8401\n",
      "Epoch [5/5], Step [5502/10336], Loss: 1.3164\n",
      "Epoch [5/5], Step [5504/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [5506/10336], Loss: 0.8096\n",
      "Epoch [5/5], Step [5508/10336], Loss: 0.9933\n",
      "Epoch [5/5], Step [5510/10336], Loss: 0.7899\n",
      "Epoch [5/5], Step [5512/10336], Loss: 0.3474\n",
      "Epoch [5/5], Step [5514/10336], Loss: 1.0421\n",
      "Epoch [5/5], Step [5516/10336], Loss: 0.0168\n",
      "Epoch [5/5], Step [5518/10336], Loss: 0.5236\n",
      "Epoch [5/5], Step [5520/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [5522/10336], Loss: 0.1205\n",
      "Epoch [5/5], Step [5524/10336], Loss: 1.7039\n",
      "Epoch [5/5], Step [5526/10336], Loss: 0.0208\n",
      "Epoch [5/5], Step [5528/10336], Loss: 0.4405\n",
      "Epoch [5/5], Step [5530/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [5532/10336], Loss: 2.3872\n",
      "Epoch [5/5], Step [5534/10336], Loss: 0.0095\n",
      "Epoch [5/5], Step [5536/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [5538/10336], Loss: 0.8562\n",
      "Epoch [5/5], Step [5540/10336], Loss: 0.5033\n",
      "Epoch [5/5], Step [5542/10336], Loss: 2.6670\n",
      "Epoch [5/5], Step [5544/10336], Loss: 1.8279\n",
      "Epoch [5/5], Step [5546/10336], Loss: 0.0541\n",
      "Epoch [5/5], Step [5548/10336], Loss: 1.2310\n",
      "Epoch [5/5], Step [5550/10336], Loss: 2.1080\n",
      "Epoch [5/5], Step [5552/10336], Loss: 0.0107\n",
      "Epoch [5/5], Step [5554/10336], Loss: 5.4025\n",
      "Epoch [5/5], Step [5556/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [5558/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [5560/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [5562/10336], Loss: 0.0190\n",
      "Epoch [5/5], Step [5564/10336], Loss: 3.1445\n",
      "Epoch [5/5], Step [5566/10336], Loss: 0.0669\n",
      "Epoch [5/5], Step [5568/10336], Loss: 0.0210\n",
      "Epoch [5/5], Step [5570/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [5572/10336], Loss: 0.0900\n",
      "Epoch [5/5], Step [5574/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [5576/10336], Loss: 1.4402\n",
      "Epoch [5/5], Step [5578/10336], Loss: 0.1244\n",
      "Epoch [5/5], Step [5580/10336], Loss: 0.9122\n",
      "Epoch [5/5], Step [5582/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [5584/10336], Loss: 1.5399\n",
      "Epoch [5/5], Step [5586/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [5588/10336], Loss: 1.4955\n",
      "Epoch [5/5], Step [5590/10336], Loss: 0.1145\n",
      "Epoch [5/5], Step [5592/10336], Loss: 2.7573\n",
      "Epoch [5/5], Step [5594/10336], Loss: 0.0655\n",
      "Epoch [5/5], Step [5596/10336], Loss: 3.0307\n",
      "Epoch [5/5], Step [5598/10336], Loss: 3.9836\n",
      "Epoch [5/5], Step [5600/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [5602/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [5604/10336], Loss: 0.0066\n",
      "Epoch [5/5], Step [5606/10336], Loss: 0.0776\n",
      "Epoch [5/5], Step [5608/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [5610/10336], Loss: 0.2373\n",
      "Epoch [5/5], Step [5612/10336], Loss: 0.2875\n",
      "Epoch [5/5], Step [5614/10336], Loss: 0.1960\n",
      "Epoch [5/5], Step [5616/10336], Loss: 2.1177\n",
      "Epoch [5/5], Step [5618/10336], Loss: 0.5255\n",
      "Epoch [5/5], Step [5620/10336], Loss: 0.0873\n",
      "Epoch [5/5], Step [5622/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [5624/10336], Loss: 1.0783\n",
      "Epoch [5/5], Step [5626/10336], Loss: 0.1277\n",
      "Epoch [5/5], Step [5628/10336], Loss: 0.0116\n",
      "Epoch [5/5], Step [5630/10336], Loss: 0.1767\n",
      "Epoch [5/5], Step [5632/10336], Loss: 0.1644\n",
      "Epoch [5/5], Step [5634/10336], Loss: 1.2253\n",
      "Epoch [5/5], Step [5636/10336], Loss: 1.4030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [5638/10336], Loss: 0.7103\n",
      "Epoch [5/5], Step [5640/10336], Loss: 0.7790\n",
      "Epoch [5/5], Step [5642/10336], Loss: 0.5759\n",
      "Epoch [5/5], Step [5644/10336], Loss: 0.2023\n",
      "Epoch [5/5], Step [5646/10336], Loss: 0.6703\n",
      "Epoch [5/5], Step [5648/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [5650/10336], Loss: 0.9565\n",
      "Epoch [5/5], Step [5652/10336], Loss: 0.0145\n",
      "Epoch [5/5], Step [5654/10336], Loss: 0.1367\n",
      "Epoch [5/5], Step [5656/10336], Loss: 0.0141\n",
      "Epoch [5/5], Step [5658/10336], Loss: 0.0851\n",
      "Epoch [5/5], Step [5660/10336], Loss: 0.0416\n",
      "Epoch [5/5], Step [5662/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [5664/10336], Loss: 0.8194\n",
      "Epoch [5/5], Step [5666/10336], Loss: 0.0196\n",
      "Epoch [5/5], Step [5668/10336], Loss: 0.0110\n",
      "Epoch [5/5], Step [5670/10336], Loss: 0.1038\n",
      "Epoch [5/5], Step [5672/10336], Loss: 0.1010\n",
      "Epoch [5/5], Step [5674/10336], Loss: 0.0095\n",
      "Epoch [5/5], Step [5676/10336], Loss: 2.4810\n",
      "Epoch [5/5], Step [5678/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [5680/10336], Loss: 2.0526\n",
      "Epoch [5/5], Step [5682/10336], Loss: 0.8380\n",
      "Epoch [5/5], Step [5684/10336], Loss: 0.1110\n",
      "Epoch [5/5], Step [5686/10336], Loss: 2.2692\n",
      "Epoch [5/5], Step [5688/10336], Loss: 0.0853\n",
      "Epoch [5/5], Step [5690/10336], Loss: 0.3694\n",
      "Epoch [5/5], Step [5692/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [5694/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [5696/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [5698/10336], Loss: 1.0109\n",
      "Epoch [5/5], Step [5700/10336], Loss: 0.0123\n",
      "Epoch [5/5], Step [5702/10336], Loss: 0.1418\n",
      "Epoch [5/5], Step [5704/10336], Loss: 3.2254\n",
      "Epoch [5/5], Step [5706/10336], Loss: 0.0275\n",
      "Epoch [5/5], Step [5708/10336], Loss: 0.0518\n",
      "Epoch [5/5], Step [5710/10336], Loss: 0.7139\n",
      "Epoch [5/5], Step [5712/10336], Loss: 0.0421\n",
      "Epoch [5/5], Step [5714/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [5716/10336], Loss: 2.8105\n",
      "Epoch [5/5], Step [5718/10336], Loss: 0.1408\n",
      "Epoch [5/5], Step [5720/10336], Loss: 2.0103\n",
      "Epoch [5/5], Step [5722/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [5724/10336], Loss: 0.0836\n",
      "Epoch [5/5], Step [5726/10336], Loss: 0.0286\n",
      "Epoch [5/5], Step [5728/10336], Loss: 0.1307\n",
      "Epoch [5/5], Step [5730/10336], Loss: 0.5048\n",
      "Epoch [5/5], Step [5732/10336], Loss: 0.0457\n",
      "Epoch [5/5], Step [5734/10336], Loss: 1.9281\n",
      "Epoch [5/5], Step [5736/10336], Loss: 0.0088\n",
      "Epoch [5/5], Step [5738/10336], Loss: 0.0122\n",
      "Epoch [5/5], Step [5740/10336], Loss: 0.1089\n",
      "Epoch [5/5], Step [5742/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [5744/10336], Loss: 0.0073\n",
      "Epoch [5/5], Step [5746/10336], Loss: 0.3926\n",
      "Epoch [5/5], Step [5748/10336], Loss: 0.0188\n",
      "Epoch [5/5], Step [5750/10336], Loss: 0.7940\n",
      "Epoch [5/5], Step [5752/10336], Loss: 2.4306\n",
      "Epoch [5/5], Step [5754/10336], Loss: 0.0461\n",
      "Epoch [5/5], Step [5756/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [5758/10336], Loss: 0.6837\n",
      "Epoch [5/5], Step [5760/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [5762/10336], Loss: 0.0999\n",
      "Epoch [5/5], Step [5764/10336], Loss: 0.4352\n",
      "Epoch [5/5], Step [5766/10336], Loss: 0.2774\n",
      "Epoch [5/5], Step [5768/10336], Loss: 3.6185\n",
      "Epoch [5/5], Step [5770/10336], Loss: 0.1276\n",
      "Epoch [5/5], Step [5772/10336], Loss: 0.1427\n",
      "Epoch [5/5], Step [5774/10336], Loss: 2.0826\n",
      "Epoch [5/5], Step [5776/10336], Loss: 0.0126\n",
      "Epoch [5/5], Step [5778/10336], Loss: 0.2291\n",
      "Epoch [5/5], Step [5780/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [5782/10336], Loss: 0.0230\n",
      "Epoch [5/5], Step [5784/10336], Loss: 0.4964\n",
      "Epoch [5/5], Step [5786/10336], Loss: 0.4699\n",
      "Epoch [5/5], Step [5788/10336], Loss: 0.0412\n",
      "Epoch [5/5], Step [5790/10336], Loss: 0.1237\n",
      "Epoch [5/5], Step [5792/10336], Loss: 0.2504\n",
      "Epoch [5/5], Step [5794/10336], Loss: 0.6843\n",
      "Epoch [5/5], Step [5796/10336], Loss: 0.9331\n",
      "Epoch [5/5], Step [5798/10336], Loss: 0.1158\n",
      "Epoch [5/5], Step [5800/10336], Loss: 0.7424\n",
      "Epoch [5/5], Step [5802/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [5804/10336], Loss: 0.1052\n",
      "Epoch [5/5], Step [5806/10336], Loss: 3.2826\n",
      "Epoch [5/5], Step [5808/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [5810/10336], Loss: 0.0140\n",
      "Epoch [5/5], Step [5812/10336], Loss: 0.0412\n",
      "Epoch [5/5], Step [5814/10336], Loss: 0.0110\n",
      "Epoch [5/5], Step [5816/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [5818/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [5820/10336], Loss: 0.1231\n",
      "Epoch [5/5], Step [5822/10336], Loss: 0.8474\n",
      "Epoch [5/5], Step [5824/10336], Loss: 0.2405\n",
      "Epoch [5/5], Step [5826/10336], Loss: 0.0307\n",
      "Epoch [5/5], Step [5828/10336], Loss: 0.1754\n",
      "Epoch [5/5], Step [5830/10336], Loss: 0.5593\n",
      "Epoch [5/5], Step [5832/10336], Loss: 0.6223\n",
      "Epoch [5/5], Step [5834/10336], Loss: 0.1963\n",
      "Epoch [5/5], Step [5836/10336], Loss: 0.4790\n",
      "Epoch [5/5], Step [5838/10336], Loss: 0.0175\n",
      "Epoch [5/5], Step [5840/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [5842/10336], Loss: 0.0529\n",
      "Epoch [5/5], Step [5844/10336], Loss: 0.1615\n",
      "Epoch [5/5], Step [5846/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [5848/10336], Loss: 0.0752\n",
      "Epoch [5/5], Step [5850/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [5852/10336], Loss: 0.4934\n",
      "Epoch [5/5], Step [5854/10336], Loss: 0.0960\n",
      "Epoch [5/5], Step [5856/10336], Loss: 0.1068\n",
      "Epoch [5/5], Step [5858/10336], Loss: 0.0689\n",
      "Epoch [5/5], Step [5860/10336], Loss: 0.5314\n",
      "Epoch [5/5], Step [5862/10336], Loss: 1.5079\n",
      "Epoch [5/5], Step [5864/10336], Loss: 0.0220\n",
      "Epoch [5/5], Step [5866/10336], Loss: 0.4058\n",
      "Epoch [5/5], Step [5868/10336], Loss: 0.0223\n",
      "Epoch [5/5], Step [5870/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [5872/10336], Loss: 0.8229\n",
      "Epoch [5/5], Step [5874/10336], Loss: 0.0186\n",
      "Epoch [5/5], Step [5876/10336], Loss: 0.7546\n",
      "Epoch [5/5], Step [5878/10336], Loss: 2.5748\n",
      "Epoch [5/5], Step [5880/10336], Loss: 0.0451\n",
      "Epoch [5/5], Step [5882/10336], Loss: 0.2757\n",
      "Epoch [5/5], Step [5884/10336], Loss: 0.2504\n",
      "Epoch [5/5], Step [5886/10336], Loss: 2.7541\n",
      "Epoch [5/5], Step [5888/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [5890/10336], Loss: 0.0150\n",
      "Epoch [5/5], Step [5892/10336], Loss: 0.0406\n",
      "Epoch [5/5], Step [5894/10336], Loss: 3.2314\n",
      "Epoch [5/5], Step [5896/10336], Loss: 0.0256\n",
      "Epoch [5/5], Step [5898/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [5900/10336], Loss: 2.4983\n",
      "Epoch [5/5], Step [5902/10336], Loss: 0.1065\n",
      "Epoch [5/5], Step [5904/10336], Loss: 0.8519\n",
      "Epoch [5/5], Step [5906/10336], Loss: 0.0122\n",
      "Epoch [5/5], Step [5908/10336], Loss: 0.1303\n",
      "Epoch [5/5], Step [5910/10336], Loss: 2.6242\n",
      "Epoch [5/5], Step [5912/10336], Loss: 0.1987\n",
      "Epoch [5/5], Step [5914/10336], Loss: 0.1682\n",
      "Epoch [5/5], Step [5916/10336], Loss: 0.1871\n",
      "Epoch [5/5], Step [5918/10336], Loss: 0.0310\n",
      "Epoch [5/5], Step [5920/10336], Loss: 0.6652\n",
      "Epoch [5/5], Step [5922/10336], Loss: 0.0462\n",
      "Epoch [5/5], Step [5924/10336], Loss: 3.3839\n",
      "Epoch [5/5], Step [5926/10336], Loss: 0.8144\n",
      "Epoch [5/5], Step [5928/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [5930/10336], Loss: 1.6010\n",
      "Epoch [5/5], Step [5932/10336], Loss: 0.1426\n",
      "Epoch [5/5], Step [5934/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [5936/10336], Loss: 0.0203\n",
      "Epoch [5/5], Step [5938/10336], Loss: 0.7155\n",
      "Epoch [5/5], Step [5940/10336], Loss: 3.7944\n",
      "Epoch [5/5], Step [5942/10336], Loss: 1.1712\n",
      "Epoch [5/5], Step [5944/10336], Loss: 0.1302\n",
      "Epoch [5/5], Step [5946/10336], Loss: 0.0854\n",
      "Epoch [5/5], Step [5948/10336], Loss: 0.2386\n",
      "Epoch [5/5], Step [5950/10336], Loss: 0.2230\n",
      "Epoch [5/5], Step [5952/10336], Loss: 0.5479\n",
      "Epoch [5/5], Step [5954/10336], Loss: 0.0782\n",
      "Epoch [5/5], Step [5956/10336], Loss: 0.5293\n",
      "Epoch [5/5], Step [5958/10336], Loss: 2.7407\n",
      "Epoch [5/5], Step [5960/10336], Loss: 0.3038\n",
      "Epoch [5/5], Step [5962/10336], Loss: 0.0974\n",
      "Epoch [5/5], Step [5964/10336], Loss: 0.3215\n",
      "Epoch [5/5], Step [5966/10336], Loss: 0.1334\n",
      "Epoch [5/5], Step [5968/10336], Loss: 0.7411\n",
      "Epoch [5/5], Step [5970/10336], Loss: 0.1848\n",
      "Epoch [5/5], Step [5972/10336], Loss: 0.8382\n",
      "Epoch [5/5], Step [5974/10336], Loss: 0.1009\n",
      "Epoch [5/5], Step [5976/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [5978/10336], Loss: 1.2251\n",
      "Epoch [5/5], Step [5980/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [5982/10336], Loss: 0.1619\n",
      "Epoch [5/5], Step [5984/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [5986/10336], Loss: 3.1625\n",
      "Epoch [5/5], Step [5988/10336], Loss: 1.3741\n",
      "Epoch [5/5], Step [5990/10336], Loss: 0.1618\n",
      "Epoch [5/5], Step [5992/10336], Loss: 2.0639\n",
      "Epoch [5/5], Step [5994/10336], Loss: 0.0309\n",
      "Epoch [5/5], Step [5996/10336], Loss: 0.1234\n",
      "Epoch [5/5], Step [5998/10336], Loss: 0.0180\n",
      "Epoch [5/5], Step [6000/10336], Loss: 0.5645\n",
      "Epoch [5/5], Step [6002/10336], Loss: 0.0353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [6004/10336], Loss: 3.0383\n",
      "Epoch [5/5], Step [6006/10336], Loss: 0.0137\n",
      "Epoch [5/5], Step [6008/10336], Loss: 0.0841\n",
      "Epoch [5/5], Step [6010/10336], Loss: 0.1373\n",
      "Epoch [5/5], Step [6012/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [6014/10336], Loss: 0.2948\n",
      "Epoch [5/5], Step [6016/10336], Loss: 0.2569\n",
      "Epoch [5/5], Step [6018/10336], Loss: 0.1889\n",
      "Epoch [5/5], Step [6020/10336], Loss: 1.0124\n",
      "Epoch [5/5], Step [6022/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [6024/10336], Loss: 0.7439\n",
      "Epoch [5/5], Step [6026/10336], Loss: 0.0804\n",
      "Epoch [5/5], Step [6028/10336], Loss: 0.1621\n",
      "Epoch [5/5], Step [6030/10336], Loss: 0.0930\n",
      "Epoch [5/5], Step [6032/10336], Loss: 0.3382\n",
      "Epoch [5/5], Step [6034/10336], Loss: 1.2453\n",
      "Epoch [5/5], Step [6036/10336], Loss: 0.5194\n",
      "Epoch [5/5], Step [6038/10336], Loss: 0.7964\n",
      "Epoch [5/5], Step [6040/10336], Loss: 0.4423\n",
      "Epoch [5/5], Step [6042/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [6044/10336], Loss: 1.2413\n",
      "Epoch [5/5], Step [6046/10336], Loss: 0.4619\n",
      "Epoch [5/5], Step [6048/10336], Loss: 0.0778\n",
      "Epoch [5/5], Step [6050/10336], Loss: 0.2646\n",
      "Epoch [5/5], Step [6052/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [6054/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [6056/10336], Loss: 1.8149\n",
      "Epoch [5/5], Step [6058/10336], Loss: 0.0170\n",
      "Epoch [5/5], Step [6060/10336], Loss: 2.9947\n",
      "Epoch [5/5], Step [6062/10336], Loss: 2.2552\n",
      "Epoch [5/5], Step [6064/10336], Loss: 0.0192\n",
      "Epoch [5/5], Step [6066/10336], Loss: 0.0159\n",
      "Epoch [5/5], Step [6068/10336], Loss: 0.0830\n",
      "Epoch [5/5], Step [6070/10336], Loss: 2.0055\n",
      "Epoch [5/5], Step [6072/10336], Loss: 0.5693\n",
      "Epoch [5/5], Step [6074/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [6076/10336], Loss: 0.6532\n",
      "Epoch [5/5], Step [6078/10336], Loss: 0.0613\n",
      "Epoch [5/5], Step [6080/10336], Loss: 1.2673\n",
      "Epoch [5/5], Step [6082/10336], Loss: 0.8296\n",
      "Epoch [5/5], Step [6084/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [6086/10336], Loss: 0.1027\n",
      "Epoch [5/5], Step [6088/10336], Loss: 1.8011\n",
      "Epoch [5/5], Step [6090/10336], Loss: 0.1407\n",
      "Epoch [5/5], Step [6092/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [6094/10336], Loss: 1.0650\n",
      "Epoch [5/5], Step [6096/10336], Loss: 1.6549\n",
      "Epoch [5/5], Step [6098/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [6100/10336], Loss: 2.9221\n",
      "Epoch [5/5], Step [6102/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [6104/10336], Loss: 1.4647\n",
      "Epoch [5/5], Step [6106/10336], Loss: 0.0136\n",
      "Epoch [5/5], Step [6108/10336], Loss: 1.5492\n",
      "Epoch [5/5], Step [6110/10336], Loss: 1.0312\n",
      "Epoch [5/5], Step [6112/10336], Loss: 1.9825\n",
      "Epoch [5/5], Step [6114/10336], Loss: 0.2284\n",
      "Epoch [5/5], Step [6116/10336], Loss: 0.8637\n",
      "Epoch [5/5], Step [6118/10336], Loss: 0.2522\n",
      "Epoch [5/5], Step [6120/10336], Loss: 0.9607\n",
      "Epoch [5/5], Step [6122/10336], Loss: 1.1502\n",
      "Epoch [5/5], Step [6124/10336], Loss: 0.0136\n",
      "Epoch [5/5], Step [6126/10336], Loss: 0.7647\n",
      "Epoch [5/5], Step [6128/10336], Loss: 0.5495\n",
      "Epoch [5/5], Step [6130/10336], Loss: 0.1088\n",
      "Epoch [5/5], Step [6132/10336], Loss: 1.7181\n",
      "Epoch [5/5], Step [6134/10336], Loss: 0.7746\n",
      "Epoch [5/5], Step [6136/10336], Loss: 0.5025\n",
      "Epoch [5/5], Step [6138/10336], Loss: 0.7763\n",
      "Epoch [5/5], Step [6140/10336], Loss: 0.0108\n",
      "Epoch [5/5], Step [6142/10336], Loss: 0.0803\n",
      "Epoch [5/5], Step [6144/10336], Loss: 0.7047\n",
      "Epoch [5/5], Step [6146/10336], Loss: 2.0077\n",
      "Epoch [5/5], Step [6148/10336], Loss: 0.0609\n",
      "Epoch [5/5], Step [6150/10336], Loss: 0.6831\n",
      "Epoch [5/5], Step [6152/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [6154/10336], Loss: 0.1117\n",
      "Epoch [5/5], Step [6156/10336], Loss: 0.0098\n",
      "Epoch [5/5], Step [6158/10336], Loss: 2.3604\n",
      "Epoch [5/5], Step [6160/10336], Loss: 0.9385\n",
      "Epoch [5/5], Step [6162/10336], Loss: 1.9764\n",
      "Epoch [5/5], Step [6164/10336], Loss: 1.8773\n",
      "Epoch [5/5], Step [6166/10336], Loss: 0.0109\n",
      "Epoch [5/5], Step [6168/10336], Loss: 0.0185\n",
      "Epoch [5/5], Step [6170/10336], Loss: 0.1252\n",
      "Epoch [5/5], Step [6172/10336], Loss: 0.4149\n",
      "Epoch [5/5], Step [6174/10336], Loss: 2.5777\n",
      "Epoch [5/5], Step [6176/10336], Loss: 0.1671\n",
      "Epoch [5/5], Step [6178/10336], Loss: 1.9377\n",
      "Epoch [5/5], Step [6180/10336], Loss: 0.0766\n",
      "Epoch [5/5], Step [6182/10336], Loss: 0.7216\n",
      "Epoch [5/5], Step [6184/10336], Loss: 0.0743\n",
      "Epoch [5/5], Step [6186/10336], Loss: 0.0228\n",
      "Epoch [5/5], Step [6188/10336], Loss: 0.6055\n",
      "Epoch [5/5], Step [6190/10336], Loss: 0.3679\n",
      "Epoch [5/5], Step [6192/10336], Loss: 1.0165\n",
      "Epoch [5/5], Step [6194/10336], Loss: 0.2269\n",
      "Epoch [5/5], Step [6196/10336], Loss: 1.5243\n",
      "Epoch [5/5], Step [6198/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [6200/10336], Loss: 0.0690\n",
      "Epoch [5/5], Step [6202/10336], Loss: 0.6076\n",
      "Epoch [5/5], Step [6204/10336], Loss: 0.6209\n",
      "Epoch [5/5], Step [6206/10336], Loss: 2.1888\n",
      "Epoch [5/5], Step [6208/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [6210/10336], Loss: 0.7856\n",
      "Epoch [5/5], Step [6212/10336], Loss: 0.0250\n",
      "Epoch [5/5], Step [6214/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [6216/10336], Loss: 0.0080\n",
      "Epoch [5/5], Step [6218/10336], Loss: 0.0211\n",
      "Epoch [5/5], Step [6220/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [6222/10336], Loss: 0.0270\n",
      "Epoch [5/5], Step [6224/10336], Loss: 0.0223\n",
      "Epoch [5/5], Step [6226/10336], Loss: 0.1414\n",
      "Epoch [5/5], Step [6228/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [6230/10336], Loss: 1.3976\n",
      "Epoch [5/5], Step [6232/10336], Loss: 0.2571\n",
      "Epoch [5/5], Step [6234/10336], Loss: 1.3287\n",
      "Epoch [5/5], Step [6236/10336], Loss: 0.9678\n",
      "Epoch [5/5], Step [6238/10336], Loss: 0.2293\n",
      "Epoch [5/5], Step [6240/10336], Loss: 1.2674\n",
      "Epoch [5/5], Step [6242/10336], Loss: 0.6626\n",
      "Epoch [5/5], Step [6244/10336], Loss: 0.0821\n",
      "Epoch [5/5], Step [6246/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [6248/10336], Loss: 0.0532\n",
      "Epoch [5/5], Step [6250/10336], Loss: 0.1202\n",
      "Epoch [5/5], Step [6252/10336], Loss: 0.2373\n",
      "Epoch [5/5], Step [6254/10336], Loss: 0.0110\n",
      "Epoch [5/5], Step [6256/10336], Loss: 0.0187\n",
      "Epoch [5/5], Step [6258/10336], Loss: 0.0254\n",
      "Epoch [5/5], Step [6260/10336], Loss: 0.2206\n",
      "Epoch [5/5], Step [6262/10336], Loss: 0.0841\n",
      "Epoch [5/5], Step [6264/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [6266/10336], Loss: 4.5690\n",
      "Epoch [5/5], Step [6268/10336], Loss: 0.0283\n",
      "Epoch [5/5], Step [6270/10336], Loss: 0.6337\n",
      "Epoch [5/5], Step [6272/10336], Loss: 1.1028\n",
      "Epoch [5/5], Step [6274/10336], Loss: 2.0970\n",
      "Epoch [5/5], Step [6276/10336], Loss: 0.4217\n",
      "Epoch [5/5], Step [6278/10336], Loss: 0.5611\n",
      "Epoch [5/5], Step [6280/10336], Loss: 0.0633\n",
      "Epoch [5/5], Step [6282/10336], Loss: 2.9335\n",
      "Epoch [5/5], Step [6284/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [6286/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [6288/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [6290/10336], Loss: 0.2836\n",
      "Epoch [5/5], Step [6292/10336], Loss: 0.7224\n",
      "Epoch [5/5], Step [6294/10336], Loss: 0.3987\n",
      "Epoch [5/5], Step [6296/10336], Loss: 0.0137\n",
      "Epoch [5/5], Step [6298/10336], Loss: 2.1075\n",
      "Epoch [5/5], Step [6300/10336], Loss: 0.1040\n",
      "Epoch [5/5], Step [6302/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [6304/10336], Loss: 0.0917\n",
      "Epoch [5/5], Step [6306/10336], Loss: 2.5527\n",
      "Epoch [5/5], Step [6308/10336], Loss: 0.0441\n",
      "Epoch [5/5], Step [6310/10336], Loss: 0.4448\n",
      "Epoch [5/5], Step [6312/10336], Loss: 2.4326\n",
      "Epoch [5/5], Step [6314/10336], Loss: 0.0414\n",
      "Epoch [5/5], Step [6316/10336], Loss: 0.0593\n",
      "Epoch [5/5], Step [6318/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [6320/10336], Loss: 0.0786\n",
      "Epoch [5/5], Step [6322/10336], Loss: 1.7507\n",
      "Epoch [5/5], Step [6324/10336], Loss: 0.5916\n",
      "Epoch [5/5], Step [6326/10336], Loss: 0.0025\n",
      "Epoch [5/5], Step [6328/10336], Loss: 0.0743\n",
      "Epoch [5/5], Step [6330/10336], Loss: 0.0367\n",
      "Epoch [5/5], Step [6332/10336], Loss: 0.0105\n",
      "Epoch [5/5], Step [6334/10336], Loss: 0.7437\n",
      "Epoch [5/5], Step [6336/10336], Loss: 0.2549\n",
      "Epoch [5/5], Step [6338/10336], Loss: 2.5894\n",
      "Epoch [5/5], Step [6340/10336], Loss: 1.0499\n",
      "Epoch [5/5], Step [6342/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [6344/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [6346/10336], Loss: 0.0070\n",
      "Epoch [5/5], Step [6348/10336], Loss: 0.5128\n",
      "Epoch [5/5], Step [6350/10336], Loss: 0.1178\n",
      "Epoch [5/5], Step [6352/10336], Loss: 0.6778\n",
      "Epoch [5/5], Step [6354/10336], Loss: 3.2020\n",
      "Epoch [5/5], Step [6356/10336], Loss: 0.0437\n",
      "Epoch [5/5], Step [6358/10336], Loss: 0.0635\n",
      "Epoch [5/5], Step [6360/10336], Loss: 0.0087\n",
      "Epoch [5/5], Step [6362/10336], Loss: 0.4848\n",
      "Epoch [5/5], Step [6364/10336], Loss: 0.2153\n",
      "Epoch [5/5], Step [6366/10336], Loss: 1.5310\n",
      "Epoch [5/5], Step [6368/10336], Loss: 1.3145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [6370/10336], Loss: 2.2898\n",
      "Epoch [5/5], Step [6372/10336], Loss: 0.3556\n",
      "Epoch [5/5], Step [6374/10336], Loss: 0.3435\n",
      "Epoch [5/5], Step [6376/10336], Loss: 0.0751\n",
      "Epoch [5/5], Step [6378/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [6380/10336], Loss: 0.3584\n",
      "Epoch [5/5], Step [6382/10336], Loss: 0.8324\n",
      "Epoch [5/5], Step [6384/10336], Loss: 2.3065\n",
      "Epoch [5/5], Step [6386/10336], Loss: 0.2134\n",
      "Epoch [5/5], Step [6388/10336], Loss: 1.6372\n",
      "Epoch [5/5], Step [6390/10336], Loss: 0.1217\n",
      "Epoch [5/5], Step [6392/10336], Loss: 0.0270\n",
      "Epoch [5/5], Step [6394/10336], Loss: 0.3411\n",
      "Epoch [5/5], Step [6396/10336], Loss: 0.1489\n",
      "Epoch [5/5], Step [6398/10336], Loss: 0.0187\n",
      "Epoch [5/5], Step [6400/10336], Loss: 0.1976\n",
      "Epoch [5/5], Step [6402/10336], Loss: 1.0930\n",
      "Epoch [5/5], Step [6404/10336], Loss: 0.0588\n",
      "Epoch [5/5], Step [6406/10336], Loss: 0.7313\n",
      "Epoch [5/5], Step [6408/10336], Loss: 0.5749\n",
      "Epoch [5/5], Step [6410/10336], Loss: 0.0185\n",
      "Epoch [5/5], Step [6412/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [6414/10336], Loss: 0.2899\n",
      "Epoch [5/5], Step [6416/10336], Loss: 0.4369\n",
      "Epoch [5/5], Step [6418/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [6420/10336], Loss: 0.5157\n",
      "Epoch [5/5], Step [6422/10336], Loss: 0.9222\n",
      "Epoch [5/5], Step [6424/10336], Loss: 0.0390\n",
      "Epoch [5/5], Step [6426/10336], Loss: 0.6185\n",
      "Epoch [5/5], Step [6428/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [6430/10336], Loss: 1.8004\n",
      "Epoch [5/5], Step [6432/10336], Loss: 0.0049\n",
      "Epoch [5/5], Step [6434/10336], Loss: 1.6965\n",
      "Epoch [5/5], Step [6436/10336], Loss: 0.2044\n",
      "Epoch [5/5], Step [6438/10336], Loss: 0.0221\n",
      "Epoch [5/5], Step [6440/10336], Loss: 0.0681\n",
      "Epoch [5/5], Step [6442/10336], Loss: 1.2808\n",
      "Epoch [5/5], Step [6444/10336], Loss: 0.3161\n",
      "Epoch [5/5], Step [6446/10336], Loss: 0.0374\n",
      "Epoch [5/5], Step [6448/10336], Loss: 2.2446\n",
      "Epoch [5/5], Step [6450/10336], Loss: 0.0071\n",
      "Epoch [5/5], Step [6452/10336], Loss: 0.0056\n",
      "Epoch [5/5], Step [6454/10336], Loss: 1.3977\n",
      "Epoch [5/5], Step [6456/10336], Loss: 0.2800\n",
      "Epoch [5/5], Step [6458/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [6460/10336], Loss: 0.1220\n",
      "Epoch [5/5], Step [6462/10336], Loss: 1.0258\n",
      "Epoch [5/5], Step [6464/10336], Loss: 0.1817\n",
      "Epoch [5/5], Step [6466/10336], Loss: 0.6399\n",
      "Epoch [5/5], Step [6468/10336], Loss: 0.9159\n",
      "Epoch [5/5], Step [6470/10336], Loss: 0.6020\n",
      "Epoch [5/5], Step [6472/10336], Loss: 0.1029\n",
      "Epoch [5/5], Step [6474/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [6476/10336], Loss: 1.2369\n",
      "Epoch [5/5], Step [6478/10336], Loss: 1.8891\n",
      "Epoch [5/5], Step [6480/10336], Loss: 3.0456\n",
      "Epoch [5/5], Step [6482/10336], Loss: 0.3158\n",
      "Epoch [5/5], Step [6484/10336], Loss: 0.0489\n",
      "Epoch [5/5], Step [6486/10336], Loss: 0.2060\n",
      "Epoch [5/5], Step [6488/10336], Loss: 1.0247\n",
      "Epoch [5/5], Step [6490/10336], Loss: 0.5228\n",
      "Epoch [5/5], Step [6492/10336], Loss: 0.0646\n",
      "Epoch [5/5], Step [6494/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [6496/10336], Loss: 0.1809\n",
      "Epoch [5/5], Step [6498/10336], Loss: 3.0643\n",
      "Epoch [5/5], Step [6500/10336], Loss: 1.8686\n",
      "Epoch [5/5], Step [6502/10336], Loss: 0.0969\n",
      "Epoch [5/5], Step [6504/10336], Loss: 0.2429\n",
      "Epoch [5/5], Step [6506/10336], Loss: 1.1663\n",
      "Epoch [5/5], Step [6508/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [6510/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [6512/10336], Loss: 0.0240\n",
      "Epoch [5/5], Step [6514/10336], Loss: 0.0092\n",
      "Epoch [5/5], Step [6516/10336], Loss: 0.6198\n",
      "Epoch [5/5], Step [6518/10336], Loss: 0.0376\n",
      "Epoch [5/5], Step [6520/10336], Loss: 0.0230\n",
      "Epoch [5/5], Step [6522/10336], Loss: 2.1866\n",
      "Epoch [5/5], Step [6524/10336], Loss: 0.3803\n",
      "Epoch [5/5], Step [6526/10336], Loss: 0.9575\n",
      "Epoch [5/5], Step [6528/10336], Loss: 0.3218\n",
      "Epoch [5/5], Step [6530/10336], Loss: 0.9582\n",
      "Epoch [5/5], Step [6532/10336], Loss: 0.2672\n",
      "Epoch [5/5], Step [6534/10336], Loss: 2.3478\n",
      "Epoch [5/5], Step [6536/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [6538/10336], Loss: 0.9930\n",
      "Epoch [5/5], Step [6540/10336], Loss: 0.4430\n",
      "Epoch [5/5], Step [6542/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [6544/10336], Loss: 0.1618\n",
      "Epoch [5/5], Step [6546/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [6548/10336], Loss: 1.3775\n",
      "Epoch [5/5], Step [6550/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [6552/10336], Loss: 0.0677\n",
      "Epoch [5/5], Step [6554/10336], Loss: 0.0110\n",
      "Epoch [5/5], Step [6556/10336], Loss: 1.1752\n",
      "Epoch [5/5], Step [6558/10336], Loss: 0.4256\n",
      "Epoch [5/5], Step [6560/10336], Loss: 0.0679\n",
      "Epoch [5/5], Step [6562/10336], Loss: 1.4943\n",
      "Epoch [5/5], Step [6564/10336], Loss: 0.2390\n",
      "Epoch [5/5], Step [6566/10336], Loss: 0.0564\n",
      "Epoch [5/5], Step [6568/10336], Loss: 0.5541\n",
      "Epoch [5/5], Step [6570/10336], Loss: 0.4844\n",
      "Epoch [5/5], Step [6572/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [6574/10336], Loss: 1.0046\n",
      "Epoch [5/5], Step [6576/10336], Loss: 0.1651\n",
      "Epoch [5/5], Step [6578/10336], Loss: 0.7007\n",
      "Epoch [5/5], Step [6580/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [6582/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [6584/10336], Loss: 1.2815\n",
      "Epoch [5/5], Step [6586/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [6588/10336], Loss: 0.0340\n",
      "Epoch [5/5], Step [6590/10336], Loss: 0.6835\n",
      "Epoch [5/5], Step [6592/10336], Loss: 0.3381\n",
      "Epoch [5/5], Step [6594/10336], Loss: 1.1704\n",
      "Epoch [5/5], Step [6596/10336], Loss: 0.4479\n",
      "Epoch [5/5], Step [6598/10336], Loss: 0.3343\n",
      "Epoch [5/5], Step [6600/10336], Loss: 0.3651\n",
      "Epoch [5/5], Step [6602/10336], Loss: 1.3611\n",
      "Epoch [5/5], Step [6604/10336], Loss: 0.0034\n",
      "Epoch [5/5], Step [6606/10336], Loss: 0.0105\n",
      "Epoch [5/5], Step [6608/10336], Loss: 0.2152\n",
      "Epoch [5/5], Step [6610/10336], Loss: 1.3832\n",
      "Epoch [5/5], Step [6612/10336], Loss: 0.3923\n",
      "Epoch [5/5], Step [6614/10336], Loss: 0.0812\n",
      "Epoch [5/5], Step [6616/10336], Loss: 0.1239\n",
      "Epoch [5/5], Step [6618/10336], Loss: 1.4606\n",
      "Epoch [5/5], Step [6620/10336], Loss: 1.1838\n",
      "Epoch [5/5], Step [6622/10336], Loss: 0.2568\n",
      "Epoch [5/5], Step [6624/10336], Loss: 0.6509\n",
      "Epoch [5/5], Step [6626/10336], Loss: 0.4958\n",
      "Epoch [5/5], Step [6628/10336], Loss: 0.0423\n",
      "Epoch [5/5], Step [6630/10336], Loss: 3.5911\n",
      "Epoch [5/5], Step [6632/10336], Loss: 0.1989\n",
      "Epoch [5/5], Step [6634/10336], Loss: 1.6329\n",
      "Epoch [5/5], Step [6636/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [6638/10336], Loss: 0.3389\n",
      "Epoch [5/5], Step [6640/10336], Loss: 0.0076\n",
      "Epoch [5/5], Step [6642/10336], Loss: 0.0650\n",
      "Epoch [5/5], Step [6644/10336], Loss: 0.3616\n",
      "Epoch [5/5], Step [6646/10336], Loss: 0.0105\n",
      "Epoch [5/5], Step [6648/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [6650/10336], Loss: 0.0252\n",
      "Epoch [5/5], Step [6652/10336], Loss: 2.5429\n",
      "Epoch [5/5], Step [6654/10336], Loss: 0.0494\n",
      "Epoch [5/5], Step [6656/10336], Loss: 0.0947\n",
      "Epoch [5/5], Step [6658/10336], Loss: 0.0400\n",
      "Epoch [5/5], Step [6660/10336], Loss: 0.2318\n",
      "Epoch [5/5], Step [6662/10336], Loss: 0.0775\n",
      "Epoch [5/5], Step [6664/10336], Loss: 1.8662\n",
      "Epoch [5/5], Step [6666/10336], Loss: 0.6524\n",
      "Epoch [5/5], Step [6668/10336], Loss: 0.3788\n",
      "Epoch [5/5], Step [6670/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [6672/10336], Loss: 0.0144\n",
      "Epoch [5/5], Step [6674/10336], Loss: 0.3134\n",
      "Epoch [5/5], Step [6676/10336], Loss: 2.1519\n",
      "Epoch [5/5], Step [6678/10336], Loss: 0.0022\n",
      "Epoch [5/5], Step [6680/10336], Loss: 0.4445\n",
      "Epoch [5/5], Step [6682/10336], Loss: 0.6431\n",
      "Epoch [5/5], Step [6684/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [6686/10336], Loss: 0.9169\n",
      "Epoch [5/5], Step [6688/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [6690/10336], Loss: 0.0160\n",
      "Epoch [5/5], Step [6692/10336], Loss: 0.0424\n",
      "Epoch [5/5], Step [6694/10336], Loss: 0.8175\n",
      "Epoch [5/5], Step [6696/10336], Loss: 0.5732\n",
      "Epoch [5/5], Step [6698/10336], Loss: 2.4305\n",
      "Epoch [5/5], Step [6700/10336], Loss: 1.8271\n",
      "Epoch [5/5], Step [6702/10336], Loss: 0.0745\n",
      "Epoch [5/5], Step [6704/10336], Loss: 2.8862\n",
      "Epoch [5/5], Step [6706/10336], Loss: 0.3410\n",
      "Epoch [5/5], Step [6708/10336], Loss: 0.0582\n",
      "Epoch [5/5], Step [6710/10336], Loss: 0.9290\n",
      "Epoch [5/5], Step [6712/10336], Loss: 0.8739\n",
      "Epoch [5/5], Step [6714/10336], Loss: 1.1706\n",
      "Epoch [5/5], Step [6716/10336], Loss: 0.0368\n",
      "Epoch [5/5], Step [6718/10336], Loss: 0.0440\n",
      "Epoch [5/5], Step [6720/10336], Loss: 0.0377\n",
      "Epoch [5/5], Step [6722/10336], Loss: 1.4866\n",
      "Epoch [5/5], Step [6724/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [6726/10336], Loss: 0.1498\n",
      "Epoch [5/5], Step [6728/10336], Loss: 0.3459\n",
      "Epoch [5/5], Step [6730/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [6732/10336], Loss: 0.4215\n",
      "Epoch [5/5], Step [6734/10336], Loss: 0.7504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [6736/10336], Loss: 0.0383\n",
      "Epoch [5/5], Step [6738/10336], Loss: 0.0535\n",
      "Epoch [5/5], Step [6740/10336], Loss: 1.7929\n",
      "Epoch [5/5], Step [6742/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [6744/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [6746/10336], Loss: 0.2465\n",
      "Epoch [5/5], Step [6748/10336], Loss: 0.0274\n",
      "Epoch [5/5], Step [6750/10336], Loss: 1.7471\n",
      "Epoch [5/5], Step [6752/10336], Loss: 0.8197\n",
      "Epoch [5/5], Step [6754/10336], Loss: 0.0806\n",
      "Epoch [5/5], Step [6756/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [6758/10336], Loss: 2.0322\n",
      "Epoch [5/5], Step [6760/10336], Loss: 1.6871\n",
      "Epoch [5/5], Step [6762/10336], Loss: 0.0319\n",
      "Epoch [5/5], Step [6764/10336], Loss: 0.6214\n",
      "Epoch [5/5], Step [6766/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [6768/10336], Loss: 1.3317\n",
      "Epoch [5/5], Step [6770/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [6772/10336], Loss: 0.1584\n",
      "Epoch [5/5], Step [6774/10336], Loss: 2.3014\n",
      "Epoch [5/5], Step [6776/10336], Loss: 0.2125\n",
      "Epoch [5/5], Step [6778/10336], Loss: 0.1326\n",
      "Epoch [5/5], Step [6780/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [6782/10336], Loss: 0.0653\n",
      "Epoch [5/5], Step [6784/10336], Loss: 0.9827\n",
      "Epoch [5/5], Step [6786/10336], Loss: 0.0197\n",
      "Epoch [5/5], Step [6788/10336], Loss: 0.0053\n",
      "Epoch [5/5], Step [6790/10336], Loss: 0.0148\n",
      "Epoch [5/5], Step [6792/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [6794/10336], Loss: 0.0109\n",
      "Epoch [5/5], Step [6796/10336], Loss: 0.0165\n",
      "Epoch [5/5], Step [6798/10336], Loss: 2.0429\n",
      "Epoch [5/5], Step [6800/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [6802/10336], Loss: 0.0658\n",
      "Epoch [5/5], Step [6804/10336], Loss: 0.2190\n",
      "Epoch [5/5], Step [6806/10336], Loss: 0.0083\n",
      "Epoch [5/5], Step [6808/10336], Loss: 0.0694\n",
      "Epoch [5/5], Step [6810/10336], Loss: 0.0714\n",
      "Epoch [5/5], Step [6812/10336], Loss: 0.0185\n",
      "Epoch [5/5], Step [6814/10336], Loss: 3.4153\n",
      "Epoch [5/5], Step [6816/10336], Loss: 0.0623\n",
      "Epoch [5/5], Step [6818/10336], Loss: 0.0266\n",
      "Epoch [5/5], Step [6820/10336], Loss: 0.1350\n",
      "Epoch [5/5], Step [6822/10336], Loss: 0.0387\n",
      "Epoch [5/5], Step [6824/10336], Loss: 1.1201\n",
      "Epoch [5/5], Step [6826/10336], Loss: 0.5550\n",
      "Epoch [5/5], Step [6828/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [6830/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [6832/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [6834/10336], Loss: 1.8967\n",
      "Epoch [5/5], Step [6836/10336], Loss: 0.3818\n",
      "Epoch [5/5], Step [6838/10336], Loss: 1.8560\n",
      "Epoch [5/5], Step [6840/10336], Loss: 0.8660\n",
      "Epoch [5/5], Step [6842/10336], Loss: 0.1412\n",
      "Epoch [5/5], Step [6844/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [6846/10336], Loss: 0.0110\n",
      "Epoch [5/5], Step [6848/10336], Loss: 0.4929\n",
      "Epoch [5/5], Step [6850/10336], Loss: 2.1886\n",
      "Epoch [5/5], Step [6852/10336], Loss: 0.3613\n",
      "Epoch [5/5], Step [6854/10336], Loss: 0.5270\n",
      "Epoch [5/5], Step [6856/10336], Loss: 0.0651\n",
      "Epoch [5/5], Step [6858/10336], Loss: 0.1461\n",
      "Epoch [5/5], Step [6860/10336], Loss: 1.4006\n",
      "Epoch [5/5], Step [6862/10336], Loss: 0.5151\n",
      "Epoch [5/5], Step [6864/10336], Loss: 0.0036\n",
      "Epoch [5/5], Step [6866/10336], Loss: 0.1562\n",
      "Epoch [5/5], Step [6868/10336], Loss: 0.0069\n",
      "Epoch [5/5], Step [6870/10336], Loss: 0.2583\n",
      "Epoch [5/5], Step [6872/10336], Loss: 2.3254\n",
      "Epoch [5/5], Step [6874/10336], Loss: 0.4083\n",
      "Epoch [5/5], Step [6876/10336], Loss: 0.5103\n",
      "Epoch [5/5], Step [6878/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [6880/10336], Loss: 0.0338\n",
      "Epoch [5/5], Step [6882/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [6884/10336], Loss: 0.4727\n",
      "Epoch [5/5], Step [6886/10336], Loss: 0.0578\n",
      "Epoch [5/5], Step [6888/10336], Loss: 0.0419\n",
      "Epoch [5/5], Step [6890/10336], Loss: 0.4616\n",
      "Epoch [5/5], Step [6892/10336], Loss: 0.0446\n",
      "Epoch [5/5], Step [6894/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [6896/10336], Loss: 0.8067\n",
      "Epoch [5/5], Step [6898/10336], Loss: 1.4064\n",
      "Epoch [5/5], Step [6900/10336], Loss: 0.2425\n",
      "Epoch [5/5], Step [6902/10336], Loss: 0.2945\n",
      "Epoch [5/5], Step [6904/10336], Loss: 0.0067\n",
      "Epoch [5/5], Step [6906/10336], Loss: 0.8805\n",
      "Epoch [5/5], Step [6908/10336], Loss: 1.2446\n",
      "Epoch [5/5], Step [6910/10336], Loss: 0.5119\n",
      "Epoch [5/5], Step [6912/10336], Loss: 0.1814\n",
      "Epoch [5/5], Step [6914/10336], Loss: 1.2393\n",
      "Epoch [5/5], Step [6916/10336], Loss: 0.5799\n",
      "Epoch [5/5], Step [6918/10336], Loss: 0.0977\n",
      "Epoch [5/5], Step [6920/10336], Loss: 0.1370\n",
      "Epoch [5/5], Step [6922/10336], Loss: 1.1440\n",
      "Epoch [5/5], Step [6924/10336], Loss: 0.0187\n",
      "Epoch [5/5], Step [6926/10336], Loss: 0.4978\n",
      "Epoch [5/5], Step [6928/10336], Loss: 1.2463\n",
      "Epoch [5/5], Step [6930/10336], Loss: 0.1364\n",
      "Epoch [5/5], Step [6932/10336], Loss: 0.0284\n",
      "Epoch [5/5], Step [6934/10336], Loss: 0.0150\n",
      "Epoch [5/5], Step [6936/10336], Loss: 0.0304\n",
      "Epoch [5/5], Step [6938/10336], Loss: 0.3720\n",
      "Epoch [5/5], Step [6940/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [6942/10336], Loss: 0.3582\n",
      "Epoch [5/5], Step [6944/10336], Loss: 1.5744\n",
      "Epoch [5/5], Step [6946/10336], Loss: 0.3470\n",
      "Epoch [5/5], Step [6948/10336], Loss: 0.0193\n",
      "Epoch [5/5], Step [6950/10336], Loss: 0.0110\n",
      "Epoch [5/5], Step [6952/10336], Loss: 0.3986\n",
      "Epoch [5/5], Step [6954/10336], Loss: 0.7120\n",
      "Epoch [5/5], Step [6956/10336], Loss: 0.0686\n",
      "Epoch [5/5], Step [6958/10336], Loss: 0.0076\n",
      "Epoch [5/5], Step [6960/10336], Loss: 1.9498\n",
      "Epoch [5/5], Step [6962/10336], Loss: 1.1667\n",
      "Epoch [5/5], Step [6964/10336], Loss: 0.0451\n",
      "Epoch [5/5], Step [6966/10336], Loss: 0.0080\n",
      "Epoch [5/5], Step [6968/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [6970/10336], Loss: 0.7631\n",
      "Epoch [5/5], Step [6972/10336], Loss: 0.0435\n",
      "Epoch [5/5], Step [6974/10336], Loss: 0.2526\n",
      "Epoch [5/5], Step [6976/10336], Loss: 0.0191\n",
      "Epoch [5/5], Step [6978/10336], Loss: 0.0140\n",
      "Epoch [5/5], Step [6980/10336], Loss: 0.0177\n",
      "Epoch [5/5], Step [6982/10336], Loss: 0.0581\n",
      "Epoch [5/5], Step [6984/10336], Loss: 0.0269\n",
      "Epoch [5/5], Step [6986/10336], Loss: 0.1260\n",
      "Epoch [5/5], Step [6988/10336], Loss: 0.0443\n",
      "Epoch [5/5], Step [6990/10336], Loss: 0.3874\n",
      "Epoch [5/5], Step [6992/10336], Loss: 0.0943\n",
      "Epoch [5/5], Step [6994/10336], Loss: 2.7610\n",
      "Epoch [5/5], Step [6996/10336], Loss: 4.2442\n",
      "Epoch [5/5], Step [6998/10336], Loss: 0.2122\n",
      "Epoch [5/5], Step [7000/10336], Loss: 0.1197\n",
      "Epoch [5/5], Step [7002/10336], Loss: 0.5767\n",
      "Epoch [5/5], Step [7004/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [7006/10336], Loss: 0.4303\n",
      "Epoch [5/5], Step [7008/10336], Loss: 0.2647\n",
      "Epoch [5/5], Step [7010/10336], Loss: 0.0098\n",
      "Epoch [5/5], Step [7012/10336], Loss: 1.9909\n",
      "Epoch [5/5], Step [7014/10336], Loss: 0.0365\n",
      "Epoch [5/5], Step [7016/10336], Loss: 0.0272\n",
      "Epoch [5/5], Step [7018/10336], Loss: 1.0276\n",
      "Epoch [5/5], Step [7020/10336], Loss: 0.9914\n",
      "Epoch [5/5], Step [7022/10336], Loss: 0.2897\n",
      "Epoch [5/5], Step [7024/10336], Loss: 1.2548\n",
      "Epoch [5/5], Step [7026/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [7028/10336], Loss: 0.0966\n",
      "Epoch [5/5], Step [7030/10336], Loss: 0.0424\n",
      "Epoch [5/5], Step [7032/10336], Loss: 0.0162\n",
      "Epoch [5/5], Step [7034/10336], Loss: 0.2160\n",
      "Epoch [5/5], Step [7036/10336], Loss: 0.0396\n",
      "Epoch [5/5], Step [7038/10336], Loss: 1.6267\n",
      "Epoch [5/5], Step [7040/10336], Loss: 0.0344\n",
      "Epoch [5/5], Step [7042/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [7044/10336], Loss: 0.5337\n",
      "Epoch [5/5], Step [7046/10336], Loss: 0.2223\n",
      "Epoch [5/5], Step [7048/10336], Loss: 1.6981\n",
      "Epoch [5/5], Step [7050/10336], Loss: 0.6142\n",
      "Epoch [5/5], Step [7052/10336], Loss: 1.1164\n",
      "Epoch [5/5], Step [7054/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [7056/10336], Loss: 0.0083\n",
      "Epoch [5/5], Step [7058/10336], Loss: 0.5998\n",
      "Epoch [5/5], Step [7060/10336], Loss: 0.1472\n",
      "Epoch [5/5], Step [7062/10336], Loss: 1.2505\n",
      "Epoch [5/5], Step [7064/10336], Loss: 0.3387\n",
      "Epoch [5/5], Step [7066/10336], Loss: 0.0035\n",
      "Epoch [5/5], Step [7068/10336], Loss: 0.0898\n",
      "Epoch [5/5], Step [7070/10336], Loss: 3.5872\n",
      "Epoch [5/5], Step [7072/10336], Loss: 1.7862\n",
      "Epoch [5/5], Step [7074/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [7076/10336], Loss: 0.2233\n",
      "Epoch [5/5], Step [7078/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [7080/10336], Loss: 0.1297\n",
      "Epoch [5/5], Step [7082/10336], Loss: 1.5250\n",
      "Epoch [5/5], Step [7084/10336], Loss: 1.0096\n",
      "Epoch [5/5], Step [7086/10336], Loss: 0.0382\n",
      "Epoch [5/5], Step [7088/10336], Loss: 3.1640\n",
      "Epoch [5/5], Step [7090/10336], Loss: 0.9257\n",
      "Epoch [5/5], Step [7092/10336], Loss: 1.5127\n",
      "Epoch [5/5], Step [7094/10336], Loss: 0.4982\n",
      "Epoch [5/5], Step [7096/10336], Loss: 1.7284\n",
      "Epoch [5/5], Step [7098/10336], Loss: 0.1137\n",
      "Epoch [5/5], Step [7100/10336], Loss: 0.6889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [7102/10336], Loss: 1.2483\n",
      "Epoch [5/5], Step [7104/10336], Loss: 0.6558\n",
      "Epoch [5/5], Step [7106/10336], Loss: 0.2446\n",
      "Epoch [5/5], Step [7108/10336], Loss: 0.2163\n",
      "Epoch [5/5], Step [7110/10336], Loss: 2.1478\n",
      "Epoch [5/5], Step [7112/10336], Loss: 0.3020\n",
      "Epoch [5/5], Step [7114/10336], Loss: 0.0249\n",
      "Epoch [5/5], Step [7116/10336], Loss: 0.0222\n",
      "Epoch [5/5], Step [7118/10336], Loss: 2.0185\n",
      "Epoch [5/5], Step [7120/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [7122/10336], Loss: 0.0160\n",
      "Epoch [5/5], Step [7124/10336], Loss: 0.1194\n",
      "Epoch [5/5], Step [7126/10336], Loss: 0.0156\n",
      "Epoch [5/5], Step [7128/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [7130/10336], Loss: 0.5185\n",
      "Epoch [5/5], Step [7132/10336], Loss: 1.7775\n",
      "Epoch [5/5], Step [7134/10336], Loss: 0.0076\n",
      "Epoch [5/5], Step [7136/10336], Loss: 0.4245\n",
      "Epoch [5/5], Step [7138/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [7140/10336], Loss: 0.6363\n",
      "Epoch [5/5], Step [7142/10336], Loss: 0.1040\n",
      "Epoch [5/5], Step [7144/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [7146/10336], Loss: 1.2870\n",
      "Epoch [5/5], Step [7148/10336], Loss: 1.0519\n",
      "Epoch [5/5], Step [7150/10336], Loss: 0.0265\n",
      "Epoch [5/5], Step [7152/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [7154/10336], Loss: 0.2248\n",
      "Epoch [5/5], Step [7156/10336], Loss: 0.0393\n",
      "Epoch [5/5], Step [7158/10336], Loss: 1.1813\n",
      "Epoch [5/5], Step [7160/10336], Loss: 0.3486\n",
      "Epoch [5/5], Step [7162/10336], Loss: 0.8117\n",
      "Epoch [5/5], Step [7164/10336], Loss: 0.0142\n",
      "Epoch [5/5], Step [7166/10336], Loss: 0.4662\n",
      "Epoch [5/5], Step [7168/10336], Loss: 1.8920\n",
      "Epoch [5/5], Step [7170/10336], Loss: 0.8468\n",
      "Epoch [5/5], Step [7172/10336], Loss: 1.5701\n",
      "Epoch [5/5], Step [7174/10336], Loss: 0.9878\n",
      "Epoch [5/5], Step [7176/10336], Loss: 0.0048\n",
      "Epoch [5/5], Step [7178/10336], Loss: 1.6939\n",
      "Epoch [5/5], Step [7180/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [7182/10336], Loss: 0.1001\n",
      "Epoch [5/5], Step [7184/10336], Loss: 0.1728\n",
      "Epoch [5/5], Step [7186/10336], Loss: 2.0349\n",
      "Epoch [5/5], Step [7188/10336], Loss: 3.2244\n",
      "Epoch [5/5], Step [7190/10336], Loss: 0.1502\n",
      "Epoch [5/5], Step [7192/10336], Loss: 0.6259\n",
      "Epoch [5/5], Step [7194/10336], Loss: 0.0316\n",
      "Epoch [5/5], Step [7196/10336], Loss: 0.0459\n",
      "Epoch [5/5], Step [7198/10336], Loss: 1.5820\n",
      "Epoch [5/5], Step [7200/10336], Loss: 0.0694\n",
      "Epoch [5/5], Step [7202/10336], Loss: 0.2059\n",
      "Epoch [5/5], Step [7204/10336], Loss: 1.1528\n",
      "Epoch [5/5], Step [7206/10336], Loss: 1.4080\n",
      "Epoch [5/5], Step [7208/10336], Loss: 1.8675\n",
      "Epoch [5/5], Step [7210/10336], Loss: 0.0254\n",
      "Epoch [5/5], Step [7212/10336], Loss: 0.7284\n",
      "Epoch [5/5], Step [7214/10336], Loss: 0.0433\n",
      "Epoch [5/5], Step [7216/10336], Loss: 1.1221\n",
      "Epoch [5/5], Step [7218/10336], Loss: 0.4190\n",
      "Epoch [5/5], Step [7220/10336], Loss: 0.0639\n",
      "Epoch [5/5], Step [7222/10336], Loss: 0.0296\n",
      "Epoch [5/5], Step [7224/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [7226/10336], Loss: 0.1003\n",
      "Epoch [5/5], Step [7228/10336], Loss: 0.0934\n",
      "Epoch [5/5], Step [7230/10336], Loss: 0.0055\n",
      "Epoch [5/5], Step [7232/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [7234/10336], Loss: 2.0964\n",
      "Epoch [5/5], Step [7236/10336], Loss: 0.2530\n",
      "Epoch [5/5], Step [7238/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [7240/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [7242/10336], Loss: 0.1098\n",
      "Epoch [5/5], Step [7244/10336], Loss: 2.5738\n",
      "Epoch [5/5], Step [7246/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [7248/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [7250/10336], Loss: 0.5105\n",
      "Epoch [5/5], Step [7252/10336], Loss: 0.0268\n",
      "Epoch [5/5], Step [7254/10336], Loss: 0.5664\n",
      "Epoch [5/5], Step [7256/10336], Loss: 1.6519\n",
      "Epoch [5/5], Step [7258/10336], Loss: 0.0367\n",
      "Epoch [5/5], Step [7260/10336], Loss: 0.0519\n",
      "Epoch [5/5], Step [7262/10336], Loss: 0.3861\n",
      "Epoch [5/5], Step [7264/10336], Loss: 1.8702\n",
      "Epoch [5/5], Step [7266/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [7268/10336], Loss: 0.1903\n",
      "Epoch [5/5], Step [7270/10336], Loss: 0.3853\n",
      "Epoch [5/5], Step [7272/10336], Loss: 0.0937\n",
      "Epoch [5/5], Step [7274/10336], Loss: 0.8154\n",
      "Epoch [5/5], Step [7276/10336], Loss: 0.6457\n",
      "Epoch [5/5], Step [7278/10336], Loss: 0.4701\n",
      "Epoch [5/5], Step [7280/10336], Loss: 0.5030\n",
      "Epoch [5/5], Step [7282/10336], Loss: 0.5310\n",
      "Epoch [5/5], Step [7284/10336], Loss: 1.0436\n",
      "Epoch [5/5], Step [7286/10336], Loss: 2.2349\n",
      "Epoch [5/5], Step [7288/10336], Loss: 0.0844\n",
      "Epoch [5/5], Step [7290/10336], Loss: 0.1180\n",
      "Epoch [5/5], Step [7292/10336], Loss: 0.0034\n",
      "Epoch [5/5], Step [7294/10336], Loss: 2.8570\n",
      "Epoch [5/5], Step [7296/10336], Loss: 0.0711\n",
      "Epoch [5/5], Step [7298/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [7300/10336], Loss: 0.0089\n",
      "Epoch [5/5], Step [7302/10336], Loss: 0.9482\n",
      "Epoch [5/5], Step [7304/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [7306/10336], Loss: 0.8936\n",
      "Epoch [5/5], Step [7308/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [7310/10336], Loss: 0.0441\n",
      "Epoch [5/5], Step [7312/10336], Loss: 0.0123\n",
      "Epoch [5/5], Step [7314/10336], Loss: 2.6000\n",
      "Epoch [5/5], Step [7316/10336], Loss: 1.8631\n",
      "Epoch [5/5], Step [7318/10336], Loss: 0.4713\n",
      "Epoch [5/5], Step [7320/10336], Loss: 0.0310\n",
      "Epoch [5/5], Step [7322/10336], Loss: 1.4593\n",
      "Epoch [5/5], Step [7324/10336], Loss: 0.1262\n",
      "Epoch [5/5], Step [7326/10336], Loss: 0.3085\n",
      "Epoch [5/5], Step [7328/10336], Loss: 0.0098\n",
      "Epoch [5/5], Step [7330/10336], Loss: 0.0701\n",
      "Epoch [5/5], Step [7332/10336], Loss: 0.0279\n",
      "Epoch [5/5], Step [7334/10336], Loss: 0.0886\n",
      "Epoch [5/5], Step [7336/10336], Loss: 0.6485\n",
      "Epoch [5/5], Step [7338/10336], Loss: 0.0502\n",
      "Epoch [5/5], Step [7340/10336], Loss: 0.7162\n",
      "Epoch [5/5], Step [7342/10336], Loss: 0.0488\n",
      "Epoch [5/5], Step [7344/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [7346/10336], Loss: 0.0105\n",
      "Epoch [5/5], Step [7348/10336], Loss: 0.0092\n",
      "Epoch [5/5], Step [7350/10336], Loss: 0.0252\n",
      "Epoch [5/5], Step [7352/10336], Loss: 0.0038\n",
      "Epoch [5/5], Step [7354/10336], Loss: 0.0284\n",
      "Epoch [5/5], Step [7356/10336], Loss: 0.8559\n",
      "Epoch [5/5], Step [7358/10336], Loss: 2.4913\n",
      "Epoch [5/5], Step [7360/10336], Loss: 0.5767\n",
      "Epoch [5/5], Step [7362/10336], Loss: 0.4835\n",
      "Epoch [5/5], Step [7364/10336], Loss: 0.7271\n",
      "Epoch [5/5], Step [7366/10336], Loss: 0.1098\n",
      "Epoch [5/5], Step [7368/10336], Loss: 1.2945\n",
      "Epoch [5/5], Step [7370/10336], Loss: 0.0017\n",
      "Epoch [5/5], Step [7372/10336], Loss: 0.0579\n",
      "Epoch [5/5], Step [7374/10336], Loss: 0.0139\n",
      "Epoch [5/5], Step [7376/10336], Loss: 0.0215\n",
      "Epoch [5/5], Step [7378/10336], Loss: 0.1730\n",
      "Epoch [5/5], Step [7380/10336], Loss: 2.8397\n",
      "Epoch [5/5], Step [7382/10336], Loss: 0.1005\n",
      "Epoch [5/5], Step [7384/10336], Loss: 0.4339\n",
      "Epoch [5/5], Step [7386/10336], Loss: 2.7892\n",
      "Epoch [5/5], Step [7388/10336], Loss: 0.0825\n",
      "Epoch [5/5], Step [7390/10336], Loss: 0.9867\n",
      "Epoch [5/5], Step [7392/10336], Loss: 0.1343\n",
      "Epoch [5/5], Step [7394/10336], Loss: 0.0239\n",
      "Epoch [5/5], Step [7396/10336], Loss: 0.3805\n",
      "Epoch [5/5], Step [7398/10336], Loss: 1.1291\n",
      "Epoch [5/5], Step [7400/10336], Loss: 0.5639\n",
      "Epoch [5/5], Step [7402/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [7404/10336], Loss: 0.6926\n",
      "Epoch [5/5], Step [7406/10336], Loss: 1.2032\n",
      "Epoch [5/5], Step [7408/10336], Loss: 0.5248\n",
      "Epoch [5/5], Step [7410/10336], Loss: 0.0776\n",
      "Epoch [5/5], Step [7412/10336], Loss: 0.5002\n",
      "Epoch [5/5], Step [7414/10336], Loss: 0.2385\n",
      "Epoch [5/5], Step [7416/10336], Loss: 0.1001\n",
      "Epoch [5/5], Step [7418/10336], Loss: 1.5274\n",
      "Epoch [5/5], Step [7420/10336], Loss: 0.0209\n",
      "Epoch [5/5], Step [7422/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [7424/10336], Loss: 0.6381\n",
      "Epoch [5/5], Step [7426/10336], Loss: 0.2201\n",
      "Epoch [5/5], Step [7428/10336], Loss: 0.1018\n",
      "Epoch [5/5], Step [7430/10336], Loss: 0.6393\n",
      "Epoch [5/5], Step [7432/10336], Loss: 0.3475\n",
      "Epoch [5/5], Step [7434/10336], Loss: 0.0544\n",
      "Epoch [5/5], Step [7436/10336], Loss: 0.8434\n",
      "Epoch [5/5], Step [7438/10336], Loss: 0.0605\n",
      "Epoch [5/5], Step [7440/10336], Loss: 0.2953\n",
      "Epoch [5/5], Step [7442/10336], Loss: 0.0355\n",
      "Epoch [5/5], Step [7444/10336], Loss: 1.6216\n",
      "Epoch [5/5], Step [7446/10336], Loss: 0.3350\n",
      "Epoch [5/5], Step [7448/10336], Loss: 0.1171\n",
      "Epoch [5/5], Step [7450/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [7452/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [7454/10336], Loss: 1.7402\n",
      "Epoch [5/5], Step [7456/10336], Loss: 0.1736\n",
      "Epoch [5/5], Step [7458/10336], Loss: 1.0136\n",
      "Epoch [5/5], Step [7460/10336], Loss: 0.8941\n",
      "Epoch [5/5], Step [7462/10336], Loss: 0.2239\n",
      "Epoch [5/5], Step [7464/10336], Loss: 0.4904\n",
      "Epoch [5/5], Step [7466/10336], Loss: 0.0144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [7468/10336], Loss: 1.2343\n",
      "Epoch [5/5], Step [7470/10336], Loss: 0.7828\n",
      "Epoch [5/5], Step [7472/10336], Loss: 0.4318\n",
      "Epoch [5/5], Step [7474/10336], Loss: 0.9812\n",
      "Epoch [5/5], Step [7476/10336], Loss: 0.0268\n",
      "Epoch [5/5], Step [7478/10336], Loss: 2.2707\n",
      "Epoch [5/5], Step [7480/10336], Loss: 0.1881\n",
      "Epoch [5/5], Step [7482/10336], Loss: 0.1061\n",
      "Epoch [5/5], Step [7484/10336], Loss: 0.1207\n",
      "Epoch [5/5], Step [7486/10336], Loss: 0.0210\n",
      "Epoch [5/5], Step [7488/10336], Loss: 0.0875\n",
      "Epoch [5/5], Step [7490/10336], Loss: 1.8011\n",
      "Epoch [5/5], Step [7492/10336], Loss: 1.3199\n",
      "Epoch [5/5], Step [7494/10336], Loss: 0.2470\n",
      "Epoch [5/5], Step [7496/10336], Loss: 0.5131\n",
      "Epoch [5/5], Step [7498/10336], Loss: 0.2462\n",
      "Epoch [5/5], Step [7500/10336], Loss: 0.1065\n",
      "Epoch [5/5], Step [7502/10336], Loss: 0.0067\n",
      "Epoch [5/5], Step [7504/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [7506/10336], Loss: 0.9561\n",
      "Epoch [5/5], Step [7508/10336], Loss: 1.5046\n",
      "Epoch [5/5], Step [7510/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [7512/10336], Loss: 0.0060\n",
      "Epoch [5/5], Step [7514/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [7516/10336], Loss: 0.1239\n",
      "Epoch [5/5], Step [7518/10336], Loss: 0.2478\n",
      "Epoch [5/5], Step [7520/10336], Loss: 0.0572\n",
      "Epoch [5/5], Step [7522/10336], Loss: 0.1460\n",
      "Epoch [5/5], Step [7524/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [7526/10336], Loss: 0.1048\n",
      "Epoch [5/5], Step [7528/10336], Loss: 0.5114\n",
      "Epoch [5/5], Step [7530/10336], Loss: 0.2460\n",
      "Epoch [5/5], Step [7532/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [7534/10336], Loss: 0.1698\n",
      "Epoch [5/5], Step [7536/10336], Loss: 0.0048\n",
      "Epoch [5/5], Step [7538/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [7540/10336], Loss: 1.9654\n",
      "Epoch [5/5], Step [7542/10336], Loss: 1.1936\n",
      "Epoch [5/5], Step [7544/10336], Loss: 0.4042\n",
      "Epoch [5/5], Step [7546/10336], Loss: 0.0415\n",
      "Epoch [5/5], Step [7548/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [7550/10336], Loss: 0.0091\n",
      "Epoch [5/5], Step [7552/10336], Loss: 1.0622\n",
      "Epoch [5/5], Step [7554/10336], Loss: 0.0773\n",
      "Epoch [5/5], Step [7556/10336], Loss: 1.3060\n",
      "Epoch [5/5], Step [7558/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [7560/10336], Loss: 0.4712\n",
      "Epoch [5/5], Step [7562/10336], Loss: 0.0271\n",
      "Epoch [5/5], Step [7564/10336], Loss: 0.0139\n",
      "Epoch [5/5], Step [7566/10336], Loss: 2.7407\n",
      "Epoch [5/5], Step [7568/10336], Loss: 1.0758\n",
      "Epoch [5/5], Step [7570/10336], Loss: 0.1309\n",
      "Epoch [5/5], Step [7572/10336], Loss: 0.3392\n",
      "Epoch [5/5], Step [7574/10336], Loss: 0.2929\n",
      "Epoch [5/5], Step [7576/10336], Loss: 0.3205\n",
      "Epoch [5/5], Step [7578/10336], Loss: 0.1370\n",
      "Epoch [5/5], Step [7580/10336], Loss: 1.5877\n",
      "Epoch [5/5], Step [7582/10336], Loss: 0.0505\n",
      "Epoch [5/5], Step [7584/10336], Loss: 0.0058\n",
      "Epoch [5/5], Step [7586/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [7588/10336], Loss: 1.9044\n",
      "Epoch [5/5], Step [7590/10336], Loss: 1.1378\n",
      "Epoch [5/5], Step [7592/10336], Loss: 0.9015\n",
      "Epoch [5/5], Step [7594/10336], Loss: 0.4114\n",
      "Epoch [5/5], Step [7596/10336], Loss: 0.0907\n",
      "Epoch [5/5], Step [7598/10336], Loss: 0.1826\n",
      "Epoch [5/5], Step [7600/10336], Loss: 0.3202\n",
      "Epoch [5/5], Step [7602/10336], Loss: 0.0505\n",
      "Epoch [5/5], Step [7604/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [7606/10336], Loss: 0.0299\n",
      "Epoch [5/5], Step [7608/10336], Loss: 0.0344\n",
      "Epoch [5/5], Step [7610/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [7612/10336], Loss: 0.3646\n",
      "Epoch [5/5], Step [7614/10336], Loss: 0.0230\n",
      "Epoch [5/5], Step [7616/10336], Loss: 1.4899\n",
      "Epoch [5/5], Step [7618/10336], Loss: 2.4439\n",
      "Epoch [5/5], Step [7620/10336], Loss: 0.0338\n",
      "Epoch [5/5], Step [7622/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [7624/10336], Loss: 0.1620\n",
      "Epoch [5/5], Step [7626/10336], Loss: 2.6093\n",
      "Epoch [5/5], Step [7628/10336], Loss: 0.1446\n",
      "Epoch [5/5], Step [7630/10336], Loss: 0.0219\n",
      "Epoch [5/5], Step [7632/10336], Loss: 0.6115\n",
      "Epoch [5/5], Step [7634/10336], Loss: 0.0353\n",
      "Epoch [5/5], Step [7636/10336], Loss: 2.2753\n",
      "Epoch [5/5], Step [7638/10336], Loss: 1.7075\n",
      "Epoch [5/5], Step [7640/10336], Loss: 2.0678\n",
      "Epoch [5/5], Step [7642/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [7644/10336], Loss: 0.8463\n",
      "Epoch [5/5], Step [7646/10336], Loss: 0.0411\n",
      "Epoch [5/5], Step [7648/10336], Loss: 0.0702\n",
      "Epoch [5/5], Step [7650/10336], Loss: 1.3071\n",
      "Epoch [5/5], Step [7652/10336], Loss: 0.5595\n",
      "Epoch [5/5], Step [7654/10336], Loss: 0.5403\n",
      "Epoch [5/5], Step [7656/10336], Loss: 1.9814\n",
      "Epoch [5/5], Step [7658/10336], Loss: 3.8648\n",
      "Epoch [5/5], Step [7660/10336], Loss: 0.5723\n",
      "Epoch [5/5], Step [7662/10336], Loss: 0.3131\n",
      "Epoch [5/5], Step [7664/10336], Loss: 2.6678\n",
      "Epoch [5/5], Step [7666/10336], Loss: 0.3526\n",
      "Epoch [5/5], Step [7668/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [7670/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [7672/10336], Loss: 0.5566\n",
      "Epoch [5/5], Step [7674/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [7676/10336], Loss: 0.0936\n",
      "Epoch [5/5], Step [7678/10336], Loss: 0.0304\n",
      "Epoch [5/5], Step [7680/10336], Loss: 0.0884\n",
      "Epoch [5/5], Step [7682/10336], Loss: 1.1503\n",
      "Epoch [5/5], Step [7684/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [7686/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [7688/10336], Loss: 0.3650\n",
      "Epoch [5/5], Step [7690/10336], Loss: 0.1125\n",
      "Epoch [5/5], Step [7692/10336], Loss: 0.2797\n",
      "Epoch [5/5], Step [7694/10336], Loss: 0.1092\n",
      "Epoch [5/5], Step [7696/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [7698/10336], Loss: 0.1538\n",
      "Epoch [5/5], Step [7700/10336], Loss: 0.1565\n",
      "Epoch [5/5], Step [7702/10336], Loss: 0.0570\n",
      "Epoch [5/5], Step [7704/10336], Loss: 0.3387\n",
      "Epoch [5/5], Step [7706/10336], Loss: 3.0884\n",
      "Epoch [5/5], Step [7708/10336], Loss: 0.0095\n",
      "Epoch [5/5], Step [7710/10336], Loss: 1.6617\n",
      "Epoch [5/5], Step [7712/10336], Loss: 0.4279\n",
      "Epoch [5/5], Step [7714/10336], Loss: 0.2893\n",
      "Epoch [5/5], Step [7716/10336], Loss: 0.3785\n",
      "Epoch [5/5], Step [7718/10336], Loss: 0.3175\n",
      "Epoch [5/5], Step [7720/10336], Loss: 0.0130\n",
      "Epoch [5/5], Step [7722/10336], Loss: 0.6500\n",
      "Epoch [5/5], Step [7724/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [7726/10336], Loss: 0.0122\n",
      "Epoch [5/5], Step [7728/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [7730/10336], Loss: 0.2059\n",
      "Epoch [5/5], Step [7732/10336], Loss: 2.5056\n",
      "Epoch [5/5], Step [7734/10336], Loss: 4.0669\n",
      "Epoch [5/5], Step [7736/10336], Loss: 1.4386\n",
      "Epoch [5/5], Step [7738/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [7740/10336], Loss: 1.0727\n",
      "Epoch [5/5], Step [7742/10336], Loss: 1.7973\n",
      "Epoch [5/5], Step [7744/10336], Loss: 2.4152\n",
      "Epoch [5/5], Step [7746/10336], Loss: 0.0696\n",
      "Epoch [5/5], Step [7748/10336], Loss: 1.7605\n",
      "Epoch [5/5], Step [7750/10336], Loss: 0.0264\n",
      "Epoch [5/5], Step [7752/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [7754/10336], Loss: 0.0231\n",
      "Epoch [5/5], Step [7756/10336], Loss: 0.5939\n",
      "Epoch [5/5], Step [7758/10336], Loss: 0.1514\n",
      "Epoch [5/5], Step [7760/10336], Loss: 0.2002\n",
      "Epoch [5/5], Step [7762/10336], Loss: 0.7923\n",
      "Epoch [5/5], Step [7764/10336], Loss: 0.0481\n",
      "Epoch [5/5], Step [7766/10336], Loss: 0.2681\n",
      "Epoch [5/5], Step [7768/10336], Loss: 0.1157\n",
      "Epoch [5/5], Step [7770/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [7772/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [7774/10336], Loss: 0.4919\n",
      "Epoch [5/5], Step [7776/10336], Loss: 0.1327\n",
      "Epoch [5/5], Step [7778/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [7780/10336], Loss: 0.1177\n",
      "Epoch [5/5], Step [7782/10336], Loss: 2.0193\n",
      "Epoch [5/5], Step [7784/10336], Loss: 1.3635\n",
      "Epoch [5/5], Step [7786/10336], Loss: 0.0222\n",
      "Epoch [5/5], Step [7788/10336], Loss: 0.9148\n",
      "Epoch [5/5], Step [7790/10336], Loss: 0.6062\n",
      "Epoch [5/5], Step [7792/10336], Loss: 0.0955\n",
      "Epoch [5/5], Step [7794/10336], Loss: 0.0181\n",
      "Epoch [5/5], Step [7796/10336], Loss: 0.0731\n",
      "Epoch [5/5], Step [7798/10336], Loss: 1.3647\n",
      "Epoch [5/5], Step [7800/10336], Loss: 0.2543\n",
      "Epoch [5/5], Step [7802/10336], Loss: 0.2771\n",
      "Epoch [5/5], Step [7804/10336], Loss: 1.4627\n",
      "Epoch [5/5], Step [7806/10336], Loss: 0.5498\n",
      "Epoch [5/5], Step [7808/10336], Loss: 0.2128\n",
      "Epoch [5/5], Step [7810/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [7812/10336], Loss: 0.0727\n",
      "Epoch [5/5], Step [7814/10336], Loss: 0.1168\n",
      "Epoch [5/5], Step [7816/10336], Loss: 1.5607\n",
      "Epoch [5/5], Step [7818/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [7820/10336], Loss: 0.0245\n",
      "Epoch [5/5], Step [7822/10336], Loss: 0.4998\n",
      "Epoch [5/5], Step [7824/10336], Loss: 0.2335\n",
      "Epoch [5/5], Step [7826/10336], Loss: 0.0289\n",
      "Epoch [5/5], Step [7828/10336], Loss: 1.4263\n",
      "Epoch [5/5], Step [7830/10336], Loss: 2.6535\n",
      "Epoch [5/5], Step [7832/10336], Loss: 1.0343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [7834/10336], Loss: 0.2840\n",
      "Epoch [5/5], Step [7836/10336], Loss: 0.4754\n",
      "Epoch [5/5], Step [7838/10336], Loss: 0.0129\n",
      "Epoch [5/5], Step [7840/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [7842/10336], Loss: 0.5924\n",
      "Epoch [5/5], Step [7844/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [7846/10336], Loss: 0.0078\n",
      "Epoch [5/5], Step [7848/10336], Loss: 0.6640\n",
      "Epoch [5/5], Step [7850/10336], Loss: 1.1692\n",
      "Epoch [5/5], Step [7852/10336], Loss: 0.0915\n",
      "Epoch [5/5], Step [7854/10336], Loss: 3.6469\n",
      "Epoch [5/5], Step [7856/10336], Loss: 0.6425\n",
      "Epoch [5/5], Step [7858/10336], Loss: 0.1387\n",
      "Epoch [5/5], Step [7860/10336], Loss: 0.6907\n",
      "Epoch [5/5], Step [7862/10336], Loss: 2.0423\n",
      "Epoch [5/5], Step [7864/10336], Loss: 0.1096\n",
      "Epoch [5/5], Step [7866/10336], Loss: 2.4252\n",
      "Epoch [5/5], Step [7868/10336], Loss: 0.8359\n",
      "Epoch [5/5], Step [7870/10336], Loss: 0.1537\n",
      "Epoch [5/5], Step [7872/10336], Loss: 1.1274\n",
      "Epoch [5/5], Step [7874/10336], Loss: 0.0710\n",
      "Epoch [5/5], Step [7876/10336], Loss: 0.1552\n",
      "Epoch [5/5], Step [7878/10336], Loss: 1.5521\n",
      "Epoch [5/5], Step [7880/10336], Loss: 0.0499\n",
      "Epoch [5/5], Step [7882/10336], Loss: 0.2774\n",
      "Epoch [5/5], Step [7884/10336], Loss: 0.7317\n",
      "Epoch [5/5], Step [7886/10336], Loss: 0.3632\n",
      "Epoch [5/5], Step [7888/10336], Loss: 0.8616\n",
      "Epoch [5/5], Step [7890/10336], Loss: 0.1998\n",
      "Epoch [5/5], Step [7892/10336], Loss: 2.9125\n",
      "Epoch [5/5], Step [7894/10336], Loss: 1.4854\n",
      "Epoch [5/5], Step [7896/10336], Loss: 1.0421\n",
      "Epoch [5/5], Step [7898/10336], Loss: 2.0188\n",
      "Epoch [5/5], Step [7900/10336], Loss: 0.1176\n",
      "Epoch [5/5], Step [7902/10336], Loss: 0.5802\n",
      "Epoch [5/5], Step [7904/10336], Loss: 0.7440\n",
      "Epoch [5/5], Step [7906/10336], Loss: 0.2254\n",
      "Epoch [5/5], Step [7908/10336], Loss: 0.7192\n",
      "Epoch [5/5], Step [7910/10336], Loss: 0.0553\n",
      "Epoch [5/5], Step [7912/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [7914/10336], Loss: 0.0678\n",
      "Epoch [5/5], Step [7916/10336], Loss: 1.3443\n",
      "Epoch [5/5], Step [7918/10336], Loss: 0.0885\n",
      "Epoch [5/5], Step [7920/10336], Loss: 0.0092\n",
      "Epoch [5/5], Step [7922/10336], Loss: 0.0741\n",
      "Epoch [5/5], Step [7924/10336], Loss: 0.3816\n",
      "Epoch [5/5], Step [7926/10336], Loss: 2.1450\n",
      "Epoch [5/5], Step [7928/10336], Loss: 0.1090\n",
      "Epoch [5/5], Step [7930/10336], Loss: 0.1251\n",
      "Epoch [5/5], Step [7932/10336], Loss: 0.3684\n",
      "Epoch [5/5], Step [7934/10336], Loss: 0.0663\n",
      "Epoch [5/5], Step [7936/10336], Loss: 1.3877\n",
      "Epoch [5/5], Step [7938/10336], Loss: 0.2243\n",
      "Epoch [5/5], Step [7940/10336], Loss: 0.1799\n",
      "Epoch [5/5], Step [7942/10336], Loss: 0.0071\n",
      "Epoch [5/5], Step [7944/10336], Loss: 1.3977\n",
      "Epoch [5/5], Step [7946/10336], Loss: 0.0741\n",
      "Epoch [5/5], Step [7948/10336], Loss: 0.8259\n",
      "Epoch [5/5], Step [7950/10336], Loss: 0.0925\n",
      "Epoch [5/5], Step [7952/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [7954/10336], Loss: 0.4812\n",
      "Epoch [5/5], Step [7956/10336], Loss: 0.0026\n",
      "Epoch [5/5], Step [7958/10336], Loss: 0.1929\n",
      "Epoch [5/5], Step [7960/10336], Loss: 0.1800\n",
      "Epoch [5/5], Step [7962/10336], Loss: 0.2430\n",
      "Epoch [5/5], Step [7964/10336], Loss: 0.0686\n",
      "Epoch [5/5], Step [7966/10336], Loss: 2.1857\n",
      "Epoch [5/5], Step [7968/10336], Loss: 2.4534\n",
      "Epoch [5/5], Step [7970/10336], Loss: 0.4216\n",
      "Epoch [5/5], Step [7972/10336], Loss: 1.3116\n",
      "Epoch [5/5], Step [7974/10336], Loss: 0.0925\n",
      "Epoch [5/5], Step [7976/10336], Loss: 0.1409\n",
      "Epoch [5/5], Step [7978/10336], Loss: 0.1169\n",
      "Epoch [5/5], Step [7980/10336], Loss: 0.0863\n",
      "Epoch [5/5], Step [7982/10336], Loss: 0.5692\n",
      "Epoch [5/5], Step [7984/10336], Loss: 0.0544\n",
      "Epoch [5/5], Step [7986/10336], Loss: 0.0561\n",
      "Epoch [5/5], Step [7988/10336], Loss: 1.9146\n",
      "Epoch [5/5], Step [7990/10336], Loss: 0.0613\n",
      "Epoch [5/5], Step [7992/10336], Loss: 0.1231\n",
      "Epoch [5/5], Step [7994/10336], Loss: 0.0455\n",
      "Epoch [5/5], Step [7996/10336], Loss: 0.5719\n",
      "Epoch [5/5], Step [7998/10336], Loss: 0.5913\n",
      "Epoch [5/5], Step [8000/10336], Loss: 0.0101\n",
      "Epoch [5/5], Step [8002/10336], Loss: 0.5709\n",
      "Epoch [5/5], Step [8004/10336], Loss: 0.0310\n",
      "Epoch [5/5], Step [8006/10336], Loss: 0.4260\n",
      "Epoch [5/5], Step [8008/10336], Loss: 0.0295\n",
      "Epoch [5/5], Step [8010/10336], Loss: 0.4302\n",
      "Epoch [5/5], Step [8012/10336], Loss: 0.3309\n",
      "Epoch [5/5], Step [8014/10336], Loss: 1.5409\n",
      "Epoch [5/5], Step [8016/10336], Loss: 0.0049\n",
      "Epoch [5/5], Step [8018/10336], Loss: 1.5842\n",
      "Epoch [5/5], Step [8020/10336], Loss: 0.1102\n",
      "Epoch [5/5], Step [8022/10336], Loss: 0.5080\n",
      "Epoch [5/5], Step [8024/10336], Loss: 0.0349\n",
      "Epoch [5/5], Step [8026/10336], Loss: 0.0411\n",
      "Epoch [5/5], Step [8028/10336], Loss: 0.0205\n",
      "Epoch [5/5], Step [8030/10336], Loss: 0.2171\n",
      "Epoch [5/5], Step [8032/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [8034/10336], Loss: 0.0417\n",
      "Epoch [5/5], Step [8036/10336], Loss: 0.0180\n",
      "Epoch [5/5], Step [8038/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [8040/10336], Loss: 0.1281\n",
      "Epoch [5/5], Step [8042/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [8044/10336], Loss: 0.0034\n",
      "Epoch [5/5], Step [8046/10336], Loss: 0.0099\n",
      "Epoch [5/5], Step [8048/10336], Loss: 1.8580\n",
      "Epoch [5/5], Step [8050/10336], Loss: 0.2324\n",
      "Epoch [5/5], Step [8052/10336], Loss: 0.2006\n",
      "Epoch [5/5], Step [8054/10336], Loss: 0.0089\n",
      "Epoch [5/5], Step [8056/10336], Loss: 0.0106\n",
      "Epoch [5/5], Step [8058/10336], Loss: 0.9611\n",
      "Epoch [5/5], Step [8060/10336], Loss: 0.4583\n",
      "Epoch [5/5], Step [8062/10336], Loss: 1.9240\n",
      "Epoch [5/5], Step [8064/10336], Loss: 1.2270\n",
      "Epoch [5/5], Step [8066/10336], Loss: 0.3142\n",
      "Epoch [5/5], Step [8068/10336], Loss: 0.0469\n",
      "Epoch [5/5], Step [8070/10336], Loss: 0.3254\n",
      "Epoch [5/5], Step [8072/10336], Loss: 0.2829\n",
      "Epoch [5/5], Step [8074/10336], Loss: 1.2018\n",
      "Epoch [5/5], Step [8076/10336], Loss: 0.3302\n",
      "Epoch [5/5], Step [8078/10336], Loss: 0.1241\n",
      "Epoch [5/5], Step [8080/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [8082/10336], Loss: 1.0642\n",
      "Epoch [5/5], Step [8084/10336], Loss: 0.0115\n",
      "Epoch [5/5], Step [8086/10336], Loss: 0.0313\n",
      "Epoch [5/5], Step [8088/10336], Loss: 0.0357\n",
      "Epoch [5/5], Step [8090/10336], Loss: 0.3533\n",
      "Epoch [5/5], Step [8092/10336], Loss: 0.0252\n",
      "Epoch [5/5], Step [8094/10336], Loss: 0.5018\n",
      "Epoch [5/5], Step [8096/10336], Loss: 0.6783\n",
      "Epoch [5/5], Step [8098/10336], Loss: 0.9551\n",
      "Epoch [5/5], Step [8100/10336], Loss: 0.1664\n",
      "Epoch [5/5], Step [8102/10336], Loss: 0.0755\n",
      "Epoch [5/5], Step [8104/10336], Loss: 0.0195\n",
      "Epoch [5/5], Step [8106/10336], Loss: 0.1389\n",
      "Epoch [5/5], Step [8108/10336], Loss: 0.0802\n",
      "Epoch [5/5], Step [8110/10336], Loss: 0.3369\n",
      "Epoch [5/5], Step [8112/10336], Loss: 0.0300\n",
      "Epoch [5/5], Step [8114/10336], Loss: 1.6822\n",
      "Epoch [5/5], Step [8116/10336], Loss: 0.6375\n",
      "Epoch [5/5], Step [8118/10336], Loss: 1.0774\n",
      "Epoch [5/5], Step [8120/10336], Loss: 0.3860\n",
      "Epoch [5/5], Step [8122/10336], Loss: 0.3160\n",
      "Epoch [5/5], Step [8124/10336], Loss: 0.0965\n",
      "Epoch [5/5], Step [8126/10336], Loss: 0.0271\n",
      "Epoch [5/5], Step [8128/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [8130/10336], Loss: 0.0233\n",
      "Epoch [5/5], Step [8132/10336], Loss: 0.6395\n",
      "Epoch [5/5], Step [8134/10336], Loss: 0.7679\n",
      "Epoch [5/5], Step [8136/10336], Loss: 0.7383\n",
      "Epoch [5/5], Step [8138/10336], Loss: 0.0363\n",
      "Epoch [5/5], Step [8140/10336], Loss: 1.3894\n",
      "Epoch [5/5], Step [8142/10336], Loss: 0.0401\n",
      "Epoch [5/5], Step [8144/10336], Loss: 0.0101\n",
      "Epoch [5/5], Step [8146/10336], Loss: 0.2777\n",
      "Epoch [5/5], Step [8148/10336], Loss: 0.7539\n",
      "Epoch [5/5], Step [8150/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [8152/10336], Loss: 0.0143\n",
      "Epoch [5/5], Step [8154/10336], Loss: 0.0449\n",
      "Epoch [5/5], Step [8156/10336], Loss: 0.0615\n",
      "Epoch [5/5], Step [8158/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [8160/10336], Loss: 0.7656\n",
      "Epoch [5/5], Step [8162/10336], Loss: 0.0720\n",
      "Epoch [5/5], Step [8164/10336], Loss: 0.0973\n",
      "Epoch [5/5], Step [8166/10336], Loss: 0.0929\n",
      "Epoch [5/5], Step [8168/10336], Loss: 0.4986\n",
      "Epoch [5/5], Step [8170/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [8172/10336], Loss: 0.0073\n",
      "Epoch [5/5], Step [8174/10336], Loss: 2.0786\n",
      "Epoch [5/5], Step [8176/10336], Loss: 0.0407\n",
      "Epoch [5/5], Step [8178/10336], Loss: 0.6059\n",
      "Epoch [5/5], Step [8180/10336], Loss: 0.0238\n",
      "Epoch [5/5], Step [8182/10336], Loss: 1.1486\n",
      "Epoch [5/5], Step [8184/10336], Loss: 0.1091\n",
      "Epoch [5/5], Step [8186/10336], Loss: 3.5862\n",
      "Epoch [5/5], Step [8188/10336], Loss: 2.2149\n",
      "Epoch [5/5], Step [8190/10336], Loss: 0.0090\n",
      "Epoch [5/5], Step [8192/10336], Loss: 0.2592\n",
      "Epoch [5/5], Step [8194/10336], Loss: 1.1811\n",
      "Epoch [5/5], Step [8196/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [8198/10336], Loss: 0.0286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [8200/10336], Loss: 0.0738\n",
      "Epoch [5/5], Step [8202/10336], Loss: 1.6643\n",
      "Epoch [5/5], Step [8204/10336], Loss: 2.1216\n",
      "Epoch [5/5], Step [8206/10336], Loss: 0.0067\n",
      "Epoch [5/5], Step [8208/10336], Loss: 0.0955\n",
      "Epoch [5/5], Step [8210/10336], Loss: 0.0305\n",
      "Epoch [5/5], Step [8212/10336], Loss: 3.8545\n",
      "Epoch [5/5], Step [8214/10336], Loss: 1.5196\n",
      "Epoch [5/5], Step [8216/10336], Loss: 0.5368\n",
      "Epoch [5/5], Step [8218/10336], Loss: 1.7563\n",
      "Epoch [5/5], Step [8220/10336], Loss: 1.5252\n",
      "Epoch [5/5], Step [8222/10336], Loss: 0.9793\n",
      "Epoch [5/5], Step [8224/10336], Loss: 0.0100\n",
      "Epoch [5/5], Step [8226/10336], Loss: 0.3679\n",
      "Epoch [5/5], Step [8228/10336], Loss: 1.3284\n",
      "Epoch [5/5], Step [8230/10336], Loss: 1.2208\n",
      "Epoch [5/5], Step [8232/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [8234/10336], Loss: 0.2164\n",
      "Epoch [5/5], Step [8236/10336], Loss: 0.0237\n",
      "Epoch [5/5], Step [8238/10336], Loss: 0.5432\n",
      "Epoch [5/5], Step [8240/10336], Loss: 1.5672\n",
      "Epoch [5/5], Step [8242/10336], Loss: 0.0425\n",
      "Epoch [5/5], Step [8244/10336], Loss: 0.0263\n",
      "Epoch [5/5], Step [8246/10336], Loss: 0.0480\n",
      "Epoch [5/5], Step [8248/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [8250/10336], Loss: 0.0157\n",
      "Epoch [5/5], Step [8252/10336], Loss: 0.2383\n",
      "Epoch [5/5], Step [8254/10336], Loss: 0.1317\n",
      "Epoch [5/5], Step [8256/10336], Loss: 0.7429\n",
      "Epoch [5/5], Step [8258/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [8260/10336], Loss: 0.4837\n",
      "Epoch [5/5], Step [8262/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [8264/10336], Loss: 0.4082\n",
      "Epoch [5/5], Step [8266/10336], Loss: 0.3882\n",
      "Epoch [5/5], Step [8268/10336], Loss: 0.0621\n",
      "Epoch [5/5], Step [8270/10336], Loss: 0.0160\n",
      "Epoch [5/5], Step [8272/10336], Loss: 0.4449\n",
      "Epoch [5/5], Step [8274/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [8276/10336], Loss: 1.2859\n",
      "Epoch [5/5], Step [8278/10336], Loss: 0.1520\n",
      "Epoch [5/5], Step [8280/10336], Loss: 0.7582\n",
      "Epoch [5/5], Step [8282/10336], Loss: 0.4554\n",
      "Epoch [5/5], Step [8284/10336], Loss: 0.3258\n",
      "Epoch [5/5], Step [8286/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [8288/10336], Loss: 4.0403\n",
      "Epoch [5/5], Step [8290/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [8292/10336], Loss: 1.2222\n",
      "Epoch [5/5], Step [8294/10336], Loss: 1.7443\n",
      "Epoch [5/5], Step [8296/10336], Loss: 0.0256\n",
      "Epoch [5/5], Step [8298/10336], Loss: 0.0687\n",
      "Epoch [5/5], Step [8300/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [8302/10336], Loss: 0.3006\n",
      "Epoch [5/5], Step [8304/10336], Loss: 0.5915\n",
      "Epoch [5/5], Step [8306/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [8308/10336], Loss: 0.0146\n",
      "Epoch [5/5], Step [8310/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [8312/10336], Loss: 1.2640\n",
      "Epoch [5/5], Step [8314/10336], Loss: 4.3067\n",
      "Epoch [5/5], Step [8316/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [8318/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [8320/10336], Loss: 0.1863\n",
      "Epoch [5/5], Step [8322/10336], Loss: 2.0863\n",
      "Epoch [5/5], Step [8324/10336], Loss: 0.0106\n",
      "Epoch [5/5], Step [8326/10336], Loss: 0.6953\n",
      "Epoch [5/5], Step [8328/10336], Loss: 0.2340\n",
      "Epoch [5/5], Step [8330/10336], Loss: 0.4057\n",
      "Epoch [5/5], Step [8332/10336], Loss: 1.0461\n",
      "Epoch [5/5], Step [8334/10336], Loss: 0.2202\n",
      "Epoch [5/5], Step [8336/10336], Loss: 1.0838\n",
      "Epoch [5/5], Step [8338/10336], Loss: 1.0242\n",
      "Epoch [5/5], Step [8340/10336], Loss: 0.6001\n",
      "Epoch [5/5], Step [8342/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [8344/10336], Loss: 0.5592\n",
      "Epoch [5/5], Step [8346/10336], Loss: 0.0339\n",
      "Epoch [5/5], Step [8348/10336], Loss: 0.0136\n",
      "Epoch [5/5], Step [8350/10336], Loss: 0.0047\n",
      "Epoch [5/5], Step [8352/10336], Loss: 0.0213\n",
      "Epoch [5/5], Step [8354/10336], Loss: 3.9459\n",
      "Epoch [5/5], Step [8356/10336], Loss: 1.6044\n",
      "Epoch [5/5], Step [8358/10336], Loss: 0.6607\n",
      "Epoch [5/5], Step [8360/10336], Loss: 0.0183\n",
      "Epoch [5/5], Step [8362/10336], Loss: 0.2814\n",
      "Epoch [5/5], Step [8364/10336], Loss: 0.0359\n",
      "Epoch [5/5], Step [8366/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [8368/10336], Loss: 0.6008\n",
      "Epoch [5/5], Step [8370/10336], Loss: 0.1888\n",
      "Epoch [5/5], Step [8372/10336], Loss: 0.0846\n",
      "Epoch [5/5], Step [8374/10336], Loss: 2.0207\n",
      "Epoch [5/5], Step [8376/10336], Loss: 0.0912\n",
      "Epoch [5/5], Step [8378/10336], Loss: 0.5585\n",
      "Epoch [5/5], Step [8380/10336], Loss: 6.2264\n",
      "Epoch [5/5], Step [8382/10336], Loss: 0.8799\n",
      "Epoch [5/5], Step [8384/10336], Loss: 0.1148\n",
      "Epoch [5/5], Step [8386/10336], Loss: 0.7088\n",
      "Epoch [5/5], Step [8388/10336], Loss: 0.0356\n",
      "Epoch [5/5], Step [8390/10336], Loss: 0.5451\n",
      "Epoch [5/5], Step [8392/10336], Loss: 0.0167\n",
      "Epoch [5/5], Step [8394/10336], Loss: 1.4124\n",
      "Epoch [5/5], Step [8396/10336], Loss: 0.0136\n",
      "Epoch [5/5], Step [8398/10336], Loss: 0.0504\n",
      "Epoch [5/5], Step [8400/10336], Loss: 0.0101\n",
      "Epoch [5/5], Step [8402/10336], Loss: 0.6753\n",
      "Epoch [5/5], Step [8404/10336], Loss: 1.5785\n",
      "Epoch [5/5], Step [8406/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [8408/10336], Loss: 0.2539\n",
      "Epoch [5/5], Step [8410/10336], Loss: 0.0991\n",
      "Epoch [5/5], Step [8412/10336], Loss: 0.1256\n",
      "Epoch [5/5], Step [8414/10336], Loss: 0.0344\n",
      "Epoch [5/5], Step [8416/10336], Loss: 0.0414\n",
      "Epoch [5/5], Step [8418/10336], Loss: 0.1756\n",
      "Epoch [5/5], Step [8420/10336], Loss: 1.9324\n",
      "Epoch [5/5], Step [8422/10336], Loss: 1.0347\n",
      "Epoch [5/5], Step [8424/10336], Loss: 1.0885\n",
      "Epoch [5/5], Step [8426/10336], Loss: 0.7559\n",
      "Epoch [5/5], Step [8428/10336], Loss: 0.1316\n",
      "Epoch [5/5], Step [8430/10336], Loss: 1.0663\n",
      "Epoch [5/5], Step [8432/10336], Loss: 2.4734\n",
      "Epoch [5/5], Step [8434/10336], Loss: 0.3374\n",
      "Epoch [5/5], Step [8436/10336], Loss: 4.3699\n",
      "Epoch [5/5], Step [8438/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [8440/10336], Loss: 1.7160\n",
      "Epoch [5/5], Step [8442/10336], Loss: 2.4822\n",
      "Epoch [5/5], Step [8444/10336], Loss: 0.0869\n",
      "Epoch [5/5], Step [8446/10336], Loss: 0.2473\n",
      "Epoch [5/5], Step [8448/10336], Loss: 0.2222\n",
      "Epoch [5/5], Step [8450/10336], Loss: 1.4632\n",
      "Epoch [5/5], Step [8452/10336], Loss: 0.7726\n",
      "Epoch [5/5], Step [8454/10336], Loss: 1.2667\n",
      "Epoch [5/5], Step [8456/10336], Loss: 0.3657\n",
      "Epoch [5/5], Step [8458/10336], Loss: 0.5683\n",
      "Epoch [5/5], Step [8460/10336], Loss: 1.1658\n",
      "Epoch [5/5], Step [8462/10336], Loss: 0.3406\n",
      "Epoch [5/5], Step [8464/10336], Loss: 2.0876\n",
      "Epoch [5/5], Step [8466/10336], Loss: 0.0319\n",
      "Epoch [5/5], Step [8468/10336], Loss: 0.2236\n",
      "Epoch [5/5], Step [8470/10336], Loss: 1.8563\n",
      "Epoch [5/5], Step [8472/10336], Loss: 0.1811\n",
      "Epoch [5/5], Step [8474/10336], Loss: 0.0433\n",
      "Epoch [5/5], Step [8476/10336], Loss: 1.1983\n",
      "Epoch [5/5], Step [8478/10336], Loss: 0.0509\n",
      "Epoch [5/5], Step [8480/10336], Loss: 0.3069\n",
      "Epoch [5/5], Step [8482/10336], Loss: 0.0652\n",
      "Epoch [5/5], Step [8484/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [8486/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [8488/10336], Loss: 2.6562\n",
      "Epoch [5/5], Step [8490/10336], Loss: 0.1360\n",
      "Epoch [5/5], Step [8492/10336], Loss: 0.5077\n",
      "Epoch [5/5], Step [8494/10336], Loss: 2.0223\n",
      "Epoch [5/5], Step [8496/10336], Loss: 0.0269\n",
      "Epoch [5/5], Step [8498/10336], Loss: 0.1152\n",
      "Epoch [5/5], Step [8500/10336], Loss: 0.0123\n",
      "Epoch [5/5], Step [8502/10336], Loss: 0.0744\n",
      "Epoch [5/5], Step [8504/10336], Loss: 3.0446\n",
      "Epoch [5/5], Step [8506/10336], Loss: 0.0235\n",
      "Epoch [5/5], Step [8508/10336], Loss: 0.1281\n",
      "Epoch [5/5], Step [8510/10336], Loss: 0.6263\n",
      "Epoch [5/5], Step [8512/10336], Loss: 0.4996\n",
      "Epoch [5/5], Step [8514/10336], Loss: 1.2041\n",
      "Epoch [5/5], Step [8516/10336], Loss: 0.3082\n",
      "Epoch [5/5], Step [8518/10336], Loss: 0.0081\n",
      "Epoch [5/5], Step [8520/10336], Loss: 0.1700\n",
      "Epoch [5/5], Step [8522/10336], Loss: 0.0060\n",
      "Epoch [5/5], Step [8524/10336], Loss: 0.0132\n",
      "Epoch [5/5], Step [8526/10336], Loss: 0.1984\n",
      "Epoch [5/5], Step [8528/10336], Loss: 0.6491\n",
      "Epoch [5/5], Step [8530/10336], Loss: 0.4996\n",
      "Epoch [5/5], Step [8532/10336], Loss: 0.0976\n",
      "Epoch [5/5], Step [8534/10336], Loss: 0.1401\n",
      "Epoch [5/5], Step [8536/10336], Loss: 0.4815\n",
      "Epoch [5/5], Step [8538/10336], Loss: 1.1704\n",
      "Epoch [5/5], Step [8540/10336], Loss: 0.1687\n",
      "Epoch [5/5], Step [8542/10336], Loss: 0.1451\n",
      "Epoch [5/5], Step [8544/10336], Loss: 0.0525\n",
      "Epoch [5/5], Step [8546/10336], Loss: 0.2874\n",
      "Epoch [5/5], Step [8548/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [8550/10336], Loss: 0.0744\n",
      "Epoch [5/5], Step [8552/10336], Loss: 0.1827\n",
      "Epoch [5/5], Step [8554/10336], Loss: 0.4267\n",
      "Epoch [5/5], Step [8556/10336], Loss: 0.1392\n",
      "Epoch [5/5], Step [8558/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [8560/10336], Loss: 0.0078\n",
      "Epoch [5/5], Step [8562/10336], Loss: 1.1736\n",
      "Epoch [5/5], Step [8564/10336], Loss: 0.6786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [8566/10336], Loss: 0.1712\n",
      "Epoch [5/5], Step [8568/10336], Loss: 0.3411\n",
      "Epoch [5/5], Step [8570/10336], Loss: 1.6316\n",
      "Epoch [5/5], Step [8572/10336], Loss: 4.2539\n",
      "Epoch [5/5], Step [8574/10336], Loss: 0.0100\n",
      "Epoch [5/5], Step [8576/10336], Loss: 0.6053\n",
      "Epoch [5/5], Step [8578/10336], Loss: 0.2891\n",
      "Epoch [5/5], Step [8580/10336], Loss: 0.0795\n",
      "Epoch [5/5], Step [8582/10336], Loss: 0.1811\n",
      "Epoch [5/5], Step [8584/10336], Loss: 1.1717\n",
      "Epoch [5/5], Step [8586/10336], Loss: 0.8315\n",
      "Epoch [5/5], Step [8588/10336], Loss: 0.0191\n",
      "Epoch [5/5], Step [8590/10336], Loss: 0.0912\n",
      "Epoch [5/5], Step [8592/10336], Loss: 0.1244\n",
      "Epoch [5/5], Step [8594/10336], Loss: 0.5980\n",
      "Epoch [5/5], Step [8596/10336], Loss: 1.8255\n",
      "Epoch [5/5], Step [8598/10336], Loss: 0.8497\n",
      "Epoch [5/5], Step [8600/10336], Loss: 1.4230\n",
      "Epoch [5/5], Step [8602/10336], Loss: 0.0524\n",
      "Epoch [5/5], Step [8604/10336], Loss: 0.1980\n",
      "Epoch [5/5], Step [8606/10336], Loss: 0.0031\n",
      "Epoch [5/5], Step [8608/10336], Loss: 0.2035\n",
      "Epoch [5/5], Step [8610/10336], Loss: 2.7172\n",
      "Epoch [5/5], Step [8612/10336], Loss: 1.4253\n",
      "Epoch [5/5], Step [8614/10336], Loss: 1.5004\n",
      "Epoch [5/5], Step [8616/10336], Loss: 0.1452\n",
      "Epoch [5/5], Step [8618/10336], Loss: 0.0936\n",
      "Epoch [5/5], Step [8620/10336], Loss: 2.3868\n",
      "Epoch [5/5], Step [8622/10336], Loss: 0.1468\n",
      "Epoch [5/5], Step [8624/10336], Loss: 0.0106\n",
      "Epoch [5/5], Step [8626/10336], Loss: 0.0115\n",
      "Epoch [5/5], Step [8628/10336], Loss: 0.3161\n",
      "Epoch [5/5], Step [8630/10336], Loss: 0.7716\n",
      "Epoch [5/5], Step [8632/10336], Loss: 1.2067\n",
      "Epoch [5/5], Step [8634/10336], Loss: 0.0015\n",
      "Epoch [5/5], Step [8636/10336], Loss: 1.5362\n",
      "Epoch [5/5], Step [8638/10336], Loss: 1.8055\n",
      "Epoch [5/5], Step [8640/10336], Loss: 0.3026\n",
      "Epoch [5/5], Step [8642/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [8644/10336], Loss: 2.0826\n",
      "Epoch [5/5], Step [8646/10336], Loss: 0.0110\n",
      "Epoch [5/5], Step [8648/10336], Loss: 0.0784\n",
      "Epoch [5/5], Step [8650/10336], Loss: 0.4037\n",
      "Epoch [5/5], Step [8652/10336], Loss: 1.2581\n",
      "Epoch [5/5], Step [8654/10336], Loss: 0.0053\n",
      "Epoch [5/5], Step [8656/10336], Loss: 2.5612\n",
      "Epoch [5/5], Step [8658/10336], Loss: 2.1779\n",
      "Epoch [5/5], Step [8660/10336], Loss: 0.7200\n",
      "Epoch [5/5], Step [8662/10336], Loss: 0.6479\n",
      "Epoch [5/5], Step [8664/10336], Loss: 0.0489\n",
      "Epoch [5/5], Step [8666/10336], Loss: 0.0236\n",
      "Epoch [5/5], Step [8668/10336], Loss: 0.0481\n",
      "Epoch [5/5], Step [8670/10336], Loss: 0.0642\n",
      "Epoch [5/5], Step [8672/10336], Loss: 0.0187\n",
      "Epoch [5/5], Step [8674/10336], Loss: 0.1745\n",
      "Epoch [5/5], Step [8676/10336], Loss: 1.4018\n",
      "Epoch [5/5], Step [8678/10336], Loss: 0.5937\n",
      "Epoch [5/5], Step [8680/10336], Loss: 0.0085\n",
      "Epoch [5/5], Step [8682/10336], Loss: 0.0356\n",
      "Epoch [5/5], Step [8684/10336], Loss: 0.1999\n",
      "Epoch [5/5], Step [8686/10336], Loss: 0.1161\n",
      "Epoch [5/5], Step [8688/10336], Loss: 0.4664\n",
      "Epoch [5/5], Step [8690/10336], Loss: 2.1324\n",
      "Epoch [5/5], Step [8692/10336], Loss: 0.1665\n",
      "Epoch [5/5], Step [8694/10336], Loss: 1.9511\n",
      "Epoch [5/5], Step [8696/10336], Loss: 0.0878\n",
      "Epoch [5/5], Step [8698/10336], Loss: 1.9757\n",
      "Epoch [5/5], Step [8700/10336], Loss: 0.0127\n",
      "Epoch [5/5], Step [8702/10336], Loss: 0.5770\n",
      "Epoch [5/5], Step [8704/10336], Loss: 0.0208\n",
      "Epoch [5/5], Step [8706/10336], Loss: 0.0127\n",
      "Epoch [5/5], Step [8708/10336], Loss: 0.1304\n",
      "Epoch [5/5], Step [8710/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [8712/10336], Loss: 0.0293\n",
      "Epoch [5/5], Step [8714/10336], Loss: 0.6807\n",
      "Epoch [5/5], Step [8716/10336], Loss: 2.3650\n",
      "Epoch [5/5], Step [8718/10336], Loss: 1.0491\n",
      "Epoch [5/5], Step [8720/10336], Loss: 0.6500\n",
      "Epoch [5/5], Step [8722/10336], Loss: 0.0318\n",
      "Epoch [5/5], Step [8724/10336], Loss: 0.0695\n",
      "Epoch [5/5], Step [8726/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [8728/10336], Loss: 0.0897\n",
      "Epoch [5/5], Step [8730/10336], Loss: 0.2735\n",
      "Epoch [5/5], Step [8732/10336], Loss: 0.2586\n",
      "Epoch [5/5], Step [8734/10336], Loss: 1.3305\n",
      "Epoch [5/5], Step [8736/10336], Loss: 0.0286\n",
      "Epoch [5/5], Step [8738/10336], Loss: 0.9042\n",
      "Epoch [5/5], Step [8740/10336], Loss: 0.2321\n",
      "Epoch [5/5], Step [8742/10336], Loss: 0.0111\n",
      "Epoch [5/5], Step [8744/10336], Loss: 3.3789\n",
      "Epoch [5/5], Step [8746/10336], Loss: 0.1502\n",
      "Epoch [5/5], Step [8748/10336], Loss: 0.0984\n",
      "Epoch [5/5], Step [8750/10336], Loss: 0.6877\n",
      "Epoch [5/5], Step [8752/10336], Loss: 0.0622\n",
      "Epoch [5/5], Step [8754/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [8756/10336], Loss: 0.4614\n",
      "Epoch [5/5], Step [8758/10336], Loss: 1.3617\n",
      "Epoch [5/5], Step [8760/10336], Loss: 0.4474\n",
      "Epoch [5/5], Step [8762/10336], Loss: 0.4522\n",
      "Epoch [5/5], Step [8764/10336], Loss: 0.2834\n",
      "Epoch [5/5], Step [8766/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [8768/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [8770/10336], Loss: 0.7610\n",
      "Epoch [5/5], Step [8772/10336], Loss: 0.0032\n",
      "Epoch [5/5], Step [8774/10336], Loss: 0.0158\n",
      "Epoch [5/5], Step [8776/10336], Loss: 2.0165\n",
      "Epoch [5/5], Step [8778/10336], Loss: 0.5876\n",
      "Epoch [5/5], Step [8780/10336], Loss: 2.4542\n",
      "Epoch [5/5], Step [8782/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [8784/10336], Loss: 1.5065\n",
      "Epoch [5/5], Step [8786/10336], Loss: 0.0883\n",
      "Epoch [5/5], Step [8788/10336], Loss: 1.6135\n",
      "Epoch [5/5], Step [8790/10336], Loss: 1.5895\n",
      "Epoch [5/5], Step [8792/10336], Loss: 0.1075\n",
      "Epoch [5/5], Step [8794/10336], Loss: 0.2692\n",
      "Epoch [5/5], Step [8796/10336], Loss: 0.0212\n",
      "Epoch [5/5], Step [8798/10336], Loss: 0.3539\n",
      "Epoch [5/5], Step [8800/10336], Loss: 0.0250\n",
      "Epoch [5/5], Step [8802/10336], Loss: 1.6534\n",
      "Epoch [5/5], Step [8804/10336], Loss: 0.7857\n",
      "Epoch [5/5], Step [8806/10336], Loss: 0.2072\n",
      "Epoch [5/5], Step [8808/10336], Loss: 0.0971\n",
      "Epoch [5/5], Step [8810/10336], Loss: 0.4549\n",
      "Epoch [5/5], Step [8812/10336], Loss: 1.9882\n",
      "Epoch [5/5], Step [8814/10336], Loss: 0.1406\n",
      "Epoch [5/5], Step [8816/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [8818/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [8820/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [8822/10336], Loss: 0.1373\n",
      "Epoch [5/5], Step [8824/10336], Loss: 0.3204\n",
      "Epoch [5/5], Step [8826/10336], Loss: 0.0190\n",
      "Epoch [5/5], Step [8828/10336], Loss: 2.4993\n",
      "Epoch [5/5], Step [8830/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [8832/10336], Loss: 2.2758\n",
      "Epoch [5/5], Step [8834/10336], Loss: 0.8633\n",
      "Epoch [5/5], Step [8836/10336], Loss: 3.4844\n",
      "Epoch [5/5], Step [8838/10336], Loss: 0.0576\n",
      "Epoch [5/5], Step [8840/10336], Loss: 0.1842\n",
      "Epoch [5/5], Step [8842/10336], Loss: 0.0111\n",
      "Epoch [5/5], Step [8844/10336], Loss: 1.5863\n",
      "Epoch [5/5], Step [8846/10336], Loss: 0.9239\n",
      "Epoch [5/5], Step [8848/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [8850/10336], Loss: 0.5375\n",
      "Epoch [5/5], Step [8852/10336], Loss: 0.0145\n",
      "Epoch [5/5], Step [8854/10336], Loss: 2.2787\n",
      "Epoch [5/5], Step [8856/10336], Loss: 0.0887\n",
      "Epoch [5/5], Step [8858/10336], Loss: 0.3984\n",
      "Epoch [5/5], Step [8860/10336], Loss: 0.8797\n",
      "Epoch [5/5], Step [8862/10336], Loss: 2.7641\n",
      "Epoch [5/5], Step [8864/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [8866/10336], Loss: 0.4173\n",
      "Epoch [5/5], Step [8868/10336], Loss: 0.0034\n",
      "Epoch [5/5], Step [8870/10336], Loss: 0.0074\n",
      "Epoch [5/5], Step [8872/10336], Loss: 0.0100\n",
      "Epoch [5/5], Step [8874/10336], Loss: 0.8472\n",
      "Epoch [5/5], Step [8876/10336], Loss: 0.0071\n",
      "Epoch [5/5], Step [8878/10336], Loss: 0.0242\n",
      "Epoch [5/5], Step [8880/10336], Loss: 2.5156\n",
      "Epoch [5/5], Step [8882/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [8884/10336], Loss: 0.1932\n",
      "Epoch [5/5], Step [8886/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [8888/10336], Loss: 0.0073\n",
      "Epoch [5/5], Step [8890/10336], Loss: 0.0162\n",
      "Epoch [5/5], Step [8892/10336], Loss: 0.6305\n",
      "Epoch [5/5], Step [8894/10336], Loss: 0.1218\n",
      "Epoch [5/5], Step [8896/10336], Loss: 0.1274\n",
      "Epoch [5/5], Step [8898/10336], Loss: 4.3858\n",
      "Epoch [5/5], Step [8900/10336], Loss: 1.1721\n",
      "Epoch [5/5], Step [8902/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [8904/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [8906/10336], Loss: 0.0298\n",
      "Epoch [5/5], Step [8908/10336], Loss: 0.1329\n",
      "Epoch [5/5], Step [8910/10336], Loss: 0.3817\n",
      "Epoch [5/5], Step [8912/10336], Loss: 0.0677\n",
      "Epoch [5/5], Step [8914/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [8916/10336], Loss: 0.7948\n",
      "Epoch [5/5], Step [8918/10336], Loss: 0.0321\n",
      "Epoch [5/5], Step [8920/10336], Loss: 2.5588\n",
      "Epoch [5/5], Step [8922/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [8924/10336], Loss: 1.0133\n",
      "Epoch [5/5], Step [8926/10336], Loss: 0.1494\n",
      "Epoch [5/5], Step [8928/10336], Loss: 0.3911\n",
      "Epoch [5/5], Step [8930/10336], Loss: 0.1594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [8932/10336], Loss: 1.0381\n",
      "Epoch [5/5], Step [8934/10336], Loss: 0.1115\n",
      "Epoch [5/5], Step [8936/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [8938/10336], Loss: 0.3484\n",
      "Epoch [5/5], Step [8940/10336], Loss: 1.0063\n",
      "Epoch [5/5], Step [8942/10336], Loss: 0.0746\n",
      "Epoch [5/5], Step [8944/10336], Loss: 1.0208\n",
      "Epoch [5/5], Step [8946/10336], Loss: 2.4754\n",
      "Epoch [5/5], Step [8948/10336], Loss: 0.0712\n",
      "Epoch [5/5], Step [8950/10336], Loss: 0.6197\n",
      "Epoch [5/5], Step [8952/10336], Loss: 0.0028\n",
      "Epoch [5/5], Step [8954/10336], Loss: 0.6687\n",
      "Epoch [5/5], Step [8956/10336], Loss: 0.0338\n",
      "Epoch [5/5], Step [8958/10336], Loss: 2.2505\n",
      "Epoch [5/5], Step [8960/10336], Loss: 0.5472\n",
      "Epoch [5/5], Step [8962/10336], Loss: 0.0158\n",
      "Epoch [5/5], Step [8964/10336], Loss: 0.1854\n",
      "Epoch [5/5], Step [8966/10336], Loss: 1.7414\n",
      "Epoch [5/5], Step [8968/10336], Loss: 0.0139\n",
      "Epoch [5/5], Step [8970/10336], Loss: 2.4515\n",
      "Epoch [5/5], Step [8972/10336], Loss: 0.7353\n",
      "Epoch [5/5], Step [8974/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [8976/10336], Loss: 0.0277\n",
      "Epoch [5/5], Step [8978/10336], Loss: 0.7307\n",
      "Epoch [5/5], Step [8980/10336], Loss: 0.8994\n",
      "Epoch [5/5], Step [8982/10336], Loss: 0.3438\n",
      "Epoch [5/5], Step [8984/10336], Loss: 1.9176\n",
      "Epoch [5/5], Step [8986/10336], Loss: 0.8962\n",
      "Epoch [5/5], Step [8988/10336], Loss: 1.0872\n",
      "Epoch [5/5], Step [8990/10336], Loss: 0.0006\n",
      "Epoch [5/5], Step [8992/10336], Loss: 0.0808\n",
      "Epoch [5/5], Step [8994/10336], Loss: 1.0822\n",
      "Epoch [5/5], Step [8996/10336], Loss: 1.0126\n",
      "Epoch [5/5], Step [8998/10336], Loss: 0.3671\n",
      "Epoch [5/5], Step [9000/10336], Loss: 0.4930\n",
      "Epoch [5/5], Step [9002/10336], Loss: 0.0391\n",
      "Epoch [5/5], Step [9004/10336], Loss: 0.2548\n",
      "Epoch [5/5], Step [9006/10336], Loss: 1.0994\n",
      "Epoch [5/5], Step [9008/10336], Loss: 0.1810\n",
      "Epoch [5/5], Step [9010/10336], Loss: 0.1060\n",
      "Epoch [5/5], Step [9012/10336], Loss: 1.9250\n",
      "Epoch [5/5], Step [9014/10336], Loss: 0.0020\n",
      "Epoch [5/5], Step [9016/10336], Loss: 0.0080\n",
      "Epoch [5/5], Step [9018/10336], Loss: 1.9995\n",
      "Epoch [5/5], Step [9020/10336], Loss: 2.8127\n",
      "Epoch [5/5], Step [9022/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [9024/10336], Loss: 2.3248\n",
      "Epoch [5/5], Step [9026/10336], Loss: 0.8622\n",
      "Epoch [5/5], Step [9028/10336], Loss: 2.6052\n",
      "Epoch [5/5], Step [9030/10336], Loss: 0.0751\n",
      "Epoch [5/5], Step [9032/10336], Loss: 3.1202\n",
      "Epoch [5/5], Step [9034/10336], Loss: 0.5939\n",
      "Epoch [5/5], Step [9036/10336], Loss: 0.3173\n",
      "Epoch [5/5], Step [9038/10336], Loss: 0.1426\n",
      "Epoch [5/5], Step [9040/10336], Loss: 0.6341\n",
      "Epoch [5/5], Step [9042/10336], Loss: 0.6768\n",
      "Epoch [5/5], Step [9044/10336], Loss: 2.4468\n",
      "Epoch [5/5], Step [9046/10336], Loss: 1.2636\n",
      "Epoch [5/5], Step [9048/10336], Loss: 0.0213\n",
      "Epoch [5/5], Step [9050/10336], Loss: 1.9113\n",
      "Epoch [5/5], Step [9052/10336], Loss: 0.0797\n",
      "Epoch [5/5], Step [9054/10336], Loss: 2.6248\n",
      "Epoch [5/5], Step [9056/10336], Loss: 0.3019\n",
      "Epoch [5/5], Step [9058/10336], Loss: 0.4327\n",
      "Epoch [5/5], Step [9060/10336], Loss: 0.2226\n",
      "Epoch [5/5], Step [9062/10336], Loss: 0.1669\n",
      "Epoch [5/5], Step [9064/10336], Loss: 0.2557\n",
      "Epoch [5/5], Step [9066/10336], Loss: 1.1493\n",
      "Epoch [5/5], Step [9068/10336], Loss: 1.3568\n",
      "Epoch [5/5], Step [9070/10336], Loss: 0.0115\n",
      "Epoch [5/5], Step [9072/10336], Loss: 1.2213\n",
      "Epoch [5/5], Step [9074/10336], Loss: 0.2516\n",
      "Epoch [5/5], Step [9076/10336], Loss: 0.1745\n",
      "Epoch [5/5], Step [9078/10336], Loss: 4.0778\n",
      "Epoch [5/5], Step [9080/10336], Loss: 0.3833\n",
      "Epoch [5/5], Step [9082/10336], Loss: 2.0427\n",
      "Epoch [5/5], Step [9084/10336], Loss: 0.0037\n",
      "Epoch [5/5], Step [9086/10336], Loss: 0.0101\n",
      "Epoch [5/5], Step [9088/10336], Loss: 1.0285\n",
      "Epoch [5/5], Step [9090/10336], Loss: 0.5310\n",
      "Epoch [5/5], Step [9092/10336], Loss: 1.5623\n",
      "Epoch [5/5], Step [9094/10336], Loss: 0.0297\n",
      "Epoch [5/5], Step [9096/10336], Loss: 0.0286\n",
      "Epoch [5/5], Step [9098/10336], Loss: 0.0219\n",
      "Epoch [5/5], Step [9100/10336], Loss: 0.9221\n",
      "Epoch [5/5], Step [9102/10336], Loss: 0.9042\n",
      "Epoch [5/5], Step [9104/10336], Loss: 1.2558\n",
      "Epoch [5/5], Step [9106/10336], Loss: 0.7613\n",
      "Epoch [5/5], Step [9108/10336], Loss: 0.5650\n",
      "Epoch [5/5], Step [9110/10336], Loss: 0.0696\n",
      "Epoch [5/5], Step [9112/10336], Loss: 0.0719\n",
      "Epoch [5/5], Step [9114/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [9116/10336], Loss: 0.1546\n",
      "Epoch [5/5], Step [9118/10336], Loss: 1.1461\n",
      "Epoch [5/5], Step [9120/10336], Loss: 0.1575\n",
      "Epoch [5/5], Step [9122/10336], Loss: 0.0563\n",
      "Epoch [5/5], Step [9124/10336], Loss: 1.2276\n",
      "Epoch [5/5], Step [9126/10336], Loss: 3.1397\n",
      "Epoch [5/5], Step [9128/10336], Loss: 0.0120\n",
      "Epoch [5/5], Step [9130/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [9132/10336], Loss: 2.4870\n",
      "Epoch [5/5], Step [9134/10336], Loss: 0.8557\n",
      "Epoch [5/5], Step [9136/10336], Loss: 0.1362\n",
      "Epoch [5/5], Step [9138/10336], Loss: 2.3545\n",
      "Epoch [5/5], Step [9140/10336], Loss: 0.0727\n",
      "Epoch [5/5], Step [9142/10336], Loss: 0.0444\n",
      "Epoch [5/5], Step [9144/10336], Loss: 0.0386\n",
      "Epoch [5/5], Step [9146/10336], Loss: 0.6172\n",
      "Epoch [5/5], Step [9148/10336], Loss: 0.0071\n",
      "Epoch [5/5], Step [9150/10336], Loss: 0.1261\n",
      "Epoch [5/5], Step [9152/10336], Loss: 0.7674\n",
      "Epoch [5/5], Step [9154/10336], Loss: 0.0241\n",
      "Epoch [5/5], Step [9156/10336], Loss: 0.0211\n",
      "Epoch [5/5], Step [9158/10336], Loss: 0.1926\n",
      "Epoch [5/5], Step [9160/10336], Loss: 0.6342\n",
      "Epoch [5/5], Step [9162/10336], Loss: 0.1702\n",
      "Epoch [5/5], Step [9164/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [9166/10336], Loss: 0.1321\n",
      "Epoch [5/5], Step [9168/10336], Loss: 0.0362\n",
      "Epoch [5/5], Step [9170/10336], Loss: 0.1560\n",
      "Epoch [5/5], Step [9172/10336], Loss: 0.3441\n",
      "Epoch [5/5], Step [9174/10336], Loss: 0.1774\n",
      "Epoch [5/5], Step [9176/10336], Loss: 0.0057\n",
      "Epoch [5/5], Step [9178/10336], Loss: 0.0420\n",
      "Epoch [5/5], Step [9180/10336], Loss: 0.0624\n",
      "Epoch [5/5], Step [9182/10336], Loss: 0.0502\n",
      "Epoch [5/5], Step [9184/10336], Loss: 0.0134\n",
      "Epoch [5/5], Step [9186/10336], Loss: 1.1988\n",
      "Epoch [5/5], Step [9188/10336], Loss: 1.6824\n",
      "Epoch [5/5], Step [9190/10336], Loss: 0.0463\n",
      "Epoch [5/5], Step [9192/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [9194/10336], Loss: 0.0393\n",
      "Epoch [5/5], Step [9196/10336], Loss: 1.9769\n",
      "Epoch [5/5], Step [9198/10336], Loss: 0.1269\n",
      "Epoch [5/5], Step [9200/10336], Loss: 0.1152\n",
      "Epoch [5/5], Step [9202/10336], Loss: 0.0042\n",
      "Epoch [5/5], Step [9204/10336], Loss: 0.4582\n",
      "Epoch [5/5], Step [9206/10336], Loss: 0.0582\n",
      "Epoch [5/5], Step [9208/10336], Loss: 0.1173\n",
      "Epoch [5/5], Step [9210/10336], Loss: 0.0271\n",
      "Epoch [5/5], Step [9212/10336], Loss: 0.4806\n",
      "Epoch [5/5], Step [9214/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [9216/10336], Loss: 3.7247\n",
      "Epoch [5/5], Step [9218/10336], Loss: 2.0599\n",
      "Epoch [5/5], Step [9220/10336], Loss: 0.0316\n",
      "Epoch [5/5], Step [9222/10336], Loss: 1.1857\n",
      "Epoch [5/5], Step [9224/10336], Loss: 0.5741\n",
      "Epoch [5/5], Step [9226/10336], Loss: 0.0370\n",
      "Epoch [5/5], Step [9228/10336], Loss: 0.2460\n",
      "Epoch [5/5], Step [9230/10336], Loss: 0.7857\n",
      "Epoch [5/5], Step [9232/10336], Loss: 0.3264\n",
      "Epoch [5/5], Step [9234/10336], Loss: 0.0055\n",
      "Epoch [5/5], Step [9236/10336], Loss: 0.2571\n",
      "Epoch [5/5], Step [9238/10336], Loss: 0.0098\n",
      "Epoch [5/5], Step [9240/10336], Loss: 0.0807\n",
      "Epoch [5/5], Step [9242/10336], Loss: 0.0163\n",
      "Epoch [5/5], Step [9244/10336], Loss: 1.4250\n",
      "Epoch [5/5], Step [9246/10336], Loss: 0.2528\n",
      "Epoch [5/5], Step [9248/10336], Loss: 0.1744\n",
      "Epoch [5/5], Step [9250/10336], Loss: 0.1118\n",
      "Epoch [5/5], Step [9252/10336], Loss: 0.8297\n",
      "Epoch [5/5], Step [9254/10336], Loss: 0.1659\n",
      "Epoch [5/5], Step [9256/10336], Loss: 0.0045\n",
      "Epoch [5/5], Step [9258/10336], Loss: 0.0380\n",
      "Epoch [5/5], Step [9260/10336], Loss: 0.1038\n",
      "Epoch [5/5], Step [9262/10336], Loss: 0.4324\n",
      "Epoch [5/5], Step [9264/10336], Loss: 3.1756\n",
      "Epoch [5/5], Step [9266/10336], Loss: 0.0788\n",
      "Epoch [5/5], Step [9268/10336], Loss: 0.7307\n",
      "Epoch [5/5], Step [9270/10336], Loss: 2.7674\n",
      "Epoch [5/5], Step [9272/10336], Loss: 0.4643\n",
      "Epoch [5/5], Step [9274/10336], Loss: 0.7922\n",
      "Epoch [5/5], Step [9276/10336], Loss: 1.6087\n",
      "Epoch [5/5], Step [9278/10336], Loss: 0.1731\n",
      "Epoch [5/5], Step [9280/10336], Loss: 0.4033\n",
      "Epoch [5/5], Step [9282/10336], Loss: 0.0139\n",
      "Epoch [5/5], Step [9284/10336], Loss: 0.0394\n",
      "Epoch [5/5], Step [9286/10336], Loss: 2.5380\n",
      "Epoch [5/5], Step [9288/10336], Loss: 0.2318\n",
      "Epoch [5/5], Step [9290/10336], Loss: 0.1780\n",
      "Epoch [5/5], Step [9292/10336], Loss: 0.0108\n",
      "Epoch [5/5], Step [9294/10336], Loss: 2.1194\n",
      "Epoch [5/5], Step [9296/10336], Loss: 0.6822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [9298/10336], Loss: 0.4521\n",
      "Epoch [5/5], Step [9300/10336], Loss: 0.4086\n",
      "Epoch [5/5], Step [9302/10336], Loss: 0.1905\n",
      "Epoch [5/5], Step [9304/10336], Loss: 0.0027\n",
      "Epoch [5/5], Step [9306/10336], Loss: 0.3427\n",
      "Epoch [5/5], Step [9308/10336], Loss: 0.0730\n",
      "Epoch [5/5], Step [9310/10336], Loss: 0.2222\n",
      "Epoch [5/5], Step [9312/10336], Loss: 0.0344\n",
      "Epoch [5/5], Step [9314/10336], Loss: 0.1048\n",
      "Epoch [5/5], Step [9316/10336], Loss: 1.0737\n",
      "Epoch [5/5], Step [9318/10336], Loss: 0.8832\n",
      "Epoch [5/5], Step [9320/10336], Loss: 1.3638\n",
      "Epoch [5/5], Step [9322/10336], Loss: 0.0151\n",
      "Epoch [5/5], Step [9324/10336], Loss: 0.6816\n",
      "Epoch [5/5], Step [9326/10336], Loss: 0.3251\n",
      "Epoch [5/5], Step [9328/10336], Loss: 0.1878\n",
      "Epoch [5/5], Step [9330/10336], Loss: 0.0557\n",
      "Epoch [5/5], Step [9332/10336], Loss: 0.2576\n",
      "Epoch [5/5], Step [9334/10336], Loss: 0.0127\n",
      "Epoch [5/5], Step [9336/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [9338/10336], Loss: 1.5756\n",
      "Epoch [5/5], Step [9340/10336], Loss: 4.2752\n",
      "Epoch [5/5], Step [9342/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [9344/10336], Loss: 0.1051\n",
      "Epoch [5/5], Step [9346/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [9348/10336], Loss: 0.0426\n",
      "Epoch [5/5], Step [9350/10336], Loss: 0.0248\n",
      "Epoch [5/5], Step [9352/10336], Loss: 0.2126\n",
      "Epoch [5/5], Step [9354/10336], Loss: 1.5594\n",
      "Epoch [5/5], Step [9356/10336], Loss: 0.0174\n",
      "Epoch [5/5], Step [9358/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [9360/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [9362/10336], Loss: 0.1074\n",
      "Epoch [5/5], Step [9364/10336], Loss: 0.9636\n",
      "Epoch [5/5], Step [9366/10336], Loss: 0.0302\n",
      "Epoch [5/5], Step [9368/10336], Loss: 0.1038\n",
      "Epoch [5/5], Step [9370/10336], Loss: 0.0936\n",
      "Epoch [5/5], Step [9372/10336], Loss: 0.2208\n",
      "Epoch [5/5], Step [9374/10336], Loss: 0.1231\n",
      "Epoch [5/5], Step [9376/10336], Loss: 2.2070\n",
      "Epoch [5/5], Step [9378/10336], Loss: 0.3458\n",
      "Epoch [5/5], Step [9380/10336], Loss: 2.2161\n",
      "Epoch [5/5], Step [9382/10336], Loss: 0.1759\n",
      "Epoch [5/5], Step [9384/10336], Loss: 0.0065\n",
      "Epoch [5/5], Step [9386/10336], Loss: 0.8839\n",
      "Epoch [5/5], Step [9388/10336], Loss: 0.1694\n",
      "Epoch [5/5], Step [9390/10336], Loss: 0.0044\n",
      "Epoch [5/5], Step [9392/10336], Loss: 0.0562\n",
      "Epoch [5/5], Step [9394/10336], Loss: 1.1375\n",
      "Epoch [5/5], Step [9396/10336], Loss: 0.0063\n",
      "Epoch [5/5], Step [9398/10336], Loss: 0.0517\n",
      "Epoch [5/5], Step [9400/10336], Loss: 0.1577\n",
      "Epoch [5/5], Step [9402/10336], Loss: 0.0148\n",
      "Epoch [5/5], Step [9404/10336], Loss: 3.0974\n",
      "Epoch [5/5], Step [9406/10336], Loss: 0.0113\n",
      "Epoch [5/5], Step [9408/10336], Loss: 0.0476\n",
      "Epoch [5/5], Step [9410/10336], Loss: 1.1043\n",
      "Epoch [5/5], Step [9412/10336], Loss: 0.7281\n",
      "Epoch [5/5], Step [9414/10336], Loss: 0.9743\n",
      "Epoch [5/5], Step [9416/10336], Loss: 0.2075\n",
      "Epoch [5/5], Step [9418/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [9420/10336], Loss: 1.1814\n",
      "Epoch [5/5], Step [9422/10336], Loss: 0.0088\n",
      "Epoch [5/5], Step [9424/10336], Loss: 0.7303\n",
      "Epoch [5/5], Step [9426/10336], Loss: 0.0133\n",
      "Epoch [5/5], Step [9428/10336], Loss: 0.0029\n",
      "Epoch [5/5], Step [9430/10336], Loss: 0.0740\n",
      "Epoch [5/5], Step [9432/10336], Loss: 0.3418\n",
      "Epoch [5/5], Step [9434/10336], Loss: 0.8094\n",
      "Epoch [5/5], Step [9436/10336], Loss: 0.3042\n",
      "Epoch [5/5], Step [9438/10336], Loss: 0.0346\n",
      "Epoch [5/5], Step [9440/10336], Loss: 2.5600\n",
      "Epoch [5/5], Step [9442/10336], Loss: 0.6133\n",
      "Epoch [5/5], Step [9444/10336], Loss: 2.7921\n",
      "Epoch [5/5], Step [9446/10336], Loss: 0.0050\n",
      "Epoch [5/5], Step [9448/10336], Loss: 0.7477\n",
      "Epoch [5/5], Step [9450/10336], Loss: 0.1794\n",
      "Epoch [5/5], Step [9452/10336], Loss: 1.4078\n",
      "Epoch [5/5], Step [9454/10336], Loss: 0.0918\n",
      "Epoch [5/5], Step [9456/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [9458/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [9460/10336], Loss: 0.0068\n",
      "Epoch [5/5], Step [9462/10336], Loss: 0.0652\n",
      "Epoch [5/5], Step [9464/10336], Loss: 1.7467\n",
      "Epoch [5/5], Step [9466/10336], Loss: 0.2605\n",
      "Epoch [5/5], Step [9468/10336], Loss: 2.6646\n",
      "Epoch [5/5], Step [9470/10336], Loss: 0.4592\n",
      "Epoch [5/5], Step [9472/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [9474/10336], Loss: 0.1071\n",
      "Epoch [5/5], Step [9476/10336], Loss: 0.3053\n",
      "Epoch [5/5], Step [9478/10336], Loss: 0.7105\n",
      "Epoch [5/5], Step [9480/10336], Loss: 0.2507\n",
      "Epoch [5/5], Step [9482/10336], Loss: 0.1778\n",
      "Epoch [5/5], Step [9484/10336], Loss: 0.5640\n",
      "Epoch [5/5], Step [9486/10336], Loss: 1.0059\n",
      "Epoch [5/5], Step [9488/10336], Loss: 0.6836\n",
      "Epoch [5/5], Step [9490/10336], Loss: 0.0411\n",
      "Epoch [5/5], Step [9492/10336], Loss: 0.3071\n",
      "Epoch [5/5], Step [9494/10336], Loss: 0.1595\n",
      "Epoch [5/5], Step [9496/10336], Loss: 0.0886\n",
      "Epoch [5/5], Step [9498/10336], Loss: 2.8625\n",
      "Epoch [5/5], Step [9500/10336], Loss: 0.4428\n",
      "Epoch [5/5], Step [9502/10336], Loss: 0.8494\n",
      "Epoch [5/5], Step [9504/10336], Loss: 0.0644\n",
      "Epoch [5/5], Step [9506/10336], Loss: 0.0094\n",
      "Epoch [5/5], Step [9508/10336], Loss: 0.8156\n",
      "Epoch [5/5], Step [9510/10336], Loss: 0.1652\n",
      "Epoch [5/5], Step [9512/10336], Loss: 0.2031\n",
      "Epoch [5/5], Step [9514/10336], Loss: 2.4877\n",
      "Epoch [5/5], Step [9516/10336], Loss: 1.2425\n",
      "Epoch [5/5], Step [9518/10336], Loss: 1.5832\n",
      "Epoch [5/5], Step [9520/10336], Loss: 0.0367\n",
      "Epoch [5/5], Step [9522/10336], Loss: 0.1003\n",
      "Epoch [5/5], Step [9524/10336], Loss: 1.6277\n",
      "Epoch [5/5], Step [9526/10336], Loss: 0.8858\n",
      "Epoch [5/5], Step [9528/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [9530/10336], Loss: 1.9983\n",
      "Epoch [5/5], Step [9532/10336], Loss: 0.0162\n",
      "Epoch [5/5], Step [9534/10336], Loss: 0.6356\n",
      "Epoch [5/5], Step [9536/10336], Loss: 0.0319\n",
      "Epoch [5/5], Step [9538/10336], Loss: 0.0991\n",
      "Epoch [5/5], Step [9540/10336], Loss: 2.1811\n",
      "Epoch [5/5], Step [9542/10336], Loss: 0.0775\n",
      "Epoch [5/5], Step [9544/10336], Loss: 0.0112\n",
      "Epoch [5/5], Step [9546/10336], Loss: 1.1459\n",
      "Epoch [5/5], Step [9548/10336], Loss: 0.3019\n",
      "Epoch [5/5], Step [9550/10336], Loss: 1.0010\n",
      "Epoch [5/5], Step [9552/10336], Loss: 0.0442\n",
      "Epoch [5/5], Step [9554/10336], Loss: 0.1289\n",
      "Epoch [5/5], Step [9556/10336], Loss: 0.7412\n",
      "Epoch [5/5], Step [9558/10336], Loss: 1.6487\n",
      "Epoch [5/5], Step [9560/10336], Loss: 0.0753\n",
      "Epoch [5/5], Step [9562/10336], Loss: 0.0362\n",
      "Epoch [5/5], Step [9564/10336], Loss: 0.0880\n",
      "Epoch [5/5], Step [9566/10336], Loss: 0.0254\n",
      "Epoch [5/5], Step [9568/10336], Loss: 2.9476\n",
      "Epoch [5/5], Step [9570/10336], Loss: 0.8013\n",
      "Epoch [5/5], Step [9572/10336], Loss: 0.0537\n",
      "Epoch [5/5], Step [9574/10336], Loss: 0.9652\n",
      "Epoch [5/5], Step [9576/10336], Loss: 0.0135\n",
      "Epoch [5/5], Step [9578/10336], Loss: 0.3124\n",
      "Epoch [5/5], Step [9580/10336], Loss: 0.1466\n",
      "Epoch [5/5], Step [9582/10336], Loss: 0.0329\n",
      "Epoch [5/5], Step [9584/10336], Loss: 0.0189\n",
      "Epoch [5/5], Step [9586/10336], Loss: 1.3654\n",
      "Epoch [5/5], Step [9588/10336], Loss: 0.0095\n",
      "Epoch [5/5], Step [9590/10336], Loss: 0.2398\n",
      "Epoch [5/5], Step [9592/10336], Loss: 0.8452\n",
      "Epoch [5/5], Step [9594/10336], Loss: 0.7170\n",
      "Epoch [5/5], Step [9596/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [9598/10336], Loss: 0.0196\n",
      "Epoch [5/5], Step [9600/10336], Loss: 1.3116\n",
      "Epoch [5/5], Step [9602/10336], Loss: 0.0194\n",
      "Epoch [5/5], Step [9604/10336], Loss: 0.7421\n",
      "Epoch [5/5], Step [9606/10336], Loss: 1.8756\n",
      "Epoch [5/5], Step [9608/10336], Loss: 3.3230\n",
      "Epoch [5/5], Step [9610/10336], Loss: 1.5105\n",
      "Epoch [5/5], Step [9612/10336], Loss: 0.1717\n",
      "Epoch [5/5], Step [9614/10336], Loss: 1.5095\n",
      "Epoch [5/5], Step [9616/10336], Loss: 0.0216\n",
      "Epoch [5/5], Step [9618/10336], Loss: 0.1018\n",
      "Epoch [5/5], Step [9620/10336], Loss: 0.7139\n",
      "Epoch [5/5], Step [9622/10336], Loss: 0.8174\n",
      "Epoch [5/5], Step [9624/10336], Loss: 0.1838\n",
      "Epoch [5/5], Step [9626/10336], Loss: 0.0040\n",
      "Epoch [5/5], Step [9628/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [9630/10336], Loss: 0.1167\n",
      "Epoch [5/5], Step [9632/10336], Loss: 0.1438\n",
      "Epoch [5/5], Step [9634/10336], Loss: 0.8582\n",
      "Epoch [5/5], Step [9636/10336], Loss: 0.0355\n",
      "Epoch [5/5], Step [9638/10336], Loss: 0.0148\n",
      "Epoch [5/5], Step [9640/10336], Loss: 0.4516\n",
      "Epoch [5/5], Step [9642/10336], Loss: 2.0503\n",
      "Epoch [5/5], Step [9644/10336], Loss: 0.5071\n",
      "Epoch [5/5], Step [9646/10336], Loss: 0.0597\n",
      "Epoch [5/5], Step [9648/10336], Loss: 0.4445\n",
      "Epoch [5/5], Step [9650/10336], Loss: 2.1136\n",
      "Epoch [5/5], Step [9652/10336], Loss: 0.0198\n",
      "Epoch [5/5], Step [9654/10336], Loss: 0.9260\n",
      "Epoch [5/5], Step [9656/10336], Loss: 0.5100\n",
      "Epoch [5/5], Step [9658/10336], Loss: 0.3569\n",
      "Epoch [5/5], Step [9660/10336], Loss: 1.9686\n",
      "Epoch [5/5], Step [9662/10336], Loss: 1.0264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [9664/10336], Loss: 0.0645\n",
      "Epoch [5/5], Step [9666/10336], Loss: 0.6926\n",
      "Epoch [5/5], Step [9668/10336], Loss: 0.7717\n",
      "Epoch [5/5], Step [9670/10336], Loss: 0.0278\n",
      "Epoch [5/5], Step [9672/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [9674/10336], Loss: 0.0533\n",
      "Epoch [5/5], Step [9676/10336], Loss: 0.6806\n",
      "Epoch [5/5], Step [9678/10336], Loss: 0.7220\n",
      "Epoch [5/5], Step [9680/10336], Loss: 1.6289\n",
      "Epoch [5/5], Step [9682/10336], Loss: 0.0459\n",
      "Epoch [5/5], Step [9684/10336], Loss: 0.1173\n",
      "Epoch [5/5], Step [9686/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [9688/10336], Loss: 0.0083\n",
      "Epoch [5/5], Step [9690/10336], Loss: 1.5271\n",
      "Epoch [5/5], Step [9692/10336], Loss: 0.0084\n",
      "Epoch [5/5], Step [9694/10336], Loss: 0.8009\n",
      "Epoch [5/5], Step [9696/10336], Loss: 0.2419\n",
      "Epoch [5/5], Step [9698/10336], Loss: 1.8596\n",
      "Epoch [5/5], Step [9700/10336], Loss: 0.1060\n",
      "Epoch [5/5], Step [9702/10336], Loss: 1.0649\n",
      "Epoch [5/5], Step [9704/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [9706/10336], Loss: 1.1045\n",
      "Epoch [5/5], Step [9708/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [9710/10336], Loss: 0.0065\n",
      "Epoch [5/5], Step [9712/10336], Loss: 2.2775\n",
      "Epoch [5/5], Step [9714/10336], Loss: 1.4030\n",
      "Epoch [5/5], Step [9716/10336], Loss: 1.0395\n",
      "Epoch [5/5], Step [9718/10336], Loss: 0.3916\n",
      "Epoch [5/5], Step [9720/10336], Loss: 0.6397\n",
      "Epoch [5/5], Step [9722/10336], Loss: 1.0175\n",
      "Epoch [5/5], Step [9724/10336], Loss: 0.1076\n",
      "Epoch [5/5], Step [9726/10336], Loss: 0.0324\n",
      "Epoch [5/5], Step [9728/10336], Loss: 1.8421\n",
      "Epoch [5/5], Step [9730/10336], Loss: 0.8842\n",
      "Epoch [5/5], Step [9732/10336], Loss: 5.0350\n",
      "Epoch [5/5], Step [9734/10336], Loss: 0.0419\n",
      "Epoch [5/5], Step [9736/10336], Loss: 0.2101\n",
      "Epoch [5/5], Step [9738/10336], Loss: 0.7016\n",
      "Epoch [5/5], Step [9740/10336], Loss: 0.3827\n",
      "Epoch [5/5], Step [9742/10336], Loss: 0.0267\n",
      "Epoch [5/5], Step [9744/10336], Loss: 0.0991\n",
      "Epoch [5/5], Step [9746/10336], Loss: 0.4872\n",
      "Epoch [5/5], Step [9748/10336], Loss: 0.2242\n",
      "Epoch [5/5], Step [9750/10336], Loss: 0.4658\n",
      "Epoch [5/5], Step [9752/10336], Loss: 0.1454\n",
      "Epoch [5/5], Step [9754/10336], Loss: 0.4509\n",
      "Epoch [5/5], Step [9756/10336], Loss: 1.2529\n",
      "Epoch [5/5], Step [9758/10336], Loss: 0.4343\n",
      "Epoch [5/5], Step [9760/10336], Loss: 0.2865\n",
      "Epoch [5/5], Step [9762/10336], Loss: 0.0914\n",
      "Epoch [5/5], Step [9764/10336], Loss: 0.1049\n",
      "Epoch [5/5], Step [9766/10336], Loss: 0.2284\n",
      "Epoch [5/5], Step [9768/10336], Loss: 0.0562\n",
      "Epoch [5/5], Step [9770/10336], Loss: 0.7877\n",
      "Epoch [5/5], Step [9772/10336], Loss: 0.3105\n",
      "Epoch [5/5], Step [9774/10336], Loss: 0.2012\n",
      "Epoch [5/5], Step [9776/10336], Loss: 0.0024\n",
      "Epoch [5/5], Step [9778/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [9780/10336], Loss: 1.6075\n",
      "Epoch [5/5], Step [9782/10336], Loss: 0.0805\n",
      "Epoch [5/5], Step [9784/10336], Loss: 0.0690\n",
      "Epoch [5/5], Step [9786/10336], Loss: 0.8825\n",
      "Epoch [5/5], Step [9788/10336], Loss: 0.1565\n",
      "Epoch [5/5], Step [9790/10336], Loss: 0.0282\n",
      "Epoch [5/5], Step [9792/10336], Loss: 0.0061\n",
      "Epoch [5/5], Step [9794/10336], Loss: 0.6063\n",
      "Epoch [5/5], Step [9796/10336], Loss: 0.9682\n",
      "Epoch [5/5], Step [9798/10336], Loss: 0.3887\n",
      "Epoch [5/5], Step [9800/10336], Loss: 1.9431\n",
      "Epoch [5/5], Step [9802/10336], Loss: 0.2979\n",
      "Epoch [5/5], Step [9804/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [9806/10336], Loss: 0.2357\n",
      "Epoch [5/5], Step [9808/10336], Loss: 0.1681\n",
      "Epoch [5/5], Step [9810/10336], Loss: 0.4173\n",
      "Epoch [5/5], Step [9812/10336], Loss: 0.0813\n",
      "Epoch [5/5], Step [9814/10336], Loss: 0.0167\n",
      "Epoch [5/5], Step [9816/10336], Loss: 0.0963\n",
      "Epoch [5/5], Step [9818/10336], Loss: 0.1148\n",
      "Epoch [5/5], Step [9820/10336], Loss: 0.4799\n",
      "Epoch [5/5], Step [9822/10336], Loss: 0.4045\n",
      "Epoch [5/5], Step [9824/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [9826/10336], Loss: 0.0021\n",
      "Epoch [5/5], Step [9828/10336], Loss: 0.9551\n",
      "Epoch [5/5], Step [9830/10336], Loss: 0.5720\n",
      "Epoch [5/5], Step [9832/10336], Loss: 0.0094\n",
      "Epoch [5/5], Step [9834/10336], Loss: 3.6267\n",
      "Epoch [5/5], Step [9836/10336], Loss: 2.8346\n",
      "Epoch [5/5], Step [9838/10336], Loss: 2.2366\n",
      "Epoch [5/5], Step [9840/10336], Loss: 0.0019\n",
      "Epoch [5/5], Step [9842/10336], Loss: 0.8231\n",
      "Epoch [5/5], Step [9844/10336], Loss: 0.7940\n",
      "Epoch [5/5], Step [9846/10336], Loss: 0.0393\n",
      "Epoch [5/5], Step [9848/10336], Loss: 0.2228\n",
      "Epoch [5/5], Step [9850/10336], Loss: 0.9398\n",
      "Epoch [5/5], Step [9852/10336], Loss: 0.1024\n",
      "Epoch [5/5], Step [9854/10336], Loss: 0.3815\n",
      "Epoch [5/5], Step [9856/10336], Loss: 0.0263\n",
      "Epoch [5/5], Step [9858/10336], Loss: 0.4115\n",
      "Epoch [5/5], Step [9860/10336], Loss: 0.0370\n",
      "Epoch [5/5], Step [9862/10336], Loss: 0.0260\n",
      "Epoch [5/5], Step [9864/10336], Loss: 0.6488\n",
      "Epoch [5/5], Step [9866/10336], Loss: 0.0892\n",
      "Epoch [5/5], Step [9868/10336], Loss: 0.7090\n",
      "Epoch [5/5], Step [9870/10336], Loss: 0.0673\n",
      "Epoch [5/5], Step [9872/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [9874/10336], Loss: 0.0350\n",
      "Epoch [5/5], Step [9876/10336], Loss: 1.6409\n",
      "Epoch [5/5], Step [9878/10336], Loss: 0.0077\n",
      "Epoch [5/5], Step [9880/10336], Loss: 0.5098\n",
      "Epoch [5/5], Step [9882/10336], Loss: 0.0630\n",
      "Epoch [5/5], Step [9884/10336], Loss: 2.1301\n",
      "Epoch [5/5], Step [9886/10336], Loss: 1.4385\n",
      "Epoch [5/5], Step [9888/10336], Loss: 0.2864\n",
      "Epoch [5/5], Step [9890/10336], Loss: 0.4707\n",
      "Epoch [5/5], Step [9892/10336], Loss: 3.2698\n",
      "Epoch [5/5], Step [9894/10336], Loss: 0.1302\n",
      "Epoch [5/5], Step [9896/10336], Loss: 0.7117\n",
      "Epoch [5/5], Step [9898/10336], Loss: 1.0051\n",
      "Epoch [5/5], Step [9900/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [9902/10336], Loss: 1.6888\n",
      "Epoch [5/5], Step [9904/10336], Loss: 0.0993\n",
      "Epoch [5/5], Step [9906/10336], Loss: 0.0012\n",
      "Epoch [5/5], Step [9908/10336], Loss: 0.0096\n",
      "Epoch [5/5], Step [9910/10336], Loss: 0.2780\n",
      "Epoch [5/5], Step [9912/10336], Loss: 0.0871\n",
      "Epoch [5/5], Step [9914/10336], Loss: 1.2199\n",
      "Epoch [5/5], Step [9916/10336], Loss: 1.0904\n",
      "Epoch [5/5], Step [9918/10336], Loss: 0.5400\n",
      "Epoch [5/5], Step [9920/10336], Loss: 0.0098\n",
      "Epoch [5/5], Step [9922/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [9924/10336], Loss: 0.0284\n",
      "Epoch [5/5], Step [9926/10336], Loss: 1.3895\n",
      "Epoch [5/5], Step [9928/10336], Loss: 0.0052\n",
      "Epoch [5/5], Step [9930/10336], Loss: 0.1002\n",
      "Epoch [5/5], Step [9932/10336], Loss: 0.9659\n",
      "Epoch [5/5], Step [9934/10336], Loss: 0.0397\n",
      "Epoch [5/5], Step [9936/10336], Loss: 0.3398\n",
      "Epoch [5/5], Step [9938/10336], Loss: 0.2866\n",
      "Epoch [5/5], Step [9940/10336], Loss: 0.6390\n",
      "Epoch [5/5], Step [9942/10336], Loss: 1.4046\n",
      "Epoch [5/5], Step [9944/10336], Loss: 0.5832\n",
      "Epoch [5/5], Step [9946/10336], Loss: 0.3176\n",
      "Epoch [5/5], Step [9948/10336], Loss: 0.0103\n",
      "Epoch [5/5], Step [9950/10336], Loss: 0.0000\n",
      "Epoch [5/5], Step [9952/10336], Loss: 0.1549\n",
      "Epoch [5/5], Step [9954/10336], Loss: 0.0182\n",
      "Epoch [5/5], Step [9956/10336], Loss: 0.0152\n",
      "Epoch [5/5], Step [9958/10336], Loss: 0.0159\n",
      "Epoch [5/5], Step [9960/10336], Loss: 2.7430\n",
      "Epoch [5/5], Step [9962/10336], Loss: 1.1232\n",
      "Epoch [5/5], Step [9964/10336], Loss: 0.4738\n",
      "Epoch [5/5], Step [9966/10336], Loss: 0.0009\n",
      "Epoch [5/5], Step [9968/10336], Loss: 0.0003\n",
      "Epoch [5/5], Step [9970/10336], Loss: 0.0018\n",
      "Epoch [5/5], Step [9972/10336], Loss: 0.0879\n",
      "Epoch [5/5], Step [9974/10336], Loss: 0.0272\n",
      "Epoch [5/5], Step [9976/10336], Loss: 0.2318\n",
      "Epoch [5/5], Step [9978/10336], Loss: 0.3825\n",
      "Epoch [5/5], Step [9980/10336], Loss: 0.7596\n",
      "Epoch [5/5], Step [9982/10336], Loss: 0.2035\n",
      "Epoch [5/5], Step [9984/10336], Loss: 1.6010\n",
      "Epoch [5/5], Step [9986/10336], Loss: 0.7615\n",
      "Epoch [5/5], Step [9988/10336], Loss: 0.4550\n",
      "Epoch [5/5], Step [9990/10336], Loss: 0.3468\n",
      "Epoch [5/5], Step [9992/10336], Loss: 0.0289\n",
      "Epoch [5/5], Step [9994/10336], Loss: 0.0632\n",
      "Epoch [5/5], Step [9996/10336], Loss: 0.6668\n",
      "Epoch [5/5], Step [9998/10336], Loss: 1.0379\n",
      "Epoch [5/5], Step [10000/10336], Loss: 0.0064\n",
      "Epoch [5/5], Step [10002/10336], Loss: 0.0089\n",
      "Epoch [5/5], Step [10004/10336], Loss: 0.3082\n",
      "Epoch [5/5], Step [10006/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [10008/10336], Loss: 0.1657\n",
      "Epoch [5/5], Step [10010/10336], Loss: 0.0303\n",
      "Epoch [5/5], Step [10012/10336], Loss: 0.1827\n",
      "Epoch [5/5], Step [10014/10336], Loss: 0.0623\n",
      "Epoch [5/5], Step [10016/10336], Loss: 0.0168\n",
      "Epoch [5/5], Step [10018/10336], Loss: 0.0476\n",
      "Epoch [5/5], Step [10020/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [10022/10336], Loss: 0.0562\n",
      "Epoch [5/5], Step [10024/10336], Loss: 0.0097\n",
      "Epoch [5/5], Step [10026/10336], Loss: 0.0177\n",
      "Epoch [5/5], Step [10028/10336], Loss: 0.0009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [10030/10336], Loss: 0.2769\n",
      "Epoch [5/5], Step [10032/10336], Loss: 1.4767\n",
      "Epoch [5/5], Step [10034/10336], Loss: 0.7046\n",
      "Epoch [5/5], Step [10036/10336], Loss: 0.2128\n",
      "Epoch [5/5], Step [10038/10336], Loss: 0.0999\n",
      "Epoch [5/5], Step [10040/10336], Loss: 0.9959\n",
      "Epoch [5/5], Step [10042/10336], Loss: 0.0151\n",
      "Epoch [5/5], Step [10044/10336], Loss: 0.0059\n",
      "Epoch [5/5], Step [10046/10336], Loss: 2.6660\n",
      "Epoch [5/5], Step [10048/10336], Loss: 0.3317\n",
      "Epoch [5/5], Step [10050/10336], Loss: 0.0260\n",
      "Epoch [5/5], Step [10052/10336], Loss: 1.4736\n",
      "Epoch [5/5], Step [10054/10336], Loss: 1.6593\n",
      "Epoch [5/5], Step [10056/10336], Loss: 0.0016\n",
      "Epoch [5/5], Step [10058/10336], Loss: 0.0825\n",
      "Epoch [5/5], Step [10060/10336], Loss: 0.0155\n",
      "Epoch [5/5], Step [10062/10336], Loss: 1.9926\n",
      "Epoch [5/5], Step [10064/10336], Loss: 0.0054\n",
      "Epoch [5/5], Step [10066/10336], Loss: 0.0014\n",
      "Epoch [5/5], Step [10068/10336], Loss: 0.2914\n",
      "Epoch [5/5], Step [10070/10336], Loss: 0.0873\n",
      "Epoch [5/5], Step [10072/10336], Loss: 1.3264\n",
      "Epoch [5/5], Step [10074/10336], Loss: 0.0046\n",
      "Epoch [5/5], Step [10076/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [10078/10336], Loss: 0.2501\n",
      "Epoch [5/5], Step [10080/10336], Loss: 1.2879\n",
      "Epoch [5/5], Step [10082/10336], Loss: 0.8148\n",
      "Epoch [5/5], Step [10084/10336], Loss: 0.5594\n",
      "Epoch [5/5], Step [10086/10336], Loss: 0.0177\n",
      "Epoch [5/5], Step [10088/10336], Loss: 0.0163\n",
      "Epoch [5/5], Step [10090/10336], Loss: 0.5213\n",
      "Epoch [5/5], Step [10092/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [10094/10336], Loss: 1.7385\n",
      "Epoch [5/5], Step [10096/10336], Loss: 0.1730\n",
      "Epoch [5/5], Step [10098/10336], Loss: 0.8055\n",
      "Epoch [5/5], Step [10100/10336], Loss: 0.0720\n",
      "Epoch [5/5], Step [10102/10336], Loss: 0.0151\n",
      "Epoch [5/5], Step [10104/10336], Loss: 0.1783\n",
      "Epoch [5/5], Step [10106/10336], Loss: 0.3755\n",
      "Epoch [5/5], Step [10108/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [10110/10336], Loss: 1.3655\n",
      "Epoch [5/5], Step [10112/10336], Loss: 0.0433\n",
      "Epoch [5/5], Step [10114/10336], Loss: 1.2714\n",
      "Epoch [5/5], Step [10116/10336], Loss: 0.0131\n",
      "Epoch [5/5], Step [10118/10336], Loss: 0.2164\n",
      "Epoch [5/5], Step [10120/10336], Loss: 1.2066\n",
      "Epoch [5/5], Step [10122/10336], Loss: 0.2803\n",
      "Epoch [5/5], Step [10124/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [10126/10336], Loss: 0.7166\n",
      "Epoch [5/5], Step [10128/10336], Loss: 0.1714\n",
      "Epoch [5/5], Step [10130/10336], Loss: 0.0030\n",
      "Epoch [5/5], Step [10132/10336], Loss: 0.0149\n",
      "Epoch [5/5], Step [10134/10336], Loss: 0.0033\n",
      "Epoch [5/5], Step [10136/10336], Loss: 0.0062\n",
      "Epoch [5/5], Step [10138/10336], Loss: 1.1657\n",
      "Epoch [5/5], Step [10140/10336], Loss: 0.6156\n",
      "Epoch [5/5], Step [10142/10336], Loss: 0.0671\n",
      "Epoch [5/5], Step [10144/10336], Loss: 0.1168\n",
      "Epoch [5/5], Step [10146/10336], Loss: 0.6808\n",
      "Epoch [5/5], Step [10148/10336], Loss: 0.0269\n",
      "Epoch [5/5], Step [10150/10336], Loss: 1.9236\n",
      "Epoch [5/5], Step [10152/10336], Loss: 2.1477\n",
      "Epoch [5/5], Step [10154/10336], Loss: 0.0002\n",
      "Epoch [5/5], Step [10156/10336], Loss: 0.1278\n",
      "Epoch [5/5], Step [10158/10336], Loss: 0.1394\n",
      "Epoch [5/5], Step [10160/10336], Loss: 0.0023\n",
      "Epoch [5/5], Step [10162/10336], Loss: 0.6651\n",
      "Epoch [5/5], Step [10164/10336], Loss: 0.1526\n",
      "Epoch [5/5], Step [10166/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [10168/10336], Loss: 1.6194\n",
      "Epoch [5/5], Step [10170/10336], Loss: 0.0618\n",
      "Epoch [5/5], Step [10172/10336], Loss: 0.0010\n",
      "Epoch [5/5], Step [10174/10336], Loss: 0.0155\n",
      "Epoch [5/5], Step [10176/10336], Loss: 0.0063\n",
      "Epoch [5/5], Step [10178/10336], Loss: 0.0005\n",
      "Epoch [5/5], Step [10180/10336], Loss: 0.0004\n",
      "Epoch [5/5], Step [10182/10336], Loss: 0.4773\n",
      "Epoch [5/5], Step [10184/10336], Loss: 0.6142\n",
      "Epoch [5/5], Step [10186/10336], Loss: 2.2801\n",
      "Epoch [5/5], Step [10188/10336], Loss: 0.1207\n",
      "Epoch [5/5], Step [10190/10336], Loss: 3.2301\n",
      "Epoch [5/5], Step [10192/10336], Loss: 0.3275\n",
      "Epoch [5/5], Step [10194/10336], Loss: 0.0820\n",
      "Epoch [5/5], Step [10196/10336], Loss: 0.6587\n",
      "Epoch [5/5], Step [10198/10336], Loss: 0.0043\n",
      "Epoch [5/5], Step [10200/10336], Loss: 0.0051\n",
      "Epoch [5/5], Step [10202/10336], Loss: 0.2302\n",
      "Epoch [5/5], Step [10204/10336], Loss: 0.0586\n",
      "Epoch [5/5], Step [10206/10336], Loss: 0.3040\n",
      "Epoch [5/5], Step [10208/10336], Loss: 0.4563\n",
      "Epoch [5/5], Step [10210/10336], Loss: 0.0169\n",
      "Epoch [5/5], Step [10212/10336], Loss: 0.0844\n",
      "Epoch [5/5], Step [10214/10336], Loss: 0.8328\n",
      "Epoch [5/5], Step [10216/10336], Loss: 0.0013\n",
      "Epoch [5/5], Step [10218/10336], Loss: 0.1342\n",
      "Epoch [5/5], Step [10220/10336], Loss: 3.4837\n",
      "Epoch [5/5], Step [10222/10336], Loss: 0.0392\n",
      "Epoch [5/5], Step [10224/10336], Loss: 0.0001\n",
      "Epoch [5/5], Step [10226/10336], Loss: 0.0440\n",
      "Epoch [5/5], Step [10228/10336], Loss: 1.0883\n",
      "Epoch [5/5], Step [10230/10336], Loss: 0.0011\n",
      "Epoch [5/5], Step [10232/10336], Loss: 0.0264\n",
      "Epoch [5/5], Step [10234/10336], Loss: 0.0926\n",
      "Epoch [5/5], Step [10236/10336], Loss: 2.6136\n",
      "Epoch [5/5], Step [10238/10336], Loss: 0.3818\n",
      "Epoch [5/5], Step [10240/10336], Loss: 0.5706\n",
      "Epoch [5/5], Step [10242/10336], Loss: 0.0456\n",
      "Epoch [5/5], Step [10244/10336], Loss: 3.1016\n",
      "Epoch [5/5], Step [10246/10336], Loss: 0.0802\n",
      "Epoch [5/5], Step [10248/10336], Loss: 1.6131\n",
      "Epoch [5/5], Step [10250/10336], Loss: 0.0008\n",
      "Epoch [5/5], Step [10252/10336], Loss: 0.0201\n",
      "Epoch [5/5], Step [10254/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [10256/10336], Loss: 1.5665\n",
      "Epoch [5/5], Step [10258/10336], Loss: 0.0237\n",
      "Epoch [5/5], Step [10260/10336], Loss: 0.3203\n",
      "Epoch [5/5], Step [10262/10336], Loss: 0.0203\n",
      "Epoch [5/5], Step [10264/10336], Loss: 0.2525\n",
      "Epoch [5/5], Step [10266/10336], Loss: 0.1868\n",
      "Epoch [5/5], Step [10268/10336], Loss: 0.8295\n",
      "Epoch [5/5], Step [10270/10336], Loss: 0.0935\n",
      "Epoch [5/5], Step [10272/10336], Loss: 0.8219\n",
      "Epoch [5/5], Step [10274/10336], Loss: 0.0148\n",
      "Epoch [5/5], Step [10276/10336], Loss: 2.0087\n",
      "Epoch [5/5], Step [10278/10336], Loss: 0.0699\n",
      "Epoch [5/5], Step [10280/10336], Loss: 0.9337\n",
      "Epoch [5/5], Step [10282/10336], Loss: 0.2543\n",
      "Epoch [5/5], Step [10284/10336], Loss: 0.0072\n",
      "Epoch [5/5], Step [10286/10336], Loss: 0.9353\n",
      "Epoch [5/5], Step [10288/10336], Loss: 0.1122\n",
      "Epoch [5/5], Step [10290/10336], Loss: 0.0041\n",
      "Epoch [5/5], Step [10292/10336], Loss: 0.2657\n",
      "Epoch [5/5], Step [10294/10336], Loss: 0.2649\n",
      "Epoch [5/5], Step [10296/10336], Loss: 0.1873\n",
      "Epoch [5/5], Step [10298/10336], Loss: 0.0117\n",
      "Epoch [5/5], Step [10300/10336], Loss: 0.0197\n",
      "Epoch [5/5], Step [10302/10336], Loss: 0.9651\n",
      "Epoch [5/5], Step [10304/10336], Loss: 0.2865\n",
      "Epoch [5/5], Step [10306/10336], Loss: 0.0294\n",
      "Epoch [5/5], Step [10308/10336], Loss: 2.1204\n",
      "Epoch [5/5], Step [10310/10336], Loss: 0.6851\n",
      "Epoch [5/5], Step [10312/10336], Loss: 0.9000\n",
      "Epoch [5/5], Step [10314/10336], Loss: 3.2454\n",
      "Epoch [5/5], Step [10316/10336], Loss: 0.1414\n",
      "Epoch [5/5], Step [10318/10336], Loss: 0.0760\n",
      "Epoch [5/5], Step [10320/10336], Loss: 0.0119\n",
      "Epoch [5/5], Step [10322/10336], Loss: 0.9304\n",
      "Epoch [5/5], Step [10324/10336], Loss: 0.1583\n",
      "Epoch [5/5], Step [10326/10336], Loss: 0.5483\n",
      "Epoch [5/5], Step [10328/10336], Loss: 0.0132\n",
      "Epoch [5/5], Step [10330/10336], Loss: 0.0455\n",
      "Epoch [5/5], Step [10332/10336], Loss: 0.0695\n",
      "Epoch [5/5], Step [10334/10336], Loss: 0.0007\n",
      "Epoch [5/5], Step [10336/10336], Loss: 0.5067\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0978ef8290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30min 56s, sys: 5min 37s, total: 36min 34s\n",
      "Wall time: 33min 22s\n"
     ]
    }
   ],
   "source": [
    "%time arya_train()\n",
    "torch.save(resnet18.state_dict(), 'resnet18.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resnet18.load_state_dict(torch.load('resnet18.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Accuracy Calculation\n",
    "Jorah then asks a question, how is this a detection task?<br/>\n",
    "As everybody wonders, Theon Greyjoy suggests a slding window method to test the above trained trained network on the detection task:<br/>\n",
    "\"We take some windows of varying size and aspect ratios\", he mumbled, \"and slide it through the test image (considering some stride of pixels) from left to right, and top to bottom, detect the class scores for each of the window, and keep only those which are above a certain threshold value!\". \"He is right\", says Samwell, \"I read a similar approach in the paper -Faster RCNN by Ross Girshick in the library, where he uses three diferent scales/sizes and three different aspect ratios, making a total of nine windows per pixel to slide\". You need to write the code and use it in testing code to find the predicted boxes and their classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def theon_sliding_window(image,aspect_ratio,size,slide_amount):\n",
    "    # Begin\n",
    "    windows = []\n",
    "    labels = []\n",
    "    for x in xrange(0,image.size(0),slide_amount):\n",
    "        for y in xrange(0,image.size(1),slide_amount):\n",
    "            box = [x,y,int(x+size*aspect_ratio),y+size]\n",
    "            crop_img = image.crop(box).convert('RGB')\n",
    "            images = Variable(crop_img)\n",
    "            crop_img = composed_transform(crop_img)\n",
    "            if(use_gpu):\n",
    "                images = images.cuda()\n",
    "            output = resnet18(images)\n",
    "            prob,label = torch.topk(output.data,1)\n",
    "            if prob > threshold:\n",
    "                windows.append(box)\n",
    "                labels.append(label)\n",
    "    return windows,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Wait\", says <b>Jon Snow</b>, \"The predicted boxes may be too many and we can't deal with all of them. So, I myself will go and apply non_maximum_supression to reduce the number of boxes\". You are free to choose the threshold value for non maximum supression, but choose wisely [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aegon_targaryen_non_maximum_supression(boxes,lables,threshold = 0.3):\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # initialize the list of picked indexes\t\n",
    "    pick = []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "\n",
    "    # compute the area of the bounding boxes and sort the bounding\n",
    "    # boxes by the bottom-right y-coordinate of the bounding box\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "    # grab the last index in the indexes list and add the\n",
    "    # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the largest (x, y) coordinates for the start of\n",
    "        # the bounding box and the smallest (x, y) coordinates\n",
    "        # for the end of the bounding box\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        # compute the width and height of the bounding box\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > threshold)[0])))\n",
    "\n",
    "    # return only the bounding boxes that were picked using the\n",
    "    # integer data type\n",
    "    return boxes[pick].astype(\"int\") , labels[pick]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daenerys, the queen, then orders her army to test out the trained model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def daenerys_test(resnet18):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # Also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    aspect_ratio = [1,0.75,1.33]\n",
    "    size = [224,112,56]\n",
    "    slide_amount = 10\n",
    "    boxes = []\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images)\n",
    "        \n",
    "        if(use_gpu):\n",
    "            images = images.cuda()\n",
    "        \n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                windows, labels = theon_sliding_window(images,aspect_ratio(i),size(j),slide_amount)\n",
    "                boxes.extend()\n",
    "        boxes, labels = aegon_targaryen_non_maximum_supression(boxes,labels,0.3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time daenerys_test(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Showdown\n",
    "After covering all the steps and passing the accuracy value to the talking crystal, they all pass through to the land of the living, with a wounded Jon Snow armed with the Dragon-axe. After a fierce battle, Jon Snow manages to go face to face with the Night king. Surrounded by battling men and falling bodies, they engage in a ferocious battle, a battle of spear and axe. After a raging fight, Jon manages to sink the axe into the Night king's heart, but not before he gets wounded by the spear. As dead men fall to bones, Daenerys and others rush to his aid, but it is too late. Everyone is in tears as they look towards the man of honour, Jon Snow, lying in Daenerys's arms when he says his last words: \"The night has ended. Winter is finally over!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"The night has ended. Winter is finally over!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
