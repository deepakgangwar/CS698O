{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Datasets and Finetuning Pre-trained Networks\n",
    "In this notebook you have to create custom datasets for PyTorch and use this dataset to finetune certain pre-trained neural networks and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "#\n",
    "# Several of the imports you will need have been added but you will need to provide the\n",
    "# rest yourself; you should be able to figure out most of the imports as you go through\n",
    "# the notebook since without proper imports your code will fail to run\n",
    "#\n",
    "# All import statements go in this block\n",
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import torchvision\n",
    "import PIL.Image\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All hyper parameters go in the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Datasets\n",
    "Your first task is to create a pipeline for the custom dataset so that you can load it using a dataloader. Download the dataset provided in the assignment webpage and complete the following block of code so that you can load it as if it was a standard dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CDATA(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # root_dir  - the root directory of the dataset\n",
    "        # train     - a boolean parameter representing whether to return the training set or the test set\n",
    "        # transform - the transforms to be applied on the images before returning them\n",
    "        #\n",
    "        # In this function store the parameters in instance variables and make a mapping\n",
    "        # from images to labels and keep it as an instance variable. Make sure to check which\n",
    "        # dataset is required; train or test; and create the mapping accordingly.\n",
    "        if(train):\n",
    "            dir = root_dir + '/train'\n",
    "        else :\n",
    "            dir = root_dir + '/test'\n",
    "        self.transform = transform\n",
    "        self.img = []\n",
    "        self.label = []\n",
    "        for file_path in glob.glob(dir+'/*/*.png'):\n",
    "            image = PIL.Image.open(file_path)\n",
    "            self.img.append(image.convert('RGB'))\n",
    "            self.label.append(ord(file_path.split('/')[-2]) - ord('A')) #ord makes A,B,C.. to 0,1,2,.. respectively\n",
    "            \n",
    "    def __len__(self):\n",
    "        # return the size of the dataset (total number of images) as an integer\n",
    "        # this should be rather easy if you created a mapping in __init__\n",
    "        return len(self.img)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # idx - the index of the sample requested\n",
    "        #\n",
    "        # Open the image correspoding to idx, apply transforms on it and return a tuple (image, label)\n",
    "        # where label is an integer from 0-9 (since notMNIST has 10 classes)\n",
    "        if self.transform is None:\n",
    "            return (self.img[idx],self.label[idx])\n",
    "        else:\n",
    "            img_transformed = self.transform(self.img[idx])\n",
    "            return (img_transformed,self.label[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now load the dataset. You just need to supply the `root_dir` in the block below and if you implemented the above block correctly, it should work without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = CDATA(root_dir='./notMNIST_small', train=True, transform=None)\n",
    "# img,label = train_dataset.__getitem__(10100)\n",
    "# print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 16854\n",
      "Size of test dataset: 1870\n",
      "Train images\n",
      "Test images\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAB3CAYAAAANSYv6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noECBgscc/z9VEqQLdrYOWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d2ed51250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((224,224)),transforms.ToTensor()])\n",
    "train_dataset = CDATA(root_dir='./notMNIST_small', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = CDATA(root_dir='./notMNIST_small', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "# Let's check the size of the datasets, if implemented correctly they should be 16854 and 1870 respectively\n",
    "print('Size of train dataset: %d' % len(train_dataset))\n",
    "print('Size of test dataset: %d' % len(test_dataset))\n",
    "\n",
    "# Create loaders for the dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Let's look at one batch of train and test images\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "train_dataiter = iter(train_loader)\n",
    "train_images, train_labels = train_dataiter.next()\n",
    "print(\"Train images\")\n",
    "imshow(torchvision.utils.make_grid(train_images))\n",
    "\n",
    "test_dataiter = iter(test_loader)\n",
    "test_images, test_labels = test_dataiter.next()\n",
    "print(\"Test images\")\n",
    "imshow(torchvision.utils.make_grid(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 and Resnet-18\n",
    "Now that you have created the dataset we can use it for training and testing neural networks. VGG-16 and Resnet-18 are both well-known deep-net architectures. VGG-16 is named as such since it has 16 layers in total (13 convolution and 3 fully-connected). Resnet-18 on the other hand is a Resnet architecture that uses skip-connections. PyTorch provides pre-trained models of both these architectures and we shall be using them directly. If you are interested in knowing how they have been defined do take a look at the source, [VGG](https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py), [Resnet](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Code to change the last layers so that they only have 10 classes as output\n",
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(512 * 7 * 7, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 10),\n",
    ")\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 10)\n",
    "\n",
    "# Add code for using CUDA here if it is available\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    vgg16.cuda()\n",
    "    resnet18.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()# Define cross-entropy loss\n",
    "optimizer_vgg16 = torch.optim.Adam(vgg16.parameters(), lr=learning_rate)# Use Adam optimizer, use learning_rate hyper parameter\n",
    "optimizer_resnet18 = torch.optim.Adam(resnet18.parameters(), lr=learning_rate)# Use Adam optimizer, use learning_rate hyper parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning\n",
    "Finetuning is nothing but training models after their weights have been loaded. This allows us to start at a better position than training from scratch. Since the models created already have weights loaded, you simply need to write a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_vgg16():\n",
    "    # Write loops so as to train the model for N epochs, use num_epochs hyper parameter\n",
    "    # Train/finetune the VGG-16 network\n",
    "    # Store the losses for every epoch and generate a graph using matplotlib\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if(use_gpu):\n",
    "                images=images.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer_vgg16.zero_grad()  # zero the gradient buffer\n",
    "            outputs = vgg16(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_vgg16.step()\n",
    "            loss_arr.append(loss.data[0])\n",
    "            if (i+1) % batch_size == 0:\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def train_resnet18():\n",
    "    # Same as above except now using the Resnet-18 network\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if(use_gpu):\n",
    "                images=images.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer_resnet18.zero_grad()  # zero the gradient buffer\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_resnet18.step()\n",
    "            loss_arr.append(loss.data[0])\n",
    "            if (i+1) % batch_size == 0:\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start the training/finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [10/1685], Loss: 1.8709\n",
      "Epoch [1/5], Step [20/1685], Loss: 0.8383\n",
      "Epoch [1/5], Step [30/1685], Loss: 0.8559\n",
      "Epoch [1/5], Step [40/1685], Loss: 0.1617\n",
      "Epoch [1/5], Step [50/1685], Loss: 0.7611\n",
      "Epoch [1/5], Step [60/1685], Loss: 0.3271\n",
      "Epoch [1/5], Step [70/1685], Loss: 1.1896\n",
      "Epoch [1/5], Step [80/1685], Loss: 0.8737\n",
      "Epoch [1/5], Step [90/1685], Loss: 0.2362\n",
      "Epoch [1/5], Step [100/1685], Loss: 0.3017\n",
      "Epoch [1/5], Step [110/1685], Loss: 0.1826\n",
      "Epoch [1/5], Step [120/1685], Loss: 0.1660\n",
      "Epoch [1/5], Step [130/1685], Loss: 0.3534\n",
      "Epoch [1/5], Step [140/1685], Loss: 0.2373\n",
      "Epoch [1/5], Step [150/1685], Loss: 0.0788\n",
      "Epoch [1/5], Step [160/1685], Loss: 0.5949\n",
      "Epoch [1/5], Step [170/1685], Loss: 0.0899\n",
      "Epoch [1/5], Step [180/1685], Loss: 0.5950\n",
      "Epoch [1/5], Step [190/1685], Loss: 0.1118\n",
      "Epoch [1/5], Step [200/1685], Loss: 0.2481\n",
      "Epoch [1/5], Step [210/1685], Loss: 0.2474\n",
      "Epoch [1/5], Step [220/1685], Loss: 0.1224\n",
      "Epoch [1/5], Step [230/1685], Loss: 0.6279\n",
      "Epoch [1/5], Step [240/1685], Loss: 0.3746\n",
      "Epoch [1/5], Step [250/1685], Loss: 0.1069\n",
      "Epoch [1/5], Step [260/1685], Loss: 0.0937\n",
      "Epoch [1/5], Step [270/1685], Loss: 0.1949\n",
      "Epoch [1/5], Step [280/1685], Loss: 0.1622\n",
      "Epoch [1/5], Step [290/1685], Loss: 0.9575\n",
      "Epoch [1/5], Step [300/1685], Loss: 0.5856\n",
      "Epoch [1/5], Step [310/1685], Loss: 1.6929\n",
      "Epoch [1/5], Step [320/1685], Loss: 1.2615\n",
      "Epoch [1/5], Step [330/1685], Loss: 0.3430\n",
      "Epoch [1/5], Step [340/1685], Loss: 0.0260\n",
      "Epoch [1/5], Step [350/1685], Loss: 0.1569\n",
      "Epoch [1/5], Step [360/1685], Loss: 0.0527\n",
      "Epoch [1/5], Step [370/1685], Loss: 0.3920\n",
      "Epoch [1/5], Step [380/1685], Loss: 0.0991\n",
      "Epoch [1/5], Step [390/1685], Loss: 0.0035\n",
      "Epoch [1/5], Step [400/1685], Loss: 0.0592\n",
      "Epoch [1/5], Step [410/1685], Loss: 0.3881\n",
      "Epoch [1/5], Step [420/1685], Loss: 0.2955\n",
      "Epoch [1/5], Step [430/1685], Loss: 0.0925\n",
      "Epoch [1/5], Step [440/1685], Loss: 0.1914\n",
      "Epoch [1/5], Step [450/1685], Loss: 0.5369\n",
      "Epoch [1/5], Step [460/1685], Loss: 0.0013\n",
      "Epoch [1/5], Step [470/1685], Loss: 0.1837\n",
      "Epoch [1/5], Step [480/1685], Loss: 0.0963\n",
      "Epoch [1/5], Step [490/1685], Loss: 0.0008\n",
      "Epoch [1/5], Step [500/1685], Loss: 0.3025\n",
      "Epoch [1/5], Step [510/1685], Loss: 0.2884\n",
      "Epoch [1/5], Step [520/1685], Loss: 1.8211\n",
      "Epoch [1/5], Step [530/1685], Loss: 0.6928\n",
      "Epoch [1/5], Step [540/1685], Loss: 0.0910\n",
      "Epoch [1/5], Step [550/1685], Loss: 0.0103\n",
      "Epoch [1/5], Step [560/1685], Loss: 0.3097\n",
      "Epoch [1/5], Step [570/1685], Loss: 0.0141\n",
      "Epoch [1/5], Step [580/1685], Loss: 0.0291\n",
      "Epoch [1/5], Step [590/1685], Loss: 0.2163\n",
      "Epoch [1/5], Step [600/1685], Loss: 0.9295\n",
      "Epoch [1/5], Step [610/1685], Loss: 0.0603\n",
      "Epoch [1/5], Step [620/1685], Loss: 0.0401\n",
      "Epoch [1/5], Step [630/1685], Loss: 0.1993\n",
      "Epoch [1/5], Step [640/1685], Loss: 0.4439\n",
      "Epoch [1/5], Step [650/1685], Loss: 0.2711\n",
      "Epoch [1/5], Step [660/1685], Loss: 0.0745\n",
      "Epoch [1/5], Step [670/1685], Loss: 0.2549\n",
      "Epoch [1/5], Step [680/1685], Loss: 0.0219\n",
      "Epoch [1/5], Step [690/1685], Loss: 1.0260\n",
      "Epoch [1/5], Step [700/1685], Loss: 0.8424\n",
      "Epoch [1/5], Step [710/1685], Loss: 0.1307\n",
      "Epoch [1/5], Step [720/1685], Loss: 0.0084\n",
      "Epoch [1/5], Step [730/1685], Loss: 0.0023\n",
      "Epoch [1/5], Step [740/1685], Loss: 0.1214\n",
      "Epoch [1/5], Step [750/1685], Loss: 0.7183\n",
      "Epoch [1/5], Step [760/1685], Loss: 0.3424\n",
      "Epoch [1/5], Step [770/1685], Loss: 0.6478\n",
      "Epoch [1/5], Step [780/1685], Loss: 0.4756\n",
      "Epoch [1/5], Step [790/1685], Loss: 0.4118\n",
      "Epoch [1/5], Step [800/1685], Loss: 0.3605\n",
      "Epoch [1/5], Step [810/1685], Loss: 1.1462\n",
      "Epoch [1/5], Step [820/1685], Loss: 0.5160\n",
      "Epoch [1/5], Step [830/1685], Loss: 0.0350\n",
      "Epoch [1/5], Step [840/1685], Loss: 0.5726\n",
      "Epoch [1/5], Step [850/1685], Loss: 0.0535\n",
      "Epoch [1/5], Step [860/1685], Loss: 0.1642\n",
      "Epoch [1/5], Step [870/1685], Loss: 0.0664\n",
      "Epoch [1/5], Step [880/1685], Loss: 0.0965\n",
      "Epoch [1/5], Step [890/1685], Loss: 0.0345\n",
      "Epoch [1/5], Step [900/1685], Loss: 0.2287\n",
      "Epoch [1/5], Step [910/1685], Loss: 0.0096\n",
      "Epoch [1/5], Step [920/1685], Loss: 0.1993\n",
      "Epoch [1/5], Step [930/1685], Loss: 0.2956\n",
      "Epoch [1/5], Step [940/1685], Loss: 0.0108\n",
      "Epoch [1/5], Step [950/1685], Loss: 0.0187\n",
      "Epoch [1/5], Step [960/1685], Loss: 0.0140\n",
      "Epoch [1/5], Step [970/1685], Loss: 0.8495\n",
      "Epoch [1/5], Step [980/1685], Loss: 0.0671\n",
      "Epoch [1/5], Step [990/1685], Loss: 0.2839\n",
      "Epoch [1/5], Step [1000/1685], Loss: 0.0916\n",
      "Epoch [1/5], Step [1010/1685], Loss: 0.1390\n",
      "Epoch [1/5], Step [1020/1685], Loss: 0.1096\n",
      "Epoch [1/5], Step [1030/1685], Loss: 0.1461\n",
      "Epoch [1/5], Step [1040/1685], Loss: 0.0149\n",
      "Epoch [1/5], Step [1050/1685], Loss: 0.1305\n",
      "Epoch [1/5], Step [1060/1685], Loss: 0.1882\n",
      "Epoch [1/5], Step [1070/1685], Loss: 0.2847\n",
      "Epoch [1/5], Step [1080/1685], Loss: 0.0015\n",
      "Epoch [1/5], Step [1090/1685], Loss: 0.0238\n",
      "Epoch [1/5], Step [1100/1685], Loss: 0.1888\n",
      "Epoch [1/5], Step [1110/1685], Loss: 0.0013\n",
      "Epoch [1/5], Step [1120/1685], Loss: 0.0302\n",
      "Epoch [1/5], Step [1130/1685], Loss: 0.1292\n",
      "Epoch [1/5], Step [1140/1685], Loss: 0.2392\n",
      "Epoch [1/5], Step [1150/1685], Loss: 0.0044\n",
      "Epoch [1/5], Step [1160/1685], Loss: 0.0687\n",
      "Epoch [1/5], Step [1170/1685], Loss: 0.6848\n",
      "Epoch [1/5], Step [1180/1685], Loss: 0.5181\n",
      "Epoch [1/5], Step [1190/1685], Loss: 0.1543\n",
      "Epoch [1/5], Step [1200/1685], Loss: 0.2159\n",
      "Epoch [1/5], Step [1210/1685], Loss: 0.1152\n",
      "Epoch [1/5], Step [1220/1685], Loss: 0.4684\n",
      "Epoch [1/5], Step [1230/1685], Loss: 0.3777\n",
      "Epoch [1/5], Step [1240/1685], Loss: 0.3487\n",
      "Epoch [1/5], Step [1250/1685], Loss: 0.0859\n",
      "Epoch [1/5], Step [1260/1685], Loss: 0.2034\n",
      "Epoch [1/5], Step [1270/1685], Loss: 0.0209\n",
      "Epoch [1/5], Step [1280/1685], Loss: 0.4134\n",
      "Epoch [1/5], Step [1290/1685], Loss: 0.0169\n",
      "Epoch [1/5], Step [1300/1685], Loss: 0.0582\n",
      "Epoch [1/5], Step [1310/1685], Loss: 0.0214\n",
      "Epoch [1/5], Step [1320/1685], Loss: 0.1072\n",
      "Epoch [1/5], Step [1330/1685], Loss: 0.0083\n",
      "Epoch [1/5], Step [1340/1685], Loss: 0.0019\n",
      "Epoch [1/5], Step [1350/1685], Loss: 0.0036\n",
      "Epoch [1/5], Step [1360/1685], Loss: 0.1141\n",
      "Epoch [1/5], Step [1370/1685], Loss: 0.1856\n",
      "Epoch [1/5], Step [1380/1685], Loss: 0.0010\n",
      "Epoch [1/5], Step [1390/1685], Loss: 0.0231\n",
      "Epoch [1/5], Step [1400/1685], Loss: 0.0086\n",
      "Epoch [1/5], Step [1410/1685], Loss: 0.0098\n",
      "Epoch [1/5], Step [1420/1685], Loss: 0.0467\n",
      "Epoch [1/5], Step [1430/1685], Loss: 0.0427\n",
      "Epoch [1/5], Step [1440/1685], Loss: 0.2720\n",
      "Epoch [1/5], Step [1450/1685], Loss: 0.0275\n",
      "Epoch [1/5], Step [1460/1685], Loss: 0.0241\n",
      "Epoch [1/5], Step [1470/1685], Loss: 0.2377\n",
      "Epoch [1/5], Step [1480/1685], Loss: 0.0404\n",
      "Epoch [1/5], Step [1490/1685], Loss: 0.3709\n",
      "Epoch [1/5], Step [1500/1685], Loss: 0.0053\n",
      "Epoch [1/5], Step [1510/1685], Loss: 0.8973\n",
      "Epoch [1/5], Step [1520/1685], Loss: 0.0356\n",
      "Epoch [1/5], Step [1530/1685], Loss: 0.0911\n",
      "Epoch [1/5], Step [1540/1685], Loss: 0.1722\n",
      "Epoch [1/5], Step [1550/1685], Loss: 0.0013\n",
      "Epoch [1/5], Step [1560/1685], Loss: 0.2609\n",
      "Epoch [1/5], Step [1570/1685], Loss: 0.1172\n",
      "Epoch [1/5], Step [1580/1685], Loss: 1.5622\n",
      "Epoch [1/5], Step [1590/1685], Loss: 0.0090\n",
      "Epoch [1/5], Step [1600/1685], Loss: 0.4832\n",
      "Epoch [1/5], Step [1610/1685], Loss: 0.0222\n",
      "Epoch [1/5], Step [1620/1685], Loss: 0.1220\n",
      "Epoch [1/5], Step [1630/1685], Loss: 0.2301\n",
      "Epoch [1/5], Step [1640/1685], Loss: 0.0569\n",
      "Epoch [1/5], Step [1650/1685], Loss: 0.0067\n",
      "Epoch [1/5], Step [1660/1685], Loss: 0.4374\n",
      "Epoch [1/5], Step [1670/1685], Loss: 0.0007\n",
      "Epoch [1/5], Step [1680/1685], Loss: 0.0087\n",
      "Epoch [2/5], Step [10/1685], Loss: 0.0413\n",
      "Epoch [2/5], Step [20/1685], Loss: 0.2501\n",
      "Epoch [2/5], Step [30/1685], Loss: 0.6262\n",
      "Epoch [2/5], Step [40/1685], Loss: 0.2741\n",
      "Epoch [2/5], Step [50/1685], Loss: 0.4857\n",
      "Epoch [2/5], Step [60/1685], Loss: 0.1006\n",
      "Epoch [2/5], Step [70/1685], Loss: 0.0219\n",
      "Epoch [2/5], Step [80/1685], Loss: 0.3350\n",
      "Epoch [2/5], Step [90/1685], Loss: 0.1363\n",
      "Epoch [2/5], Step [100/1685], Loss: 0.0135\n",
      "Epoch [2/5], Step [110/1685], Loss: 0.0003\n",
      "Epoch [2/5], Step [120/1685], Loss: 0.0725\n",
      "Epoch [2/5], Step [130/1685], Loss: 0.0027\n",
      "Epoch [2/5], Step [140/1685], Loss: 0.0003\n",
      "Epoch [2/5], Step [150/1685], Loss: 0.3140\n",
      "Epoch [2/5], Step [160/1685], Loss: 0.0844\n",
      "Epoch [2/5], Step [170/1685], Loss: 0.0007\n",
      "Epoch [2/5], Step [180/1685], Loss: 1.1614\n",
      "Epoch [2/5], Step [190/1685], Loss: 0.2532\n",
      "Epoch [2/5], Step [200/1685], Loss: 0.0130\n",
      "Epoch [2/5], Step [210/1685], Loss: 0.0007\n",
      "Epoch [2/5], Step [220/1685], Loss: 0.0288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [230/1685], Loss: 0.2328\n",
      "Epoch [2/5], Step [240/1685], Loss: 0.0733\n",
      "Epoch [2/5], Step [250/1685], Loss: 0.0017\n",
      "Epoch [2/5], Step [260/1685], Loss: 0.0363\n",
      "Epoch [2/5], Step [270/1685], Loss: 0.1969\n",
      "Epoch [2/5], Step [280/1685], Loss: 0.0243\n",
      "Epoch [2/5], Step [290/1685], Loss: 0.0954\n",
      "Epoch [2/5], Step [300/1685], Loss: 0.4884\n",
      "Epoch [2/5], Step [310/1685], Loss: 0.0828\n",
      "Epoch [2/5], Step [320/1685], Loss: 0.0059\n",
      "Epoch [2/5], Step [330/1685], Loss: 0.0003\n",
      "Epoch [2/5], Step [340/1685], Loss: 0.0035\n",
      "Epoch [2/5], Step [350/1685], Loss: 0.0152\n",
      "Epoch [2/5], Step [360/1685], Loss: 0.0321\n",
      "Epoch [2/5], Step [370/1685], Loss: 0.4560\n",
      "Epoch [2/5], Step [380/1685], Loss: 0.0039\n",
      "Epoch [2/5], Step [390/1685], Loss: 0.0152\n",
      "Epoch [2/5], Step [400/1685], Loss: 0.1276\n",
      "Epoch [2/5], Step [410/1685], Loss: 0.0475\n",
      "Epoch [2/5], Step [420/1685], Loss: 0.1177\n",
      "Epoch [2/5], Step [430/1685], Loss: 0.0153\n",
      "Epoch [2/5], Step [440/1685], Loss: 0.0001\n",
      "Epoch [2/5], Step [450/1685], Loss: 1.2015\n",
      "Epoch [2/5], Step [460/1685], Loss: 0.0427\n",
      "Epoch [2/5], Step [470/1685], Loss: 0.2561\n",
      "Epoch [2/5], Step [480/1685], Loss: 0.1721\n",
      "Epoch [2/5], Step [490/1685], Loss: 0.0051\n",
      "Epoch [2/5], Step [500/1685], Loss: 0.3252\n",
      "Epoch [2/5], Step [510/1685], Loss: 0.2110\n",
      "Epoch [2/5], Step [520/1685], Loss: 0.0047\n",
      "Epoch [2/5], Step [530/1685], Loss: 0.1096\n",
      "Epoch [2/5], Step [540/1685], Loss: 0.1115\n",
      "Epoch [2/5], Step [550/1685], Loss: 0.0030\n",
      "Epoch [2/5], Step [560/1685], Loss: 0.4007\n",
      "Epoch [2/5], Step [570/1685], Loss: 0.0064\n",
      "Epoch [2/5], Step [580/1685], Loss: 0.0018\n",
      "Epoch [2/5], Step [590/1685], Loss: 0.0692\n",
      "Epoch [2/5], Step [600/1685], Loss: 0.5605\n",
      "Epoch [2/5], Step [610/1685], Loss: 0.4698\n",
      "Epoch [2/5], Step [620/1685], Loss: 0.1288\n",
      "Epoch [2/5], Step [630/1685], Loss: 0.0013\n",
      "Epoch [2/5], Step [640/1685], Loss: 0.0343\n",
      "Epoch [2/5], Step [650/1685], Loss: 0.0014\n",
      "Epoch [2/5], Step [660/1685], Loss: 0.0626\n",
      "Epoch [2/5], Step [670/1685], Loss: 0.2207\n",
      "Epoch [2/5], Step [680/1685], Loss: 0.1038\n",
      "Epoch [2/5], Step [690/1685], Loss: 0.0587\n",
      "Epoch [2/5], Step [700/1685], Loss: 0.0185\n",
      "Epoch [2/5], Step [710/1685], Loss: 0.2831\n",
      "Epoch [2/5], Step [720/1685], Loss: 0.2591\n",
      "Epoch [2/5], Step [730/1685], Loss: 0.0025\n",
      "Epoch [2/5], Step [740/1685], Loss: 0.0103\n",
      "Epoch [2/5], Step [750/1685], Loss: 0.0250\n",
      "Epoch [2/5], Step [760/1685], Loss: 0.4641\n",
      "Epoch [2/5], Step [770/1685], Loss: 0.3403\n",
      "Epoch [2/5], Step [780/1685], Loss: 0.0046\n",
      "Epoch [2/5], Step [790/1685], Loss: 0.3871\n",
      "Epoch [2/5], Step [800/1685], Loss: 0.4480\n",
      "Epoch [2/5], Step [810/1685], Loss: 0.0058\n",
      "Epoch [2/5], Step [820/1685], Loss: 0.3343\n",
      "Epoch [2/5], Step [830/1685], Loss: 0.0048\n",
      "Epoch [2/5], Step [840/1685], Loss: 0.0689\n",
      "Epoch [2/5], Step [850/1685], Loss: 0.0290\n",
      "Epoch [2/5], Step [860/1685], Loss: 0.2599\n",
      "Epoch [2/5], Step [870/1685], Loss: 1.0503\n",
      "Epoch [2/5], Step [880/1685], Loss: 0.0054\n",
      "Epoch [2/5], Step [890/1685], Loss: 0.0010\n",
      "Epoch [2/5], Step [900/1685], Loss: 0.2753\n",
      "Epoch [2/5], Step [910/1685], Loss: 0.3566\n",
      "Epoch [2/5], Step [920/1685], Loss: 0.4494\n",
      "Epoch [2/5], Step [930/1685], Loss: 0.0786\n",
      "Epoch [2/5], Step [940/1685], Loss: 0.0309\n",
      "Epoch [2/5], Step [950/1685], Loss: 0.0161\n",
      "Epoch [2/5], Step [960/1685], Loss: 0.0949\n",
      "Epoch [2/5], Step [970/1685], Loss: 0.1010\n",
      "Epoch [2/5], Step [980/1685], Loss: 0.2904\n",
      "Epoch [2/5], Step [990/1685], Loss: 0.1570\n",
      "Epoch [2/5], Step [1000/1685], Loss: 0.0025\n",
      "Epoch [2/5], Step [1010/1685], Loss: 0.6876\n",
      "Epoch [2/5], Step [1020/1685], Loss: 0.5094\n",
      "Epoch [2/5], Step [1030/1685], Loss: 0.0033\n",
      "Epoch [2/5], Step [1040/1685], Loss: 0.3659\n",
      "Epoch [2/5], Step [1050/1685], Loss: 0.0160\n",
      "Epoch [2/5], Step [1060/1685], Loss: 0.0010\n",
      "Epoch [2/5], Step [1070/1685], Loss: 0.2458\n",
      "Epoch [2/5], Step [1080/1685], Loss: 0.0154\n",
      "Epoch [2/5], Step [1090/1685], Loss: 0.7269\n",
      "Epoch [2/5], Step [1100/1685], Loss: 0.0002\n",
      "Epoch [2/5], Step [1110/1685], Loss: 0.5502\n",
      "Epoch [2/5], Step [1120/1685], Loss: 0.2735\n",
      "Epoch [2/5], Step [1130/1685], Loss: 0.4998\n",
      "Epoch [2/5], Step [1140/1685], Loss: 0.2072\n",
      "Epoch [2/5], Step [1150/1685], Loss: 0.1669\n",
      "Epoch [2/5], Step [1160/1685], Loss: 0.4398\n",
      "Epoch [2/5], Step [1170/1685], Loss: 0.0839\n",
      "Epoch [2/5], Step [1180/1685], Loss: 0.4323\n",
      "Epoch [2/5], Step [1190/1685], Loss: 0.6572\n",
      "Epoch [2/5], Step [1200/1685], Loss: 0.0086\n",
      "Epoch [2/5], Step [1210/1685], Loss: 0.2399\n",
      "Epoch [2/5], Step [1220/1685], Loss: 0.1583\n",
      "Epoch [2/5], Step [1230/1685], Loss: 0.0261\n",
      "Epoch [2/5], Step [1240/1685], Loss: 0.0923\n",
      "Epoch [2/5], Step [1250/1685], Loss: 0.5125\n",
      "Epoch [2/5], Step [1260/1685], Loss: 0.0010\n",
      "Epoch [2/5], Step [1270/1685], Loss: 0.0979\n",
      "Epoch [2/5], Step [1280/1685], Loss: 0.4043\n",
      "Epoch [2/5], Step [1290/1685], Loss: 0.0001\n",
      "Epoch [2/5], Step [1300/1685], Loss: 0.0007\n",
      "Epoch [2/5], Step [1310/1685], Loss: 0.0043\n",
      "Epoch [2/5], Step [1320/1685], Loss: 0.1437\n",
      "Epoch [2/5], Step [1330/1685], Loss: 0.2391\n",
      "Epoch [2/5], Step [1340/1685], Loss: 0.2733\n",
      "Epoch [2/5], Step [1350/1685], Loss: 0.0303\n",
      "Epoch [2/5], Step [1360/1685], Loss: 0.0213\n",
      "Epoch [2/5], Step [1370/1685], Loss: 0.3928\n",
      "Epoch [2/5], Step [1380/1685], Loss: 0.0392\n",
      "Epoch [2/5], Step [1390/1685], Loss: 0.1319\n",
      "Epoch [2/5], Step [1400/1685], Loss: 0.0012\n",
      "Epoch [2/5], Step [1410/1685], Loss: 0.0042\n",
      "Epoch [2/5], Step [1420/1685], Loss: 0.4494\n",
      "Epoch [2/5], Step [1430/1685], Loss: 0.0039\n",
      "Epoch [2/5], Step [1440/1685], Loss: 0.0075\n",
      "Epoch [2/5], Step [1450/1685], Loss: 0.0000\n",
      "Epoch [2/5], Step [1460/1685], Loss: 0.2238\n",
      "Epoch [2/5], Step [1470/1685], Loss: 0.0435\n",
      "Epoch [2/5], Step [1480/1685], Loss: 0.1809\n",
      "Epoch [2/5], Step [1490/1685], Loss: 0.0122\n",
      "Epoch [2/5], Step [1500/1685], Loss: 0.1096\n",
      "Epoch [2/5], Step [1510/1685], Loss: 0.0282\n",
      "Epoch [2/5], Step [1520/1685], Loss: 0.0065\n",
      "Epoch [2/5], Step [1530/1685], Loss: 0.0110\n",
      "Epoch [2/5], Step [1540/1685], Loss: 0.0965\n",
      "Epoch [2/5], Step [1550/1685], Loss: 0.3147\n",
      "Epoch [2/5], Step [1560/1685], Loss: 0.0173\n",
      "Epoch [2/5], Step [1570/1685], Loss: 0.0154\n",
      "Epoch [2/5], Step [1580/1685], Loss: 0.2201\n",
      "Epoch [2/5], Step [1590/1685], Loss: 0.0018\n",
      "Epoch [2/5], Step [1600/1685], Loss: 0.1173\n",
      "Epoch [2/5], Step [1610/1685], Loss: 0.0197\n",
      "Epoch [2/5], Step [1620/1685], Loss: 0.0779\n",
      "Epoch [2/5], Step [1630/1685], Loss: 0.0179\n",
      "Epoch [2/5], Step [1640/1685], Loss: 0.0195\n",
      "Epoch [2/5], Step [1650/1685], Loss: 0.0001\n",
      "Epoch [2/5], Step [1660/1685], Loss: 0.3932\n",
      "Epoch [2/5], Step [1670/1685], Loss: 0.0429\n",
      "Epoch [2/5], Step [1680/1685], Loss: 0.1750\n",
      "Epoch [3/5], Step [10/1685], Loss: 0.3396\n",
      "Epoch [3/5], Step [20/1685], Loss: 0.2998\n",
      "Epoch [3/5], Step [30/1685], Loss: 0.0004\n",
      "Epoch [3/5], Step [40/1685], Loss: 0.0001\n",
      "Epoch [3/5], Step [50/1685], Loss: 0.0002\n",
      "Epoch [3/5], Step [60/1685], Loss: 0.0057\n",
      "Epoch [3/5], Step [70/1685], Loss: 0.0063\n",
      "Epoch [3/5], Step [80/1685], Loss: 0.0010\n",
      "Epoch [3/5], Step [90/1685], Loss: 0.2104\n",
      "Epoch [3/5], Step [100/1685], Loss: 0.0300\n",
      "Epoch [3/5], Step [110/1685], Loss: 0.0425\n",
      "Epoch [3/5], Step [120/1685], Loss: 0.0089\n",
      "Epoch [3/5], Step [130/1685], Loss: 0.1211\n",
      "Epoch [3/5], Step [140/1685], Loss: 0.3128\n",
      "Epoch [3/5], Step [150/1685], Loss: 0.0052\n",
      "Epoch [3/5], Step [160/1685], Loss: 0.0538\n",
      "Epoch [3/5], Step [170/1685], Loss: 0.3873\n",
      "Epoch [3/5], Step [180/1685], Loss: 0.0010\n",
      "Epoch [3/5], Step [190/1685], Loss: 0.7088\n",
      "Epoch [3/5], Step [200/1685], Loss: 0.1375\n",
      "Epoch [3/5], Step [210/1685], Loss: 0.0003\n",
      "Epoch [3/5], Step [220/1685], Loss: 0.0004\n",
      "Epoch [3/5], Step [230/1685], Loss: 0.0975\n",
      "Epoch [3/5], Step [240/1685], Loss: 0.0783\n",
      "Epoch [3/5], Step [250/1685], Loss: 0.0001\n",
      "Epoch [3/5], Step [260/1685], Loss: 0.0842\n",
      "Epoch [3/5], Step [270/1685], Loss: 0.0000\n",
      "Epoch [3/5], Step [280/1685], Loss: 0.4079\n",
      "Epoch [3/5], Step [290/1685], Loss: 0.0077\n",
      "Epoch [3/5], Step [300/1685], Loss: 0.1332\n",
      "Epoch [3/5], Step [310/1685], Loss: 0.2408\n",
      "Epoch [3/5], Step [320/1685], Loss: 0.1272\n",
      "Epoch [3/5], Step [330/1685], Loss: 0.1287\n",
      "Epoch [3/5], Step [340/1685], Loss: 0.0163\n",
      "Epoch [3/5], Step [350/1685], Loss: 0.1038\n",
      "Epoch [3/5], Step [360/1685], Loss: 0.0164\n",
      "Epoch [3/5], Step [370/1685], Loss: 0.5331\n",
      "Epoch [3/5], Step [380/1685], Loss: 0.0022\n",
      "Epoch [3/5], Step [390/1685], Loss: 0.0038\n",
      "Epoch [3/5], Step [400/1685], Loss: 0.1151\n",
      "Epoch [3/5], Step [410/1685], Loss: 0.1590\n",
      "Epoch [3/5], Step [420/1685], Loss: 0.0473\n",
      "Epoch [3/5], Step [430/1685], Loss: 0.0060\n",
      "Epoch [3/5], Step [440/1685], Loss: 0.1322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [450/1685], Loss: 0.0001\n",
      "Epoch [3/5], Step [460/1685], Loss: 0.0380\n",
      "Epoch [3/5], Step [470/1685], Loss: 0.0009\n",
      "Epoch [3/5], Step [480/1685], Loss: 0.0020\n",
      "Epoch [3/5], Step [490/1685], Loss: 0.1929\n",
      "Epoch [3/5], Step [500/1685], Loss: 0.0002\n",
      "Epoch [3/5], Step [510/1685], Loss: 0.1869\n",
      "Epoch [3/5], Step [520/1685], Loss: 0.2659\n",
      "Epoch [3/5], Step [530/1685], Loss: 0.4002\n",
      "Epoch [3/5], Step [540/1685], Loss: 0.2875\n",
      "Epoch [3/5], Step [550/1685], Loss: 0.0118\n",
      "Epoch [3/5], Step [560/1685], Loss: 0.0270\n",
      "Epoch [3/5], Step [570/1685], Loss: 0.1193\n",
      "Epoch [3/5], Step [580/1685], Loss: 0.0026\n",
      "Epoch [3/5], Step [590/1685], Loss: 0.4208\n",
      "Epoch [3/5], Step [600/1685], Loss: 0.0060\n",
      "Epoch [3/5], Step [610/1685], Loss: 0.0887\n",
      "Epoch [3/5], Step [620/1685], Loss: 0.1700\n",
      "Epoch [3/5], Step [630/1685], Loss: 0.0399\n",
      "Epoch [3/5], Step [640/1685], Loss: 0.4844\n",
      "Epoch [3/5], Step [650/1685], Loss: 0.0002\n",
      "Epoch [3/5], Step [660/1685], Loss: 0.0053\n",
      "Epoch [3/5], Step [670/1685], Loss: 0.0007\n",
      "Epoch [3/5], Step [680/1685], Loss: 0.0004\n",
      "Epoch [3/5], Step [690/1685], Loss: 0.0114\n",
      "Epoch [3/5], Step [700/1685], Loss: 0.2401\n",
      "Epoch [3/5], Step [710/1685], Loss: 0.1370\n",
      "Epoch [3/5], Step [720/1685], Loss: 0.3316\n",
      "Epoch [3/5], Step [730/1685], Loss: 0.0978\n",
      "Epoch [3/5], Step [740/1685], Loss: 0.3019\n",
      "Epoch [3/5], Step [750/1685], Loss: 0.0005\n",
      "Epoch [3/5], Step [760/1685], Loss: 0.0435\n",
      "Epoch [3/5], Step [770/1685], Loss: 0.0008\n",
      "Epoch [3/5], Step [780/1685], Loss: 0.0084\n",
      "Epoch [3/5], Step [790/1685], Loss: 0.3516\n",
      "Epoch [3/5], Step [800/1685], Loss: 0.0200\n",
      "Epoch [3/5], Step [810/1685], Loss: 0.2603\n",
      "Epoch [3/5], Step [820/1685], Loss: 0.0163\n",
      "Epoch [3/5], Step [830/1685], Loss: 0.0368\n",
      "Epoch [3/5], Step [840/1685], Loss: 0.1637\n",
      "Epoch [3/5], Step [850/1685], Loss: 0.0003\n",
      "Epoch [3/5], Step [860/1685], Loss: 0.0818\n",
      "Epoch [3/5], Step [870/1685], Loss: 0.0213\n",
      "Epoch [3/5], Step [880/1685], Loss: 0.0034\n",
      "Epoch [3/5], Step [890/1685], Loss: 0.2741\n",
      "Epoch [3/5], Step [900/1685], Loss: 0.0813\n",
      "Epoch [3/5], Step [910/1685], Loss: 0.0947\n",
      "Epoch [3/5], Step [920/1685], Loss: 0.2173\n",
      "Epoch [3/5], Step [930/1685], Loss: 0.0000\n",
      "Epoch [3/5], Step [940/1685], Loss: 0.0000\n",
      "Epoch [3/5], Step [950/1685], Loss: 0.0820\n",
      "Epoch [3/5], Step [960/1685], Loss: 0.0027\n",
      "Epoch [3/5], Step [970/1685], Loss: 0.0747\n",
      "Epoch [3/5], Step [980/1685], Loss: 0.0410\n",
      "Epoch [3/5], Step [990/1685], Loss: 0.2255\n",
      "Epoch [3/5], Step [1000/1685], Loss: 0.0234\n",
      "Epoch [3/5], Step [1010/1685], Loss: 0.0001\n",
      "Epoch [3/5], Step [1020/1685], Loss: 0.0467\n",
      "Epoch [3/5], Step [1030/1685], Loss: 0.3370\n",
      "Epoch [3/5], Step [1040/1685], Loss: 0.4488\n",
      "Epoch [3/5], Step [1050/1685], Loss: 0.0005\n",
      "Epoch [3/5], Step [1060/1685], Loss: 0.0097\n",
      "Epoch [3/5], Step [1070/1685], Loss: 0.0037\n",
      "Epoch [3/5], Step [1080/1685], Loss: 0.1823\n",
      "Epoch [3/5], Step [1090/1685], Loss: 0.0004\n",
      "Epoch [3/5], Step [1100/1685], Loss: 0.0245\n",
      "Epoch [3/5], Step [1110/1685], Loss: 0.0361\n",
      "Epoch [3/5], Step [1120/1685], Loss: 0.1026\n",
      "Epoch [3/5], Step [1130/1685], Loss: 0.1350\n",
      "Epoch [3/5], Step [1140/1685], Loss: 0.0001\n",
      "Epoch [3/5], Step [1150/1685], Loss: 0.0000\n",
      "Epoch [3/5], Step [1160/1685], Loss: 0.1098\n",
      "Epoch [3/5], Step [1170/1685], Loss: 0.0095\n",
      "Epoch [3/5], Step [1180/1685], Loss: 0.4298\n",
      "Epoch [3/5], Step [1190/1685], Loss: 0.0001\n",
      "Epoch [3/5], Step [1200/1685], Loss: 0.0967\n",
      "Epoch [3/5], Step [1210/1685], Loss: 0.5298\n",
      "Epoch [3/5], Step [1220/1685], Loss: 0.1380\n",
      "Epoch [3/5], Step [1230/1685], Loss: 0.0915\n",
      "Epoch [3/5], Step [1240/1685], Loss: 0.0621\n",
      "Epoch [3/5], Step [1250/1685], Loss: 0.1542\n",
      "Epoch [3/5], Step [1260/1685], Loss: 0.0223\n",
      "Epoch [3/5], Step [1270/1685], Loss: 0.1124\n",
      "Epoch [3/5], Step [1280/1685], Loss: 0.0105\n",
      "Epoch [3/5], Step [1290/1685], Loss: 0.2952\n",
      "Epoch [3/5], Step [1300/1685], Loss: 0.0256\n",
      "Epoch [3/5], Step [1310/1685], Loss: 0.0064\n",
      "Epoch [3/5], Step [1320/1685], Loss: 0.0551\n",
      "Epoch [3/5], Step [1330/1685], Loss: 0.0060\n",
      "Epoch [3/5], Step [1340/1685], Loss: 0.0057\n",
      "Epoch [3/5], Step [1350/1685], Loss: 0.0067\n",
      "Epoch [3/5], Step [1360/1685], Loss: 0.0403\n",
      "Epoch [3/5], Step [1370/1685], Loss: 0.3003\n",
      "Epoch [3/5], Step [1380/1685], Loss: 0.0003\n",
      "Epoch [3/5], Step [1390/1685], Loss: 0.0023\n",
      "Epoch [3/5], Step [1400/1685], Loss: 0.0002\n",
      "Epoch [3/5], Step [1410/1685], Loss: 0.0003\n",
      "Epoch [3/5], Step [1420/1685], Loss: 0.4680\n",
      "Epoch [3/5], Step [1430/1685], Loss: 0.0852\n",
      "Epoch [3/5], Step [1440/1685], Loss: 0.0158\n",
      "Epoch [3/5], Step [1450/1685], Loss: 0.3005\n",
      "Epoch [3/5], Step [1460/1685], Loss: 0.0007\n",
      "Epoch [3/5], Step [1470/1685], Loss: 0.0667\n",
      "Epoch [3/5], Step [1480/1685], Loss: 0.0251\n",
      "Epoch [3/5], Step [1490/1685], Loss: 0.2168\n",
      "Epoch [3/5], Step [1500/1685], Loss: 0.0029\n",
      "Epoch [3/5], Step [1510/1685], Loss: 0.0328\n",
      "Epoch [3/5], Step [1520/1685], Loss: 0.0107\n",
      "Epoch [3/5], Step [1530/1685], Loss: 0.0069\n",
      "Epoch [3/5], Step [1540/1685], Loss: 0.8792\n",
      "Epoch [3/5], Step [1550/1685], Loss: 0.0012\n",
      "Epoch [3/5], Step [1560/1685], Loss: 0.0032\n",
      "Epoch [3/5], Step [1570/1685], Loss: 0.1716\n",
      "Epoch [3/5], Step [1580/1685], Loss: 0.0012\n",
      "Epoch [3/5], Step [1590/1685], Loss: 0.0645\n",
      "Epoch [3/5], Step [1600/1685], Loss: 0.0001\n",
      "Epoch [3/5], Step [1610/1685], Loss: 0.3043\n",
      "Epoch [3/5], Step [1620/1685], Loss: 0.1530\n",
      "Epoch [3/5], Step [1630/1685], Loss: 0.0385\n",
      "Epoch [3/5], Step [1640/1685], Loss: 0.1978\n",
      "Epoch [3/5], Step [1650/1685], Loss: 0.0004\n",
      "Epoch [3/5], Step [1660/1685], Loss: 0.6083\n",
      "Epoch [3/5], Step [1670/1685], Loss: 0.1181\n",
      "Epoch [3/5], Step [1680/1685], Loss: 0.0006\n",
      "Epoch [4/5], Step [10/1685], Loss: 0.0500\n",
      "Epoch [4/5], Step [20/1685], Loss: 0.0264\n",
      "Epoch [4/5], Step [30/1685], Loss: 0.0001\n",
      "Epoch [4/5], Step [40/1685], Loss: 0.0438\n",
      "Epoch [4/5], Step [50/1685], Loss: 0.0491\n",
      "Epoch [4/5], Step [60/1685], Loss: 0.0202\n",
      "Epoch [4/5], Step [70/1685], Loss: 0.0143\n",
      "Epoch [4/5], Step [80/1685], Loss: 0.0708\n",
      "Epoch [4/5], Step [90/1685], Loss: 0.0002\n",
      "Epoch [4/5], Step [100/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [110/1685], Loss: 0.0006\n",
      "Epoch [4/5], Step [120/1685], Loss: 0.0200\n",
      "Epoch [4/5], Step [130/1685], Loss: 0.0463\n",
      "Epoch [4/5], Step [140/1685], Loss: 0.0010\n",
      "Epoch [4/5], Step [150/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [160/1685], Loss: 0.0052\n",
      "Epoch [4/5], Step [170/1685], Loss: 0.1712\n",
      "Epoch [4/5], Step [180/1685], Loss: 0.0048\n",
      "Epoch [4/5], Step [190/1685], Loss: 0.0002\n",
      "Epoch [4/5], Step [200/1685], Loss: 0.0033\n",
      "Epoch [4/5], Step [210/1685], Loss: 0.0583\n",
      "Epoch [4/5], Step [220/1685], Loss: 0.0660\n",
      "Epoch [4/5], Step [230/1685], Loss: 0.0008\n",
      "Epoch [4/5], Step [240/1685], Loss: 0.3154\n",
      "Epoch [4/5], Step [250/1685], Loss: 0.0496\n",
      "Epoch [4/5], Step [260/1685], Loss: 0.2070\n",
      "Epoch [4/5], Step [270/1685], Loss: 0.1160\n",
      "Epoch [4/5], Step [280/1685], Loss: 0.0007\n",
      "Epoch [4/5], Step [290/1685], Loss: 0.1854\n",
      "Epoch [4/5], Step [300/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [310/1685], Loss: 0.0005\n",
      "Epoch [4/5], Step [320/1685], Loss: 0.0841\n",
      "Epoch [4/5], Step [330/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [340/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [350/1685], Loss: 0.0008\n",
      "Epoch [4/5], Step [360/1685], Loss: 0.0012\n",
      "Epoch [4/5], Step [370/1685], Loss: 0.0023\n",
      "Epoch [4/5], Step [380/1685], Loss: 0.0489\n",
      "Epoch [4/5], Step [390/1685], Loss: 0.0001\n",
      "Epoch [4/5], Step [400/1685], Loss: 0.0026\n",
      "Epoch [4/5], Step [410/1685], Loss: 0.1103\n",
      "Epoch [4/5], Step [420/1685], Loss: 0.0641\n",
      "Epoch [4/5], Step [430/1685], Loss: 0.0023\n",
      "Epoch [4/5], Step [440/1685], Loss: 0.1685\n",
      "Epoch [4/5], Step [450/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [460/1685], Loss: 0.1122\n",
      "Epoch [4/5], Step [470/1685], Loss: 0.0136\n",
      "Epoch [4/5], Step [480/1685], Loss: 0.8037\n",
      "Epoch [4/5], Step [490/1685], Loss: 0.2911\n",
      "Epoch [4/5], Step [500/1685], Loss: 0.0015\n",
      "Epoch [4/5], Step [510/1685], Loss: 0.0785\n",
      "Epoch [4/5], Step [520/1685], Loss: 0.0145\n",
      "Epoch [4/5], Step [530/1685], Loss: 0.0021\n",
      "Epoch [4/5], Step [540/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [550/1685], Loss: 0.0015\n",
      "Epoch [4/5], Step [560/1685], Loss: 0.0005\n",
      "Epoch [4/5], Step [570/1685], Loss: 0.0280\n",
      "Epoch [4/5], Step [580/1685], Loss: 0.0060\n",
      "Epoch [4/5], Step [590/1685], Loss: 0.0054\n",
      "Epoch [4/5], Step [600/1685], Loss: 0.0045\n",
      "Epoch [4/5], Step [610/1685], Loss: 0.1108\n",
      "Epoch [4/5], Step [620/1685], Loss: 0.2644\n",
      "Epoch [4/5], Step [630/1685], Loss: 0.0022\n",
      "Epoch [4/5], Step [640/1685], Loss: 0.0133\n",
      "Epoch [4/5], Step [650/1685], Loss: 0.0123\n",
      "Epoch [4/5], Step [660/1685], Loss: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [670/1685], Loss: 0.0206\n",
      "Epoch [4/5], Step [680/1685], Loss: 0.0009\n",
      "Epoch [4/5], Step [690/1685], Loss: 0.0006\n",
      "Epoch [4/5], Step [700/1685], Loss: 0.0170\n",
      "Epoch [4/5], Step [710/1685], Loss: 0.0117\n",
      "Epoch [4/5], Step [720/1685], Loss: 0.0001\n",
      "Epoch [4/5], Step [730/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [740/1685], Loss: 0.2918\n",
      "Epoch [4/5], Step [750/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [760/1685], Loss: 0.0021\n",
      "Epoch [4/5], Step [770/1685], Loss: 0.3118\n",
      "Epoch [4/5], Step [780/1685], Loss: 0.0439\n",
      "Epoch [4/5], Step [790/1685], Loss: 0.0749\n",
      "Epoch [4/5], Step [800/1685], Loss: 0.0773\n",
      "Epoch [4/5], Step [810/1685], Loss: 0.0002\n",
      "Epoch [4/5], Step [820/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [830/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [840/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [850/1685], Loss: 0.0037\n",
      "Epoch [4/5], Step [860/1685], Loss: 0.8434\n",
      "Epoch [4/5], Step [870/1685], Loss: 0.0002\n",
      "Epoch [4/5], Step [880/1685], Loss: 0.0003\n",
      "Epoch [4/5], Step [890/1685], Loss: 0.0591\n",
      "Epoch [4/5], Step [900/1685], Loss: 0.0013\n",
      "Epoch [4/5], Step [910/1685], Loss: 0.2068\n",
      "Epoch [4/5], Step [920/1685], Loss: 0.0684\n",
      "Epoch [4/5], Step [930/1685], Loss: 0.0064\n",
      "Epoch [4/5], Step [940/1685], Loss: 0.0005\n",
      "Epoch [4/5], Step [950/1685], Loss: 0.0014\n",
      "Epoch [4/5], Step [960/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [970/1685], Loss: 0.0001\n",
      "Epoch [4/5], Step [980/1685], Loss: 0.0002\n",
      "Epoch [4/5], Step [990/1685], Loss: 0.0017\n",
      "Epoch [4/5], Step [1000/1685], Loss: 0.1632\n",
      "Epoch [4/5], Step [1010/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [1020/1685], Loss: 0.0574\n",
      "Epoch [4/5], Step [1030/1685], Loss: 0.0601\n",
      "Epoch [4/5], Step [1040/1685], Loss: 0.0186\n",
      "Epoch [4/5], Step [1050/1685], Loss: 0.0386\n",
      "Epoch [4/5], Step [1060/1685], Loss: 0.0029\n",
      "Epoch [4/5], Step [1070/1685], Loss: 0.1709\n",
      "Epoch [4/5], Step [1080/1685], Loss: 0.0042\n",
      "Epoch [4/5], Step [1090/1685], Loss: 0.0190\n",
      "Epoch [4/5], Step [1100/1685], Loss: 0.0040\n",
      "Epoch [4/5], Step [1110/1685], Loss: 0.0007\n",
      "Epoch [4/5], Step [1120/1685], Loss: 0.0068\n",
      "Epoch [4/5], Step [1130/1685], Loss: 0.0295\n",
      "Epoch [4/5], Step [1140/1685], Loss: 0.0010\n",
      "Epoch [4/5], Step [1150/1685], Loss: 0.0021\n",
      "Epoch [4/5], Step [1160/1685], Loss: 0.0426\n",
      "Epoch [4/5], Step [1170/1685], Loss: 0.0263\n",
      "Epoch [4/5], Step [1180/1685], Loss: 0.2674\n",
      "Epoch [4/5], Step [1190/1685], Loss: 0.4055\n",
      "Epoch [4/5], Step [1200/1685], Loss: 0.6613\n",
      "Epoch [4/5], Step [1210/1685], Loss: 0.0103\n",
      "Epoch [4/5], Step [1220/1685], Loss: 0.1621\n",
      "Epoch [4/5], Step [1230/1685], Loss: 0.0002\n",
      "Epoch [4/5], Step [1240/1685], Loss: 0.0067\n",
      "Epoch [4/5], Step [1250/1685], Loss: 0.0048\n",
      "Epoch [4/5], Step [1260/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [1270/1685], Loss: 0.0134\n",
      "Epoch [4/5], Step [1280/1685], Loss: 0.0101\n",
      "Epoch [4/5], Step [1290/1685], Loss: 0.0284\n",
      "Epoch [4/5], Step [1300/1685], Loss: 0.0029\n",
      "Epoch [4/5], Step [1310/1685], Loss: 0.1205\n",
      "Epoch [4/5], Step [1320/1685], Loss: 0.0012\n",
      "Epoch [4/5], Step [1330/1685], Loss: 0.0972\n",
      "Epoch [4/5], Step [1340/1685], Loss: 0.0088\n",
      "Epoch [4/5], Step [1350/1685], Loss: 0.0011\n",
      "Epoch [4/5], Step [1360/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [1370/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [1380/1685], Loss: 0.3389\n",
      "Epoch [4/5], Step [1390/1685], Loss: 0.3294\n",
      "Epoch [4/5], Step [1400/1685], Loss: 0.0336\n",
      "Epoch [4/5], Step [1410/1685], Loss: 0.0394\n",
      "Epoch [4/5], Step [1420/1685], Loss: 0.0196\n",
      "Epoch [4/5], Step [1430/1685], Loss: 0.1458\n",
      "Epoch [4/5], Step [1440/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [1450/1685], Loss: 0.0012\n",
      "Epoch [4/5], Step [1460/1685], Loss: 0.0000\n",
      "Epoch [4/5], Step [1470/1685], Loss: 0.0016\n",
      "Epoch [4/5], Step [1480/1685], Loss: 0.0035\n",
      "Epoch [4/5], Step [1490/1685], Loss: 0.1168\n",
      "Epoch [4/5], Step [1500/1685], Loss: 0.0453\n",
      "Epoch [4/5], Step [1510/1685], Loss: 0.2298\n",
      "Epoch [4/5], Step [1520/1685], Loss: 0.0165\n",
      "Epoch [4/5], Step [1530/1685], Loss: 0.1205\n",
      "Epoch [4/5], Step [1540/1685], Loss: 0.4087\n",
      "Epoch [4/5], Step [1550/1685], Loss: 0.0008\n",
      "Epoch [4/5], Step [1560/1685], Loss: 0.0003\n",
      "Epoch [4/5], Step [1570/1685], Loss: 0.0340\n",
      "Epoch [4/5], Step [1580/1685], Loss: 0.1078\n",
      "Epoch [4/5], Step [1590/1685], Loss: 0.0607\n",
      "Epoch [4/5], Step [1600/1685], Loss: 0.0396\n",
      "Epoch [4/5], Step [1610/1685], Loss: 0.1795\n",
      "Epoch [4/5], Step [1620/1685], Loss: 0.0005\n",
      "Epoch [4/5], Step [1630/1685], Loss: 0.2654\n",
      "Epoch [4/5], Step [1640/1685], Loss: 0.0008\n",
      "Epoch [4/5], Step [1650/1685], Loss: 0.0417\n",
      "Epoch [4/5], Step [1660/1685], Loss: 0.0340\n",
      "Epoch [4/5], Step [1670/1685], Loss: 0.4249\n",
      "Epoch [4/5], Step [1680/1685], Loss: 0.0010\n",
      "Epoch [5/5], Step [10/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [20/1685], Loss: 0.1855\n",
      "Epoch [5/5], Step [30/1685], Loss: 0.0002\n",
      "Epoch [5/5], Step [40/1685], Loss: 0.0108\n",
      "Epoch [5/5], Step [50/1685], Loss: 0.0811\n",
      "Epoch [5/5], Step [60/1685], Loss: 0.0167\n",
      "Epoch [5/5], Step [70/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [80/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [90/1685], Loss: 0.0285\n",
      "Epoch [5/5], Step [100/1685], Loss: 0.4710\n",
      "Epoch [5/5], Step [110/1685], Loss: 0.1771\n",
      "Epoch [5/5], Step [120/1685], Loss: 0.0542\n",
      "Epoch [5/5], Step [130/1685], Loss: 0.1091\n",
      "Epoch [5/5], Step [140/1685], Loss: 0.3814\n",
      "Epoch [5/5], Step [150/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [160/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [170/1685], Loss: 0.2130\n",
      "Epoch [5/5], Step [180/1685], Loss: 0.1666\n",
      "Epoch [5/5], Step [190/1685], Loss: 0.0003\n",
      "Epoch [5/5], Step [200/1685], Loss: 0.0761\n",
      "Epoch [5/5], Step [210/1685], Loss: 0.0004\n",
      "Epoch [5/5], Step [220/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [230/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [240/1685], Loss: 0.0012\n",
      "Epoch [5/5], Step [250/1685], Loss: 0.0687\n",
      "Epoch [5/5], Step [260/1685], Loss: 0.0062\n",
      "Epoch [5/5], Step [270/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [280/1685], Loss: 0.0005\n",
      "Epoch [5/5], Step [290/1685], Loss: 0.2520\n",
      "Epoch [5/5], Step [300/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [310/1685], Loss: 0.0016\n",
      "Epoch [5/5], Step [320/1685], Loss: 0.0118\n",
      "Epoch [5/5], Step [330/1685], Loss: 0.0030\n",
      "Epoch [5/5], Step [340/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [350/1685], Loss: 0.3282\n",
      "Epoch [5/5], Step [360/1685], Loss: 0.0095\n",
      "Epoch [5/5], Step [370/1685], Loss: 0.0670\n",
      "Epoch [5/5], Step [380/1685], Loss: 0.0013\n",
      "Epoch [5/5], Step [390/1685], Loss: 0.3500\n",
      "Epoch [5/5], Step [400/1685], Loss: 0.2067\n",
      "Epoch [5/5], Step [410/1685], Loss: 0.0059\n",
      "Epoch [5/5], Step [420/1685], Loss: 0.0036\n",
      "Epoch [5/5], Step [430/1685], Loss: 0.0011\n",
      "Epoch [5/5], Step [440/1685], Loss: 0.0018\n",
      "Epoch [5/5], Step [450/1685], Loss: 0.0059\n",
      "Epoch [5/5], Step [460/1685], Loss: 0.0004\n",
      "Epoch [5/5], Step [470/1685], Loss: 0.0008\n",
      "Epoch [5/5], Step [480/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [490/1685], Loss: 0.0021\n",
      "Epoch [5/5], Step [500/1685], Loss: 0.1188\n",
      "Epoch [5/5], Step [510/1685], Loss: 0.0119\n",
      "Epoch [5/5], Step [520/1685], Loss: 0.5320\n",
      "Epoch [5/5], Step [530/1685], Loss: 0.0217\n",
      "Epoch [5/5], Step [540/1685], Loss: 0.1386\n",
      "Epoch [5/5], Step [550/1685], Loss: 0.0012\n",
      "Epoch [5/5], Step [560/1685], Loss: 0.0418\n",
      "Epoch [5/5], Step [570/1685], Loss: 0.0976\n",
      "Epoch [5/5], Step [580/1685], Loss: 0.2649\n",
      "Epoch [5/5], Step [590/1685], Loss: 0.0010\n",
      "Epoch [5/5], Step [600/1685], Loss: 0.0003\n",
      "Epoch [5/5], Step [610/1685], Loss: 0.0624\n",
      "Epoch [5/5], Step [620/1685], Loss: 0.2085\n",
      "Epoch [5/5], Step [630/1685], Loss: 0.2129\n",
      "Epoch [5/5], Step [640/1685], Loss: 0.0069\n",
      "Epoch [5/5], Step [650/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [660/1685], Loss: 0.0069\n",
      "Epoch [5/5], Step [670/1685], Loss: 0.0007\n",
      "Epoch [5/5], Step [680/1685], Loss: 0.0264\n",
      "Epoch [5/5], Step [690/1685], Loss: 0.0027\n",
      "Epoch [5/5], Step [700/1685], Loss: 0.0019\n",
      "Epoch [5/5], Step [710/1685], Loss: 0.8188\n",
      "Epoch [5/5], Step [720/1685], Loss: 0.0594\n",
      "Epoch [5/5], Step [730/1685], Loss: 0.0102\n",
      "Epoch [5/5], Step [740/1685], Loss: 0.0627\n",
      "Epoch [5/5], Step [750/1685], Loss: 0.0147\n",
      "Epoch [5/5], Step [760/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [770/1685], Loss: 0.7683\n",
      "Epoch [5/5], Step [780/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [790/1685], Loss: 0.0260\n",
      "Epoch [5/5], Step [800/1685], Loss: 0.2035\n",
      "Epoch [5/5], Step [810/1685], Loss: 0.0352\n",
      "Epoch [5/5], Step [820/1685], Loss: 0.0726\n",
      "Epoch [5/5], Step [830/1685], Loss: 0.0002\n",
      "Epoch [5/5], Step [840/1685], Loss: 0.0161\n",
      "Epoch [5/5], Step [850/1685], Loss: 0.0046\n",
      "Epoch [5/5], Step [860/1685], Loss: 0.0007\n",
      "Epoch [5/5], Step [870/1685], Loss: 0.1713\n",
      "Epoch [5/5], Step [880/1685], Loss: 0.0035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [890/1685], Loss: 0.0183\n",
      "Epoch [5/5], Step [900/1685], Loss: 0.0291\n",
      "Epoch [5/5], Step [910/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [920/1685], Loss: 0.0975\n",
      "Epoch [5/5], Step [930/1685], Loss: 0.0010\n",
      "Epoch [5/5], Step [940/1685], Loss: 0.0002\n",
      "Epoch [5/5], Step [950/1685], Loss: 0.1650\n",
      "Epoch [5/5], Step [960/1685], Loss: 0.0450\n",
      "Epoch [5/5], Step [970/1685], Loss: 0.0002\n",
      "Epoch [5/5], Step [980/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [990/1685], Loss: 0.4499\n",
      "Epoch [5/5], Step [1000/1685], Loss: 0.0831\n",
      "Epoch [5/5], Step [1010/1685], Loss: 0.0205\n",
      "Epoch [5/5], Step [1020/1685], Loss: 0.3422\n",
      "Epoch [5/5], Step [1030/1685], Loss: 0.0037\n",
      "Epoch [5/5], Step [1040/1685], Loss: 0.0055\n",
      "Epoch [5/5], Step [1050/1685], Loss: 0.0048\n",
      "Epoch [5/5], Step [1060/1685], Loss: 0.0011\n",
      "Epoch [5/5], Step [1070/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [1080/1685], Loss: 0.3024\n",
      "Epoch [5/5], Step [1090/1685], Loss: 0.0506\n",
      "Epoch [5/5], Step [1100/1685], Loss: 0.0082\n",
      "Epoch [5/5], Step [1110/1685], Loss: 0.0073\n",
      "Epoch [5/5], Step [1120/1685], Loss: 0.0216\n",
      "Epoch [5/5], Step [1130/1685], Loss: 0.0081\n",
      "Epoch [5/5], Step [1140/1685], Loss: 0.0082\n",
      "Epoch [5/5], Step [1150/1685], Loss: 0.0876\n",
      "Epoch [5/5], Step [1160/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [1170/1685], Loss: 0.0150\n",
      "Epoch [5/5], Step [1180/1685], Loss: 0.0449\n",
      "Epoch [5/5], Step [1190/1685], Loss: 0.3202\n",
      "Epoch [5/5], Step [1200/1685], Loss: 0.0509\n",
      "Epoch [5/5], Step [1210/1685], Loss: 0.0144\n",
      "Epoch [5/5], Step [1220/1685], Loss: 0.2358\n",
      "Epoch [5/5], Step [1230/1685], Loss: 0.0003\n",
      "Epoch [5/5], Step [1240/1685], Loss: 0.0107\n",
      "Epoch [5/5], Step [1250/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [1260/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [1270/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [1280/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [1290/1685], Loss: 0.0008\n",
      "Epoch [5/5], Step [1300/1685], Loss: 0.0146\n",
      "Epoch [5/5], Step [1310/1685], Loss: 0.5177\n",
      "Epoch [5/5], Step [1320/1685], Loss: 0.0393\n",
      "Epoch [5/5], Step [1330/1685], Loss: 0.0081\n",
      "Epoch [5/5], Step [1340/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [1350/1685], Loss: 0.1356\n",
      "Epoch [5/5], Step [1360/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [1370/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [1380/1685], Loss: 0.0186\n",
      "Epoch [5/5], Step [1390/1685], Loss: 0.0081\n",
      "Epoch [5/5], Step [1400/1685], Loss: 0.0047\n",
      "Epoch [5/5], Step [1410/1685], Loss: 0.0595\n",
      "Epoch [5/5], Step [1420/1685], Loss: 0.0257\n",
      "Epoch [5/5], Step [1430/1685], Loss: 0.0063\n",
      "Epoch [5/5], Step [1440/1685], Loss: 0.0361\n",
      "Epoch [5/5], Step [1450/1685], Loss: 0.0005\n",
      "Epoch [5/5], Step [1460/1685], Loss: 0.3013\n",
      "Epoch [5/5], Step [1470/1685], Loss: 0.0019\n",
      "Epoch [5/5], Step [1480/1685], Loss: 0.0130\n",
      "Epoch [5/5], Step [1490/1685], Loss: 0.0772\n",
      "Epoch [5/5], Step [1500/1685], Loss: 0.0026\n",
      "Epoch [5/5], Step [1510/1685], Loss: 0.0170\n",
      "Epoch [5/5], Step [1520/1685], Loss: 0.1790\n",
      "Epoch [5/5], Step [1530/1685], Loss: 0.0030\n",
      "Epoch [5/5], Step [1540/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [1550/1685], Loss: 0.0002\n",
      "Epoch [5/5], Step [1560/1685], Loss: 0.0006\n",
      "Epoch [5/5], Step [1570/1685], Loss: 0.0004\n",
      "Epoch [5/5], Step [1580/1685], Loss: 0.0004\n",
      "Epoch [5/5], Step [1590/1685], Loss: 0.0000\n",
      "Epoch [5/5], Step [1600/1685], Loss: 0.3764\n",
      "Epoch [5/5], Step [1610/1685], Loss: 0.0879\n",
      "Epoch [5/5], Step [1620/1685], Loss: 0.0308\n",
      "Epoch [5/5], Step [1630/1685], Loss: 0.0002\n",
      "Epoch [5/5], Step [1640/1685], Loss: 0.0543\n",
      "Epoch [5/5], Step [1650/1685], Loss: 0.3205\n",
      "Epoch [5/5], Step [1660/1685], Loss: 0.0664\n",
      "Epoch [5/5], Step [1670/1685], Loss: 0.0006\n",
      "Epoch [5/5], Step [1680/1685], Loss: 0.0078\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIqIIYnAnIoogBnciogj6f+HM0N0DbySPAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ce75dca10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 24min 24s, sys: 23min 31s, total: 1h 47min 55s\n",
      "Wall time: 1h 45min 55s\n",
      "Epoch [1/5], Step [10/1685], Loss: 1.1559\n",
      "Epoch [1/5], Step [20/1685], Loss: 1.2208\n",
      "Epoch [1/5], Step [30/1685], Loss: 0.6937\n",
      "Epoch [1/5], Step [40/1685], Loss: 0.7589\n",
      "Epoch [1/5], Step [50/1685], Loss: 0.3285\n",
      "Epoch [1/5], Step [60/1685], Loss: 0.7161\n",
      "Epoch [1/5], Step [70/1685], Loss: 0.7294\n",
      "Epoch [1/5], Step [80/1685], Loss: 0.0515\n",
      "Epoch [1/5], Step [90/1685], Loss: 0.4287\n",
      "Epoch [1/5], Step [100/1685], Loss: 0.3745\n",
      "Epoch [1/5], Step [110/1685], Loss: 1.0108\n",
      "Epoch [1/5], Step [120/1685], Loss: 0.1670\n",
      "Epoch [1/5], Step [130/1685], Loss: 0.5750\n",
      "Epoch [1/5], Step [140/1685], Loss: 0.2065\n",
      "Epoch [1/5], Step [150/1685], Loss: 0.5113\n",
      "Epoch [1/5], Step [160/1685], Loss: 0.1648\n",
      "Epoch [1/5], Step [170/1685], Loss: 0.1559\n",
      "Epoch [1/5], Step [180/1685], Loss: 0.4603\n",
      "Epoch [1/5], Step [190/1685], Loss: 0.4126\n",
      "Epoch [1/5], Step [200/1685], Loss: 0.2340\n",
      "Epoch [1/5], Step [210/1685], Loss: 0.2981\n",
      "Epoch [1/5], Step [220/1685], Loss: 0.1445\n",
      "Epoch [1/5], Step [230/1685], Loss: 0.1292\n",
      "Epoch [1/5], Step [240/1685], Loss: 0.3162\n",
      "Epoch [1/5], Step [250/1685], Loss: 0.0434\n",
      "Epoch [1/5], Step [260/1685], Loss: 0.0907\n",
      "Epoch [1/5], Step [270/1685], Loss: 0.6356\n",
      "Epoch [1/5], Step [280/1685], Loss: 0.0491\n",
      "Epoch [1/5], Step [290/1685], Loss: 0.0914\n",
      "Epoch [1/5], Step [300/1685], Loss: 0.0215\n",
      "Epoch [1/5], Step [310/1685], Loss: 0.2713\n",
      "Epoch [1/5], Step [320/1685], Loss: 0.1749\n",
      "Epoch [1/5], Step [330/1685], Loss: 0.1318\n",
      "Epoch [1/5], Step [340/1685], Loss: 0.1783\n",
      "Epoch [1/5], Step [350/1685], Loss: 0.3373\n",
      "Epoch [1/5], Step [360/1685], Loss: 0.8719\n",
      "Epoch [1/5], Step [370/1685], Loss: 0.1115\n",
      "Epoch [1/5], Step [380/1685], Loss: 0.1245\n",
      "Epoch [1/5], Step [390/1685], Loss: 0.6582\n",
      "Epoch [1/5], Step [400/1685], Loss: 0.4239\n",
      "Epoch [1/5], Step [410/1685], Loss: 0.4310\n",
      "Epoch [1/5], Step [420/1685], Loss: 0.1861\n",
      "Epoch [1/5], Step [430/1685], Loss: 0.1894\n",
      "Epoch [1/5], Step [440/1685], Loss: 0.0668\n",
      "Epoch [1/5], Step [450/1685], Loss: 0.0228\n",
      "Epoch [1/5], Step [460/1685], Loss: 0.3701\n",
      "Epoch [1/5], Step [470/1685], Loss: 0.2590\n",
      "Epoch [1/5], Step [480/1685], Loss: 0.0461\n",
      "Epoch [1/5], Step [490/1685], Loss: 0.0686\n",
      "Epoch [1/5], Step [500/1685], Loss: 0.1634\n",
      "Epoch [1/5], Step [510/1685], Loss: 0.2193\n",
      "Epoch [1/5], Step [520/1685], Loss: 0.1831\n",
      "Epoch [1/5], Step [530/1685], Loss: 0.2817\n",
      "Epoch [1/5], Step [540/1685], Loss: 0.2702\n",
      "Epoch [1/5], Step [550/1685], Loss: 0.2010\n",
      "Epoch [1/5], Step [560/1685], Loss: 0.5184\n",
      "Epoch [1/5], Step [570/1685], Loss: 0.7049\n",
      "Epoch [1/5], Step [580/1685], Loss: 0.1465\n",
      "Epoch [1/5], Step [590/1685], Loss: 0.0476\n",
      "Epoch [1/5], Step [600/1685], Loss: 0.0506\n",
      "Epoch [1/5], Step [610/1685], Loss: 0.3005\n",
      "Epoch [1/5], Step [620/1685], Loss: 0.3252\n",
      "Epoch [1/5], Step [630/1685], Loss: 0.1482\n",
      "Epoch [1/5], Step [640/1685], Loss: 0.6733\n",
      "Epoch [1/5], Step [650/1685], Loss: 0.0845\n",
      "Epoch [1/5], Step [660/1685], Loss: 0.9182\n",
      "Epoch [1/5], Step [670/1685], Loss: 0.1744\n",
      "Epoch [1/5], Step [680/1685], Loss: 0.0163\n",
      "Epoch [1/5], Step [690/1685], Loss: 0.5156\n",
      "Epoch [1/5], Step [700/1685], Loss: 0.3157\n",
      "Epoch [1/5], Step [710/1685], Loss: 0.0333\n",
      "Epoch [1/5], Step [720/1685], Loss: 0.5434\n",
      "Epoch [1/5], Step [730/1685], Loss: 0.0226\n",
      "Epoch [1/5], Step [740/1685], Loss: 0.0533\n",
      "Epoch [1/5], Step [750/1685], Loss: 0.1347\n",
      "Epoch [1/5], Step [760/1685], Loss: 0.2850\n",
      "Epoch [1/5], Step [770/1685], Loss: 0.4484\n",
      "Epoch [1/5], Step [780/1685], Loss: 0.0171\n",
      "Epoch [1/5], Step [790/1685], Loss: 0.0215\n",
      "Epoch [1/5], Step [800/1685], Loss: 1.1322\n",
      "Epoch [1/5], Step [810/1685], Loss: 0.0250\n",
      "Epoch [1/5], Step [820/1685], Loss: 0.0201\n",
      "Epoch [1/5], Step [830/1685], Loss: 0.4947\n",
      "Epoch [1/5], Step [840/1685], Loss: 0.0420\n",
      "Epoch [1/5], Step [850/1685], Loss: 0.4603\n",
      "Epoch [1/5], Step [860/1685], Loss: 0.0633\n",
      "Epoch [1/5], Step [870/1685], Loss: 0.0113\n",
      "Epoch [1/5], Step [880/1685], Loss: 0.0278\n",
      "Epoch [1/5], Step [890/1685], Loss: 0.5318\n",
      "Epoch [1/5], Step [900/1685], Loss: 0.0634\n",
      "Epoch [1/5], Step [910/1685], Loss: 0.1106\n",
      "Epoch [1/5], Step [920/1685], Loss: 0.0525\n",
      "Epoch [1/5], Step [930/1685], Loss: 0.2035\n",
      "Epoch [1/5], Step [940/1685], Loss: 0.0085\n",
      "Epoch [1/5], Step [950/1685], Loss: 0.0948\n",
      "Epoch [1/5], Step [960/1685], Loss: 0.0237\n",
      "Epoch [1/5], Step [970/1685], Loss: 0.1586\n",
      "Epoch [1/5], Step [980/1685], Loss: 0.0333\n",
      "Epoch [1/5], Step [990/1685], Loss: 0.2406\n",
      "Epoch [1/5], Step [1000/1685], Loss: 0.0231\n",
      "Epoch [1/5], Step [1010/1685], Loss: 0.0616\n",
      "Epoch [1/5], Step [1020/1685], Loss: 0.0547\n",
      "Epoch [1/5], Step [1030/1685], Loss: 0.0535\n",
      "Epoch [1/5], Step [1040/1685], Loss: 0.5021\n",
      "Epoch [1/5], Step [1050/1685], Loss: 0.2079\n",
      "Epoch [1/5], Step [1060/1685], Loss: 0.0113\n",
      "Epoch [1/5], Step [1070/1685], Loss: 0.0559\n",
      "Epoch [1/5], Step [1080/1685], Loss: 0.0550\n",
      "Epoch [1/5], Step [1090/1685], Loss: 0.8089\n",
      "Epoch [1/5], Step [1100/1685], Loss: 0.0610\n",
      "Epoch [1/5], Step [1110/1685], Loss: 0.2321\n",
      "Epoch [1/5], Step [1120/1685], Loss: 0.1666\n",
      "Epoch [1/5], Step [1130/1685], Loss: 0.0429\n",
      "Epoch [1/5], Step [1140/1685], Loss: 0.0752\n",
      "Epoch [1/5], Step [1150/1685], Loss: 0.1962\n",
      "Epoch [1/5], Step [1160/1685], Loss: 0.0086\n",
      "Epoch [1/5], Step [1170/1685], Loss: 0.7903\n",
      "Epoch [1/5], Step [1180/1685], Loss: 0.0109\n",
      "Epoch [1/5], Step [1190/1685], Loss: 0.1832\n",
      "Epoch [1/5], Step [1200/1685], Loss: 0.4128\n",
      "Epoch [1/5], Step [1210/1685], Loss: 0.3069\n",
      "Epoch [1/5], Step [1220/1685], Loss: 0.5802\n",
      "Epoch [1/5], Step [1230/1685], Loss: 0.0435\n",
      "Epoch [1/5], Step [1240/1685], Loss: 0.2913\n",
      "Epoch [1/5], Step [1250/1685], Loss: 0.1495\n",
      "Epoch [1/5], Step [1260/1685], Loss: 0.3048\n",
      "Epoch [1/5], Step [1270/1685], Loss: 0.0162\n",
      "Epoch [1/5], Step [1280/1685], Loss: 0.1330\n",
      "Epoch [1/5], Step [1290/1685], Loss: 0.0985\n",
      "Epoch [1/5], Step [1300/1685], Loss: 0.3966\n",
      "Epoch [1/5], Step [1310/1685], Loss: 0.0139\n",
      "Epoch [1/5], Step [1320/1685], Loss: 0.0356\n",
      "Epoch [1/5], Step [1330/1685], Loss: 0.2178\n",
      "Epoch [1/5], Step [1340/1685], Loss: 0.1095\n",
      "Epoch [1/5], Step [1350/1685], Loss: 0.4303\n",
      "Epoch [1/5], Step [1360/1685], Loss: 0.0366\n",
      "Epoch [1/5], Step [1370/1685], Loss: 0.0263\n",
      "Epoch [1/5], Step [1380/1685], Loss: 0.2069\n",
      "Epoch [1/5], Step [1390/1685], Loss: 0.0376\n",
      "Epoch [1/5], Step [1400/1685], Loss: 0.2613\n",
      "Epoch [1/5], Step [1410/1685], Loss: 0.0537\n",
      "Epoch [1/5], Step [1420/1685], Loss: 0.0097\n",
      "Epoch [1/5], Step [1430/1685], Loss: 0.0821\n",
      "Epoch [1/5], Step [1440/1685], Loss: 0.2099\n",
      "Epoch [1/5], Step [1450/1685], Loss: 0.2357\n",
      "Epoch [1/5], Step [1460/1685], Loss: 0.2775\n",
      "Epoch [1/5], Step [1470/1685], Loss: 0.3088\n",
      "Epoch [1/5], Step [1480/1685], Loss: 0.0120\n",
      "Epoch [1/5], Step [1490/1685], Loss: 0.3709\n",
      "Epoch [1/5], Step [1500/1685], Loss: 0.0425\n",
      "Epoch [1/5], Step [1510/1685], Loss: 0.4487\n",
      "Epoch [1/5], Step [1520/1685], Loss: 0.3348\n",
      "Epoch [1/5], Step [1530/1685], Loss: 0.0812\n",
      "Epoch [1/5], Step [1540/1685], Loss: 0.5563\n",
      "Epoch [1/5], Step [1550/1685], Loss: 0.0291\n",
      "Epoch [1/5], Step [1560/1685], Loss: 0.0877\n",
      "Epoch [1/5], Step [1570/1685], Loss: 0.4193\n",
      "Epoch [1/5], Step [1580/1685], Loss: 0.5183\n",
      "Epoch [1/5], Step [1590/1685], Loss: 0.0495\n",
      "Epoch [1/5], Step [1600/1685], Loss: 0.5178\n",
      "Epoch [1/5], Step [1610/1685], Loss: 0.4806\n",
      "Epoch [1/5], Step [1620/1685], Loss: 0.9229\n",
      "Epoch [1/5], Step [1630/1685], Loss: 0.0271\n",
      "Epoch [1/5], Step [1640/1685], Loss: 0.0186\n",
      "Epoch [1/5], Step [1650/1685], Loss: 0.2191\n",
      "Epoch [1/5], Step [1660/1685], Loss: 0.4240\n",
      "Epoch [1/5], Step [1670/1685], Loss: 0.0065\n",
      "Epoch [1/5], Step [1680/1685], Loss: 0.0524\n",
      "Epoch [2/5], Step [10/1685], Loss: 0.7878\n",
      "Epoch [2/5], Step [20/1685], Loss: 0.0484\n",
      "Epoch [2/5], Step [30/1685], Loss: 0.0282\n",
      "Epoch [2/5], Step [40/1685], Loss: 0.0315\n",
      "Epoch [2/5], Step [50/1685], Loss: 0.1458\n",
      "Epoch [2/5], Step [60/1685], Loss: 0.0887\n",
      "Epoch [2/5], Step [70/1685], Loss: 0.0355\n",
      "Epoch [2/5], Step [80/1685], Loss: 0.8183\n",
      "Epoch [2/5], Step [90/1685], Loss: 0.0577\n",
      "Epoch [2/5], Step [100/1685], Loss: 0.0052\n",
      "Epoch [2/5], Step [110/1685], Loss: 0.0252\n",
      "Epoch [2/5], Step [120/1685], Loss: 0.1404\n",
      "Epoch [2/5], Step [130/1685], Loss: 0.0052\n",
      "Epoch [2/5], Step [140/1685], Loss: 0.4458\n",
      "Epoch [2/5], Step [150/1685], Loss: 0.0249\n",
      "Epoch [2/5], Step [160/1685], Loss: 0.1557\n",
      "Epoch [2/5], Step [170/1685], Loss: 0.1615\n",
      "Epoch [2/5], Step [180/1685], Loss: 0.2616\n",
      "Epoch [2/5], Step [190/1685], Loss: 0.0691\n",
      "Epoch [2/5], Step [200/1685], Loss: 0.1056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [210/1685], Loss: 0.0813\n",
      "Epoch [2/5], Step [220/1685], Loss: 0.1621\n",
      "Epoch [2/5], Step [230/1685], Loss: 0.3073\n",
      "Epoch [2/5], Step [240/1685], Loss: 0.2350\n",
      "Epoch [2/5], Step [250/1685], Loss: 0.0173\n",
      "Epoch [2/5], Step [260/1685], Loss: 0.1221\n",
      "Epoch [2/5], Step [270/1685], Loss: 0.0774\n",
      "Epoch [2/5], Step [280/1685], Loss: 0.0046\n",
      "Epoch [2/5], Step [290/1685], Loss: 0.0275\n",
      "Epoch [2/5], Step [300/1685], Loss: 0.3803\n",
      "Epoch [2/5], Step [310/1685], Loss: 0.1575\n",
      "Epoch [2/5], Step [320/1685], Loss: 0.4336\n",
      "Epoch [2/5], Step [330/1685], Loss: 0.1423\n",
      "Epoch [2/5], Step [340/1685], Loss: 0.0131\n",
      "Epoch [2/5], Step [350/1685], Loss: 0.0057\n",
      "Epoch [2/5], Step [360/1685], Loss: 0.2948\n",
      "Epoch [2/5], Step [370/1685], Loss: 0.0111\n",
      "Epoch [2/5], Step [380/1685], Loss: 0.0105\n",
      "Epoch [2/5], Step [390/1685], Loss: 0.1925\n",
      "Epoch [2/5], Step [400/1685], Loss: 0.0524\n",
      "Epoch [2/5], Step [410/1685], Loss: 0.0137\n",
      "Epoch [2/5], Step [420/1685], Loss: 0.0132\n",
      "Epoch [2/5], Step [430/1685], Loss: 0.3563\n",
      "Epoch [2/5], Step [440/1685], Loss: 0.0183\n",
      "Epoch [2/5], Step [450/1685], Loss: 0.0071\n",
      "Epoch [2/5], Step [460/1685], Loss: 0.0561\n",
      "Epoch [2/5], Step [470/1685], Loss: 0.0157\n",
      "Epoch [2/5], Step [480/1685], Loss: 0.0107\n",
      "Epoch [2/5], Step [490/1685], Loss: 0.1339\n",
      "Epoch [2/5], Step [500/1685], Loss: 0.2257\n",
      "Epoch [2/5], Step [510/1685], Loss: 0.0266\n",
      "Epoch [2/5], Step [520/1685], Loss: 0.0873\n",
      "Epoch [2/5], Step [530/1685], Loss: 0.1739\n",
      "Epoch [2/5], Step [540/1685], Loss: 0.4450\n",
      "Epoch [2/5], Step [550/1685], Loss: 0.0116\n",
      "Epoch [2/5], Step [560/1685], Loss: 0.0541\n",
      "Epoch [2/5], Step [570/1685], Loss: 0.0694\n",
      "Epoch [2/5], Step [580/1685], Loss: 0.2501\n",
      "Epoch [2/5], Step [590/1685], Loss: 0.3613\n",
      "Epoch [2/5], Step [600/1685], Loss: 0.0206\n",
      "Epoch [2/5], Step [610/1685], Loss: 0.0274\n",
      "Epoch [2/5], Step [620/1685], Loss: 0.2116\n",
      "Epoch [2/5], Step [630/1685], Loss: 0.0550\n",
      "Epoch [2/5], Step [640/1685], Loss: 0.4656\n",
      "Epoch [2/5], Step [650/1685], Loss: 0.0157\n",
      "Epoch [2/5], Step [660/1685], Loss: 0.0128\n",
      "Epoch [2/5], Step [670/1685], Loss: 0.2497\n",
      "Epoch [2/5], Step [680/1685], Loss: 0.0323\n",
      "Epoch [2/5], Step [690/1685], Loss: 0.3122\n",
      "Epoch [2/5], Step [700/1685], Loss: 1.0955\n",
      "Epoch [2/5], Step [710/1685], Loss: 0.0164\n",
      "Epoch [2/5], Step [720/1685], Loss: 0.0067\n",
      "Epoch [2/5], Step [730/1685], Loss: 0.0270\n",
      "Epoch [2/5], Step [740/1685], Loss: 0.1204\n",
      "Epoch [2/5], Step [750/1685], Loss: 0.1195\n",
      "Epoch [2/5], Step [760/1685], Loss: 0.1085\n",
      "Epoch [2/5], Step [770/1685], Loss: 0.0060\n",
      "Epoch [2/5], Step [780/1685], Loss: 0.0314\n",
      "Epoch [2/5], Step [790/1685], Loss: 0.0109\n",
      "Epoch [2/5], Step [800/1685], Loss: 0.1913\n",
      "Epoch [2/5], Step [810/1685], Loss: 0.0622\n",
      "Epoch [2/5], Step [820/1685], Loss: 0.0223\n",
      "Epoch [2/5], Step [830/1685], Loss: 0.4414\n",
      "Epoch [2/5], Step [840/1685], Loss: 0.2881\n",
      "Epoch [2/5], Step [850/1685], Loss: 0.5197\n",
      "Epoch [2/5], Step [860/1685], Loss: 0.2963\n",
      "Epoch [2/5], Step [870/1685], Loss: 0.1025\n",
      "Epoch [2/5], Step [880/1685], Loss: 0.0175\n",
      "Epoch [2/5], Step [890/1685], Loss: 1.1089\n",
      "Epoch [2/5], Step [900/1685], Loss: 0.0671\n",
      "Epoch [2/5], Step [910/1685], Loss: 0.2340\n",
      "Epoch [2/5], Step [920/1685], Loss: 0.0104\n",
      "Epoch [2/5], Step [930/1685], Loss: 0.0059\n",
      "Epoch [2/5], Step [940/1685], Loss: 0.0385\n",
      "Epoch [2/5], Step [950/1685], Loss: 0.1311\n",
      "Epoch [2/5], Step [960/1685], Loss: 0.1016\n",
      "Epoch [2/5], Step [970/1685], Loss: 0.0072\n",
      "Epoch [2/5], Step [980/1685], Loss: 0.2401\n",
      "Epoch [2/5], Step [990/1685], Loss: 0.1842\n",
      "Epoch [2/5], Step [1000/1685], Loss: 0.0652\n",
      "Epoch [2/5], Step [1010/1685], Loss: 0.5328\n",
      "Epoch [2/5], Step [1020/1685], Loss: 0.5193\n",
      "Epoch [2/5], Step [1030/1685], Loss: 0.1868\n",
      "Epoch [2/5], Step [1040/1685], Loss: 0.0205\n",
      "Epoch [2/5], Step [1050/1685], Loss: 0.0677\n",
      "Epoch [2/5], Step [1060/1685], Loss: 0.5025\n",
      "Epoch [2/5], Step [1070/1685], Loss: 0.0446\n",
      "Epoch [2/5], Step [1080/1685], Loss: 0.0213\n",
      "Epoch [2/5], Step [1090/1685], Loss: 0.0414\n",
      "Epoch [2/5], Step [1100/1685], Loss: 0.0478\n",
      "Epoch [2/5], Step [1110/1685], Loss: 0.0081\n",
      "Epoch [2/5], Step [1120/1685], Loss: 0.0064\n",
      "Epoch [2/5], Step [1130/1685], Loss: 0.0046\n",
      "Epoch [2/5], Step [1140/1685], Loss: 0.0667\n",
      "Epoch [2/5], Step [1150/1685], Loss: 0.0304\n",
      "Epoch [2/5], Step [1160/1685], Loss: 0.1567\n",
      "Epoch [2/5], Step [1170/1685], Loss: 0.0103\n",
      "Epoch [2/5], Step [1180/1685], Loss: 0.6816\n",
      "Epoch [2/5], Step [1190/1685], Loss: 0.2019\n",
      "Epoch [2/5], Step [1200/1685], Loss: 0.3949\n",
      "Epoch [2/5], Step [1210/1685], Loss: 0.0861\n",
      "Epoch [2/5], Step [1220/1685], Loss: 0.5481\n",
      "Epoch [2/5], Step [1230/1685], Loss: 0.0830\n",
      "Epoch [2/5], Step [1240/1685], Loss: 0.8247\n",
      "Epoch [2/5], Step [1250/1685], Loss: 0.0199\n",
      "Epoch [2/5], Step [1260/1685], Loss: 0.0174\n",
      "Epoch [2/5], Step [1270/1685], Loss: 0.0259\n",
      "Epoch [2/5], Step [1280/1685], Loss: 0.3242\n",
      "Epoch [2/5], Step [1290/1685], Loss: 0.0120\n",
      "Epoch [2/5], Step [1300/1685], Loss: 0.0126\n",
      "Epoch [2/5], Step [1310/1685], Loss: 0.0490\n",
      "Epoch [2/5], Step [1320/1685], Loss: 0.1347\n",
      "Epoch [2/5], Step [1330/1685], Loss: 0.2727\n",
      "Epoch [2/5], Step [1340/1685], Loss: 0.5119\n",
      "Epoch [2/5], Step [1350/1685], Loss: 0.0725\n",
      "Epoch [2/5], Step [1360/1685], Loss: 0.1968\n",
      "Epoch [2/5], Step [1370/1685], Loss: 0.0321\n",
      "Epoch [2/5], Step [1380/1685], Loss: 0.8181\n",
      "Epoch [2/5], Step [1390/1685], Loss: 0.0148\n",
      "Epoch [2/5], Step [1400/1685], Loss: 0.7182\n",
      "Epoch [2/5], Step [1410/1685], Loss: 0.1058\n",
      "Epoch [2/5], Step [1420/1685], Loss: 0.1662\n",
      "Epoch [2/5], Step [1430/1685], Loss: 0.0190\n",
      "Epoch [2/5], Step [1440/1685], Loss: 0.1971\n",
      "Epoch [2/5], Step [1450/1685], Loss: 0.0819\n",
      "Epoch [2/5], Step [1460/1685], Loss: 0.0976\n",
      "Epoch [2/5], Step [1470/1685], Loss: 0.0046\n",
      "Epoch [2/5], Step [1480/1685], Loss: 0.0213\n",
      "Epoch [2/5], Step [1490/1685], Loss: 0.0089\n",
      "Epoch [2/5], Step [1500/1685], Loss: 0.2019\n",
      "Epoch [2/5], Step [1510/1685], Loss: 0.3320\n",
      "Epoch [2/5], Step [1520/1685], Loss: 0.2431\n",
      "Epoch [2/5], Step [1530/1685], Loss: 0.0108\n",
      "Epoch [2/5], Step [1540/1685], Loss: 0.6647\n",
      "Epoch [2/5], Step [1550/1685], Loss: 0.1381\n",
      "Epoch [2/5], Step [1560/1685], Loss: 0.0110\n",
      "Epoch [2/5], Step [1570/1685], Loss: 0.0340\n",
      "Epoch [2/5], Step [1580/1685], Loss: 0.0966\n",
      "Epoch [2/5], Step [1590/1685], Loss: 0.0345\n",
      "Epoch [2/5], Step [1600/1685], Loss: 0.0328\n",
      "Epoch [2/5], Step [1610/1685], Loss: 0.0309\n",
      "Epoch [2/5], Step [1620/1685], Loss: 0.0615\n",
      "Epoch [2/5], Step [1630/1685], Loss: 0.3376\n",
      "Epoch [2/5], Step [1640/1685], Loss: 0.0171\n",
      "Epoch [2/5], Step [1650/1685], Loss: 0.1016\n",
      "Epoch [2/5], Step [1660/1685], Loss: 0.0075\n",
      "Epoch [2/5], Step [1670/1685], Loss: 0.1498\n",
      "Epoch [2/5], Step [1680/1685], Loss: 0.0023\n",
      "Epoch [3/5], Step [10/1685], Loss: 0.1921\n",
      "Epoch [3/5], Step [20/1685], Loss: 0.0459\n",
      "Epoch [3/5], Step [30/1685], Loss: 0.0614\n",
      "Epoch [3/5], Step [40/1685], Loss: 0.0163\n",
      "Epoch [3/5], Step [50/1685], Loss: 0.1455\n",
      "Epoch [3/5], Step [60/1685], Loss: 0.0260\n",
      "Epoch [3/5], Step [70/1685], Loss: 0.0073\n",
      "Epoch [3/5], Step [80/1685], Loss: 0.0180\n",
      "Epoch [3/5], Step [90/1685], Loss: 0.0116\n",
      "Epoch [3/5], Step [100/1685], Loss: 0.0130\n",
      "Epoch [3/5], Step [110/1685], Loss: 0.0970\n",
      "Epoch [3/5], Step [120/1685], Loss: 0.2207\n",
      "Epoch [3/5], Step [130/1685], Loss: 0.2607\n",
      "Epoch [3/5], Step [140/1685], Loss: 0.0318\n",
      "Epoch [3/5], Step [150/1685], Loss: 0.0107\n",
      "Epoch [3/5], Step [160/1685], Loss: 0.4353\n",
      "Epoch [3/5], Step [170/1685], Loss: 0.0172\n",
      "Epoch [3/5], Step [180/1685], Loss: 0.0896\n",
      "Epoch [3/5], Step [190/1685], Loss: 0.1489\n",
      "Epoch [3/5], Step [200/1685], Loss: 0.0225\n",
      "Epoch [3/5], Step [210/1685], Loss: 0.2374\n",
      "Epoch [3/5], Step [220/1685], Loss: 0.1539\n",
      "Epoch [3/5], Step [230/1685], Loss: 0.0504\n",
      "Epoch [3/5], Step [240/1685], Loss: 0.0056\n",
      "Epoch [3/5], Step [250/1685], Loss: 0.0842\n",
      "Epoch [3/5], Step [260/1685], Loss: 0.0179\n",
      "Epoch [3/5], Step [270/1685], Loss: 0.0071\n",
      "Epoch [3/5], Step [280/1685], Loss: 0.0458\n",
      "Epoch [3/5], Step [290/1685], Loss: 0.0221\n",
      "Epoch [3/5], Step [300/1685], Loss: 0.0107\n",
      "Epoch [3/5], Step [310/1685], Loss: 0.0098\n",
      "Epoch [3/5], Step [320/1685], Loss: 0.1787\n",
      "Epoch [3/5], Step [330/1685], Loss: 0.0992\n",
      "Epoch [3/5], Step [340/1685], Loss: 0.0860\n",
      "Epoch [3/5], Step [350/1685], Loss: 0.0070\n",
      "Epoch [3/5], Step [360/1685], Loss: 0.0061\n",
      "Epoch [3/5], Step [370/1685], Loss: 0.4472\n",
      "Epoch [3/5], Step [380/1685], Loss: 0.2291\n",
      "Epoch [3/5], Step [390/1685], Loss: 0.0026\n",
      "Epoch [3/5], Step [400/1685], Loss: 0.0559\n",
      "Epoch [3/5], Step [410/1685], Loss: 0.2115\n",
      "Epoch [3/5], Step [420/1685], Loss: 0.0049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [430/1685], Loss: 0.0596\n",
      "Epoch [3/5], Step [440/1685], Loss: 0.0144\n",
      "Epoch [3/5], Step [450/1685], Loss: 0.3095\n",
      "Epoch [3/5], Step [460/1685], Loss: 0.1996\n",
      "Epoch [3/5], Step [470/1685], Loss: 0.2553\n",
      "Epoch [3/5], Step [480/1685], Loss: 0.0036\n",
      "Epoch [3/5], Step [490/1685], Loss: 0.0033\n",
      "Epoch [3/5], Step [500/1685], Loss: 0.1546\n",
      "Epoch [3/5], Step [510/1685], Loss: 0.1483\n",
      "Epoch [3/5], Step [520/1685], Loss: 0.0457\n",
      "Epoch [3/5], Step [530/1685], Loss: 0.0022\n",
      "Epoch [3/5], Step [540/1685], Loss: 0.2137\n",
      "Epoch [3/5], Step [550/1685], Loss: 0.0087\n",
      "Epoch [3/5], Step [560/1685], Loss: 0.0028\n",
      "Epoch [3/5], Step [570/1685], Loss: 0.2046\n",
      "Epoch [3/5], Step [580/1685], Loss: 0.0054\n",
      "Epoch [3/5], Step [590/1685], Loss: 0.4515\n",
      "Epoch [3/5], Step [600/1685], Loss: 0.3653\n",
      "Epoch [3/5], Step [610/1685], Loss: 0.0056\n",
      "Epoch [3/5], Step [620/1685], Loss: 0.0102\n",
      "Epoch [3/5], Step [630/1685], Loss: 0.0114\n",
      "Epoch [3/5], Step [640/1685], Loss: 0.0845\n",
      "Epoch [3/5], Step [650/1685], Loss: 0.0624\n",
      "Epoch [3/5], Step [660/1685], Loss: 0.0243\n",
      "Epoch [3/5], Step [670/1685], Loss: 0.1126\n",
      "Epoch [3/5], Step [680/1685], Loss: 0.0011\n",
      "Epoch [3/5], Step [690/1685], Loss: 0.1077\n",
      "Epoch [3/5], Step [700/1685], Loss: 0.0587\n",
      "Epoch [3/5], Step [710/1685], Loss: 0.0606\n",
      "Epoch [3/5], Step [720/1685], Loss: 0.4053\n",
      "Epoch [3/5], Step [730/1685], Loss: 0.1056\n",
      "Epoch [3/5], Step [740/1685], Loss: 0.0857\n",
      "Epoch [3/5], Step [750/1685], Loss: 0.0339\n",
      "Epoch [3/5], Step [760/1685], Loss: 0.0038\n",
      "Epoch [3/5], Step [770/1685], Loss: 0.0307\n",
      "Epoch [3/5], Step [780/1685], Loss: 0.0068\n",
      "Epoch [3/5], Step [790/1685], Loss: 0.2063\n",
      "Epoch [3/5], Step [800/1685], Loss: 0.1136\n",
      "Epoch [3/5], Step [810/1685], Loss: 0.7047\n",
      "Epoch [3/5], Step [820/1685], Loss: 0.0206\n",
      "Epoch [3/5], Step [830/1685], Loss: 0.2329\n",
      "Epoch [3/5], Step [840/1685], Loss: 0.0030\n",
      "Epoch [3/5], Step [850/1685], Loss: 0.2020\n",
      "Epoch [3/5], Step [860/1685], Loss: 0.2685\n",
      "Epoch [3/5], Step [870/1685], Loss: 0.2809\n",
      "Epoch [3/5], Step [880/1685], Loss: 0.0349\n",
      "Epoch [3/5], Step [890/1685], Loss: 0.0049\n",
      "Epoch [3/5], Step [900/1685], Loss: 0.0105\n",
      "Epoch [3/5], Step [910/1685], Loss: 0.0103\n",
      "Epoch [3/5], Step [920/1685], Loss: 0.0174\n",
      "Epoch [3/5], Step [930/1685], Loss: 0.2442\n",
      "Epoch [3/5], Step [940/1685], Loss: 0.0045\n",
      "Epoch [3/5], Step [950/1685], Loss: 0.0230\n",
      "Epoch [3/5], Step [960/1685], Loss: 0.0382\n",
      "Epoch [3/5], Step [970/1685], Loss: 0.0544\n",
      "Epoch [3/5], Step [980/1685], Loss: 0.0397\n",
      "Epoch [3/5], Step [990/1685], Loss: 0.4049\n",
      "Epoch [3/5], Step [1000/1685], Loss: 0.0093\n",
      "Epoch [3/5], Step [1010/1685], Loss: 0.0146\n",
      "Epoch [3/5], Step [1020/1685], Loss: 0.0157\n",
      "Epoch [3/5], Step [1030/1685], Loss: 0.5630\n",
      "Epoch [3/5], Step [1040/1685], Loss: 0.0088\n",
      "Epoch [3/5], Step [1050/1685], Loss: 0.0111\n",
      "Epoch [3/5], Step [1060/1685], Loss: 0.0321\n",
      "Epoch [3/5], Step [1070/1685], Loss: 0.1936\n",
      "Epoch [3/5], Step [1080/1685], Loss: 0.0068\n",
      "Epoch [3/5], Step [1090/1685], Loss: 0.0507\n",
      "Epoch [3/5], Step [1100/1685], Loss: 0.0111\n",
      "Epoch [3/5], Step [1110/1685], Loss: 0.0012\n",
      "Epoch [3/5], Step [1120/1685], Loss: 0.0482\n",
      "Epoch [3/5], Step [1130/1685], Loss: 0.1205\n",
      "Epoch [3/5], Step [1140/1685], Loss: 0.0103\n",
      "Epoch [3/5], Step [1150/1685], Loss: 0.0040\n",
      "Epoch [3/5], Step [1160/1685], Loss: 0.5307\n",
      "Epoch [3/5], Step [1170/1685], Loss: 0.0149\n",
      "Epoch [3/5], Step [1180/1685], Loss: 0.0031\n",
      "Epoch [3/5], Step [1190/1685], Loss: 0.0055\n",
      "Epoch [3/5], Step [1200/1685], Loss: 0.3220\n",
      "Epoch [3/5], Step [1210/1685], Loss: 0.0043\n",
      "Epoch [3/5], Step [1220/1685], Loss: 0.0025\n",
      "Epoch [3/5], Step [1230/1685], Loss: 0.0022\n",
      "Epoch [3/5], Step [1240/1685], Loss: 0.0481\n",
      "Epoch [3/5], Step [1250/1685], Loss: 0.0174\n",
      "Epoch [3/5], Step [1260/1685], Loss: 0.0157\n",
      "Epoch [3/5], Step [1270/1685], Loss: 0.2805\n",
      "Epoch [3/5], Step [1280/1685], Loss: 0.0047\n",
      "Epoch [3/5], Step [1290/1685], Loss: 0.0463\n",
      "Epoch [3/5], Step [1300/1685], Loss: 0.0490\n",
      "Epoch [3/5], Step [1310/1685], Loss: 0.0110\n",
      "Epoch [3/5], Step [1320/1685], Loss: 0.0291\n",
      "Epoch [3/5], Step [1330/1685], Loss: 0.0208\n",
      "Epoch [3/5], Step [1340/1685], Loss: 0.0411\n",
      "Epoch [3/5], Step [1350/1685], Loss: 0.0045\n",
      "Epoch [3/5], Step [1360/1685], Loss: 0.1112\n",
      "Epoch [3/5], Step [1370/1685], Loss: 0.0948\n",
      "Epoch [3/5], Step [1380/1685], Loss: 0.7874\n",
      "Epoch [3/5], Step [1390/1685], Loss: 0.1492\n",
      "Epoch [3/5], Step [1400/1685], Loss: 0.1552\n",
      "Epoch [3/5], Step [1410/1685], Loss: 0.1259\n",
      "Epoch [3/5], Step [1420/1685], Loss: 0.2137\n",
      "Epoch [3/5], Step [1430/1685], Loss: 0.0077\n",
      "Epoch [3/5], Step [1440/1685], Loss: 0.0509\n",
      "Epoch [3/5], Step [1450/1685], Loss: 0.2112\n",
      "Epoch [3/5], Step [1460/1685], Loss: 0.2682\n",
      "Epoch [3/5], Step [1470/1685], Loss: 0.0237\n",
      "Epoch [3/5], Step [1480/1685], Loss: 0.0220\n",
      "Epoch [3/5], Step [1490/1685], Loss: 0.0504\n",
      "Epoch [3/5], Step [1500/1685], Loss: 0.0445\n",
      "Epoch [3/5], Step [1510/1685], Loss: 0.0085\n",
      "Epoch [3/5], Step [1520/1685], Loss: 0.4790\n",
      "Epoch [3/5], Step [1530/1685], Loss: 0.3138\n",
      "Epoch [3/5], Step [1540/1685], Loss: 0.2189\n",
      "Epoch [3/5], Step [1550/1685], Loss: 0.0330\n",
      "Epoch [3/5], Step [1560/1685], Loss: 0.1664\n",
      "Epoch [3/5], Step [1570/1685], Loss: 0.0267\n",
      "Epoch [3/5], Step [1580/1685], Loss: 0.0976\n",
      "Epoch [3/5], Step [1590/1685], Loss: 0.0383\n",
      "Epoch [3/5], Step [1600/1685], Loss: 0.0290\n",
      "Epoch [3/5], Step [1610/1685], Loss: 0.0203\n",
      "Epoch [3/5], Step [1620/1685], Loss: 0.0087\n",
      "Epoch [3/5], Step [1630/1685], Loss: 0.0459\n",
      "Epoch [3/5], Step [1640/1685], Loss: 0.0712\n",
      "Epoch [3/5], Step [1650/1685], Loss: 0.0061\n",
      "Epoch [3/5], Step [1660/1685], Loss: 0.2665\n",
      "Epoch [3/5], Step [1670/1685], Loss: 0.0173\n",
      "Epoch [3/5], Step [1680/1685], Loss: 0.2799\n",
      "Epoch [4/5], Step [10/1685], Loss: 0.0461\n",
      "Epoch [4/5], Step [20/1685], Loss: 0.0036\n",
      "Epoch [4/5], Step [30/1685], Loss: 0.0817\n",
      "Epoch [4/5], Step [40/1685], Loss: 0.0746\n",
      "Epoch [4/5], Step [50/1685], Loss: 0.0104\n",
      "Epoch [4/5], Step [60/1685], Loss: 0.0151\n",
      "Epoch [4/5], Step [70/1685], Loss: 0.0698\n",
      "Epoch [4/5], Step [80/1685], Loss: 0.3599\n",
      "Epoch [4/5], Step [90/1685], Loss: 0.0755\n",
      "Epoch [4/5], Step [100/1685], Loss: 0.0046\n",
      "Epoch [4/5], Step [110/1685], Loss: 0.0075\n",
      "Epoch [4/5], Step [120/1685], Loss: 0.0228\n",
      "Epoch [4/5], Step [130/1685], Loss: 0.0254\n",
      "Epoch [4/5], Step [140/1685], Loss: 0.0103\n",
      "Epoch [4/5], Step [150/1685], Loss: 0.0045\n",
      "Epoch [4/5], Step [160/1685], Loss: 0.0028\n",
      "Epoch [4/5], Step [170/1685], Loss: 0.0044\n",
      "Epoch [4/5], Step [180/1685], Loss: 0.0686\n",
      "Epoch [4/5], Step [190/1685], Loss: 0.0056\n",
      "Epoch [4/5], Step [200/1685], Loss: 0.4497\n",
      "Epoch [4/5], Step [210/1685], Loss: 0.5122\n",
      "Epoch [4/5], Step [220/1685], Loss: 0.0514\n",
      "Epoch [4/5], Step [230/1685], Loss: 0.0174\n",
      "Epoch [4/5], Step [240/1685], Loss: 0.0009\n",
      "Epoch [4/5], Step [250/1685], Loss: 0.0008\n",
      "Epoch [4/5], Step [260/1685], Loss: 0.0522\n",
      "Epoch [4/5], Step [270/1685], Loss: 0.0072\n",
      "Epoch [4/5], Step [280/1685], Loss: 0.0036\n",
      "Epoch [4/5], Step [290/1685], Loss: 0.0108\n",
      "Epoch [4/5], Step [300/1685], Loss: 0.0225\n",
      "Epoch [4/5], Step [310/1685], Loss: 0.0315\n",
      "Epoch [4/5], Step [320/1685], Loss: 0.0372\n",
      "Epoch [4/5], Step [330/1685], Loss: 0.0019\n",
      "Epoch [4/5], Step [340/1685], Loss: 0.0035\n",
      "Epoch [4/5], Step [350/1685], Loss: 0.0132\n",
      "Epoch [4/5], Step [360/1685], Loss: 0.0034\n",
      "Epoch [4/5], Step [370/1685], Loss: 0.2590\n",
      "Epoch [4/5], Step [380/1685], Loss: 0.0031\n",
      "Epoch [4/5], Step [390/1685], Loss: 0.1845\n",
      "Epoch [4/5], Step [400/1685], Loss: 0.0024\n",
      "Epoch [4/5], Step [410/1685], Loss: 0.1062\n",
      "Epoch [4/5], Step [420/1685], Loss: 0.0039\n",
      "Epoch [4/5], Step [430/1685], Loss: 0.0072\n",
      "Epoch [4/5], Step [440/1685], Loss: 0.0721\n",
      "Epoch [4/5], Step [450/1685], Loss: 0.0014\n",
      "Epoch [4/5], Step [460/1685], Loss: 0.0059\n",
      "Epoch [4/5], Step [470/1685], Loss: 0.1469\n",
      "Epoch [4/5], Step [480/1685], Loss: 0.5585\n",
      "Epoch [4/5], Step [490/1685], Loss: 0.0137\n",
      "Epoch [4/5], Step [500/1685], Loss: 0.0106\n",
      "Epoch [4/5], Step [510/1685], Loss: 0.0130\n",
      "Epoch [4/5], Step [520/1685], Loss: 0.0206\n",
      "Epoch [4/5], Step [530/1685], Loss: 0.0257\n",
      "Epoch [4/5], Step [540/1685], Loss: 0.0161\n",
      "Epoch [4/5], Step [550/1685], Loss: 0.0132\n",
      "Epoch [4/5], Step [560/1685], Loss: 0.0057\n",
      "Epoch [4/5], Step [570/1685], Loss: 0.0122\n",
      "Epoch [4/5], Step [580/1685], Loss: 0.0444\n",
      "Epoch [4/5], Step [590/1685], Loss: 0.0583\n",
      "Epoch [4/5], Step [600/1685], Loss: 0.1388\n",
      "Epoch [4/5], Step [610/1685], Loss: 0.0107\n",
      "Epoch [4/5], Step [620/1685], Loss: 0.0495\n",
      "Epoch [4/5], Step [630/1685], Loss: 0.0032\n",
      "Epoch [4/5], Step [640/1685], Loss: 0.0037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [650/1685], Loss: 0.0124\n",
      "Epoch [4/5], Step [660/1685], Loss: 0.0172\n",
      "Epoch [4/5], Step [670/1685], Loss: 0.0042\n",
      "Epoch [4/5], Step [680/1685], Loss: 0.1326\n",
      "Epoch [4/5], Step [690/1685], Loss: 0.0050\n",
      "Epoch [4/5], Step [700/1685], Loss: 0.0156\n",
      "Epoch [4/5], Step [710/1685], Loss: 0.1042\n",
      "Epoch [4/5], Step [720/1685], Loss: 0.1186\n",
      "Epoch [4/5], Step [730/1685], Loss: 0.0066\n",
      "Epoch [4/5], Step [740/1685], Loss: 0.0503\n",
      "Epoch [4/5], Step [750/1685], Loss: 0.0040\n",
      "Epoch [4/5], Step [760/1685], Loss: 0.0034\n",
      "Epoch [4/5], Step [770/1685], Loss: 0.0181\n",
      "Epoch [4/5], Step [780/1685], Loss: 0.0100\n",
      "Epoch [4/5], Step [790/1685], Loss: 0.1446\n",
      "Epoch [4/5], Step [800/1685], Loss: 0.0289\n",
      "Epoch [4/5], Step [810/1685], Loss: 0.0239\n",
      "Epoch [4/5], Step [820/1685], Loss: 0.0056\n",
      "Epoch [4/5], Step [830/1685], Loss: 0.0026\n",
      "Epoch [4/5], Step [840/1685], Loss: 0.0346\n",
      "Epoch [4/5], Step [850/1685], Loss: 0.0741\n",
      "Epoch [4/5], Step [860/1685], Loss: 0.0291\n",
      "Epoch [4/5], Step [870/1685], Loss: 0.0049\n",
      "Epoch [4/5], Step [880/1685], Loss: 0.0874\n",
      "Epoch [4/5], Step [890/1685], Loss: 0.0037\n",
      "Epoch [4/5], Step [900/1685], Loss: 0.0068\n",
      "Epoch [4/5], Step [910/1685], Loss: 0.2163\n",
      "Epoch [4/5], Step [920/1685], Loss: 0.0347\n",
      "Epoch [4/5], Step [930/1685], Loss: 0.1302\n",
      "Epoch [4/5], Step [940/1685], Loss: 0.0334\n",
      "Epoch [4/5], Step [950/1685], Loss: 0.0166\n",
      "Epoch [4/5], Step [960/1685], Loss: 0.0469\n",
      "Epoch [4/5], Step [970/1685], Loss: 0.2751\n",
      "Epoch [4/5], Step [980/1685], Loss: 0.0011\n",
      "Epoch [4/5], Step [990/1685], Loss: 0.0975\n",
      "Epoch [4/5], Step [1000/1685], Loss: 0.1901\n",
      "Epoch [4/5], Step [1010/1685], Loss: 0.0055\n",
      "Epoch [4/5], Step [1020/1685], Loss: 0.0067\n",
      "Epoch [4/5], Step [1030/1685], Loss: 0.0050\n",
      "Epoch [4/5], Step [1040/1685], Loss: 0.0186\n",
      "Epoch [4/5], Step [1050/1685], Loss: 0.0038\n",
      "Epoch [4/5], Step [1060/1685], Loss: 0.0052\n",
      "Epoch [4/5], Step [1070/1685], Loss: 0.0030\n",
      "Epoch [4/5], Step [1080/1685], Loss: 0.0113\n",
      "Epoch [4/5], Step [1090/1685], Loss: 0.0155\n",
      "Epoch [4/5], Step [1100/1685], Loss: 0.0061\n",
      "Epoch [4/5], Step [1110/1685], Loss: 0.0222\n",
      "Epoch [4/5], Step [1120/1685], Loss: 0.1228\n",
      "Epoch [4/5], Step [1130/1685], Loss: 0.0266\n",
      "Epoch [4/5], Step [1140/1685], Loss: 0.0148\n",
      "Epoch [4/5], Step [1150/1685], Loss: 0.0978\n",
      "Epoch [4/5], Step [1160/1685], Loss: 0.0652\n",
      "Epoch [4/5], Step [1170/1685], Loss: 0.3186\n",
      "Epoch [4/5], Step [1180/1685], Loss: 0.0222\n",
      "Epoch [4/5], Step [1190/1685], Loss: 0.0451\n",
      "Epoch [4/5], Step [1200/1685], Loss: 0.0078\n",
      "Epoch [4/5], Step [1210/1685], Loss: 0.1069\n",
      "Epoch [4/5], Step [1220/1685], Loss: 0.0184\n",
      "Epoch [4/5], Step [1230/1685], Loss: 0.0081\n",
      "Epoch [4/5], Step [1240/1685], Loss: 0.1420\n",
      "Epoch [4/5], Step [1250/1685], Loss: 0.0598\n",
      "Epoch [4/5], Step [1260/1685], Loss: 0.0875\n",
      "Epoch [4/5], Step [1270/1685], Loss: 0.0154\n",
      "Epoch [4/5], Step [1280/1685], Loss: 0.0151\n",
      "Epoch [4/5], Step [1290/1685], Loss: 0.0012\n",
      "Epoch [4/5], Step [1300/1685], Loss: 0.0011\n",
      "Epoch [4/5], Step [1310/1685], Loss: 0.4376\n",
      "Epoch [4/5], Step [1320/1685], Loss: 0.0183\n",
      "Epoch [4/5], Step [1330/1685], Loss: 0.1523\n",
      "Epoch [4/5], Step [1340/1685], Loss: 0.0199\n",
      "Epoch [4/5], Step [1350/1685], Loss: 0.0040\n",
      "Epoch [4/5], Step [1360/1685], Loss: 0.0010\n",
      "Epoch [4/5], Step [1370/1685], Loss: 0.0517\n",
      "Epoch [4/5], Step [1380/1685], Loss: 0.0045\n",
      "Epoch [4/5], Step [1390/1685], Loss: 0.1566\n",
      "Epoch [4/5], Step [1400/1685], Loss: 0.0211\n",
      "Epoch [4/5], Step [1410/1685], Loss: 0.0007\n",
      "Epoch [4/5], Step [1420/1685], Loss: 0.0145\n",
      "Epoch [4/5], Step [1430/1685], Loss: 0.0058\n",
      "Epoch [4/5], Step [1440/1685], Loss: 0.0025\n",
      "Epoch [4/5], Step [1450/1685], Loss: 0.0033\n",
      "Epoch [4/5], Step [1460/1685], Loss: 0.0085\n",
      "Epoch [4/5], Step [1470/1685], Loss: 0.0237\n",
      "Epoch [4/5], Step [1480/1685], Loss: 0.1105\n",
      "Epoch [4/5], Step [1490/1685], Loss: 0.0013\n",
      "Epoch [4/5], Step [1500/1685], Loss: 0.0010\n",
      "Epoch [4/5], Step [1510/1685], Loss: 0.0099\n",
      "Epoch [4/5], Step [1520/1685], Loss: 0.1071\n",
      "Epoch [4/5], Step [1530/1685], Loss: 0.0195\n",
      "Epoch [4/5], Step [1540/1685], Loss: 0.2834\n",
      "Epoch [4/5], Step [1550/1685], Loss: 0.0015\n",
      "Epoch [4/5], Step [1560/1685], Loss: 0.0323\n",
      "Epoch [4/5], Step [1570/1685], Loss: 0.0201\n",
      "Epoch [4/5], Step [1580/1685], Loss: 0.0007\n",
      "Epoch [4/5], Step [1590/1685], Loss: 0.0006\n",
      "Epoch [4/5], Step [1600/1685], Loss: 0.0397\n",
      "Epoch [4/5], Step [1610/1685], Loss: 0.0710\n",
      "Epoch [4/5], Step [1620/1685], Loss: 0.0430\n",
      "Epoch [4/5], Step [1630/1685], Loss: 0.0171\n",
      "Epoch [4/5], Step [1640/1685], Loss: 0.0100\n",
      "Epoch [4/5], Step [1650/1685], Loss: 0.0031\n",
      "Epoch [4/5], Step [1660/1685], Loss: 0.0583\n",
      "Epoch [4/5], Step [1670/1685], Loss: 0.0094\n",
      "Epoch [4/5], Step [1680/1685], Loss: 0.0106\n",
      "Epoch [5/5], Step [10/1685], Loss: 0.0008\n",
      "Epoch [5/5], Step [20/1685], Loss: 0.0521\n",
      "Epoch [5/5], Step [30/1685], Loss: 0.0010\n",
      "Epoch [5/5], Step [40/1685], Loss: 0.0063\n",
      "Epoch [5/5], Step [50/1685], Loss: 0.0160\n",
      "Epoch [5/5], Step [60/1685], Loss: 0.0052\n",
      "Epoch [5/5], Step [70/1685], Loss: 0.0093\n",
      "Epoch [5/5], Step [80/1685], Loss: 0.0260\n",
      "Epoch [5/5], Step [90/1685], Loss: 0.0041\n",
      "Epoch [5/5], Step [100/1685], Loss: 0.0014\n",
      "Epoch [5/5], Step [110/1685], Loss: 0.0038\n",
      "Epoch [5/5], Step [120/1685], Loss: 0.0012\n",
      "Epoch [5/5], Step [130/1685], Loss: 0.0105\n",
      "Epoch [5/5], Step [140/1685], Loss: 0.0172\n",
      "Epoch [5/5], Step [150/1685], Loss: 0.0075\n",
      "Epoch [5/5], Step [160/1685], Loss: 0.0010\n",
      "Epoch [5/5], Step [170/1685], Loss: 0.0940\n",
      "Epoch [5/5], Step [180/1685], Loss: 0.0050\n",
      "Epoch [5/5], Step [190/1685], Loss: 0.0123\n",
      "Epoch [5/5], Step [200/1685], Loss: 0.0251\n",
      "Epoch [5/5], Step [210/1685], Loss: 0.0023\n",
      "Epoch [5/5], Step [220/1685], Loss: 0.0044\n",
      "Epoch [5/5], Step [230/1685], Loss: 0.0238\n",
      "Epoch [5/5], Step [240/1685], Loss: 0.0019\n",
      "Epoch [5/5], Step [250/1685], Loss: 0.0044\n",
      "Epoch [5/5], Step [260/1685], Loss: 0.0258\n",
      "Epoch [5/5], Step [270/1685], Loss: 0.0115\n",
      "Epoch [5/5], Step [280/1685], Loss: 0.2126\n",
      "Epoch [5/5], Step [290/1685], Loss: 0.0794\n",
      "Epoch [5/5], Step [300/1685], Loss: 0.0031\n",
      "Epoch [5/5], Step [310/1685], Loss: 0.1041\n",
      "Epoch [5/5], Step [320/1685], Loss: 0.0118\n",
      "Epoch [5/5], Step [330/1685], Loss: 0.0081\n",
      "Epoch [5/5], Step [340/1685], Loss: 0.3080\n",
      "Epoch [5/5], Step [350/1685], Loss: 0.0233\n",
      "Epoch [5/5], Step [360/1685], Loss: 0.0088\n",
      "Epoch [5/5], Step [370/1685], Loss: 0.1687\n",
      "Epoch [5/5], Step [380/1685], Loss: 0.6883\n",
      "Epoch [5/5], Step [390/1685], Loss: 0.1238\n",
      "Epoch [5/5], Step [400/1685], Loss: 0.0038\n",
      "Epoch [5/5], Step [410/1685], Loss: 0.0010\n",
      "Epoch [5/5], Step [420/1685], Loss: 0.0051\n",
      "Epoch [5/5], Step [430/1685], Loss: 0.0656\n",
      "Epoch [5/5], Step [440/1685], Loss: 0.0041\n",
      "Epoch [5/5], Step [450/1685], Loss: 0.0180\n",
      "Epoch [5/5], Step [460/1685], Loss: 0.0031\n",
      "Epoch [5/5], Step [470/1685], Loss: 0.1096\n",
      "Epoch [5/5], Step [480/1685], Loss: 0.0378\n",
      "Epoch [5/5], Step [490/1685], Loss: 0.0014\n",
      "Epoch [5/5], Step [500/1685], Loss: 0.3719\n",
      "Epoch [5/5], Step [510/1685], Loss: 0.0054\n",
      "Epoch [5/5], Step [520/1685], Loss: 0.0049\n",
      "Epoch [5/5], Step [530/1685], Loss: 0.0013\n",
      "Epoch [5/5], Step [540/1685], Loss: 0.0684\n",
      "Epoch [5/5], Step [550/1685], Loss: 0.1309\n",
      "Epoch [5/5], Step [560/1685], Loss: 0.0022\n",
      "Epoch [5/5], Step [570/1685], Loss: 0.0412\n",
      "Epoch [5/5], Step [580/1685], Loss: 0.0008\n",
      "Epoch [5/5], Step [590/1685], Loss: 0.0234\n",
      "Epoch [5/5], Step [600/1685], Loss: 0.0345\n",
      "Epoch [5/5], Step [610/1685], Loss: 0.0015\n",
      "Epoch [5/5], Step [620/1685], Loss: 0.2391\n",
      "Epoch [5/5], Step [630/1685], Loss: 0.0260\n",
      "Epoch [5/5], Step [640/1685], Loss: 0.0299\n",
      "Epoch [5/5], Step [650/1685], Loss: 0.0178\n",
      "Epoch [5/5], Step [660/1685], Loss: 0.0104\n",
      "Epoch [5/5], Step [670/1685], Loss: 0.0170\n",
      "Epoch [5/5], Step [680/1685], Loss: 0.0050\n",
      "Epoch [5/5], Step [690/1685], Loss: 0.0037\n",
      "Epoch [5/5], Step [700/1685], Loss: 0.0085\n",
      "Epoch [5/5], Step [710/1685], Loss: 0.0019\n",
      "Epoch [5/5], Step [720/1685], Loss: 0.0069\n",
      "Epoch [5/5], Step [730/1685], Loss: 0.0134\n",
      "Epoch [5/5], Step [740/1685], Loss: 0.0288\n",
      "Epoch [5/5], Step [750/1685], Loss: 0.0638\n",
      "Epoch [5/5], Step [760/1685], Loss: 0.0685\n",
      "Epoch [5/5], Step [770/1685], Loss: 0.0079\n",
      "Epoch [5/5], Step [780/1685], Loss: 0.0144\n",
      "Epoch [5/5], Step [790/1685], Loss: 0.0052\n",
      "Epoch [5/5], Step [800/1685], Loss: 0.0037\n",
      "Epoch [5/5], Step [810/1685], Loss: 0.0181\n",
      "Epoch [5/5], Step [820/1685], Loss: 0.0267\n",
      "Epoch [5/5], Step [830/1685], Loss: 0.0078\n",
      "Epoch [5/5], Step [840/1685], Loss: 0.0061\n",
      "Epoch [5/5], Step [850/1685], Loss: 0.0010\n",
      "Epoch [5/5], Step [860/1685], Loss: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [870/1685], Loss: 0.0019\n",
      "Epoch [5/5], Step [880/1685], Loss: 0.1416\n",
      "Epoch [5/5], Step [890/1685], Loss: 0.0650\n",
      "Epoch [5/5], Step [900/1685], Loss: 0.0016\n",
      "Epoch [5/5], Step [910/1685], Loss: 0.0013\n",
      "Epoch [5/5], Step [920/1685], Loss: 0.3039\n",
      "Epoch [5/5], Step [930/1685], Loss: 0.0059\n",
      "Epoch [5/5], Step [940/1685], Loss: 0.0035\n",
      "Epoch [5/5], Step [950/1685], Loss: 0.3044\n",
      "Epoch [5/5], Step [960/1685], Loss: 0.0337\n",
      "Epoch [5/5], Step [970/1685], Loss: 0.0417\n",
      "Epoch [5/5], Step [980/1685], Loss: 0.0041\n",
      "Epoch [5/5], Step [990/1685], Loss: 0.3784\n",
      "Epoch [5/5], Step [1000/1685], Loss: 0.0877\n",
      "Epoch [5/5], Step [1010/1685], Loss: 0.2024\n",
      "Epoch [5/5], Step [1020/1685], Loss: 0.0182\n",
      "Epoch [5/5], Step [1030/1685], Loss: 0.0161\n",
      "Epoch [5/5], Step [1040/1685], Loss: 0.0025\n",
      "Epoch [5/5], Step [1050/1685], Loss: 0.0024\n",
      "Epoch [5/5], Step [1060/1685], Loss: 0.0027\n",
      "Epoch [5/5], Step [1070/1685], Loss: 0.4565\n",
      "Epoch [5/5], Step [1080/1685], Loss: 0.0083\n",
      "Epoch [5/5], Step [1090/1685], Loss: 0.0057\n",
      "Epoch [5/5], Step [1100/1685], Loss: 0.0126\n",
      "Epoch [5/5], Step [1110/1685], Loss: 0.0057\n",
      "Epoch [5/5], Step [1120/1685], Loss: 0.0234\n",
      "Epoch [5/5], Step [1130/1685], Loss: 0.0016\n",
      "Epoch [5/5], Step [1140/1685], Loss: 0.0009\n",
      "Epoch [5/5], Step [1150/1685], Loss: 0.0115\n",
      "Epoch [5/5], Step [1160/1685], Loss: 0.2330\n",
      "Epoch [5/5], Step [1170/1685], Loss: 0.0107\n",
      "Epoch [5/5], Step [1180/1685], Loss: 0.0084\n",
      "Epoch [5/5], Step [1190/1685], Loss: 0.0159\n",
      "Epoch [5/5], Step [1200/1685], Loss: 0.0028\n",
      "Epoch [5/5], Step [1210/1685], Loss: 0.2953\n",
      "Epoch [5/5], Step [1220/1685], Loss: 0.0296\n",
      "Epoch [5/5], Step [1230/1685], Loss: 0.2262\n",
      "Epoch [5/5], Step [1240/1685], Loss: 0.0039\n",
      "Epoch [5/5], Step [1250/1685], Loss: 0.0086\n",
      "Epoch [5/5], Step [1260/1685], Loss: 0.0081\n",
      "Epoch [5/5], Step [1270/1685], Loss: 0.0039\n",
      "Epoch [5/5], Step [1280/1685], Loss: 0.2454\n",
      "Epoch [5/5], Step [1290/1685], Loss: 0.0146\n",
      "Epoch [5/5], Step [1300/1685], Loss: 0.0136\n",
      "Epoch [5/5], Step [1310/1685], Loss: 0.0035\n",
      "Epoch [5/5], Step [1320/1685], Loss: 0.0050\n",
      "Epoch [5/5], Step [1330/1685], Loss: 0.0020\n",
      "Epoch [5/5], Step [1340/1685], Loss: 0.0155\n",
      "Epoch [5/5], Step [1350/1685], Loss: 0.0063\n",
      "Epoch [5/5], Step [1360/1685], Loss: 0.0378\n",
      "Epoch [5/5], Step [1370/1685], Loss: 0.0072\n",
      "Epoch [5/5], Step [1380/1685], Loss: 0.1827\n",
      "Epoch [5/5], Step [1390/1685], Loss: 0.0018\n",
      "Epoch [5/5], Step [1400/1685], Loss: 0.3580\n",
      "Epoch [5/5], Step [1410/1685], Loss: 0.2580\n",
      "Epoch [5/5], Step [1420/1685], Loss: 0.0697\n",
      "Epoch [5/5], Step [1430/1685], Loss: 0.2055\n",
      "Epoch [5/5], Step [1440/1685], Loss: 0.0004\n",
      "Epoch [5/5], Step [1450/1685], Loss: 0.0122\n",
      "Epoch [5/5], Step [1460/1685], Loss: 0.0036\n",
      "Epoch [5/5], Step [1470/1685], Loss: 0.2860\n",
      "Epoch [5/5], Step [1480/1685], Loss: 0.0109\n",
      "Epoch [5/5], Step [1490/1685], Loss: 0.0129\n",
      "Epoch [5/5], Step [1500/1685], Loss: 0.0143\n",
      "Epoch [5/5], Step [1510/1685], Loss: 0.0072\n",
      "Epoch [5/5], Step [1520/1685], Loss: 0.1096\n",
      "Epoch [5/5], Step [1530/1685], Loss: 0.0018\n",
      "Epoch [5/5], Step [1540/1685], Loss: 0.0644\n",
      "Epoch [5/5], Step [1550/1685], Loss: 0.0038\n",
      "Epoch [5/5], Step [1560/1685], Loss: 0.0017\n",
      "Epoch [5/5], Step [1570/1685], Loss: 0.0152\n",
      "Epoch [5/5], Step [1580/1685], Loss: 0.0307\n",
      "Epoch [5/5], Step [1590/1685], Loss: 0.3986\n",
      "Epoch [5/5], Step [1600/1685], Loss: 0.0014\n",
      "Epoch [5/5], Step [1610/1685], Loss: 0.0036\n",
      "Epoch [5/5], Step [1620/1685], Loss: 0.0031\n",
      "Epoch [5/5], Step [1630/1685], Loss: 0.0295\n",
      "Epoch [5/5], Step [1640/1685], Loss: 0.0106\n",
      "Epoch [5/5], Step [1650/1685], Loss: 0.0020\n",
      "Epoch [5/5], Step [1660/1685], Loss: 0.0011\n",
      "Epoch [5/5], Step [1670/1685], Loss: 0.1255\n",
      "Epoch [5/5], Step [1680/1685], Loss: 0.0624\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8bwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3ce744c910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17min 2s, sys: 4min 8s, total: 21min 10s\n",
      "Wall time: 19min 11s\n"
     ]
    }
   ],
   "source": [
    "%time train_vgg16()\n",
    "torch.save(vgg16.state_dict(), 'vgg16.pkl')\n",
    "%time train_resnet18()\n",
    "torch.save(resnet18.state_dict(), 'resnet18.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Once finetuning is done we need to test it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images)\n",
    "        \n",
    "        if(use_gpu):\n",
    "            images = images.cuda()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "    print('Accuracy of the network on the ' + str(total) +' test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1870 test images: 95 %\n",
      "CPU times: user 36.7 s, sys: 9.96 s, total: 46.7 s\n",
      "Wall time: 44.1 s\n",
      "Accuracy of the network on the 1870 test images: 31 %\n",
      "CPU times: user 10.5 s, sys: 2.44 s, total: 13 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%time test(vgg16)\n",
    "%time test(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add more code to save the models if you want but otherwise this notebook is complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
