{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Datasets and Finetuning Pre-trained Networks\n",
    "In this notebook you have to create custom datasets for PyTorch and use this dataset to finetune certain pre-trained neural networks and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "#\n",
    "# Several of the imports you will need have been added but you will need to provide the\n",
    "# rest yourself; you should be able to figure out most of the imports as you go through\n",
    "# the notebook since without proper imports your code will fail to run\n",
    "#\n",
    "# All import statements go in this block\n",
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import torchvision\n",
    "import PIL.Image\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All hyper parameters go in the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Datasets\n",
    "Your first task is to create a pipeline for the custom dataset so that you can load it using a dataloader. Download the dataset provided in the assignment webpage and complete the following block of code so that you can load it as if it was a standard dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CDATA(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # root_dir  - the root directory of the dataset\n",
    "        # train     - a boolean parameter representing whether to return the training set or the test set\n",
    "        # transform - the transforms to be applied on the images before returning them\n",
    "        #\n",
    "        # In this function store the parameters in instance variables and make a mapping\n",
    "        # from images to labels and keep it as an instance variable. Make sure to check which\n",
    "        # dataset is required; train or test; and create the mapping accordingly.\n",
    "        if(train):\n",
    "            dir = root_dir + '/train'\n",
    "        else :\n",
    "            dir = root_dir + '/test'\n",
    "        self.transform = transform\n",
    "        self.img = []\n",
    "        self.label = []\n",
    "        for file_path in glob.glob(dir+'/*/*.png'):\n",
    "            image = PIL.Image.open(file_path)\n",
    "            self.img.append(image.convert('RGB'))\n",
    "            self.label.append(ord(file_path.split('/')[-2]) - ord('A')) #ord makes A,B,C.. to 0,1,2,.. respectively\n",
    "            \n",
    "    def __len__(self):\n",
    "        # return the size of the dataset (total number of images) as an integer\n",
    "        # this should be rather easy if you created a mapping in __init__\n",
    "        return len(self.img)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # idx - the index of the sample requested\n",
    "        #\n",
    "        # Open the image correspoding to idx, apply transforms on it and return a tuple (image, label)\n",
    "        # where label is an integer from 0-9 (since notMNIST has 10 classes)\n",
    "        if self.transform is None:\n",
    "            return (self.img[idx],self.label[idx])\n",
    "        else:\n",
    "            img_transformed = self.transform(self.img[idx])\n",
    "            return (img_transformed,self.label[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now load the dataset. You just need to supply the `root_dir` in the block below and if you implemented the above block correctly, it should work without any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# train_dataset = CDATA(root_dir='./notMNIST_small', train=True, transform=None)\n",
    "# img,label = train_dataset.__getitem__(10100)\n",
    "# print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train dataset: 16854\n",
      "Size of test dataset: 1870\n",
      "Train images\n",
      "Test images\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAADHCAYAAADvY2N4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTTOmYzEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6e711362d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "composed_transform = transforms.Compose([transforms.Scale((224,224)),transforms.ToTensor()])\n",
    "train_dataset = CDATA(root_dir='./notMNIST_small', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = CDATA(root_dir='./notMNIST_small', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "# Let's check the size of the datasets, if implemented correctly they should be 16854 and 1870 respectively\n",
    "print('Size of train dataset: %d' % len(train_dataset))\n",
    "print('Size of test dataset: %d' % len(test_dataset))\n",
    "\n",
    "# Create loaders for the dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Let's look at one batch of train and test images\n",
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "train_dataiter = iter(train_loader)\n",
    "train_images, train_labels = train_dataiter.next()\n",
    "print(\"Train images\")\n",
    "imshow(torchvision.utils.make_grid(train_images))\n",
    "\n",
    "test_dataiter = iter(test_loader)\n",
    "test_images, test_labels = test_dataiter.next()\n",
    "print(\"Test images\")\n",
    "imshow(torchvision.utils.make_grid(test_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 and Resnet-18\n",
    "Now that you have created the dataset we can use it for training and testing neural networks. VGG-16 and Resnet-18 are both well-known deep-net architectures. VGG-16 is named as such since it has 16 layers in total (13 convolution and 3 fully-connected). Resnet-18 on the other hand is a Resnet architecture that uses skip-connections. PyTorch provides pre-trained models of both these architectures and we shall be using them directly. If you are interested in knowing how they have been defined do take a look at the source, [VGG](https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py), [Resnet](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Code to change the last layers so that they only have 10 classes as output\n",
    "vgg16.classifier = nn.Sequential(\n",
    "    nn.Linear(512 * 7 * 7, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(True),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(4096, 10),\n",
    ")\n",
    "resnet18.fc = nn.Linear(resnet18.fc.in_features, 10)\n",
    "\n",
    "# Add code for using CUDA here if it is available\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    vgg16.cuda()\n",
    "    resnet18.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()# Define cross-entropy loss\n",
    "optimizer_vgg16 = torch.optim.Adam(vgg16.parameters(), lr=learning_rate)# Use Adam optimizer, use learning_rate hyper parameter\n",
    "optimizer_resnet18 = torch.optim.Adam(resnet18.parameters(), lr=learning_rate)# Use Adam optimizer, use learning_rate hyper parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning\n",
    "Finetuning is nothing but training models after their weights have been loaded. This allows us to start at a better position than training from scratch. Since the models created already have weights loaded, you simply need to write a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_vgg16():\n",
    "    # Write loops so as to train the model for N epochs, use num_epochs hyper parameter\n",
    "    # Train/finetune the VGG-16 network\n",
    "    # Store the losses for every epoch and generate a graph using matplotlib\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if(use_gpu):\n",
    "                images=images.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer_vgg16.zero_grad()  # zero the gradient buffer\n",
    "            outputs = vgg16(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_vgg16.step()\n",
    "            if (i+1) % batch_size == 0:\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    \n",
    "def train_resnet18():\n",
    "    # Same as above except now using the Resnet-18 network\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if(use_gpu):\n",
    "                images=images.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer_resnet18.zero_grad()  # zero the gradient buffer\n",
    "            outputs = resnet18(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer_resnet18.step()\n",
    "            if (i+1) % batch_size == 0:\n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us start the training/finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2/8427], Loss: 2.4374\n",
      "Epoch [1/5], Step [4/8427], Loss: 2.5287\n",
      "Epoch [1/5], Step [6/8427], Loss: 2.5667\n",
      "Epoch [1/5], Step [8/8427], Loss: 1.8923\n",
      "Epoch [1/5], Step [10/8427], Loss: 2.5046\n",
      "Epoch [1/5], Step [12/8427], Loss: 2.2757\n",
      "Epoch [1/5], Step [14/8427], Loss: 2.1004\n",
      "Epoch [1/5], Step [16/8427], Loss: 1.6551\n",
      "Epoch [1/5], Step [18/8427], Loss: 2.2935\n",
      "Epoch [1/5], Step [20/8427], Loss: 2.1512\n",
      "Epoch [1/5], Step [22/8427], Loss: 2.5595\n",
      "Epoch [1/5], Step [24/8427], Loss: 1.7083\n",
      "Epoch [1/5], Step [26/8427], Loss: 2.7490\n",
      "Epoch [1/5], Step [28/8427], Loss: 1.3301\n",
      "Epoch [1/5], Step [30/8427], Loss: 1.7037\n",
      "Epoch [1/5], Step [32/8427], Loss: 1.8923\n",
      "Epoch [1/5], Step [34/8427], Loss: 2.5736\n",
      "Epoch [1/5], Step [36/8427], Loss: 2.1615\n",
      "Epoch [1/5], Step [38/8427], Loss: 1.9640\n",
      "Epoch [1/5], Step [40/8427], Loss: 1.8615\n",
      "Epoch [1/5], Step [42/8427], Loss: 1.3782\n",
      "Epoch [1/5], Step [44/8427], Loss: 1.6080\n",
      "Epoch [1/5], Step [46/8427], Loss: 0.3345\n",
      "Epoch [1/5], Step [48/8427], Loss: 2.5661\n",
      "Epoch [1/5], Step [50/8427], Loss: 0.5295\n",
      "Epoch [1/5], Step [52/8427], Loss: 1.2033\n",
      "Epoch [1/5], Step [54/8427], Loss: 1.3749\n",
      "Epoch [1/5], Step [56/8427], Loss: 0.8200\n",
      "Epoch [1/5], Step [58/8427], Loss: 1.5763\n",
      "Epoch [1/5], Step [60/8427], Loss: 0.2486\n",
      "Epoch [1/5], Step [62/8427], Loss: 0.1823\n",
      "Epoch [1/5], Step [64/8427], Loss: 0.0470\n",
      "Epoch [1/5], Step [66/8427], Loss: 0.8490\n",
      "Epoch [1/5], Step [68/8427], Loss: 1.4256\n",
      "Epoch [1/5], Step [70/8427], Loss: 1.8598\n",
      "Epoch [1/5], Step [72/8427], Loss: 0.2609\n",
      "Epoch [1/5], Step [74/8427], Loss: 2.6902\n",
      "Epoch [1/5], Step [76/8427], Loss: 0.9935\n",
      "Epoch [1/5], Step [78/8427], Loss: 1.9445\n",
      "Epoch [1/5], Step [80/8427], Loss: 0.1162\n",
      "Epoch [1/5], Step [82/8427], Loss: 0.2551\n",
      "Epoch [1/5], Step [84/8427], Loss: 0.5562\n",
      "Epoch [1/5], Step [86/8427], Loss: 2.7796\n",
      "Epoch [1/5], Step [88/8427], Loss: 0.1694\n",
      "Epoch [1/5], Step [90/8427], Loss: 0.0868\n",
      "Epoch [1/5], Step [92/8427], Loss: 4.0388\n",
      "Epoch [1/5], Step [94/8427], Loss: 0.1647\n",
      "Epoch [1/5], Step [96/8427], Loss: 0.1624\n",
      "Epoch [1/5], Step [98/8427], Loss: 0.6047\n",
      "Epoch [1/5], Step [100/8427], Loss: 2.3953\n",
      "Epoch [1/5], Step [102/8427], Loss: 2.2654\n",
      "Epoch [1/5], Step [104/8427], Loss: 2.4526\n",
      "Epoch [1/5], Step [106/8427], Loss: 0.6896\n",
      "Epoch [1/5], Step [108/8427], Loss: 1.0694\n",
      "Epoch [1/5], Step [110/8427], Loss: 0.9814\n",
      "Epoch [1/5], Step [112/8427], Loss: 1.6521\n",
      "Epoch [1/5], Step [114/8427], Loss: 1.4379\n",
      "Epoch [1/5], Step [116/8427], Loss: 1.1328\n",
      "Epoch [1/5], Step [118/8427], Loss: 0.6302\n",
      "Epoch [1/5], Step [120/8427], Loss: 0.5706\n",
      "Epoch [1/5], Step [122/8427], Loss: 0.9285\n",
      "Epoch [1/5], Step [124/8427], Loss: 0.3025\n",
      "Epoch [1/5], Step [126/8427], Loss: 0.2096\n",
      "Epoch [1/5], Step [128/8427], Loss: 0.6137\n",
      "Epoch [1/5], Step [130/8427], Loss: 2.6137\n",
      "Epoch [1/5], Step [132/8427], Loss: 0.0735\n",
      "Epoch [1/5], Step [134/8427], Loss: 1.3753\n",
      "Epoch [1/5], Step [136/8427], Loss: 1.4203\n",
      "Epoch [1/5], Step [138/8427], Loss: 0.0537\n",
      "Epoch [1/5], Step [140/8427], Loss: 0.7246\n",
      "Epoch [1/5], Step [142/8427], Loss: 0.3481\n",
      "Epoch [1/5], Step [144/8427], Loss: 0.0862\n",
      "Epoch [1/5], Step [146/8427], Loss: 2.5857\n",
      "Epoch [1/5], Step [148/8427], Loss: 0.1158\n",
      "Epoch [1/5], Step [150/8427], Loss: 0.4872\n",
      "Epoch [1/5], Step [152/8427], Loss: 0.2369\n",
      "Epoch [1/5], Step [154/8427], Loss: 0.0619\n",
      "Epoch [1/5], Step [156/8427], Loss: 1.5434\n",
      "Epoch [1/5], Step [158/8427], Loss: 0.0400\n",
      "Epoch [1/5], Step [160/8427], Loss: 0.3820\n",
      "Epoch [1/5], Step [162/8427], Loss: 2.2033\n",
      "Epoch [1/5], Step [164/8427], Loss: 0.0561\n",
      "Epoch [1/5], Step [166/8427], Loss: 2.1486\n",
      "Epoch [1/5], Step [168/8427], Loss: 0.9785\n",
      "Epoch [1/5], Step [170/8427], Loss: 1.4650\n",
      "Epoch [1/5], Step [172/8427], Loss: 0.9673\n",
      "Epoch [1/5], Step [174/8427], Loss: 0.3610\n",
      "Epoch [1/5], Step [176/8427], Loss: 0.0940\n",
      "Epoch [1/5], Step [178/8427], Loss: 1.4869\n",
      "Epoch [1/5], Step [180/8427], Loss: 0.4468\n",
      "Epoch [1/5], Step [182/8427], Loss: 0.2371\n",
      "Epoch [1/5], Step [184/8427], Loss: 1.6299\n",
      "Epoch [1/5], Step [186/8427], Loss: 0.0883\n",
      "Epoch [1/5], Step [188/8427], Loss: 0.3827\n",
      "Epoch [1/5], Step [190/8427], Loss: 1.5242\n",
      "Epoch [1/5], Step [192/8427], Loss: 0.0625\n",
      "Epoch [1/5], Step [194/8427], Loss: 0.0024\n",
      "Epoch [1/5], Step [196/8427], Loss: 0.5313\n",
      "Epoch [1/5], Step [198/8427], Loss: 0.0658\n",
      "Epoch [1/5], Step [200/8427], Loss: 1.8400\n",
      "Epoch [1/5], Step [202/8427], Loss: 2.3024\n",
      "Epoch [1/5], Step [204/8427], Loss: 0.5268\n",
      "Epoch [1/5], Step [206/8427], Loss: 1.2650\n",
      "Epoch [1/5], Step [208/8427], Loss: 1.4355\n",
      "Epoch [1/5], Step [210/8427], Loss: 2.6539\n",
      "Epoch [1/5], Step [212/8427], Loss: 0.0013\n",
      "Epoch [1/5], Step [214/8427], Loss: 0.0099\n",
      "Epoch [1/5], Step [216/8427], Loss: 0.0267\n",
      "Epoch [1/5], Step [218/8427], Loss: 1.0042\n",
      "Epoch [1/5], Step [220/8427], Loss: 0.3589\n",
      "Epoch [1/5], Step [222/8427], Loss: 0.6095\n",
      "Epoch [1/5], Step [224/8427], Loss: 1.5994\n",
      "Epoch [1/5], Step [226/8427], Loss: 0.5184\n",
      "Epoch [1/5], Step [228/8427], Loss: 0.2446\n",
      "Epoch [1/5], Step [230/8427], Loss: 0.0233\n",
      "Epoch [1/5], Step [232/8427], Loss: 0.1335\n",
      "Epoch [1/5], Step [234/8427], Loss: 1.8373\n",
      "Epoch [1/5], Step [236/8427], Loss: 1.4696\n",
      "Epoch [1/5], Step [238/8427], Loss: 1.3887\n",
      "Epoch [1/5], Step [240/8427], Loss: 0.0246\n",
      "Epoch [1/5], Step [242/8427], Loss: 0.1006\n",
      "Epoch [1/5], Step [244/8427], Loss: 1.1412\n",
      "Epoch [1/5], Step [246/8427], Loss: 0.0021\n",
      "Epoch [1/5], Step [248/8427], Loss: 0.5110\n",
      "Epoch [1/5], Step [250/8427], Loss: 0.8620\n",
      "Epoch [1/5], Step [252/8427], Loss: 0.3926\n",
      "Epoch [1/5], Step [254/8427], Loss: 0.2747\n",
      "Epoch [1/5], Step [256/8427], Loss: 0.0199\n",
      "Epoch [1/5], Step [258/8427], Loss: 1.2149\n",
      "Epoch [1/5], Step [260/8427], Loss: 0.5039\n",
      "Epoch [1/5], Step [262/8427], Loss: 0.8340\n",
      "Epoch [1/5], Step [264/8427], Loss: 11.6840\n",
      "Epoch [1/5], Step [266/8427], Loss: 0.4117\n",
      "Epoch [1/5], Step [268/8427], Loss: 0.0000\n",
      "Epoch [1/5], Step [270/8427], Loss: 0.5399\n",
      "Epoch [1/5], Step [272/8427], Loss: 0.4022\n",
      "Epoch [1/5], Step [274/8427], Loss: 2.1128\n",
      "Epoch [1/5], Step [276/8427], Loss: 0.1098\n",
      "Epoch [1/5], Step [278/8427], Loss: 0.8171\n",
      "Epoch [1/5], Step [280/8427], Loss: 0.0099\n",
      "Epoch [1/5], Step [282/8427], Loss: 1.1205\n",
      "Epoch [1/5], Step [284/8427], Loss: 0.0503\n",
      "Epoch [1/5], Step [286/8427], Loss: 1.1629\n",
      "Epoch [1/5], Step [288/8427], Loss: 0.9351\n",
      "Epoch [1/5], Step [290/8427], Loss: 1.5837\n",
      "Epoch [1/5], Step [292/8427], Loss: 0.1100\n",
      "Epoch [1/5], Step [294/8427], Loss: 0.0007\n",
      "Epoch [1/5], Step [296/8427], Loss: 0.2685\n",
      "Epoch [1/5], Step [298/8427], Loss: 0.0845\n",
      "Epoch [1/5], Step [300/8427], Loss: 1.0615\n",
      "Epoch [1/5], Step [302/8427], Loss: 1.3030\n",
      "Epoch [1/5], Step [304/8427], Loss: 0.4203\n",
      "Epoch [1/5], Step [306/8427], Loss: 0.2306\n",
      "Epoch [1/5], Step [308/8427], Loss: 0.2324\n",
      "Epoch [1/5], Step [310/8427], Loss: 0.2387\n",
      "Epoch [1/5], Step [312/8427], Loss: 0.6532\n",
      "Epoch [1/5], Step [314/8427], Loss: 1.7879\n",
      "Epoch [1/5], Step [316/8427], Loss: 0.0825\n",
      "Epoch [1/5], Step [318/8427], Loss: 0.0017\n",
      "Epoch [1/5], Step [320/8427], Loss: 0.7958\n",
      "Epoch [1/5], Step [322/8427], Loss: 0.0683\n",
      "Epoch [1/5], Step [324/8427], Loss: 0.1247\n",
      "Epoch [1/5], Step [326/8427], Loss: 0.0058\n",
      "Epoch [1/5], Step [328/8427], Loss: 0.7525\n",
      "Epoch [1/5], Step [330/8427], Loss: 0.0218\n",
      "Epoch [1/5], Step [332/8427], Loss: 0.1529\n",
      "Epoch [1/5], Step [334/8427], Loss: 0.0239\n",
      "Epoch [1/5], Step [336/8427], Loss: 0.0025\n",
      "Epoch [1/5], Step [338/8427], Loss: 0.2378\n",
      "Epoch [1/5], Step [340/8427], Loss: 0.0000\n",
      "Epoch [1/5], Step [342/8427], Loss: 0.0136\n",
      "Epoch [1/5], Step [344/8427], Loss: 0.0013\n",
      "Epoch [1/5], Step [346/8427], Loss: 0.4069\n",
      "Epoch [1/5], Step [348/8427], Loss: 1.7954\n",
      "Epoch [1/5], Step [350/8427], Loss: 0.3139\n",
      "Epoch [1/5], Step [352/8427], Loss: 0.0641\n",
      "Epoch [1/5], Step [354/8427], Loss: 0.0386\n",
      "Epoch [1/5], Step [356/8427], Loss: 0.0050\n",
      "Epoch [1/5], Step [358/8427], Loss: 0.5858\n",
      "Epoch [1/5], Step [360/8427], Loss: 0.0088\n",
      "Epoch [1/5], Step [362/8427], Loss: 0.0006\n",
      "Epoch [1/5], Step [364/8427], Loss: 0.0057\n",
      "Epoch [1/5], Step [366/8427], Loss: 1.6712\n",
      "Epoch [1/5], Step [368/8427], Loss: 0.0006\n",
      "Epoch [1/5], Step [370/8427], Loss: 3.4634\n",
      "Epoch [1/5], Step [372/8427], Loss: 0.2922\n",
      "Epoch [1/5], Step [374/8427], Loss: 0.0064\n",
      "Epoch [1/5], Step [376/8427], Loss: 0.0251\n",
      "Epoch [1/5], Step [378/8427], Loss: 0.3979\n",
      "Epoch [1/5], Step [380/8427], Loss: 0.0791\n",
      "Epoch [1/5], Step [382/8427], Loss: 0.6780\n",
      "Epoch [1/5], Step [384/8427], Loss: 1.9498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [386/8427], Loss: 0.0040\n",
      "Epoch [1/5], Step [388/8427], Loss: 0.3099\n",
      "Epoch [1/5], Step [390/8427], Loss: 0.0197\n",
      "Epoch [1/5], Step [392/8427], Loss: 5.9081\n",
      "Epoch [1/5], Step [394/8427], Loss: 0.0212\n",
      "Epoch [1/5], Step [396/8427], Loss: 0.4711\n",
      "Epoch [1/5], Step [398/8427], Loss: 0.0703\n",
      "Epoch [1/5], Step [400/8427], Loss: 0.0017\n",
      "Epoch [1/5], Step [402/8427], Loss: 0.0025\n",
      "Epoch [1/5], Step [404/8427], Loss: 1.5550\n",
      "Epoch [1/5], Step [406/8427], Loss: 2.3523\n",
      "Epoch [1/5], Step [408/8427], Loss: 3.5203\n",
      "Epoch [1/5], Step [410/8427], Loss: 1.3036\n",
      "Epoch [1/5], Step [412/8427], Loss: 0.0605\n",
      "Epoch [1/5], Step [414/8427], Loss: 2.3299\n",
      "Epoch [1/5], Step [416/8427], Loss: 2.3240\n",
      "Epoch [1/5], Step [418/8427], Loss: 1.8716\n",
      "Epoch [1/5], Step [420/8427], Loss: 0.1540\n",
      "Epoch [1/5], Step [422/8427], Loss: 0.0825\n",
      "Epoch [1/5], Step [424/8427], Loss: 0.0236\n",
      "Epoch [1/5], Step [426/8427], Loss: 0.6947\n",
      "Epoch [1/5], Step [428/8427], Loss: 0.0703\n",
      "Epoch [1/5], Step [430/8427], Loss: 0.3542\n",
      "Epoch [1/5], Step [432/8427], Loss: 0.5356\n",
      "Epoch [1/5], Step [434/8427], Loss: 0.7245\n",
      "Epoch [1/5], Step [436/8427], Loss: 0.0001\n",
      "Epoch [1/5], Step [438/8427], Loss: 0.0005\n",
      "Epoch [1/5], Step [440/8427], Loss: 0.0006\n",
      "Epoch [1/5], Step [442/8427], Loss: 0.0005\n",
      "Epoch [1/5], Step [444/8427], Loss: 0.0095\n",
      "Epoch [1/5], Step [446/8427], Loss: 0.0001\n",
      "Epoch [1/5], Step [448/8427], Loss: 0.3981\n",
      "Epoch [1/5], Step [450/8427], Loss: 1.5558\n",
      "Epoch [1/5], Step [452/8427], Loss: 0.7834\n",
      "Epoch [1/5], Step [454/8427], Loss: 1.9406\n",
      "Epoch [1/5], Step [456/8427], Loss: 0.0050\n",
      "Epoch [1/5], Step [458/8427], Loss: 0.1055\n",
      "Epoch [1/5], Step [460/8427], Loss: 0.0008\n",
      "Epoch [1/5], Step [462/8427], Loss: 1.5215\n",
      "Epoch [1/5], Step [464/8427], Loss: 0.0205\n",
      "Epoch [1/5], Step [466/8427], Loss: 0.1132\n",
      "Epoch [1/5], Step [468/8427], Loss: 0.1459\n",
      "Epoch [1/5], Step [470/8427], Loss: 0.0107\n",
      "Epoch [1/5], Step [472/8427], Loss: 0.0098\n",
      "Epoch [1/5], Step [474/8427], Loss: 0.0055\n",
      "Epoch [1/5], Step [476/8427], Loss: 3.5381\n",
      "Epoch [1/5], Step [478/8427], Loss: 0.0009\n",
      "Epoch [1/5], Step [480/8427], Loss: 0.0101\n",
      "Epoch [1/5], Step [482/8427], Loss: 1.2694\n",
      "Epoch [1/5], Step [484/8427], Loss: 1.0663\n",
      "Epoch [1/5], Step [486/8427], Loss: 0.1007\n",
      "Epoch [1/5], Step [488/8427], Loss: 0.0022\n"
     ]
    }
   ],
   "source": [
    "%time train_vgg16()\n",
    "%time train_resnet18()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Once finetuning is done we need to test it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images)\n",
    "        \n",
    "        if(use_gpu):\n",
    "            images = images.cuda()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "    print('Accuracy of the network on the ' + total +' test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%time test(vgg16)\n",
    "%time test(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add more code to save the models if you want but otherwise this notebook is complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
