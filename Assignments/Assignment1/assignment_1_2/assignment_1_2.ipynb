{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Custom Networks\n",
    "In this notebook you have to create a custom network whose architecture has been given, and use the dataset you created earlier to train and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Statements\n",
    "#\n",
    "# Several of the imports you will need have been added but you will need to provide the\n",
    "# rest yourself; you should be able to figure out most of the imports as you go through\n",
    "# the notebook since without proper imports your code will fail to run\n",
    "#\n",
    "# All import statements go in this block\n",
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import glob\n",
    "import torchvision\n",
    "import PIL.Image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All hyper parameters go in the next block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_epochs = 5\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Custom Dataset and Loader\n",
    "This is the same as part 1. Simply use the same code to create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CDATA(torch.utils.data.Dataset): # Extend PyTorch's Dataset class\n",
    "    def __init__(self, root_dir, train, transform=None):\n",
    "        # root_dir  - the root directory of the dataset\n",
    "        # train     - a boolean parameter representing whether to return the training set or the test set\n",
    "        # transform - the transforms to be applied on the images before returning them\n",
    "        #\n",
    "        # In this function store the parameters in instance variables and make a mapping\n",
    "        # from images to labels and keep it as an instance variable. Make sure to check which\n",
    "        # dataset is required; train or test; and create the mapping accordingly.\n",
    "        if(train):\n",
    "            dir = root_dir + '/train'\n",
    "        else :\n",
    "            dir = root_dir + '/test'\n",
    "        self.transform = transform\n",
    "        self.img = []\n",
    "        self.label = []\n",
    "        for file_path in glob.glob(dir+'/*/*.png'):\n",
    "            image = PIL.Image.open(file_path)\n",
    "            self.img.append(image.convert('RGB'))\n",
    "            self.label.append(ord(file_path.split('/')[-2]) - ord('A')) #ord makes A,B,C.. to 0,1,2,.. respectively\n",
    "            \n",
    "    def __len__(self):\n",
    "        # return the size of the dataset (total number of images) as an integer\n",
    "        # this should be rather easy if you created a mapping in __init__\n",
    "        return len(self.img)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # idx - the index of the sample requested\n",
    "        #\n",
    "        # Open the image correspoding to idx, apply transforms on it and return a tuple (image, label)\n",
    "        # where label is an integer from 0-9 (since notMNIST has 10 classes)\n",
    "        if self.transform is None:\n",
    "            return (self.img[idx],self.label[idx])\n",
    "        else:\n",
    "            img_transformed = self.transform(self.img[idx])\n",
    "            return (img_transformed,self.label[idx])\n",
    "    \n",
    "composed_transform = transforms.Compose([transforms.Scale((32,32)),transforms.ToTensor()])\n",
    "train_dataset = CDATA(root_dir='../notMNIST_small', train=True, transform=composed_transform) # Supply proper root_dir\n",
    "test_dataset = CDATA(root_dir='../notMNIST_small', train=False, transform=composed_transform) # Supply proper root_dir\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Network\n",
    "It's time to create a new custom network. This network is based on Resnet (indeed it is a resnet since it uses skip connections). The architecture of the network is provided in the diagram. It specifies the layer names, layer types as well as their parameters.\n",
    "<img src=\"architecture.png\" width=100>\n",
    "[Full size image](architecture.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomResnet(nn.Module): # Extend PyTorch's Module class\n",
    "    def __init__(self, num_classes = 10):\n",
    "        super(CustomResnet, self).__init__() # Must call super __init__()\n",
    "       \n",
    "        self.conv1 = nn.Conv2d(3,64,kernel_size=7,stride=2,padding=3,bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        \n",
    "        self.lyr1conv1 = nn.Conv2d(64,64,kernel_size=3,stride=1,padding=1,bias =True)\n",
    "        self.lyr1bn1 = nn.BatchNorm2d(64)\n",
    "        self.lyr1relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.lyr1conv2 = nn.Conv2d(64,64,kernel_size=3,stride=1,padding=1,bias = True)\n",
    "        self.lyr1bn2 = nn.BatchNorm2d(64)\n",
    "        self.lyr1relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.lyr2conv1 = nn.Conv2d(64,64,kernel_size=3,stride=1,padding=1,bias = True)\n",
    "        self.lyr2bn1 = nn.BatchNorm2d(64)\n",
    "        self.lyr2relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.lyr2conv2 = nn.Conv2d(64,64,kernel_size=3,stride=1,padding=1,bias =True)\n",
    "        self.lyr2bn2 = nn.BatchNorm2d(64)\n",
    "        self.lyr2relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        \n",
    "        self.fc = nn.Linear(4096,num_classes)\n",
    "        \n",
    "        # Define the layers of the network here\n",
    "        # There should be 17 total layers as evident from the diagram\n",
    "        # The parameters and names for the layers are provided in the diagram\n",
    "        # The variable names have to be the same as the ones in the diagram\n",
    "        # Otherwise, the weights will not load\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Here you have to define the forward pass\n",
    "        # Make sure you take care of the skip connections\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "#         print (x.size())\n",
    "        \n",
    "        temp = self.lyr1conv1(x)\n",
    "        temp = self.lyr1bn1(temp)\n",
    "        temp = self.lyr1relu1(temp)\n",
    "        temp = self.lyr1conv2(temp)\n",
    "        temp = self.lyr1bn2(temp)\n",
    "       \n",
    "       \n",
    "        x = self.lyr1relu2(temp + x) \n",
    "       \n",
    "        temp = self.lyr2conv1(x)\n",
    "        temp = self.lyr2bn1(temp)\n",
    "        temp = self.lyr2relu1(temp)\n",
    "        temp = self.lyr2conv2(temp)\n",
    "        temp = self.lyr2bn2(temp)\n",
    "       \n",
    "        x = self.lyr2relu2(temp + x)\n",
    "    \n",
    "        x= self.fc(x.view(-1,4096))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune on pre-trained CIFAR-100 weights\n",
    "We shall now finetune our model using pretrained CIFAR-100 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CustomResnet(num_classes = 100) # 100 classes since CIFAR-100 has 100 classes\n",
    "\n",
    "# Load CIFAR-100 weights. (Download them from assignment page)\n",
    "# If network was properly implemented, weights should load without any problems\n",
    "model.load_state_dict(torch.load('../CIFAR-100_weights')) # Supply the path to the weight file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional\n",
    "As a sanity check you may load the CIFAR-100 test dataset and test the above model. You should get an accuracy of ~41%. This part is optional and is meant for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Block for optionally running the model on CIFAR-100 test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finetune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change last layer to output 10 classes since our dataset has 10 classes\n",
    "# model.fc = nn.Linear(100, 10)# Complete this statement. It is similar to the resnet18 case\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "\n",
    "# Add code for using CUDA here if it is available\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    model.cuda()\n",
    "\n",
    "# Loss function and optimizers\n",
    "criterion = nn.CrossEntropyLoss()# Define cross-entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)# Use Adam optimizer, use learning_rate hyper parameter\n",
    "\n",
    "def train():\n",
    "    # Code for training the model\n",
    "    # Make sure to output a matplotlib graph of training losses\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Convert torch tensor to Variable\n",
    "            images = Variable(images)\n",
    "            labels = Variable(labels)\n",
    "            if(use_gpu):\n",
    "                images=images.cuda()\n",
    "                labels=labels.cuda()\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_arr.append(loss.data[0])\n",
    "            if (i+1) % batch_size == 0:       \n",
    "                print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                       %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))\n",
    "    \n",
    "    plt.plot( np.array(range(1,len(loss_arr)+1)), np.array(loss_arr))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [10/1685], Loss: 1.4218\n",
      "Epoch [1/5], Step [20/1685], Loss: 1.8422\n",
      "Epoch [1/5], Step [30/1685], Loss: 0.6770\n",
      "Epoch [1/5], Step [40/1685], Loss: 0.5182\n",
      "Epoch [1/5], Step [50/1685], Loss: 0.5723\n",
      "Epoch [1/5], Step [60/1685], Loss: 0.5402\n",
      "Epoch [1/5], Step [70/1685], Loss: 0.2804\n",
      "Epoch [1/5], Step [80/1685], Loss: 0.4188\n",
      "Epoch [1/5], Step [90/1685], Loss: 0.3125\n",
      "Epoch [1/5], Step [100/1685], Loss: 0.7396\n",
      "Epoch [1/5], Step [110/1685], Loss: 0.3790\n",
      "Epoch [1/5], Step [120/1685], Loss: 0.6113\n",
      "Epoch [1/5], Step [130/1685], Loss: 0.6799\n",
      "Epoch [1/5], Step [140/1685], Loss: 0.4393\n",
      "Epoch [1/5], Step [150/1685], Loss: 0.2033\n",
      "Epoch [1/5], Step [160/1685], Loss: 0.2594\n",
      "Epoch [1/5], Step [170/1685], Loss: 0.2721\n",
      "Epoch [1/5], Step [180/1685], Loss: 0.5298\n",
      "Epoch [1/5], Step [190/1685], Loss: 0.6627\n",
      "Epoch [1/5], Step [200/1685], Loss: 0.2873\n",
      "Epoch [1/5], Step [210/1685], Loss: 0.2240\n",
      "Epoch [1/5], Step [220/1685], Loss: 0.1417\n",
      "Epoch [1/5], Step [230/1685], Loss: 0.4211\n",
      "Epoch [1/5], Step [240/1685], Loss: 0.5124\n",
      "Epoch [1/5], Step [250/1685], Loss: 0.4151\n",
      "Epoch [1/5], Step [260/1685], Loss: 0.2662\n",
      "Epoch [1/5], Step [270/1685], Loss: 0.1239\n",
      "Epoch [1/5], Step [280/1685], Loss: 0.8058\n",
      "Epoch [1/5], Step [290/1685], Loss: 0.6853\n",
      "Epoch [1/5], Step [300/1685], Loss: 0.0742\n",
      "Epoch [1/5], Step [310/1685], Loss: 0.0454\n",
      "Epoch [1/5], Step [320/1685], Loss: 0.1924\n",
      "Epoch [1/5], Step [330/1685], Loss: 0.3961\n",
      "Epoch [1/5], Step [340/1685], Loss: 0.0979\n",
      "Epoch [1/5], Step [350/1685], Loss: 0.0416\n",
      "Epoch [1/5], Step [360/1685], Loss: 0.0414\n",
      "Epoch [1/5], Step [370/1685], Loss: 0.2437\n",
      "Epoch [1/5], Step [380/1685], Loss: 1.0106\n",
      "Epoch [1/5], Step [390/1685], Loss: 0.1598\n",
      "Epoch [1/5], Step [400/1685], Loss: 0.1006\n",
      "Epoch [1/5], Step [410/1685], Loss: 0.1434\n",
      "Epoch [1/5], Step [420/1685], Loss: 0.0589\n",
      "Epoch [1/5], Step [430/1685], Loss: 0.0688\n",
      "Epoch [1/5], Step [440/1685], Loss: 0.3682\n",
      "Epoch [1/5], Step [450/1685], Loss: 0.5425\n",
      "Epoch [1/5], Step [460/1685], Loss: 0.1825\n",
      "Epoch [1/5], Step [470/1685], Loss: 0.0498\n",
      "Epoch [1/5], Step [480/1685], Loss: 0.1945\n",
      "Epoch [1/5], Step [490/1685], Loss: 0.0084\n",
      "Epoch [1/5], Step [500/1685], Loss: 0.0104\n",
      "Epoch [1/5], Step [510/1685], Loss: 0.2260\n",
      "Epoch [1/5], Step [520/1685], Loss: 0.9119\n",
      "Epoch [1/5], Step [530/1685], Loss: 0.1091\n",
      "Epoch [1/5], Step [540/1685], Loss: 0.6354\n",
      "Epoch [1/5], Step [550/1685], Loss: 0.2704\n",
      "Epoch [1/5], Step [560/1685], Loss: 0.0448\n",
      "Epoch [1/5], Step [570/1685], Loss: 0.6875\n",
      "Epoch [1/5], Step [580/1685], Loss: 0.0361\n",
      "Epoch [1/5], Step [590/1685], Loss: 0.2246\n",
      "Epoch [1/5], Step [600/1685], Loss: 0.0550\n",
      "Epoch [1/5], Step [610/1685], Loss: 0.3405\n",
      "Epoch [1/5], Step [620/1685], Loss: 0.0165\n",
      "Epoch [1/5], Step [630/1685], Loss: 0.2260\n",
      "Epoch [1/5], Step [640/1685], Loss: 0.1466\n",
      "Epoch [1/5], Step [650/1685], Loss: 1.1924\n",
      "Epoch [1/5], Step [660/1685], Loss: 0.0101\n",
      "Epoch [1/5], Step [670/1685], Loss: 0.2557\n",
      "Epoch [1/5], Step [680/1685], Loss: 0.4063\n",
      "Epoch [1/5], Step [690/1685], Loss: 0.1676\n",
      "Epoch [1/5], Step [700/1685], Loss: 0.0489\n",
      "Epoch [1/5], Step [710/1685], Loss: 0.8255\n",
      "Epoch [1/5], Step [720/1685], Loss: 0.1089\n",
      "Epoch [1/5], Step [730/1685], Loss: 0.5877\n",
      "Epoch [1/5], Step [740/1685], Loss: 0.0186\n",
      "Epoch [1/5], Step [750/1685], Loss: 0.0213\n",
      "Epoch [1/5], Step [760/1685], Loss: 0.6011\n",
      "Epoch [1/5], Step [770/1685], Loss: 0.0564\n",
      "Epoch [1/5], Step [780/1685], Loss: 0.0549\n",
      "Epoch [1/5], Step [790/1685], Loss: 0.0563\n",
      "Epoch [1/5], Step [800/1685], Loss: 0.0407\n",
      "Epoch [1/5], Step [810/1685], Loss: 0.4316\n",
      "Epoch [1/5], Step [820/1685], Loss: 0.5937\n",
      "Epoch [1/5], Step [830/1685], Loss: 0.7133\n",
      "Epoch [1/5], Step [840/1685], Loss: 0.0158\n",
      "Epoch [1/5], Step [850/1685], Loss: 0.0161\n",
      "Epoch [1/5], Step [860/1685], Loss: 1.0309\n",
      "Epoch [1/5], Step [870/1685], Loss: 0.5222\n",
      "Epoch [1/5], Step [880/1685], Loss: 0.0253\n",
      "Epoch [1/5], Step [890/1685], Loss: 0.6542\n",
      "Epoch [1/5], Step [900/1685], Loss: 0.4269\n",
      "Epoch [1/5], Step [910/1685], Loss: 0.0715\n",
      "Epoch [1/5], Step [920/1685], Loss: 0.0944\n",
      "Epoch [1/5], Step [930/1685], Loss: 0.2498\n",
      "Epoch [1/5], Step [940/1685], Loss: 0.0804\n",
      "Epoch [1/5], Step [950/1685], Loss: 0.4700\n",
      "Epoch [1/5], Step [960/1685], Loss: 0.0194\n",
      "Epoch [1/5], Step [970/1685], Loss: 0.3887\n",
      "Epoch [1/5], Step [980/1685], Loss: 0.0250\n",
      "Epoch [1/5], Step [990/1685], Loss: 0.0110\n",
      "Epoch [1/5], Step [1000/1685], Loss: 0.0449\n",
      "Epoch [1/5], Step [1010/1685], Loss: 0.8463\n",
      "Epoch [1/5], Step [1020/1685], Loss: 0.6865\n",
      "Epoch [1/5], Step [1030/1685], Loss: 0.0280\n",
      "Epoch [1/5], Step [1040/1685], Loss: 0.0335\n",
      "Epoch [1/5], Step [1050/1685], Loss: 0.1630\n",
      "Epoch [1/5], Step [1060/1685], Loss: 0.0562\n",
      "Epoch [1/5], Step [1070/1685], Loss: 0.3126\n",
      "Epoch [1/5], Step [1080/1685], Loss: 0.8860\n",
      "Epoch [1/5], Step [1090/1685], Loss: 0.1287\n",
      "Epoch [1/5], Step [1100/1685], Loss: 0.1657\n",
      "Epoch [1/5], Step [1110/1685], Loss: 0.4095\n",
      "Epoch [1/5], Step [1120/1685], Loss: 0.0644\n",
      "Epoch [1/5], Step [1130/1685], Loss: 0.7112\n",
      "Epoch [1/5], Step [1140/1685], Loss: 1.3574\n",
      "Epoch [1/5], Step [1150/1685], Loss: 0.0254\n",
      "Epoch [1/5], Step [1160/1685], Loss: 0.2177\n",
      "Epoch [1/5], Step [1170/1685], Loss: 0.0832\n",
      "Epoch [1/5], Step [1180/1685], Loss: 0.0421\n",
      "Epoch [1/5], Step [1190/1685], Loss: 1.4538\n",
      "Epoch [1/5], Step [1200/1685], Loss: 0.4380\n",
      "Epoch [1/5], Step [1210/1685], Loss: 0.1156\n",
      "Epoch [1/5], Step [1220/1685], Loss: 0.0130\n",
      "Epoch [1/5], Step [1230/1685], Loss: 0.3853\n",
      "Epoch [1/5], Step [1240/1685], Loss: 0.0490\n",
      "Epoch [1/5], Step [1250/1685], Loss: 0.1684\n",
      "Epoch [1/5], Step [1260/1685], Loss: 0.0102\n",
      "Epoch [1/5], Step [1270/1685], Loss: 0.0790\n",
      "Epoch [1/5], Step [1280/1685], Loss: 0.0093\n",
      "Epoch [1/5], Step [1290/1685], Loss: 0.0353\n",
      "Epoch [1/5], Step [1300/1685], Loss: 0.4398\n",
      "Epoch [1/5], Step [1310/1685], Loss: 0.0360\n",
      "Epoch [1/5], Step [1320/1685], Loss: 0.4641\n",
      "Epoch [1/5], Step [1330/1685], Loss: 0.6997\n",
      "Epoch [1/5], Step [1340/1685], Loss: 0.0197\n",
      "Epoch [1/5], Step [1350/1685], Loss: 0.0202\n",
      "Epoch [1/5], Step [1360/1685], Loss: 0.1161\n",
      "Epoch [1/5], Step [1370/1685], Loss: 0.0035\n",
      "Epoch [1/5], Step [1380/1685], Loss: 0.0040\n",
      "Epoch [1/5], Step [1390/1685], Loss: 0.3126\n",
      "Epoch [1/5], Step [1400/1685], Loss: 0.1194\n",
      "Epoch [1/5], Step [1410/1685], Loss: 0.4602\n",
      "Epoch [1/5], Step [1420/1685], Loss: 0.1381\n",
      "Epoch [1/5], Step [1430/1685], Loss: 0.0392\n",
      "Epoch [1/5], Step [1440/1685], Loss: 0.0345\n",
      "Epoch [1/5], Step [1450/1685], Loss: 0.9149\n",
      "Epoch [1/5], Step [1460/1685], Loss: 0.0166\n",
      "Epoch [1/5], Step [1470/1685], Loss: 0.0793\n",
      "Epoch [1/5], Step [1480/1685], Loss: 0.5199\n",
      "Epoch [1/5], Step [1490/1685], Loss: 0.0175\n",
      "Epoch [1/5], Step [1500/1685], Loss: 0.2877\n",
      "Epoch [1/5], Step [1510/1685], Loss: 0.2370\n",
      "Epoch [1/5], Step [1520/1685], Loss: 0.4320\n",
      "Epoch [1/5], Step [1530/1685], Loss: 0.1282\n",
      "Epoch [1/5], Step [1540/1685], Loss: 0.0217\n",
      "Epoch [1/5], Step [1550/1685], Loss: 0.4461\n",
      "Epoch [1/5], Step [1560/1685], Loss: 0.2094\n",
      "Epoch [1/5], Step [1570/1685], Loss: 0.0719\n",
      "Epoch [1/5], Step [1580/1685], Loss: 0.2284\n",
      "Epoch [1/5], Step [1590/1685], Loss: 0.4932\n",
      "Epoch [1/5], Step [1600/1685], Loss: 0.2346\n",
      "Epoch [1/5], Step [1610/1685], Loss: 0.0203\n",
      "Epoch [1/5], Step [1620/1685], Loss: 0.0048\n",
      "Epoch [1/5], Step [1630/1685], Loss: 0.5781\n",
      "Epoch [1/5], Step [1640/1685], Loss: 0.1362\n",
      "Epoch [1/5], Step [1650/1685], Loss: 0.1195\n",
      "Epoch [1/5], Step [1660/1685], Loss: 0.0119\n",
      "Epoch [1/5], Step [1670/1685], Loss: 0.0033\n",
      "Epoch [1/5], Step [1680/1685], Loss: 1.0069\n",
      "Epoch [2/5], Step [10/1685], Loss: 0.0023\n",
      "Epoch [2/5], Step [20/1685], Loss: 0.0712\n",
      "Epoch [2/5], Step [30/1685], Loss: 0.3064\n",
      "Epoch [2/5], Step [40/1685], Loss: 0.4565\n",
      "Epoch [2/5], Step [50/1685], Loss: 0.2314\n",
      "Epoch [2/5], Step [60/1685], Loss: 0.2302\n",
      "Epoch [2/5], Step [70/1685], Loss: 0.1323\n",
      "Epoch [2/5], Step [80/1685], Loss: 0.1166\n",
      "Epoch [2/5], Step [90/1685], Loss: 0.1597\n",
      "Epoch [2/5], Step [100/1685], Loss: 0.4763\n",
      "Epoch [2/5], Step [110/1685], Loss: 0.2331\n",
      "Epoch [2/5], Step [120/1685], Loss: 0.1140\n",
      "Epoch [2/5], Step [130/1685], Loss: 0.1528\n",
      "Epoch [2/5], Step [140/1685], Loss: 0.1361\n",
      "Epoch [2/5], Step [150/1685], Loss: 0.0047\n",
      "Epoch [2/5], Step [160/1685], Loss: 0.0334\n",
      "Epoch [2/5], Step [170/1685], Loss: 0.0205\n",
      "Epoch [2/5], Step [180/1685], Loss: 0.0082\n",
      "Epoch [2/5], Step [190/1685], Loss: 0.1860\n",
      "Epoch [2/5], Step [200/1685], Loss: 0.1460\n",
      "Epoch [2/5], Step [210/1685], Loss: 0.0231\n",
      "Epoch [2/5], Step [220/1685], Loss: 0.4825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [230/1685], Loss: 0.0294\n",
      "Epoch [2/5], Step [240/1685], Loss: 0.1234\n",
      "Epoch [2/5], Step [250/1685], Loss: 0.0413\n",
      "Epoch [2/5], Step [260/1685], Loss: 0.0748\n",
      "Epoch [2/5], Step [270/1685], Loss: 0.0023\n",
      "Epoch [2/5], Step [280/1685], Loss: 0.1074\n",
      "Epoch [2/5], Step [290/1685], Loss: 0.3905\n",
      "Epoch [2/5], Step [300/1685], Loss: 0.0019\n",
      "Epoch [2/5], Step [310/1685], Loss: 0.0276\n",
      "Epoch [2/5], Step [320/1685], Loss: 0.3768\n",
      "Epoch [2/5], Step [330/1685], Loss: 0.1980\n",
      "Epoch [2/5], Step [340/1685], Loss: 0.2160\n",
      "Epoch [2/5], Step [350/1685], Loss: 0.0442\n",
      "Epoch [2/5], Step [360/1685], Loss: 0.0319\n",
      "Epoch [2/5], Step [370/1685], Loss: 0.3279\n",
      "Epoch [2/5], Step [380/1685], Loss: 0.1031\n",
      "Epoch [2/5], Step [390/1685], Loss: 0.4245\n",
      "Epoch [2/5], Step [400/1685], Loss: 0.0807\n",
      "Epoch [2/5], Step [410/1685], Loss: 0.0729\n",
      "Epoch [2/5], Step [420/1685], Loss: 0.0140\n",
      "Epoch [2/5], Step [430/1685], Loss: 0.0794\n",
      "Epoch [2/5], Step [440/1685], Loss: 0.0379\n",
      "Epoch [2/5], Step [450/1685], Loss: 0.0296\n",
      "Epoch [2/5], Step [460/1685], Loss: 0.1678\n",
      "Epoch [2/5], Step [470/1685], Loss: 0.0072\n",
      "Epoch [2/5], Step [480/1685], Loss: 0.0427\n",
      "Epoch [2/5], Step [490/1685], Loss: 0.1962\n",
      "Epoch [2/5], Step [500/1685], Loss: 0.5345\n",
      "Epoch [2/5], Step [510/1685], Loss: 0.0058\n",
      "Epoch [2/5], Step [520/1685], Loss: 0.3218\n",
      "Epoch [2/5], Step [530/1685], Loss: 0.0130\n",
      "Epoch [2/5], Step [540/1685], Loss: 0.0417\n",
      "Epoch [2/5], Step [550/1685], Loss: 0.2268\n",
      "Epoch [2/5], Step [560/1685], Loss: 0.0032\n",
      "Epoch [2/5], Step [570/1685], Loss: 0.0292\n",
      "Epoch [2/5], Step [580/1685], Loss: 0.4537\n",
      "Epoch [2/5], Step [590/1685], Loss: 0.1934\n",
      "Epoch [2/5], Step [600/1685], Loss: 0.0012\n",
      "Epoch [2/5], Step [610/1685], Loss: 0.0079\n",
      "Epoch [2/5], Step [620/1685], Loss: 0.0062\n",
      "Epoch [2/5], Step [630/1685], Loss: 0.0044\n",
      "Epoch [2/5], Step [640/1685], Loss: 0.0374\n",
      "Epoch [2/5], Step [650/1685], Loss: 0.0416\n",
      "Epoch [2/5], Step [660/1685], Loss: 0.2489\n",
      "Epoch [2/5], Step [670/1685], Loss: 0.1910\n",
      "Epoch [2/5], Step [680/1685], Loss: 0.0980\n",
      "Epoch [2/5], Step [690/1685], Loss: 0.9573\n",
      "Epoch [2/5], Step [700/1685], Loss: 0.1401\n",
      "Epoch [2/5], Step [710/1685], Loss: 0.0339\n",
      "Epoch [2/5], Step [720/1685], Loss: 0.4693\n",
      "Epoch [2/5], Step [730/1685], Loss: 0.0134\n",
      "Epoch [2/5], Step [740/1685], Loss: 0.2865\n",
      "Epoch [2/5], Step [750/1685], Loss: 0.0024\n",
      "Epoch [2/5], Step [760/1685], Loss: 0.0525\n",
      "Epoch [2/5], Step [770/1685], Loss: 0.7009\n",
      "Epoch [2/5], Step [780/1685], Loss: 0.0082\n",
      "Epoch [2/5], Step [790/1685], Loss: 0.5486\n",
      "Epoch [2/5], Step [800/1685], Loss: 0.0214\n",
      "Epoch [2/5], Step [810/1685], Loss: 0.0006\n",
      "Epoch [2/5], Step [820/1685], Loss: 0.0100\n",
      "Epoch [2/5], Step [830/1685], Loss: 0.0211\n",
      "Epoch [2/5], Step [840/1685], Loss: 0.6694\n",
      "Epoch [2/5], Step [850/1685], Loss: 0.7994\n",
      "Epoch [2/5], Step [860/1685], Loss: 0.2856\n",
      "Epoch [2/5], Step [870/1685], Loss: 0.4490\n",
      "Epoch [2/5], Step [880/1685], Loss: 0.3874\n",
      "Epoch [2/5], Step [890/1685], Loss: 0.0975\n",
      "Epoch [2/5], Step [900/1685], Loss: 0.0121\n",
      "Epoch [2/5], Step [910/1685], Loss: 0.0464\n",
      "Epoch [2/5], Step [920/1685], Loss: 1.0167\n",
      "Epoch [2/5], Step [930/1685], Loss: 0.1162\n",
      "Epoch [2/5], Step [940/1685], Loss: 0.0763\n",
      "Epoch [2/5], Step [950/1685], Loss: 0.8155\n",
      "Epoch [2/5], Step [960/1685], Loss: 0.2007\n",
      "Epoch [2/5], Step [970/1685], Loss: 0.3898\n",
      "Epoch [2/5], Step [980/1685], Loss: 0.2146\n",
      "Epoch [2/5], Step [990/1685], Loss: 0.0068\n",
      "Epoch [2/5], Step [1000/1685], Loss: 0.1701\n",
      "Epoch [2/5], Step [1010/1685], Loss: 0.0399\n",
      "Epoch [2/5], Step [1020/1685], Loss: 0.1994\n",
      "Epoch [2/5], Step [1030/1685], Loss: 0.0027\n",
      "Epoch [2/5], Step [1040/1685], Loss: 0.2302\n",
      "Epoch [2/5], Step [1050/1685], Loss: 0.3483\n",
      "Epoch [2/5], Step [1060/1685], Loss: 0.1028\n",
      "Epoch [2/5], Step [1070/1685], Loss: 0.1042\n",
      "Epoch [2/5], Step [1080/1685], Loss: 0.5149\n",
      "Epoch [2/5], Step [1090/1685], Loss: 0.0203\n",
      "Epoch [2/5], Step [1100/1685], Loss: 0.1580\n",
      "Epoch [2/5], Step [1110/1685], Loss: 0.4189\n",
      "Epoch [2/5], Step [1120/1685], Loss: 0.0167\n",
      "Epoch [2/5], Step [1130/1685], Loss: 0.1830\n",
      "Epoch [2/5], Step [1140/1685], Loss: 0.0243\n",
      "Epoch [2/5], Step [1150/1685], Loss: 0.5257\n",
      "Epoch [2/5], Step [1160/1685], Loss: 1.0517\n",
      "Epoch [2/5], Step [1170/1685], Loss: 0.1600\n",
      "Epoch [2/5], Step [1180/1685], Loss: 0.0059\n",
      "Epoch [2/5], Step [1190/1685], Loss: 0.5114\n",
      "Epoch [2/5], Step [1200/1685], Loss: 0.3041\n",
      "Epoch [2/5], Step [1210/1685], Loss: 0.0039\n",
      "Epoch [2/5], Step [1220/1685], Loss: 0.4949\n",
      "Epoch [2/5], Step [1230/1685], Loss: 0.2053\n",
      "Epoch [2/5], Step [1240/1685], Loss: 0.0027\n",
      "Epoch [2/5], Step [1250/1685], Loss: 0.0011\n",
      "Epoch [2/5], Step [1260/1685], Loss: 0.0549\n",
      "Epoch [2/5], Step [1270/1685], Loss: 0.0032\n",
      "Epoch [2/5], Step [1280/1685], Loss: 0.2790\n",
      "Epoch [2/5], Step [1290/1685], Loss: 0.3215\n",
      "Epoch [2/5], Step [1300/1685], Loss: 0.0115\n",
      "Epoch [2/5], Step [1310/1685], Loss: 0.1567\n",
      "Epoch [2/5], Step [1320/1685], Loss: 0.1925\n",
      "Epoch [2/5], Step [1330/1685], Loss: 0.1387\n",
      "Epoch [2/5], Step [1340/1685], Loss: 0.4576\n",
      "Epoch [2/5], Step [1350/1685], Loss: 0.2618\n",
      "Epoch [2/5], Step [1360/1685], Loss: 0.2562\n",
      "Epoch [2/5], Step [1370/1685], Loss: 0.0388\n",
      "Epoch [2/5], Step [1380/1685], Loss: 0.1237\n",
      "Epoch [2/5], Step [1390/1685], Loss: 0.0560\n",
      "Epoch [2/5], Step [1400/1685], Loss: 0.2745\n",
      "Epoch [2/5], Step [1410/1685], Loss: 0.1638\n",
      "Epoch [2/5], Step [1420/1685], Loss: 0.0396\n",
      "Epoch [2/5], Step [1430/1685], Loss: 0.0038\n",
      "Epoch [2/5], Step [1440/1685], Loss: 0.0230\n",
      "Epoch [2/5], Step [1450/1685], Loss: 0.7916\n",
      "Epoch [2/5], Step [1460/1685], Loss: 1.4493\n",
      "Epoch [2/5], Step [1470/1685], Loss: 0.0750\n",
      "Epoch [2/5], Step [1480/1685], Loss: 0.0009\n",
      "Epoch [2/5], Step [1490/1685], Loss: 0.0523\n",
      "Epoch [2/5], Step [1500/1685], Loss: 0.0965\n",
      "Epoch [2/5], Step [1510/1685], Loss: 0.0019\n",
      "Epoch [2/5], Step [1520/1685], Loss: 0.0004\n",
      "Epoch [2/5], Step [1530/1685], Loss: 0.5086\n",
      "Epoch [2/5], Step [1540/1685], Loss: 0.2753\n",
      "Epoch [2/5], Step [1550/1685], Loss: 0.0111\n",
      "Epoch [2/5], Step [1560/1685], Loss: 0.0052\n",
      "Epoch [2/5], Step [1570/1685], Loss: 0.2970\n",
      "Epoch [2/5], Step [1580/1685], Loss: 0.1605\n",
      "Epoch [2/5], Step [1590/1685], Loss: 0.8273\n",
      "Epoch [2/5], Step [1600/1685], Loss: 0.0124\n",
      "Epoch [2/5], Step [1610/1685], Loss: 0.0149\n",
      "Epoch [2/5], Step [1620/1685], Loss: 0.0646\n",
      "Epoch [2/5], Step [1630/1685], Loss: 0.4335\n",
      "Epoch [2/5], Step [1640/1685], Loss: 0.0708\n",
      "Epoch [2/5], Step [1650/1685], Loss: 0.0015\n",
      "Epoch [2/5], Step [1660/1685], Loss: 0.0398\n",
      "Epoch [2/5], Step [1670/1685], Loss: 0.6391\n",
      "Epoch [2/5], Step [1680/1685], Loss: 0.2619\n",
      "Epoch [3/5], Step [10/1685], Loss: 0.0401\n",
      "Epoch [3/5], Step [20/1685], Loss: 0.1691\n",
      "Epoch [3/5], Step [30/1685], Loss: 0.0253\n",
      "Epoch [3/5], Step [40/1685], Loss: 0.2835\n",
      "Epoch [3/5], Step [50/1685], Loss: 0.0589\n",
      "Epoch [3/5], Step [60/1685], Loss: 0.0951\n",
      "Epoch [3/5], Step [70/1685], Loss: 0.0842\n",
      "Epoch [3/5], Step [80/1685], Loss: 0.3755\n",
      "Epoch [3/5], Step [90/1685], Loss: 0.1349\n",
      "Epoch [3/5], Step [100/1685], Loss: 0.0070\n",
      "Epoch [3/5], Step [110/1685], Loss: 0.1037\n",
      "Epoch [3/5], Step [120/1685], Loss: 0.0244\n",
      "Epoch [3/5], Step [130/1685], Loss: 0.0156\n",
      "Epoch [3/5], Step [140/1685], Loss: 0.0057\n",
      "Epoch [3/5], Step [150/1685], Loss: 0.2170\n",
      "Epoch [3/5], Step [160/1685], Loss: 0.2467\n",
      "Epoch [3/5], Step [170/1685], Loss: 0.7613\n",
      "Epoch [3/5], Step [180/1685], Loss: 0.0104\n",
      "Epoch [3/5], Step [190/1685], Loss: 0.3218\n",
      "Epoch [3/5], Step [200/1685], Loss: 0.0094\n",
      "Epoch [3/5], Step [210/1685], Loss: 0.0623\n",
      "Epoch [3/5], Step [220/1685], Loss: 0.0135\n",
      "Epoch [3/5], Step [230/1685], Loss: 0.4825\n",
      "Epoch [3/5], Step [240/1685], Loss: 0.2272\n",
      "Epoch [3/5], Step [250/1685], Loss: 0.0115\n",
      "Epoch [3/5], Step [260/1685], Loss: 0.0434\n",
      "Epoch [3/5], Step [270/1685], Loss: 0.3492\n",
      "Epoch [3/5], Step [280/1685], Loss: 0.2447\n",
      "Epoch [3/5], Step [290/1685], Loss: 0.0439\n",
      "Epoch [3/5], Step [300/1685], Loss: 0.1975\n",
      "Epoch [3/5], Step [310/1685], Loss: 0.3182\n",
      "Epoch [3/5], Step [320/1685], Loss: 0.0022\n",
      "Epoch [3/5], Step [330/1685], Loss: 0.2192\n",
      "Epoch [3/5], Step [340/1685], Loss: 0.0338\n",
      "Epoch [3/5], Step [350/1685], Loss: 0.0082\n",
      "Epoch [3/5], Step [360/1685], Loss: 0.2446\n",
      "Epoch [3/5], Step [370/1685], Loss: 0.0731\n",
      "Epoch [3/5], Step [380/1685], Loss: 0.0033\n",
      "Epoch [3/5], Step [390/1685], Loss: 0.2743\n",
      "Epoch [3/5], Step [400/1685], Loss: 0.1975\n",
      "Epoch [3/5], Step [410/1685], Loss: 0.2962\n",
      "Epoch [3/5], Step [420/1685], Loss: 0.0133\n",
      "Epoch [3/5], Step [430/1685], Loss: 0.0040\n",
      "Epoch [3/5], Step [440/1685], Loss: 0.0066\n",
      "Epoch [3/5], Step [450/1685], Loss: 0.0119\n",
      "Epoch [3/5], Step [460/1685], Loss: 0.0765\n",
      "Epoch [3/5], Step [470/1685], Loss: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [480/1685], Loss: 0.2550\n",
      "Epoch [3/5], Step [490/1685], Loss: 0.0025\n",
      "Epoch [3/5], Step [500/1685], Loss: 0.0063\n",
      "Epoch [3/5], Step [510/1685], Loss: 0.0090\n",
      "Epoch [3/5], Step [520/1685], Loss: 0.3105\n",
      "Epoch [3/5], Step [530/1685], Loss: 0.0338\n",
      "Epoch [3/5], Step [540/1685], Loss: 0.3179\n",
      "Epoch [3/5], Step [550/1685], Loss: 0.1424\n",
      "Epoch [3/5], Step [560/1685], Loss: 0.1240\n",
      "Epoch [3/5], Step [570/1685], Loss: 0.0008\n",
      "Epoch [3/5], Step [580/1685], Loss: 0.0022\n",
      "Epoch [3/5], Step [590/1685], Loss: 0.0030\n",
      "Epoch [3/5], Step [600/1685], Loss: 0.0761\n",
      "Epoch [3/5], Step [610/1685], Loss: 0.5363\n",
      "Epoch [3/5], Step [620/1685], Loss: 0.1778\n",
      "Epoch [3/5], Step [630/1685], Loss: 0.0011\n",
      "Epoch [3/5], Step [640/1685], Loss: 0.0650\n",
      "Epoch [3/5], Step [650/1685], Loss: 0.0030\n",
      "Epoch [3/5], Step [660/1685], Loss: 0.1263\n",
      "Epoch [3/5], Step [670/1685], Loss: 0.0937\n",
      "Epoch [3/5], Step [680/1685], Loss: 0.4095\n",
      "Epoch [3/5], Step [690/1685], Loss: 0.0201\n",
      "Epoch [3/5], Step [700/1685], Loss: 0.0778\n",
      "Epoch [3/5], Step [710/1685], Loss: 0.0445\n",
      "Epoch [3/5], Step [720/1685], Loss: 0.0063\n",
      "Epoch [3/5], Step [730/1685], Loss: 0.0536\n",
      "Epoch [3/5], Step [740/1685], Loss: 0.0625\n",
      "Epoch [3/5], Step [750/1685], Loss: 0.5274\n",
      "Epoch [3/5], Step [760/1685], Loss: 0.1092\n",
      "Epoch [3/5], Step [770/1685], Loss: 0.0879\n",
      "Epoch [3/5], Step [780/1685], Loss: 0.2189\n",
      "Epoch [3/5], Step [790/1685], Loss: 0.0294\n",
      "Epoch [3/5], Step [800/1685], Loss: 0.0011\n",
      "Epoch [3/5], Step [810/1685], Loss: 0.2746\n",
      "Epoch [3/5], Step [820/1685], Loss: 0.0156\n",
      "Epoch [3/5], Step [830/1685], Loss: 0.0041\n",
      "Epoch [3/5], Step [840/1685], Loss: 0.0699\n",
      "Epoch [3/5], Step [850/1685], Loss: 0.1322\n",
      "Epoch [3/5], Step [860/1685], Loss: 0.0127\n",
      "Epoch [3/5], Step [870/1685], Loss: 0.0201\n",
      "Epoch [3/5], Step [880/1685], Loss: 0.1596\n",
      "Epoch [3/5], Step [890/1685], Loss: 0.5073\n",
      "Epoch [3/5], Step [900/1685], Loss: 0.0237\n",
      "Epoch [3/5], Step [910/1685], Loss: 0.4464\n",
      "Epoch [3/5], Step [920/1685], Loss: 0.0078\n",
      "Epoch [3/5], Step [930/1685], Loss: 0.3232\n",
      "Epoch [3/5], Step [940/1685], Loss: 0.0187\n",
      "Epoch [3/5], Step [950/1685], Loss: 0.0021\n",
      "Epoch [3/5], Step [960/1685], Loss: 0.0733\n",
      "Epoch [3/5], Step [970/1685], Loss: 0.2438\n",
      "Epoch [3/5], Step [980/1685], Loss: 0.0114\n",
      "Epoch [3/5], Step [990/1685], Loss: 0.0326\n",
      "Epoch [3/5], Step [1000/1685], Loss: 0.0098\n",
      "Epoch [3/5], Step [1010/1685], Loss: 0.0009\n",
      "Epoch [3/5], Step [1020/1685], Loss: 0.4713\n",
      "Epoch [3/5], Step [1030/1685], Loss: 0.1955\n",
      "Epoch [3/5], Step [1040/1685], Loss: 0.0532\n",
      "Epoch [3/5], Step [1050/1685], Loss: 0.1834\n",
      "Epoch [3/5], Step [1060/1685], Loss: 0.3344\n",
      "Epoch [3/5], Step [1070/1685], Loss: 0.0209\n",
      "Epoch [3/5], Step [1080/1685], Loss: 0.0047\n",
      "Epoch [3/5], Step [1090/1685], Loss: 0.1285\n",
      "Epoch [3/5], Step [1100/1685], Loss: 0.0166\n",
      "Epoch [3/5], Step [1110/1685], Loss: 0.4264\n",
      "Epoch [3/5], Step [1120/1685], Loss: 0.2090\n",
      "Epoch [3/5], Step [1130/1685], Loss: 0.0754\n",
      "Epoch [3/5], Step [1140/1685], Loss: 0.0701\n",
      "Epoch [3/5], Step [1150/1685], Loss: 0.1135\n",
      "Epoch [3/5], Step [1160/1685], Loss: 0.3293\n",
      "Epoch [3/5], Step [1170/1685], Loss: 0.0032\n",
      "Epoch [3/5], Step [1180/1685], Loss: 0.0012\n",
      "Epoch [3/5], Step [1190/1685], Loss: 0.5018\n",
      "Epoch [3/5], Step [1200/1685], Loss: 0.1064\n",
      "Epoch [3/5], Step [1210/1685], Loss: 0.0116\n",
      "Epoch [3/5], Step [1220/1685], Loss: 0.1333\n",
      "Epoch [3/5], Step [1230/1685], Loss: 0.0925\n",
      "Epoch [3/5], Step [1240/1685], Loss: 0.1257\n",
      "Epoch [3/5], Step [1250/1685], Loss: 0.0024\n",
      "Epoch [3/5], Step [1260/1685], Loss: 0.0054\n",
      "Epoch [3/5], Step [1270/1685], Loss: 0.0023\n",
      "Epoch [3/5], Step [1280/1685], Loss: 0.0331\n",
      "Epoch [3/5], Step [1290/1685], Loss: 0.0307\n",
      "Epoch [3/5], Step [1300/1685], Loss: 0.0094\n",
      "Epoch [3/5], Step [1310/1685], Loss: 0.6512\n",
      "Epoch [3/5], Step [1320/1685], Loss: 0.3790\n",
      "Epoch [3/5], Step [1330/1685], Loss: 0.2605\n",
      "Epoch [3/5], Step [1340/1685], Loss: 0.1538\n",
      "Epoch [3/5], Step [1350/1685], Loss: 0.0105\n",
      "Epoch [3/5], Step [1360/1685], Loss: 0.0084\n",
      "Epoch [3/5], Step [1370/1685], Loss: 0.0011\n",
      "Epoch [3/5], Step [1380/1685], Loss: 1.0607\n",
      "Epoch [3/5], Step [1390/1685], Loss: 0.0814\n",
      "Epoch [3/5], Step [1400/1685], Loss: 0.0079\n",
      "Epoch [3/5], Step [1410/1685], Loss: 0.4403\n",
      "Epoch [3/5], Step [1420/1685], Loss: 0.0060\n",
      "Epoch [3/5], Step [1430/1685], Loss: 0.0031\n",
      "Epoch [3/5], Step [1440/1685], Loss: 0.0372\n",
      "Epoch [3/5], Step [1450/1685], Loss: 0.2244\n",
      "Epoch [3/5], Step [1460/1685], Loss: 0.1071\n",
      "Epoch [3/5], Step [1470/1685], Loss: 0.0027\n",
      "Epoch [3/5], Step [1480/1685], Loss: 0.0102\n",
      "Epoch [3/5], Step [1490/1685], Loss: 0.0191\n",
      "Epoch [3/5], Step [1500/1685], Loss: 0.8150\n",
      "Epoch [3/5], Step [1510/1685], Loss: 0.2958\n",
      "Epoch [3/5], Step [1520/1685], Loss: 0.0820\n",
      "Epoch [3/5], Step [1530/1685], Loss: 0.0209\n",
      "Epoch [3/5], Step [1540/1685], Loss: 0.0571\n",
      "Epoch [3/5], Step [1550/1685], Loss: 0.0044\n",
      "Epoch [3/5], Step [1560/1685], Loss: 0.0007\n",
      "Epoch [3/5], Step [1570/1685], Loss: 0.1496\n",
      "Epoch [3/5], Step [1580/1685], Loss: 0.0028\n",
      "Epoch [3/5], Step [1590/1685], Loss: 0.0154\n",
      "Epoch [3/5], Step [1600/1685], Loss: 0.2422\n",
      "Epoch [3/5], Step [1610/1685], Loss: 0.4398\n",
      "Epoch [3/5], Step [1620/1685], Loss: 0.2930\n",
      "Epoch [3/5], Step [1630/1685], Loss: 0.0178\n",
      "Epoch [3/5], Step [1640/1685], Loss: 0.0150\n",
      "Epoch [3/5], Step [1650/1685], Loss: 0.0086\n",
      "Epoch [3/5], Step [1660/1685], Loss: 0.2612\n",
      "Epoch [3/5], Step [1670/1685], Loss: 0.3984\n",
      "Epoch [3/5], Step [1680/1685], Loss: 0.1178\n",
      "Epoch [4/5], Step [10/1685], Loss: 0.0854\n",
      "Epoch [4/5], Step [20/1685], Loss: 0.0011\n",
      "Epoch [4/5], Step [30/1685], Loss: 0.0317\n",
      "Epoch [4/5], Step [40/1685], Loss: 0.0051\n",
      "Epoch [4/5], Step [50/1685], Loss: 0.0733\n",
      "Epoch [4/5], Step [60/1685], Loss: 0.0404\n",
      "Epoch [4/5], Step [70/1685], Loss: 0.1889\n",
      "Epoch [4/5], Step [80/1685], Loss: 0.0104\n",
      "Epoch [4/5], Step [90/1685], Loss: 0.0003\n",
      "Epoch [4/5], Step [100/1685], Loss: 0.0568\n",
      "Epoch [4/5], Step [110/1685], Loss: 0.2516\n",
      "Epoch [4/5], Step [120/1685], Loss: 0.0328\n",
      "Epoch [4/5], Step [130/1685], Loss: 0.1406\n",
      "Epoch [4/5], Step [140/1685], Loss: 0.2003\n",
      "Epoch [4/5], Step [150/1685], Loss: 0.3026\n",
      "Epoch [4/5], Step [160/1685], Loss: 0.0438\n",
      "Epoch [4/5], Step [170/1685], Loss: 0.0088\n",
      "Epoch [4/5], Step [180/1685], Loss: 0.0528\n",
      "Epoch [4/5], Step [190/1685], Loss: 0.0623\n",
      "Epoch [4/5], Step [200/1685], Loss: 0.1553\n",
      "Epoch [4/5], Step [210/1685], Loss: 0.0276\n",
      "Epoch [4/5], Step [220/1685], Loss: 0.0121\n",
      "Epoch [4/5], Step [230/1685], Loss: 0.0077\n",
      "Epoch [4/5], Step [240/1685], Loss: 0.0366\n",
      "Epoch [4/5], Step [250/1685], Loss: 0.0186\n",
      "Epoch [4/5], Step [260/1685], Loss: 0.8944\n",
      "Epoch [4/5], Step [270/1685], Loss: 0.0959\n",
      "Epoch [4/5], Step [280/1685], Loss: 0.0404\n",
      "Epoch [4/5], Step [290/1685], Loss: 0.0014\n",
      "Epoch [4/5], Step [300/1685], Loss: 0.0025\n",
      "Epoch [4/5], Step [310/1685], Loss: 0.0042\n",
      "Epoch [4/5], Step [320/1685], Loss: 0.0165\n",
      "Epoch [4/5], Step [330/1685], Loss: 0.0807\n",
      "Epoch [4/5], Step [340/1685], Loss: 0.0254\n",
      "Epoch [4/5], Step [350/1685], Loss: 0.0146\n",
      "Epoch [4/5], Step [360/1685], Loss: 0.0124\n",
      "Epoch [4/5], Step [370/1685], Loss: 0.2399\n",
      "Epoch [4/5], Step [380/1685], Loss: 0.4408\n",
      "Epoch [4/5], Step [390/1685], Loss: 0.0607\n",
      "Epoch [4/5], Step [400/1685], Loss: 0.3479\n",
      "Epoch [4/5], Step [410/1685], Loss: 0.0429\n",
      "Epoch [4/5], Step [420/1685], Loss: 0.0536\n",
      "Epoch [4/5], Step [430/1685], Loss: 0.0638\n",
      "Epoch [4/5], Step [440/1685], Loss: 0.1147\n",
      "Epoch [4/5], Step [450/1685], Loss: 0.0015\n",
      "Epoch [4/5], Step [460/1685], Loss: 0.0130\n",
      "Epoch [4/5], Step [470/1685], Loss: 0.0007\n",
      "Epoch [4/5], Step [480/1685], Loss: 0.0006\n",
      "Epoch [4/5], Step [490/1685], Loss: 0.0211\n",
      "Epoch [4/5], Step [500/1685], Loss: 0.0908\n",
      "Epoch [4/5], Step [510/1685], Loss: 0.0052\n",
      "Epoch [4/5], Step [520/1685], Loss: 0.0058\n",
      "Epoch [4/5], Step [530/1685], Loss: 0.0023\n",
      "Epoch [4/5], Step [540/1685], Loss: 0.0163\n",
      "Epoch [4/5], Step [550/1685], Loss: 0.0116\n",
      "Epoch [4/5], Step [560/1685], Loss: 0.2669\n",
      "Epoch [4/5], Step [570/1685], Loss: 0.0492\n",
      "Epoch [4/5], Step [580/1685], Loss: 0.2203\n",
      "Epoch [4/5], Step [590/1685], Loss: 0.0019\n",
      "Epoch [4/5], Step [600/1685], Loss: 0.0660\n",
      "Epoch [4/5], Step [610/1685], Loss: 0.1834\n",
      "Epoch [4/5], Step [620/1685], Loss: 0.0889\n",
      "Epoch [4/5], Step [630/1685], Loss: 0.0737\n",
      "Epoch [4/5], Step [640/1685], Loss: 0.2091\n",
      "Epoch [4/5], Step [650/1685], Loss: 0.0099\n",
      "Epoch [4/5], Step [660/1685], Loss: 0.0007\n",
      "Epoch [4/5], Step [670/1685], Loss: 0.0011\n",
      "Epoch [4/5], Step [680/1685], Loss: 0.3263\n",
      "Epoch [4/5], Step [690/1685], Loss: 0.2037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [700/1685], Loss: 0.2307\n",
      "Epoch [4/5], Step [710/1685], Loss: 0.0030\n",
      "Epoch [4/5], Step [720/1685], Loss: 0.0149\n",
      "Epoch [4/5], Step [730/1685], Loss: 0.0067\n",
      "Epoch [4/5], Step [740/1685], Loss: 0.0009\n",
      "Epoch [4/5], Step [750/1685], Loss: 0.3525\n",
      "Epoch [4/5], Step [760/1685], Loss: 0.0083\n",
      "Epoch [4/5], Step [770/1685], Loss: 0.4094\n",
      "Epoch [4/5], Step [780/1685], Loss: 0.0018\n",
      "Epoch [4/5], Step [790/1685], Loss: 0.0021\n",
      "Epoch [4/5], Step [800/1685], Loss: 0.0406\n",
      "Epoch [4/5], Step [810/1685], Loss: 0.1833\n",
      "Epoch [4/5], Step [820/1685], Loss: 0.0527\n",
      "Epoch [4/5], Step [830/1685], Loss: 0.0404\n",
      "Epoch [4/5], Step [840/1685], Loss: 0.0054\n",
      "Epoch [4/5], Step [850/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [860/1685], Loss: 0.0033\n",
      "Epoch [4/5], Step [870/1685], Loss: 0.0139\n",
      "Epoch [4/5], Step [880/1685], Loss: 0.0308\n",
      "Epoch [4/5], Step [890/1685], Loss: 0.0302\n",
      "Epoch [4/5], Step [900/1685], Loss: 0.0011\n",
      "Epoch [4/5], Step [910/1685], Loss: 0.2249\n",
      "Epoch [4/5], Step [920/1685], Loss: 0.6641\n",
      "Epoch [4/5], Step [930/1685], Loss: 0.0309\n",
      "Epoch [4/5], Step [940/1685], Loss: 0.0788\n",
      "Epoch [4/5], Step [950/1685], Loss: 0.1821\n",
      "Epoch [4/5], Step [960/1685], Loss: 0.2608\n",
      "Epoch [4/5], Step [970/1685], Loss: 0.0453\n",
      "Epoch [4/5], Step [980/1685], Loss: 0.0022\n",
      "Epoch [4/5], Step [990/1685], Loss: 0.1190\n",
      "Epoch [4/5], Step [1000/1685], Loss: 0.0016\n",
      "Epoch [4/5], Step [1010/1685], Loss: 0.0077\n",
      "Epoch [4/5], Step [1020/1685], Loss: 0.0409\n",
      "Epoch [4/5], Step [1030/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [1040/1685], Loss: 0.0546\n",
      "Epoch [4/5], Step [1050/1685], Loss: 0.0080\n",
      "Epoch [4/5], Step [1060/1685], Loss: 0.0007\n",
      "Epoch [4/5], Step [1070/1685], Loss: 0.0025\n",
      "Epoch [4/5], Step [1080/1685], Loss: 0.0080\n",
      "Epoch [4/5], Step [1090/1685], Loss: 0.4750\n",
      "Epoch [4/5], Step [1100/1685], Loss: 0.6183\n",
      "Epoch [4/5], Step [1110/1685], Loss: 0.0028\n",
      "Epoch [4/5], Step [1120/1685], Loss: 0.0004\n",
      "Epoch [4/5], Step [1130/1685], Loss: 0.1060\n",
      "Epoch [4/5], Step [1140/1685], Loss: 0.0754\n",
      "Epoch [4/5], Step [1150/1685], Loss: 0.3061\n",
      "Epoch [4/5], Step [1160/1685], Loss: 0.0011\n",
      "Epoch [4/5], Step [1170/1685], Loss: 0.0968\n",
      "Epoch [4/5], Step [1180/1685], Loss: 0.0545\n",
      "Epoch [4/5], Step [1190/1685], Loss: 0.1594\n",
      "Epoch [4/5], Step [1200/1685], Loss: 0.0046\n",
      "Epoch [4/5], Step [1210/1685], Loss: 0.0008\n",
      "Epoch [4/5], Step [1220/1685], Loss: 0.0437\n",
      "Epoch [4/5], Step [1230/1685], Loss: 0.3912\n",
      "Epoch [4/5], Step [1240/1685], Loss: 0.7053\n",
      "Epoch [4/5], Step [1250/1685], Loss: 0.1380\n",
      "Epoch [4/5], Step [1260/1685], Loss: 0.0005\n",
      "Epoch [4/5], Step [1270/1685], Loss: 0.0296\n",
      "Epoch [4/5], Step [1280/1685], Loss: 0.0795\n",
      "Epoch [4/5], Step [1290/1685], Loss: 0.0753\n",
      "Epoch [4/5], Step [1300/1685], Loss: 0.0010\n",
      "Epoch [4/5], Step [1310/1685], Loss: 0.0008\n",
      "Epoch [4/5], Step [1320/1685], Loss: 0.0041\n",
      "Epoch [4/5], Step [1330/1685], Loss: 1.0935\n",
      "Epoch [4/5], Step [1340/1685], Loss: 0.0012\n",
      "Epoch [4/5], Step [1350/1685], Loss: 0.0192\n",
      "Epoch [4/5], Step [1360/1685], Loss: 0.1919\n",
      "Epoch [4/5], Step [1370/1685], Loss: 0.0009\n",
      "Epoch [4/5], Step [1380/1685], Loss: 0.4684\n",
      "Epoch [4/5], Step [1390/1685], Loss: 0.3308\n",
      "Epoch [4/5], Step [1400/1685], Loss: 0.0376\n",
      "Epoch [4/5], Step [1410/1685], Loss: 0.3857\n",
      "Epoch [4/5], Step [1420/1685], Loss: 0.3358\n",
      "Epoch [4/5], Step [1430/1685], Loss: 0.0303\n",
      "Epoch [4/5], Step [1440/1685], Loss: 0.0385\n",
      "Epoch [4/5], Step [1450/1685], Loss: 0.1403\n",
      "Epoch [4/5], Step [1460/1685], Loss: 0.0059\n",
      "Epoch [4/5], Step [1470/1685], Loss: 0.0392\n",
      "Epoch [4/5], Step [1480/1685], Loss: 0.0294\n",
      "Epoch [4/5], Step [1490/1685], Loss: 0.0449\n",
      "Epoch [4/5], Step [1500/1685], Loss: 0.0960\n",
      "Epoch [4/5], Step [1510/1685], Loss: 0.2874\n",
      "Epoch [4/5], Step [1520/1685], Loss: 0.0011\n",
      "Epoch [4/5], Step [1530/1685], Loss: 0.0084\n",
      "Epoch [4/5], Step [1540/1685], Loss: 0.0719\n",
      "Epoch [4/5], Step [1550/1685], Loss: 0.2903\n",
      "Epoch [4/5], Step [1560/1685], Loss: 0.0277\n",
      "Epoch [4/5], Step [1570/1685], Loss: 0.1183\n",
      "Epoch [4/5], Step [1580/1685], Loss: 0.0022\n",
      "Epoch [4/5], Step [1590/1685], Loss: 0.0088\n",
      "Epoch [4/5], Step [1600/1685], Loss: 0.0775\n",
      "Epoch [4/5], Step [1610/1685], Loss: 0.0163\n",
      "Epoch [4/5], Step [1620/1685], Loss: 0.0001\n",
      "Epoch [4/5], Step [1630/1685], Loss: 0.0246\n",
      "Epoch [4/5], Step [1640/1685], Loss: 0.6144\n",
      "Epoch [4/5], Step [1650/1685], Loss: 0.4175\n",
      "Epoch [4/5], Step [1660/1685], Loss: 0.0082\n",
      "Epoch [4/5], Step [1670/1685], Loss: 0.0005\n",
      "Epoch [4/5], Step [1680/1685], Loss: 0.0011\n",
      "Epoch [5/5], Step [10/1685], Loss: 0.0097\n",
      "Epoch [5/5], Step [20/1685], Loss: 0.0006\n",
      "Epoch [5/5], Step [30/1685], Loss: 0.0030\n",
      "Epoch [5/5], Step [40/1685], Loss: 0.0334\n",
      "Epoch [5/5], Step [50/1685], Loss: 0.0615\n",
      "Epoch [5/5], Step [60/1685], Loss: 0.0377\n",
      "Epoch [5/5], Step [70/1685], Loss: 0.1487\n",
      "Epoch [5/5], Step [80/1685], Loss: 0.4670\n",
      "Epoch [5/5], Step [90/1685], Loss: 0.0028\n",
      "Epoch [5/5], Step [100/1685], Loss: 0.1074\n",
      "Epoch [5/5], Step [110/1685], Loss: 0.0842\n",
      "Epoch [5/5], Step [120/1685], Loss: 0.1918\n",
      "Epoch [5/5], Step [130/1685], Loss: 0.0015\n",
      "Epoch [5/5], Step [140/1685], Loss: 0.0802\n",
      "Epoch [5/5], Step [150/1685], Loss: 0.0021\n",
      "Epoch [5/5], Step [160/1685], Loss: 0.0094\n",
      "Epoch [5/5], Step [170/1685], Loss: 0.2745\n",
      "Epoch [5/5], Step [180/1685], Loss: 0.0164\n",
      "Epoch [5/5], Step [190/1685], Loss: 0.0047\n",
      "Epoch [5/5], Step [200/1685], Loss: 0.1921\n",
      "Epoch [5/5], Step [210/1685], Loss: 0.1517\n",
      "Epoch [5/5], Step [220/1685], Loss: 0.2739\n",
      "Epoch [5/5], Step [230/1685], Loss: 0.0127\n",
      "Epoch [5/5], Step [240/1685], Loss: 0.4481\n",
      "Epoch [5/5], Step [250/1685], Loss: 0.0125\n",
      "Epoch [5/5], Step [260/1685], Loss: 0.0050\n",
      "Epoch [5/5], Step [270/1685], Loss: 0.0091\n",
      "Epoch [5/5], Step [280/1685], Loss: 0.0281\n",
      "Epoch [5/5], Step [290/1685], Loss: 0.0465\n",
      "Epoch [5/5], Step [300/1685], Loss: 0.0032\n",
      "Epoch [5/5], Step [310/1685], Loss: 0.0003\n",
      "Epoch [5/5], Step [320/1685], Loss: 0.1736\n",
      "Epoch [5/5], Step [330/1685], Loss: 0.0176\n",
      "Epoch [5/5], Step [340/1685], Loss: 0.0081\n",
      "Epoch [5/5], Step [350/1685], Loss: 0.0245\n",
      "Epoch [5/5], Step [360/1685], Loss: 0.2844\n",
      "Epoch [5/5], Step [370/1685], Loss: 0.0120\n",
      "Epoch [5/5], Step [380/1685], Loss: 0.0561\n",
      "Epoch [5/5], Step [390/1685], Loss: 0.2019\n",
      "Epoch [5/5], Step [400/1685], Loss: 0.0006\n",
      "Epoch [5/5], Step [410/1685], Loss: 0.0006\n",
      "Epoch [5/5], Step [420/1685], Loss: 0.0554\n",
      "Epoch [5/5], Step [430/1685], Loss: 0.0014\n",
      "Epoch [5/5], Step [440/1685], Loss: 0.0146\n",
      "Epoch [5/5], Step [450/1685], Loss: 0.0251\n",
      "Epoch [5/5], Step [460/1685], Loss: 0.0022\n",
      "Epoch [5/5], Step [470/1685], Loss: 0.0005\n",
      "Epoch [5/5], Step [480/1685], Loss: 0.3480\n",
      "Epoch [5/5], Step [490/1685], Loss: 0.0061\n",
      "Epoch [5/5], Step [500/1685], Loss: 0.0413\n",
      "Epoch [5/5], Step [510/1685], Loss: 0.0272\n",
      "Epoch [5/5], Step [520/1685], Loss: 0.0061\n",
      "Epoch [5/5], Step [530/1685], Loss: 0.1154\n",
      "Epoch [5/5], Step [540/1685], Loss: 0.0025\n",
      "Epoch [5/5], Step [550/1685], Loss: 0.1114\n",
      "Epoch [5/5], Step [560/1685], Loss: 0.0120\n",
      "Epoch [5/5], Step [570/1685], Loss: 0.0027\n",
      "Epoch [5/5], Step [580/1685], Loss: 0.0002\n",
      "Epoch [5/5], Step [590/1685], Loss: 0.0062\n",
      "Epoch [5/5], Step [600/1685], Loss: 0.0183\n",
      "Epoch [5/5], Step [610/1685], Loss: 0.1262\n",
      "Epoch [5/5], Step [620/1685], Loss: 0.0018\n",
      "Epoch [5/5], Step [630/1685], Loss: 0.1058\n",
      "Epoch [5/5], Step [640/1685], Loss: 0.1378\n",
      "Epoch [5/5], Step [650/1685], Loss: 0.1960\n",
      "Epoch [5/5], Step [660/1685], Loss: 0.0057\n",
      "Epoch [5/5], Step [670/1685], Loss: 0.0189\n",
      "Epoch [5/5], Step [680/1685], Loss: 0.0005\n",
      "Epoch [5/5], Step [690/1685], Loss: 0.7973\n",
      "Epoch [5/5], Step [700/1685], Loss: 0.0065\n",
      "Epoch [5/5], Step [710/1685], Loss: 0.4482\n",
      "Epoch [5/5], Step [720/1685], Loss: 0.0012\n",
      "Epoch [5/5], Step [730/1685], Loss: 0.0624\n",
      "Epoch [5/5], Step [740/1685], Loss: 0.1337\n",
      "Epoch [5/5], Step [750/1685], Loss: 0.1968\n",
      "Epoch [5/5], Step [760/1685], Loss: 0.0220\n",
      "Epoch [5/5], Step [770/1685], Loss: 0.0020\n",
      "Epoch [5/5], Step [780/1685], Loss: 0.0587\n",
      "Epoch [5/5], Step [790/1685], Loss: 0.0032\n",
      "Epoch [5/5], Step [800/1685], Loss: 0.0919\n",
      "Epoch [5/5], Step [810/1685], Loss: 0.0240\n",
      "Epoch [5/5], Step [820/1685], Loss: 0.0009\n",
      "Epoch [5/5], Step [830/1685], Loss: 0.0163\n",
      "Epoch [5/5], Step [840/1685], Loss: 0.0016\n",
      "Epoch [5/5], Step [850/1685], Loss: 0.0019\n",
      "Epoch [5/5], Step [860/1685], Loss: 0.2918\n",
      "Epoch [5/5], Step [870/1685], Loss: 0.0004\n",
      "Epoch [5/5], Step [880/1685], Loss: 0.0314\n",
      "Epoch [5/5], Step [890/1685], Loss: 0.0042\n",
      "Epoch [5/5], Step [900/1685], Loss: 0.0261\n",
      "Epoch [5/5], Step [910/1685], Loss: 0.0005\n",
      "Epoch [5/5], Step [920/1685], Loss: 0.0378\n",
      "Epoch [5/5], Step [930/1685], Loss: 0.0005\n",
      "Epoch [5/5], Step [940/1685], Loss: 0.0063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [950/1685], Loss: 0.0023\n",
      "Epoch [5/5], Step [960/1685], Loss: 0.0092\n",
      "Epoch [5/5], Step [970/1685], Loss: 0.0021\n",
      "Epoch [5/5], Step [980/1685], Loss: 0.4303\n",
      "Epoch [5/5], Step [990/1685], Loss: 0.0056\n",
      "Epoch [5/5], Step [1000/1685], Loss: 0.0581\n",
      "Epoch [5/5], Step [1010/1685], Loss: 0.0033\n",
      "Epoch [5/5], Step [1020/1685], Loss: 0.0682\n",
      "Epoch [5/5], Step [1030/1685], Loss: 0.0055\n",
      "Epoch [5/5], Step [1040/1685], Loss: 0.1156\n",
      "Epoch [5/5], Step [1050/1685], Loss: 0.0586\n",
      "Epoch [5/5], Step [1060/1685], Loss: 0.0078\n",
      "Epoch [5/5], Step [1070/1685], Loss: 0.5473\n",
      "Epoch [5/5], Step [1080/1685], Loss: 0.4040\n",
      "Epoch [5/5], Step [1090/1685], Loss: 0.4522\n",
      "Epoch [5/5], Step [1100/1685], Loss: 0.0033\n",
      "Epoch [5/5], Step [1110/1685], Loss: 0.1288\n",
      "Epoch [5/5], Step [1120/1685], Loss: 0.0177\n",
      "Epoch [5/5], Step [1130/1685], Loss: 0.0072\n",
      "Epoch [5/5], Step [1140/1685], Loss: 0.0465\n",
      "Epoch [5/5], Step [1150/1685], Loss: 0.2432\n",
      "Epoch [5/5], Step [1160/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [1170/1685], Loss: 0.0015\n",
      "Epoch [5/5], Step [1180/1685], Loss: 0.0978\n",
      "Epoch [5/5], Step [1190/1685], Loss: 0.1957\n",
      "Epoch [5/5], Step [1200/1685], Loss: 0.1334\n",
      "Epoch [5/5], Step [1210/1685], Loss: 0.0061\n",
      "Epoch [5/5], Step [1220/1685], Loss: 0.0001\n",
      "Epoch [5/5], Step [1230/1685], Loss: 0.0395\n",
      "Epoch [5/5], Step [1240/1685], Loss: 0.0850\n",
      "Epoch [5/5], Step [1250/1685], Loss: 0.8174\n",
      "Epoch [5/5], Step [1260/1685], Loss: 0.3166\n",
      "Epoch [5/5], Step [1270/1685], Loss: 0.0036\n",
      "Epoch [5/5], Step [1280/1685], Loss: 0.0013\n",
      "Epoch [5/5], Step [1290/1685], Loss: 0.1859\n",
      "Epoch [5/5], Step [1300/1685], Loss: 0.0141\n",
      "Epoch [5/5], Step [1310/1685], Loss: 0.0017\n",
      "Epoch [5/5], Step [1320/1685], Loss: 0.0004\n",
      "Epoch [5/5], Step [1330/1685], Loss: 0.2155\n",
      "Epoch [5/5], Step [1340/1685], Loss: 0.1094\n",
      "Epoch [5/5], Step [1350/1685], Loss: 0.0631\n",
      "Epoch [5/5], Step [1360/1685], Loss: 0.1433\n",
      "Epoch [5/5], Step [1370/1685], Loss: 0.6104\n",
      "Epoch [5/5], Step [1380/1685], Loss: 0.0607\n",
      "Epoch [5/5], Step [1390/1685], Loss: 0.0956\n",
      "Epoch [5/5], Step [1400/1685], Loss: 0.0011\n",
      "Epoch [5/5], Step [1410/1685], Loss: 0.0104\n",
      "Epoch [5/5], Step [1420/1685], Loss: 0.0012\n",
      "Epoch [5/5], Step [1430/1685], Loss: 0.2879\n",
      "Epoch [5/5], Step [1440/1685], Loss: 0.0046\n",
      "Epoch [5/5], Step [1450/1685], Loss: 0.1902\n",
      "Epoch [5/5], Step [1460/1685], Loss: 0.3519\n",
      "Epoch [5/5], Step [1470/1685], Loss: 0.4807\n",
      "Epoch [5/5], Step [1480/1685], Loss: 0.0040\n",
      "Epoch [5/5], Step [1490/1685], Loss: 0.0161\n",
      "Epoch [5/5], Step [1500/1685], Loss: 0.0337\n",
      "Epoch [5/5], Step [1510/1685], Loss: 0.0381\n",
      "Epoch [5/5], Step [1520/1685], Loss: 0.1152\n",
      "Epoch [5/5], Step [1530/1685], Loss: 0.0242\n",
      "Epoch [5/5], Step [1540/1685], Loss: 0.3301\n",
      "Epoch [5/5], Step [1550/1685], Loss: 0.3094\n",
      "Epoch [5/5], Step [1560/1685], Loss: 0.0139\n",
      "Epoch [5/5], Step [1570/1685], Loss: 0.0066\n",
      "Epoch [5/5], Step [1580/1685], Loss: 0.2923\n",
      "Epoch [5/5], Step [1590/1685], Loss: 0.1185\n",
      "Epoch [5/5], Step [1600/1685], Loss: 0.1717\n",
      "Epoch [5/5], Step [1610/1685], Loss: 0.2183\n",
      "Epoch [5/5], Step [1620/1685], Loss: 0.0265\n",
      "Epoch [5/5], Step [1630/1685], Loss: 0.0263\n",
      "Epoch [5/5], Step [1640/1685], Loss: 0.1879\n",
      "Epoch [5/5], Step [1650/1685], Loss: 0.0071\n",
      "Epoch [5/5], Step [1660/1685], Loss: 0.0107\n",
      "Epoch [5/5], Step [1670/1685], Loss: 0.0087\n",
      "Epoch [5/5], Step [1680/1685], Loss: 0.0083\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuBMR+RCDOxGRD/1/VEsH4k2o59UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f597cf07510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.9 s, sys: 1.24 s, total: 44.1 s\n",
      "Wall time: 43 s\n"
     ]
    }
   ],
   "source": [
    "%time train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    # Write loops for testing the model on the test set\n",
    "    # You should also print out the accuracy of the model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images)\n",
    "        \n",
    "        if(use_gpu):\n",
    "            images = images.cuda()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "    print('Accuracy of the network on the ' + str(total) +' test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1870 test images: 93 %\n",
      "CPU times: user 476 ms, sys: 4 ms, total: 480 ms\n",
      "Wall time: 479 ms\n"
     ]
    }
   ],
   "source": [
    "%time test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training from scratch\n",
    "Now we shall try training the model from scratch and observe the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reinstantiate the model and optimizer\n",
    "model = CustomResnet(num_classes = 10)\n",
    "use_gpu = False\n",
    "if(torch.cuda.is_available()):\n",
    "    use_gpu = True\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # Use Adam optimizer, use learning_rate hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [10/1685], Loss: 2.0222\n",
      "Epoch [1/5], Step [20/1685], Loss: 1.7721\n",
      "Epoch [1/5], Step [30/1685], Loss: 1.5709\n",
      "Epoch [1/5], Step [40/1685], Loss: 1.1590\n",
      "Epoch [1/5], Step [50/1685], Loss: 1.1239\n",
      "Epoch [1/5], Step [60/1685], Loss: 1.0065\n",
      "Epoch [1/5], Step [70/1685], Loss: 0.5045\n",
      "Epoch [1/5], Step [80/1685], Loss: 0.6897\n",
      "Epoch [1/5], Step [90/1685], Loss: 0.8144\n",
      "Epoch [1/5], Step [100/1685], Loss: 0.5093\n",
      "Epoch [1/5], Step [110/1685], Loss: 0.4225\n",
      "Epoch [1/5], Step [120/1685], Loss: 0.6171\n",
      "Epoch [1/5], Step [130/1685], Loss: 0.5063\n",
      "Epoch [1/5], Step [140/1685], Loss: 0.5847\n",
      "Epoch [1/5], Step [150/1685], Loss: 0.4232\n",
      "Epoch [1/5], Step [160/1685], Loss: 0.4646\n",
      "Epoch [1/5], Step [170/1685], Loss: 0.4566\n",
      "Epoch [1/5], Step [180/1685], Loss: 0.1341\n",
      "Epoch [1/5], Step [190/1685], Loss: 0.5362\n",
      "Epoch [1/5], Step [200/1685], Loss: 0.7433\n",
      "Epoch [1/5], Step [210/1685], Loss: 0.3382\n",
      "Epoch [1/5], Step [220/1685], Loss: 0.2675\n",
      "Epoch [1/5], Step [230/1685], Loss: 0.3082\n",
      "Epoch [1/5], Step [240/1685], Loss: 0.8020\n",
      "Epoch [1/5], Step [250/1685], Loss: 0.0863\n",
      "Epoch [1/5], Step [260/1685], Loss: 0.2517\n",
      "Epoch [1/5], Step [270/1685], Loss: 0.1253\n",
      "Epoch [1/5], Step [280/1685], Loss: 0.1605\n",
      "Epoch [1/5], Step [290/1685], Loss: 0.3024\n",
      "Epoch [1/5], Step [300/1685], Loss: 0.3119\n",
      "Epoch [1/5], Step [310/1685], Loss: 0.3072\n",
      "Epoch [1/5], Step [320/1685], Loss: 0.5523\n",
      "Epoch [1/5], Step [330/1685], Loss: 0.4073\n",
      "Epoch [1/5], Step [340/1685], Loss: 0.2949\n",
      "Epoch [1/5], Step [350/1685], Loss: 0.2756\n",
      "Epoch [1/5], Step [360/1685], Loss: 0.2818\n",
      "Epoch [1/5], Step [370/1685], Loss: 0.3629\n",
      "Epoch [1/5], Step [380/1685], Loss: 0.4930\n",
      "Epoch [1/5], Step [390/1685], Loss: 0.2088\n",
      "Epoch [1/5], Step [400/1685], Loss: 1.1734\n",
      "Epoch [1/5], Step [410/1685], Loss: 0.2920\n",
      "Epoch [1/5], Step [420/1685], Loss: 0.2910\n",
      "Epoch [1/5], Step [430/1685], Loss: 0.2163\n",
      "Epoch [1/5], Step [440/1685], Loss: 0.3945\n",
      "Epoch [1/5], Step [450/1685], Loss: 0.0488\n",
      "Epoch [1/5], Step [460/1685], Loss: 0.9135\n",
      "Epoch [1/5], Step [470/1685], Loss: 0.5017\n",
      "Epoch [1/5], Step [480/1685], Loss: 0.4958\n",
      "Epoch [1/5], Step [490/1685], Loss: 0.1782\n",
      "Epoch [1/5], Step [500/1685], Loss: 0.5982\n",
      "Epoch [1/5], Step [510/1685], Loss: 0.6535\n",
      "Epoch [1/5], Step [520/1685], Loss: 0.1028\n",
      "Epoch [1/5], Step [530/1685], Loss: 0.0495\n",
      "Epoch [1/5], Step [540/1685], Loss: 0.3442\n",
      "Epoch [1/5], Step [550/1685], Loss: 0.5746\n",
      "Epoch [1/5], Step [560/1685], Loss: 0.2534\n",
      "Epoch [1/5], Step [570/1685], Loss: 0.2651\n",
      "Epoch [1/5], Step [580/1685], Loss: 0.0382\n",
      "Epoch [1/5], Step [590/1685], Loss: 1.0388\n",
      "Epoch [1/5], Step [600/1685], Loss: 0.3803\n",
      "Epoch [1/5], Step [610/1685], Loss: 0.1553\n",
      "Epoch [1/5], Step [620/1685], Loss: 0.4917\n",
      "Epoch [1/5], Step [630/1685], Loss: 0.4367\n",
      "Epoch [1/5], Step [640/1685], Loss: 0.3695\n",
      "Epoch [1/5], Step [650/1685], Loss: 0.0501\n",
      "Epoch [1/5], Step [660/1685], Loss: 0.4656\n",
      "Epoch [1/5], Step [670/1685], Loss: 0.1379\n",
      "Epoch [1/5], Step [680/1685], Loss: 0.0483\n",
      "Epoch [1/5], Step [690/1685], Loss: 0.4097\n",
      "Epoch [1/5], Step [700/1685], Loss: 0.2003\n",
      "Epoch [1/5], Step [710/1685], Loss: 0.3818\n",
      "Epoch [1/5], Step [720/1685], Loss: 0.4597\n",
      "Epoch [1/5], Step [730/1685], Loss: 0.0887\n",
      "Epoch [1/5], Step [740/1685], Loss: 0.5992\n",
      "Epoch [1/5], Step [750/1685], Loss: 0.0880\n",
      "Epoch [1/5], Step [760/1685], Loss: 0.8999\n",
      "Epoch [1/5], Step [770/1685], Loss: 0.3678\n",
      "Epoch [1/5], Step [780/1685], Loss: 0.1064\n",
      "Epoch [1/5], Step [790/1685], Loss: 0.0175\n",
      "Epoch [1/5], Step [800/1685], Loss: 0.4585\n",
      "Epoch [1/5], Step [810/1685], Loss: 0.6428\n",
      "Epoch [1/5], Step [820/1685], Loss: 0.4680\n",
      "Epoch [1/5], Step [830/1685], Loss: 0.7406\n",
      "Epoch [1/5], Step [840/1685], Loss: 0.3483\n",
      "Epoch [1/5], Step [850/1685], Loss: 0.3024\n",
      "Epoch [1/5], Step [860/1685], Loss: 0.7305\n",
      "Epoch [1/5], Step [870/1685], Loss: 0.1149\n",
      "Epoch [1/5], Step [880/1685], Loss: 0.5062\n",
      "Epoch [1/5], Step [890/1685], Loss: 0.0554\n",
      "Epoch [1/5], Step [900/1685], Loss: 0.0490\n",
      "Epoch [1/5], Step [910/1685], Loss: 0.6738\n",
      "Epoch [1/5], Step [920/1685], Loss: 0.1638\n",
      "Epoch [1/5], Step [930/1685], Loss: 0.1267\n",
      "Epoch [1/5], Step [940/1685], Loss: 0.8786\n",
      "Epoch [1/5], Step [950/1685], Loss: 0.3305\n",
      "Epoch [1/5], Step [960/1685], Loss: 0.2743\n",
      "Epoch [1/5], Step [970/1685], Loss: 0.1450\n",
      "Epoch [1/5], Step [980/1685], Loss: 0.0337\n",
      "Epoch [1/5], Step [990/1685], Loss: 0.2807\n",
      "Epoch [1/5], Step [1000/1685], Loss: 0.0306\n",
      "Epoch [1/5], Step [1010/1685], Loss: 0.5401\n",
      "Epoch [1/5], Step [1020/1685], Loss: 0.6427\n",
      "Epoch [1/5], Step [1030/1685], Loss: 0.1700\n",
      "Epoch [1/5], Step [1040/1685], Loss: 0.6038\n",
      "Epoch [1/5], Step [1050/1685], Loss: 0.0914\n",
      "Epoch [1/5], Step [1060/1685], Loss: 0.3087\n",
      "Epoch [1/5], Step [1070/1685], Loss: 0.0403\n",
      "Epoch [1/5], Step [1080/1685], Loss: 0.2002\n",
      "Epoch [1/5], Step [1090/1685], Loss: 0.0521\n",
      "Epoch [1/5], Step [1100/1685], Loss: 0.0133\n",
      "Epoch [1/5], Step [1110/1685], Loss: 0.0146\n",
      "Epoch [1/5], Step [1120/1685], Loss: 0.1197\n",
      "Epoch [1/5], Step [1130/1685], Loss: 0.0555\n",
      "Epoch [1/5], Step [1140/1685], Loss: 0.1485\n",
      "Epoch [1/5], Step [1150/1685], Loss: 0.3396\n",
      "Epoch [1/5], Step [1160/1685], Loss: 0.0240\n",
      "Epoch [1/5], Step [1170/1685], Loss: 0.0192\n",
      "Epoch [1/5], Step [1180/1685], Loss: 0.4731\n",
      "Epoch [1/5], Step [1190/1685], Loss: 0.0096\n",
      "Epoch [1/5], Step [1200/1685], Loss: 0.2672\n",
      "Epoch [1/5], Step [1210/1685], Loss: 0.5825\n",
      "Epoch [1/5], Step [1220/1685], Loss: 0.2633\n",
      "Epoch [1/5], Step [1230/1685], Loss: 0.0383\n",
      "Epoch [1/5], Step [1240/1685], Loss: 0.4596\n",
      "Epoch [1/5], Step [1250/1685], Loss: 0.1901\n",
      "Epoch [1/5], Step [1260/1685], Loss: 0.3160\n",
      "Epoch [1/5], Step [1270/1685], Loss: 0.0683\n",
      "Epoch [1/5], Step [1280/1685], Loss: 0.1091\n",
      "Epoch [1/5], Step [1290/1685], Loss: 0.0244\n",
      "Epoch [1/5], Step [1300/1685], Loss: 0.0846\n",
      "Epoch [1/5], Step [1310/1685], Loss: 0.2239\n",
      "Epoch [1/5], Step [1320/1685], Loss: 0.1218\n",
      "Epoch [1/5], Step [1330/1685], Loss: 0.3437\n",
      "Epoch [1/5], Step [1340/1685], Loss: 0.8633\n",
      "Epoch [1/5], Step [1350/1685], Loss: 0.4322\n",
      "Epoch [1/5], Step [1360/1685], Loss: 0.1014\n",
      "Epoch [1/5], Step [1370/1685], Loss: 0.7497\n",
      "Epoch [1/5], Step [1380/1685], Loss: 0.3041\n",
      "Epoch [1/5], Step [1390/1685], Loss: 0.0577\n",
      "Epoch [1/5], Step [1400/1685], Loss: 0.0454\n",
      "Epoch [1/5], Step [1410/1685], Loss: 0.1094\n",
      "Epoch [1/5], Step [1420/1685], Loss: 0.0167\n",
      "Epoch [1/5], Step [1430/1685], Loss: 0.0534\n",
      "Epoch [1/5], Step [1440/1685], Loss: 0.0697\n",
      "Epoch [1/5], Step [1450/1685], Loss: 0.1385\n",
      "Epoch [1/5], Step [1460/1685], Loss: 0.7659\n",
      "Epoch [1/5], Step [1470/1685], Loss: 0.6787\n",
      "Epoch [1/5], Step [1480/1685], Loss: 0.0973\n",
      "Epoch [1/5], Step [1490/1685], Loss: 0.0604\n",
      "Epoch [1/5], Step [1500/1685], Loss: 0.0956\n",
      "Epoch [1/5], Step [1510/1685], Loss: 0.0114\n",
      "Epoch [1/5], Step [1520/1685], Loss: 0.0205\n",
      "Epoch [1/5], Step [1530/1685], Loss: 0.0464\n",
      "Epoch [1/5], Step [1540/1685], Loss: 0.0747\n",
      "Epoch [1/5], Step [1550/1685], Loss: 0.4700\n",
      "Epoch [1/5], Step [1560/1685], Loss: 0.1130\n",
      "Epoch [1/5], Step [1570/1685], Loss: 0.0726\n",
      "Epoch [1/5], Step [1580/1685], Loss: 0.2016\n",
      "Epoch [1/5], Step [1590/1685], Loss: 0.0413\n",
      "Epoch [1/5], Step [1600/1685], Loss: 0.1518\n",
      "Epoch [1/5], Step [1610/1685], Loss: 0.0611\n",
      "Epoch [1/5], Step [1620/1685], Loss: 0.1104\n",
      "Epoch [1/5], Step [1630/1685], Loss: 0.0311\n",
      "Epoch [1/5], Step [1640/1685], Loss: 0.4324\n",
      "Epoch [1/5], Step [1650/1685], Loss: 0.0531\n",
      "Epoch [1/5], Step [1660/1685], Loss: 0.3327\n",
      "Epoch [1/5], Step [1670/1685], Loss: 0.0120\n",
      "Epoch [1/5], Step [1680/1685], Loss: 0.1759\n",
      "Epoch [2/5], Step [10/1685], Loss: 0.2217\n",
      "Epoch [2/5], Step [20/1685], Loss: 0.6226\n",
      "Epoch [2/5], Step [30/1685], Loss: 0.1895\n",
      "Epoch [2/5], Step [40/1685], Loss: 0.4150\n",
      "Epoch [2/5], Step [50/1685], Loss: 0.0128\n",
      "Epoch [2/5], Step [60/1685], Loss: 0.3581\n",
      "Epoch [2/5], Step [70/1685], Loss: 0.1987\n",
      "Epoch [2/5], Step [80/1685], Loss: 0.2511\n",
      "Epoch [2/5], Step [90/1685], Loss: 0.1571\n",
      "Epoch [2/5], Step [100/1685], Loss: 0.1454\n",
      "Epoch [2/5], Step [110/1685], Loss: 0.4444\n",
      "Epoch [2/5], Step [120/1685], Loss: 0.3133\n",
      "Epoch [2/5], Step [130/1685], Loss: 0.0108\n",
      "Epoch [2/5], Step [140/1685], Loss: 0.3076\n",
      "Epoch [2/5], Step [150/1685], Loss: 0.0422\n",
      "Epoch [2/5], Step [160/1685], Loss: 0.4275\n",
      "Epoch [2/5], Step [170/1685], Loss: 0.0549\n",
      "Epoch [2/5], Step [180/1685], Loss: 0.0319\n",
      "Epoch [2/5], Step [190/1685], Loss: 0.0268\n",
      "Epoch [2/5], Step [200/1685], Loss: 0.0162\n",
      "Epoch [2/5], Step [210/1685], Loss: 0.0806\n",
      "Epoch [2/5], Step [220/1685], Loss: 0.4916\n",
      "Epoch [2/5], Step [230/1685], Loss: 0.3021\n",
      "Epoch [2/5], Step [240/1685], Loss: 0.2064\n",
      "Epoch [2/5], Step [250/1685], Loss: 0.6035\n",
      "Epoch [2/5], Step [260/1685], Loss: 0.0706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Step [270/1685], Loss: 0.3015\n",
      "Epoch [2/5], Step [280/1685], Loss: 0.0271\n",
      "Epoch [2/5], Step [290/1685], Loss: 0.0534\n",
      "Epoch [2/5], Step [300/1685], Loss: 0.3927\n",
      "Epoch [2/5], Step [310/1685], Loss: 0.1033\n",
      "Epoch [2/5], Step [320/1685], Loss: 0.3887\n",
      "Epoch [2/5], Step [330/1685], Loss: 0.4449\n",
      "Epoch [2/5], Step [340/1685], Loss: 0.2444\n",
      "Epoch [2/5], Step [350/1685], Loss: 0.0765\n",
      "Epoch [2/5], Step [360/1685], Loss: 0.0213\n",
      "Epoch [2/5], Step [370/1685], Loss: 0.0398\n",
      "Epoch [2/5], Step [380/1685], Loss: 0.0237\n",
      "Epoch [2/5], Step [390/1685], Loss: 0.0761\n",
      "Epoch [2/5], Step [400/1685], Loss: 0.2136\n",
      "Epoch [2/5], Step [410/1685], Loss: 0.2478\n",
      "Epoch [2/5], Step [420/1685], Loss: 0.0691\n",
      "Epoch [2/5], Step [430/1685], Loss: 0.3087\n",
      "Epoch [2/5], Step [440/1685], Loss: 0.0642\n",
      "Epoch [2/5], Step [450/1685], Loss: 1.1088\n",
      "Epoch [2/5], Step [460/1685], Loss: 0.9450\n",
      "Epoch [2/5], Step [470/1685], Loss: 0.2776\n",
      "Epoch [2/5], Step [480/1685], Loss: 0.0073\n",
      "Epoch [2/5], Step [490/1685], Loss: 0.0498\n",
      "Epoch [2/5], Step [500/1685], Loss: 0.1347\n",
      "Epoch [2/5], Step [510/1685], Loss: 0.0101\n",
      "Epoch [2/5], Step [520/1685], Loss: 0.2938\n",
      "Epoch [2/5], Step [530/1685], Loss: 0.0357\n",
      "Epoch [2/5], Step [540/1685], Loss: 0.0227\n",
      "Epoch [2/5], Step [550/1685], Loss: 0.1906\n",
      "Epoch [2/5], Step [560/1685], Loss: 0.2523\n",
      "Epoch [2/5], Step [570/1685], Loss: 0.2654\n",
      "Epoch [2/5], Step [580/1685], Loss: 0.0419\n",
      "Epoch [2/5], Step [590/1685], Loss: 0.2246\n",
      "Epoch [2/5], Step [600/1685], Loss: 0.3362\n",
      "Epoch [2/5], Step [610/1685], Loss: 0.0068\n",
      "Epoch [2/5], Step [620/1685], Loss: 0.0203\n",
      "Epoch [2/5], Step [630/1685], Loss: 0.0222\n",
      "Epoch [2/5], Step [640/1685], Loss: 0.1708\n",
      "Epoch [2/5], Step [650/1685], Loss: 0.5330\n",
      "Epoch [2/5], Step [660/1685], Loss: 0.3003\n",
      "Epoch [2/5], Step [670/1685], Loss: 0.1095\n",
      "Epoch [2/5], Step [680/1685], Loss: 0.0530\n",
      "Epoch [2/5], Step [690/1685], Loss: 0.2841\n",
      "Epoch [2/5], Step [700/1685], Loss: 0.7188\n",
      "Epoch [2/5], Step [710/1685], Loss: 0.2642\n",
      "Epoch [2/5], Step [720/1685], Loss: 0.0729\n",
      "Epoch [2/5], Step [730/1685], Loss: 0.0067\n",
      "Epoch [2/5], Step [740/1685], Loss: 0.0308\n",
      "Epoch [2/5], Step [750/1685], Loss: 0.1064\n",
      "Epoch [2/5], Step [760/1685], Loss: 0.0223\n",
      "Epoch [2/5], Step [770/1685], Loss: 0.0223\n",
      "Epoch [2/5], Step [780/1685], Loss: 0.0327\n",
      "Epoch [2/5], Step [790/1685], Loss: 0.5066\n",
      "Epoch [2/5], Step [800/1685], Loss: 0.3177\n",
      "Epoch [2/5], Step [810/1685], Loss: 0.0958\n",
      "Epoch [2/5], Step [820/1685], Loss: 0.0196\n",
      "Epoch [2/5], Step [830/1685], Loss: 0.0823\n",
      "Epoch [2/5], Step [840/1685], Loss: 0.1286\n",
      "Epoch [2/5], Step [850/1685], Loss: 0.0921\n",
      "Epoch [2/5], Step [860/1685], Loss: 0.0436\n",
      "Epoch [2/5], Step [870/1685], Loss: 0.0751\n",
      "Epoch [2/5], Step [880/1685], Loss: 0.0220\n",
      "Epoch [2/5], Step [890/1685], Loss: 0.0198\n",
      "Epoch [2/5], Step [900/1685], Loss: 0.3702\n",
      "Epoch [2/5], Step [910/1685], Loss: 0.5728\n",
      "Epoch [2/5], Step [920/1685], Loss: 0.0072\n",
      "Epoch [2/5], Step [930/1685], Loss: 0.7463\n",
      "Epoch [2/5], Step [940/1685], Loss: 0.3258\n",
      "Epoch [2/5], Step [950/1685], Loss: 0.0753\n",
      "Epoch [2/5], Step [960/1685], Loss: 0.0625\n",
      "Epoch [2/5], Step [970/1685], Loss: 0.5756\n",
      "Epoch [2/5], Step [980/1685], Loss: 0.0261\n",
      "Epoch [2/5], Step [990/1685], Loss: 0.0598\n",
      "Epoch [2/5], Step [1000/1685], Loss: 0.5800\n",
      "Epoch [2/5], Step [1010/1685], Loss: 0.4813\n",
      "Epoch [2/5], Step [1020/1685], Loss: 0.3311\n",
      "Epoch [2/5], Step [1030/1685], Loss: 0.3465\n",
      "Epoch [2/5], Step [1040/1685], Loss: 0.1378\n",
      "Epoch [2/5], Step [1050/1685], Loss: 0.3125\n",
      "Epoch [2/5], Step [1060/1685], Loss: 0.0143\n",
      "Epoch [2/5], Step [1070/1685], Loss: 0.0152\n",
      "Epoch [2/5], Step [1080/1685], Loss: 0.0104\n",
      "Epoch [2/5], Step [1090/1685], Loss: 0.1669\n",
      "Epoch [2/5], Step [1100/1685], Loss: 0.0451\n",
      "Epoch [2/5], Step [1110/1685], Loss: 0.7275\n",
      "Epoch [2/5], Step [1120/1685], Loss: 0.0247\n",
      "Epoch [2/5], Step [1130/1685], Loss: 0.5044\n",
      "Epoch [2/5], Step [1140/1685], Loss: 0.2726\n",
      "Epoch [2/5], Step [1150/1685], Loss: 0.0167\n",
      "Epoch [2/5], Step [1160/1685], Loss: 0.0119\n",
      "Epoch [2/5], Step [1170/1685], Loss: 0.0785\n",
      "Epoch [2/5], Step [1180/1685], Loss: 0.1513\n",
      "Epoch [2/5], Step [1190/1685], Loss: 0.0156\n",
      "Epoch [2/5], Step [1200/1685], Loss: 0.0542\n",
      "Epoch [2/5], Step [1210/1685], Loss: 0.7902\n",
      "Epoch [2/5], Step [1220/1685], Loss: 0.1528\n",
      "Epoch [2/5], Step [1230/1685], Loss: 0.0801\n",
      "Epoch [2/5], Step [1240/1685], Loss: 0.1077\n",
      "Epoch [2/5], Step [1250/1685], Loss: 0.0015\n",
      "Epoch [2/5], Step [1260/1685], Loss: 0.2988\n",
      "Epoch [2/5], Step [1270/1685], Loss: 0.0459\n",
      "Epoch [2/5], Step [1280/1685], Loss: 0.0308\n",
      "Epoch [2/5], Step [1290/1685], Loss: 0.0643\n",
      "Epoch [2/5], Step [1300/1685], Loss: 0.3425\n",
      "Epoch [2/5], Step [1310/1685], Loss: 0.2182\n",
      "Epoch [2/5], Step [1320/1685], Loss: 0.3459\n",
      "Epoch [2/5], Step [1330/1685], Loss: 0.1391\n",
      "Epoch [2/5], Step [1340/1685], Loss: 0.0403\n",
      "Epoch [2/5], Step [1350/1685], Loss: 0.0090\n",
      "Epoch [2/5], Step [1360/1685], Loss: 0.0136\n",
      "Epoch [2/5], Step [1370/1685], Loss: 0.0460\n",
      "Epoch [2/5], Step [1380/1685], Loss: 0.0035\n",
      "Epoch [2/5], Step [1390/1685], Loss: 0.0330\n",
      "Epoch [2/5], Step [1400/1685], Loss: 0.2092\n",
      "Epoch [2/5], Step [1410/1685], Loss: 0.0724\n",
      "Epoch [2/5], Step [1420/1685], Loss: 0.0740\n",
      "Epoch [2/5], Step [1430/1685], Loss: 0.0150\n",
      "Epoch [2/5], Step [1440/1685], Loss: 0.6643\n",
      "Epoch [2/5], Step [1450/1685], Loss: 0.1427\n",
      "Epoch [2/5], Step [1460/1685], Loss: 0.1029\n",
      "Epoch [2/5], Step [1470/1685], Loss: 0.5284\n",
      "Epoch [2/5], Step [1480/1685], Loss: 0.1085\n",
      "Epoch [2/5], Step [1490/1685], Loss: 0.8015\n",
      "Epoch [2/5], Step [1500/1685], Loss: 0.0184\n",
      "Epoch [2/5], Step [1510/1685], Loss: 0.0323\n",
      "Epoch [2/5], Step [1520/1685], Loss: 0.5694\n",
      "Epoch [2/5], Step [1530/1685], Loss: 0.0198\n",
      "Epoch [2/5], Step [1540/1685], Loss: 0.3736\n",
      "Epoch [2/5], Step [1550/1685], Loss: 0.1209\n",
      "Epoch [2/5], Step [1560/1685], Loss: 0.1883\n",
      "Epoch [2/5], Step [1570/1685], Loss: 0.0120\n",
      "Epoch [2/5], Step [1580/1685], Loss: 0.1055\n",
      "Epoch [2/5], Step [1590/1685], Loss: 0.0044\n",
      "Epoch [2/5], Step [1600/1685], Loss: 0.6262\n",
      "Epoch [2/5], Step [1610/1685], Loss: 0.2677\n",
      "Epoch [2/5], Step [1620/1685], Loss: 0.0273\n",
      "Epoch [2/5], Step [1630/1685], Loss: 0.2693\n",
      "Epoch [2/5], Step [1640/1685], Loss: 0.1608\n",
      "Epoch [2/5], Step [1650/1685], Loss: 0.3564\n",
      "Epoch [2/5], Step [1660/1685], Loss: 0.9358\n",
      "Epoch [2/5], Step [1670/1685], Loss: 0.2173\n",
      "Epoch [2/5], Step [1680/1685], Loss: 0.0068\n",
      "Epoch [3/5], Step [10/1685], Loss: 0.0849\n",
      "Epoch [3/5], Step [20/1685], Loss: 0.5406\n",
      "Epoch [3/5], Step [30/1685], Loss: 0.0611\n",
      "Epoch [3/5], Step [40/1685], Loss: 0.0843\n",
      "Epoch [3/5], Step [50/1685], Loss: 0.0785\n",
      "Epoch [3/5], Step [60/1685], Loss: 0.0075\n",
      "Epoch [3/5], Step [70/1685], Loss: 0.4681\n",
      "Epoch [3/5], Step [80/1685], Loss: 0.0413\n",
      "Epoch [3/5], Step [90/1685], Loss: 0.0050\n",
      "Epoch [3/5], Step [100/1685], Loss: 0.0313\n",
      "Epoch [3/5], Step [110/1685], Loss: 0.1579\n",
      "Epoch [3/5], Step [120/1685], Loss: 0.6450\n",
      "Epoch [3/5], Step [130/1685], Loss: 0.7034\n",
      "Epoch [3/5], Step [140/1685], Loss: 0.0118\n",
      "Epoch [3/5], Step [150/1685], Loss: 0.5168\n",
      "Epoch [3/5], Step [160/1685], Loss: 0.0094\n",
      "Epoch [3/5], Step [170/1685], Loss: 0.7157\n",
      "Epoch [3/5], Step [180/1685], Loss: 0.0325\n",
      "Epoch [3/5], Step [190/1685], Loss: 0.0312\n",
      "Epoch [3/5], Step [200/1685], Loss: 0.1151\n",
      "Epoch [3/5], Step [210/1685], Loss: 0.1803\n",
      "Epoch [3/5], Step [220/1685], Loss: 0.2918\n",
      "Epoch [3/5], Step [230/1685], Loss: 0.2544\n",
      "Epoch [3/5], Step [240/1685], Loss: 0.0304\n",
      "Epoch [3/5], Step [250/1685], Loss: 0.0025\n",
      "Epoch [3/5], Step [260/1685], Loss: 0.1323\n",
      "Epoch [3/5], Step [270/1685], Loss: 0.5386\n",
      "Epoch [3/5], Step [280/1685], Loss: 0.0091\n",
      "Epoch [3/5], Step [290/1685], Loss: 0.4847\n",
      "Epoch [3/5], Step [300/1685], Loss: 0.0901\n",
      "Epoch [3/5], Step [310/1685], Loss: 0.0024\n",
      "Epoch [3/5], Step [320/1685], Loss: 0.0383\n",
      "Epoch [3/5], Step [330/1685], Loss: 0.3951\n",
      "Epoch [3/5], Step [340/1685], Loss: 0.0396\n",
      "Epoch [3/5], Step [350/1685], Loss: 0.2558\n",
      "Epoch [3/5], Step [360/1685], Loss: 0.1140\n",
      "Epoch [3/5], Step [370/1685], Loss: 0.0601\n",
      "Epoch [3/5], Step [380/1685], Loss: 0.0831\n",
      "Epoch [3/5], Step [390/1685], Loss: 0.4662\n",
      "Epoch [3/5], Step [400/1685], Loss: 0.0099\n",
      "Epoch [3/5], Step [410/1685], Loss: 0.1138\n",
      "Epoch [3/5], Step [420/1685], Loss: 0.1365\n",
      "Epoch [3/5], Step [430/1685], Loss: 0.0324\n",
      "Epoch [3/5], Step [440/1685], Loss: 0.0107\n",
      "Epoch [3/5], Step [450/1685], Loss: 0.0636\n",
      "Epoch [3/5], Step [460/1685], Loss: 0.0156\n",
      "Epoch [3/5], Step [470/1685], Loss: 0.0033\n",
      "Epoch [3/5], Step [480/1685], Loss: 0.3963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [490/1685], Loss: 0.2408\n",
      "Epoch [3/5], Step [500/1685], Loss: 0.0668\n",
      "Epoch [3/5], Step [510/1685], Loss: 0.0447\n",
      "Epoch [3/5], Step [520/1685], Loss: 0.2452\n",
      "Epoch [3/5], Step [530/1685], Loss: 0.1087\n",
      "Epoch [3/5], Step [540/1685], Loss: 0.0138\n",
      "Epoch [3/5], Step [550/1685], Loss: 0.2513\n",
      "Epoch [3/5], Step [560/1685], Loss: 0.0186\n",
      "Epoch [3/5], Step [570/1685], Loss: 0.1545\n",
      "Epoch [3/5], Step [580/1685], Loss: 1.1573\n",
      "Epoch [3/5], Step [590/1685], Loss: 0.0048\n",
      "Epoch [3/5], Step [600/1685], Loss: 0.0511\n",
      "Epoch [3/5], Step [610/1685], Loss: 0.0646\n",
      "Epoch [3/5], Step [620/1685], Loss: 0.0025\n",
      "Epoch [3/5], Step [630/1685], Loss: 0.1069\n",
      "Epoch [3/5], Step [640/1685], Loss: 0.0580\n",
      "Epoch [3/5], Step [650/1685], Loss: 0.2930\n",
      "Epoch [3/5], Step [660/1685], Loss: 0.0109\n",
      "Epoch [3/5], Step [670/1685], Loss: 0.0793\n",
      "Epoch [3/5], Step [680/1685], Loss: 0.3286\n",
      "Epoch [3/5], Step [690/1685], Loss: 0.0167\n",
      "Epoch [3/5], Step [700/1685], Loss: 0.0043\n",
      "Epoch [3/5], Step [710/1685], Loss: 0.0885\n",
      "Epoch [3/5], Step [720/1685], Loss: 0.0217\n",
      "Epoch [3/5], Step [730/1685], Loss: 0.1637\n",
      "Epoch [3/5], Step [740/1685], Loss: 0.0346\n",
      "Epoch [3/5], Step [750/1685], Loss: 0.0290\n",
      "Epoch [3/5], Step [760/1685], Loss: 0.0299\n",
      "Epoch [3/5], Step [770/1685], Loss: 0.2755\n",
      "Epoch [3/5], Step [780/1685], Loss: 0.0074\n",
      "Epoch [3/5], Step [790/1685], Loss: 0.0411\n",
      "Epoch [3/5], Step [800/1685], Loss: 0.0970\n",
      "Epoch [3/5], Step [810/1685], Loss: 0.0813\n",
      "Epoch [3/5], Step [820/1685], Loss: 0.4439\n",
      "Epoch [3/5], Step [830/1685], Loss: 0.0088\n",
      "Epoch [3/5], Step [840/1685], Loss: 0.0134\n",
      "Epoch [3/5], Step [850/1685], Loss: 0.0355\n",
      "Epoch [3/5], Step [860/1685], Loss: 0.0987\n",
      "Epoch [3/5], Step [870/1685], Loss: 0.1562\n",
      "Epoch [3/5], Step [880/1685], Loss: 0.0034\n",
      "Epoch [3/5], Step [890/1685], Loss: 0.0496\n",
      "Epoch [3/5], Step [900/1685], Loss: 0.0114\n",
      "Epoch [3/5], Step [910/1685], Loss: 0.1829\n",
      "Epoch [3/5], Step [920/1685], Loss: 0.0616\n",
      "Epoch [3/5], Step [930/1685], Loss: 0.0294\n",
      "Epoch [3/5], Step [940/1685], Loss: 0.0062\n",
      "Epoch [3/5], Step [950/1685], Loss: 0.0418\n",
      "Epoch [3/5], Step [960/1685], Loss: 0.0544\n",
      "Epoch [3/5], Step [970/1685], Loss: 0.0575\n",
      "Epoch [3/5], Step [980/1685], Loss: 0.3672\n",
      "Epoch [3/5], Step [990/1685], Loss: 0.2134\n",
      "Epoch [3/5], Step [1000/1685], Loss: 0.5741\n",
      "Epoch [3/5], Step [1010/1685], Loss: 0.5523\n",
      "Epoch [3/5], Step [1020/1685], Loss: 0.2964\n",
      "Epoch [3/5], Step [1030/1685], Loss: 0.0114\n",
      "Epoch [3/5], Step [1040/1685], Loss: 0.0528\n",
      "Epoch [3/5], Step [1050/1685], Loss: 0.2849\n",
      "Epoch [3/5], Step [1060/1685], Loss: 0.0364\n",
      "Epoch [3/5], Step [1070/1685], Loss: 0.2991\n",
      "Epoch [3/5], Step [1080/1685], Loss: 0.0081\n",
      "Epoch [3/5], Step [1090/1685], Loss: 0.0174\n",
      "Epoch [3/5], Step [1100/1685], Loss: 0.0975\n",
      "Epoch [3/5], Step [1110/1685], Loss: 0.2884\n",
      "Epoch [3/5], Step [1120/1685], Loss: 0.0204\n",
      "Epoch [3/5], Step [1130/1685], Loss: 0.2474\n",
      "Epoch [3/5], Step [1140/1685], Loss: 0.4426\n",
      "Epoch [3/5], Step [1150/1685], Loss: 0.1925\n",
      "Epoch [3/5], Step [1160/1685], Loss: 0.2526\n",
      "Epoch [3/5], Step [1170/1685], Loss: 0.2492\n",
      "Epoch [3/5], Step [1180/1685], Loss: 0.0398\n",
      "Epoch [3/5], Step [1190/1685], Loss: 0.0990\n",
      "Epoch [3/5], Step [1200/1685], Loss: 0.2890\n",
      "Epoch [3/5], Step [1210/1685], Loss: 0.0356\n",
      "Epoch [3/5], Step [1220/1685], Loss: 0.0509\n",
      "Epoch [3/5], Step [1230/1685], Loss: 0.0385\n",
      "Epoch [3/5], Step [1240/1685], Loss: 0.2079\n",
      "Epoch [3/5], Step [1250/1685], Loss: 0.3073\n",
      "Epoch [3/5], Step [1260/1685], Loss: 0.3650\n",
      "Epoch [3/5], Step [1270/1685], Loss: 0.0157\n",
      "Epoch [3/5], Step [1280/1685], Loss: 0.2216\n",
      "Epoch [3/5], Step [1290/1685], Loss: 0.0489\n",
      "Epoch [3/5], Step [1300/1685], Loss: 0.1889\n",
      "Epoch [3/5], Step [1310/1685], Loss: 0.1944\n",
      "Epoch [3/5], Step [1320/1685], Loss: 0.0327\n",
      "Epoch [3/5], Step [1330/1685], Loss: 0.1846\n",
      "Epoch [3/5], Step [1340/1685], Loss: 0.1693\n",
      "Epoch [3/5], Step [1350/1685], Loss: 0.7561\n",
      "Epoch [3/5], Step [1360/1685], Loss: 0.0056\n",
      "Epoch [3/5], Step [1370/1685], Loss: 0.3184\n",
      "Epoch [3/5], Step [1380/1685], Loss: 0.0178\n",
      "Epoch [3/5], Step [1390/1685], Loss: 0.0182\n",
      "Epoch [3/5], Step [1400/1685], Loss: 0.0329\n",
      "Epoch [3/5], Step [1410/1685], Loss: 0.0517\n",
      "Epoch [3/5], Step [1420/1685], Loss: 0.2407\n",
      "Epoch [3/5], Step [1430/1685], Loss: 0.0916\n",
      "Epoch [3/5], Step [1440/1685], Loss: 0.2416\n",
      "Epoch [3/5], Step [1450/1685], Loss: 0.0029\n",
      "Epoch [3/5], Step [1460/1685], Loss: 0.0503\n",
      "Epoch [3/5], Step [1470/1685], Loss: 0.0156\n",
      "Epoch [3/5], Step [1480/1685], Loss: 0.2753\n",
      "Epoch [3/5], Step [1490/1685], Loss: 0.0405\n",
      "Epoch [3/5], Step [1500/1685], Loss: 0.0690\n",
      "Epoch [3/5], Step [1510/1685], Loss: 0.0028\n",
      "Epoch [3/5], Step [1520/1685], Loss: 0.2562\n",
      "Epoch [3/5], Step [1530/1685], Loss: 0.0626\n",
      "Epoch [3/5], Step [1540/1685], Loss: 0.3101\n",
      "Epoch [3/5], Step [1550/1685], Loss: 0.0349\n",
      "Epoch [3/5], Step [1560/1685], Loss: 0.4724\n",
      "Epoch [3/5], Step [1570/1685], Loss: 0.0151\n",
      "Epoch [3/5], Step [1580/1685], Loss: 0.0094\n",
      "Epoch [3/5], Step [1590/1685], Loss: 0.2764\n",
      "Epoch [3/5], Step [1600/1685], Loss: 0.1664\n",
      "Epoch [3/5], Step [1610/1685], Loss: 0.1630\n",
      "Epoch [3/5], Step [1620/1685], Loss: 0.4682\n",
      "Epoch [3/5], Step [1630/1685], Loss: 0.0120\n",
      "Epoch [3/5], Step [1640/1685], Loss: 0.0112\n",
      "Epoch [3/5], Step [1650/1685], Loss: 0.0074\n",
      "Epoch [3/5], Step [1660/1685], Loss: 0.0543\n",
      "Epoch [3/5], Step [1670/1685], Loss: 0.0081\n",
      "Epoch [3/5], Step [1680/1685], Loss: 0.2403\n",
      "Epoch [4/5], Step [10/1685], Loss: 0.1814\n",
      "Epoch [4/5], Step [20/1685], Loss: 0.0028\n",
      "Epoch [4/5], Step [30/1685], Loss: 0.1770\n",
      "Epoch [4/5], Step [40/1685], Loss: 0.0138\n",
      "Epoch [4/5], Step [50/1685], Loss: 0.0097\n",
      "Epoch [4/5], Step [60/1685], Loss: 0.0519\n",
      "Epoch [4/5], Step [70/1685], Loss: 0.2908\n",
      "Epoch [4/5], Step [80/1685], Loss: 0.0613\n",
      "Epoch [4/5], Step [90/1685], Loss: 0.4412\n",
      "Epoch [4/5], Step [100/1685], Loss: 0.1241\n",
      "Epoch [4/5], Step [110/1685], Loss: 0.2385\n",
      "Epoch [4/5], Step [120/1685], Loss: 0.0018\n",
      "Epoch [4/5], Step [130/1685], Loss: 0.0293\n",
      "Epoch [4/5], Step [140/1685], Loss: 0.0155\n",
      "Epoch [4/5], Step [150/1685], Loss: 0.0012\n",
      "Epoch [4/5], Step [160/1685], Loss: 0.0020\n",
      "Epoch [4/5], Step [170/1685], Loss: 0.4629\n",
      "Epoch [4/5], Step [180/1685], Loss: 0.0373\n",
      "Epoch [4/5], Step [190/1685], Loss: 0.0907\n",
      "Epoch [4/5], Step [200/1685], Loss: 0.0715\n",
      "Epoch [4/5], Step [210/1685], Loss: 0.0797\n",
      "Epoch [4/5], Step [220/1685], Loss: 0.0042\n",
      "Epoch [4/5], Step [230/1685], Loss: 0.0663\n",
      "Epoch [4/5], Step [240/1685], Loss: 0.1409\n",
      "Epoch [4/5], Step [250/1685], Loss: 0.0120\n",
      "Epoch [4/5], Step [260/1685], Loss: 0.0152\n",
      "Epoch [4/5], Step [270/1685], Loss: 0.1386\n",
      "Epoch [4/5], Step [280/1685], Loss: 0.1922\n",
      "Epoch [4/5], Step [290/1685], Loss: 0.3647\n",
      "Epoch [4/5], Step [300/1685], Loss: 0.0632\n",
      "Epoch [4/5], Step [310/1685], Loss: 0.0043\n",
      "Epoch [4/5], Step [320/1685], Loss: 0.0039\n",
      "Epoch [4/5], Step [330/1685], Loss: 0.0926\n",
      "Epoch [4/5], Step [340/1685], Loss: 0.0061\n",
      "Epoch [4/5], Step [350/1685], Loss: 0.0252\n",
      "Epoch [4/5], Step [360/1685], Loss: 0.0081\n",
      "Epoch [4/5], Step [370/1685], Loss: 0.4209\n",
      "Epoch [4/5], Step [380/1685], Loss: 0.0776\n",
      "Epoch [4/5], Step [390/1685], Loss: 0.0152\n",
      "Epoch [4/5], Step [400/1685], Loss: 0.0730\n",
      "Epoch [4/5], Step [410/1685], Loss: 0.1847\n",
      "Epoch [4/5], Step [420/1685], Loss: 0.0511\n",
      "Epoch [4/5], Step [430/1685], Loss: 0.0630\n",
      "Epoch [4/5], Step [440/1685], Loss: 0.0854\n",
      "Epoch [4/5], Step [450/1685], Loss: 0.2507\n",
      "Epoch [4/5], Step [460/1685], Loss: 0.0086\n",
      "Epoch [4/5], Step [470/1685], Loss: 0.0200\n",
      "Epoch [4/5], Step [480/1685], Loss: 0.0524\n",
      "Epoch [4/5], Step [490/1685], Loss: 0.0114\n",
      "Epoch [4/5], Step [500/1685], Loss: 0.0652\n",
      "Epoch [4/5], Step [510/1685], Loss: 0.0205\n",
      "Epoch [4/5], Step [520/1685], Loss: 0.0152\n",
      "Epoch [4/5], Step [530/1685], Loss: 0.4063\n",
      "Epoch [4/5], Step [540/1685], Loss: 0.0151\n",
      "Epoch [4/5], Step [550/1685], Loss: 0.1994\n",
      "Epoch [4/5], Step [560/1685], Loss: 0.0138\n",
      "Epoch [4/5], Step [570/1685], Loss: 0.0879\n",
      "Epoch [4/5], Step [580/1685], Loss: 0.0820\n",
      "Epoch [4/5], Step [590/1685], Loss: 0.1163\n",
      "Epoch [4/5], Step [600/1685], Loss: 0.1645\n",
      "Epoch [4/5], Step [610/1685], Loss: 0.0254\n",
      "Epoch [4/5], Step [620/1685], Loss: 0.0050\n",
      "Epoch [4/5], Step [630/1685], Loss: 0.1209\n",
      "Epoch [4/5], Step [640/1685], Loss: 0.0105\n",
      "Epoch [4/5], Step [650/1685], Loss: 0.0900\n",
      "Epoch [4/5], Step [660/1685], Loss: 0.1539\n",
      "Epoch [4/5], Step [670/1685], Loss: 0.0044\n",
      "Epoch [4/5], Step [680/1685], Loss: 0.0474\n",
      "Epoch [4/5], Step [690/1685], Loss: 0.0051\n",
      "Epoch [4/5], Step [700/1685], Loss: 0.0109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Step [710/1685], Loss: 0.1421\n",
      "Epoch [4/5], Step [720/1685], Loss: 0.2889\n",
      "Epoch [4/5], Step [730/1685], Loss: 0.3887\n",
      "Epoch [4/5], Step [740/1685], Loss: 0.0047\n",
      "Epoch [4/5], Step [750/1685], Loss: 0.0514\n",
      "Epoch [4/5], Step [760/1685], Loss: 0.0054\n",
      "Epoch [4/5], Step [770/1685], Loss: 0.4155\n",
      "Epoch [4/5], Step [780/1685], Loss: 0.0059\n",
      "Epoch [4/5], Step [790/1685], Loss: 0.0735\n",
      "Epoch [4/5], Step [800/1685], Loss: 0.2480\n",
      "Epoch [4/5], Step [810/1685], Loss: 0.1075\n",
      "Epoch [4/5], Step [820/1685], Loss: 0.3773\n",
      "Epoch [4/5], Step [830/1685], Loss: 0.1943\n",
      "Epoch [4/5], Step [840/1685], Loss: 0.1009\n",
      "Epoch [4/5], Step [850/1685], Loss: 0.0090\n",
      "Epoch [4/5], Step [860/1685], Loss: 0.2542\n",
      "Epoch [4/5], Step [870/1685], Loss: 0.2200\n",
      "Epoch [4/5], Step [880/1685], Loss: 0.0050\n",
      "Epoch [4/5], Step [890/1685], Loss: 0.0537\n",
      "Epoch [4/5], Step [900/1685], Loss: 0.2215\n",
      "Epoch [4/5], Step [910/1685], Loss: 0.0548\n",
      "Epoch [4/5], Step [920/1685], Loss: 0.0146\n",
      "Epoch [4/5], Step [930/1685], Loss: 0.1439\n",
      "Epoch [4/5], Step [940/1685], Loss: 0.0071\n",
      "Epoch [4/5], Step [950/1685], Loss: 0.2138\n",
      "Epoch [4/5], Step [960/1685], Loss: 0.4744\n",
      "Epoch [4/5], Step [970/1685], Loss: 0.0555\n",
      "Epoch [4/5], Step [980/1685], Loss: 0.4176\n",
      "Epoch [4/5], Step [990/1685], Loss: 0.2589\n",
      "Epoch [4/5], Step [1000/1685], Loss: 0.0045\n",
      "Epoch [4/5], Step [1010/1685], Loss: 0.0475\n",
      "Epoch [4/5], Step [1020/1685], Loss: 0.0079\n",
      "Epoch [4/5], Step [1030/1685], Loss: 0.0264\n",
      "Epoch [4/5], Step [1040/1685], Loss: 0.0301\n",
      "Epoch [4/5], Step [1050/1685], Loss: 0.0070\n",
      "Epoch [4/5], Step [1060/1685], Loss: 0.1246\n",
      "Epoch [4/5], Step [1070/1685], Loss: 0.0091\n",
      "Epoch [4/5], Step [1080/1685], Loss: 0.1164\n",
      "Epoch [4/5], Step [1090/1685], Loss: 0.0158\n",
      "Epoch [4/5], Step [1100/1685], Loss: 0.4454\n",
      "Epoch [4/5], Step [1110/1685], Loss: 0.1859\n",
      "Epoch [4/5], Step [1120/1685], Loss: 0.0891\n",
      "Epoch [4/5], Step [1130/1685], Loss: 0.0658\n",
      "Epoch [4/5], Step [1140/1685], Loss: 0.2849\n",
      "Epoch [4/5], Step [1150/1685], Loss: 0.0054\n",
      "Epoch [4/5], Step [1160/1685], Loss: 0.6439\n",
      "Epoch [4/5], Step [1170/1685], Loss: 0.3447\n",
      "Epoch [4/5], Step [1180/1685], Loss: 0.0230\n",
      "Epoch [4/5], Step [1190/1685], Loss: 0.0011\n",
      "Epoch [4/5], Step [1200/1685], Loss: 0.0392\n",
      "Epoch [4/5], Step [1210/1685], Loss: 0.0346\n",
      "Epoch [4/5], Step [1220/1685], Loss: 0.2433\n",
      "Epoch [4/5], Step [1230/1685], Loss: 0.0460\n",
      "Epoch [4/5], Step [1240/1685], Loss: 0.0176\n",
      "Epoch [4/5], Step [1250/1685], Loss: 0.7214\n",
      "Epoch [4/5], Step [1260/1685], Loss: 0.0092\n",
      "Epoch [4/5], Step [1270/1685], Loss: 0.0044\n",
      "Epoch [4/5], Step [1280/1685], Loss: 0.0014\n",
      "Epoch [4/5], Step [1290/1685], Loss: 0.0169\n",
      "Epoch [4/5], Step [1300/1685], Loss: 0.0261\n",
      "Epoch [4/5], Step [1310/1685], Loss: 0.0013\n",
      "Epoch [4/5], Step [1320/1685], Loss: 1.1182\n",
      "Epoch [4/5], Step [1330/1685], Loss: 0.4974\n",
      "Epoch [4/5], Step [1340/1685], Loss: 0.0650\n",
      "Epoch [4/5], Step [1350/1685], Loss: 0.0129\n",
      "Epoch [4/5], Step [1360/1685], Loss: 0.1221\n",
      "Epoch [4/5], Step [1370/1685], Loss: 0.0022\n",
      "Epoch [4/5], Step [1380/1685], Loss: 0.0167\n",
      "Epoch [4/5], Step [1390/1685], Loss: 0.2006\n",
      "Epoch [4/5], Step [1400/1685], Loss: 0.0125\n",
      "Epoch [4/5], Step [1410/1685], Loss: 0.2086\n",
      "Epoch [4/5], Step [1420/1685], Loss: 0.0400\n",
      "Epoch [4/5], Step [1430/1685], Loss: 0.0201\n",
      "Epoch [4/5], Step [1440/1685], Loss: 0.4144\n",
      "Epoch [4/5], Step [1450/1685], Loss: 0.2367\n",
      "Epoch [4/5], Step [1460/1685], Loss: 0.5790\n",
      "Epoch [4/5], Step [1470/1685], Loss: 0.0281\n",
      "Epoch [4/5], Step [1480/1685], Loss: 0.4366\n",
      "Epoch [4/5], Step [1490/1685], Loss: 0.0810\n",
      "Epoch [4/5], Step [1500/1685], Loss: 0.1546\n",
      "Epoch [4/5], Step [1510/1685], Loss: 0.0022\n",
      "Epoch [4/5], Step [1520/1685], Loss: 0.0784\n",
      "Epoch [4/5], Step [1530/1685], Loss: 0.0160\n",
      "Epoch [4/5], Step [1540/1685], Loss: 0.2361\n",
      "Epoch [4/5], Step [1550/1685], Loss: 0.0303\n",
      "Epoch [4/5], Step [1560/1685], Loss: 0.1694\n",
      "Epoch [4/5], Step [1570/1685], Loss: 0.0549\n",
      "Epoch [4/5], Step [1580/1685], Loss: 0.1894\n",
      "Epoch [4/5], Step [1590/1685], Loss: 0.0022\n",
      "Epoch [4/5], Step [1600/1685], Loss: 0.0703\n",
      "Epoch [4/5], Step [1610/1685], Loss: 0.1058\n",
      "Epoch [4/5], Step [1620/1685], Loss: 0.0229\n",
      "Epoch [4/5], Step [1630/1685], Loss: 0.1279\n",
      "Epoch [4/5], Step [1640/1685], Loss: 0.2695\n",
      "Epoch [4/5], Step [1650/1685], Loss: 0.0069\n",
      "Epoch [4/5], Step [1660/1685], Loss: 0.0483\n",
      "Epoch [4/5], Step [1670/1685], Loss: 0.1177\n",
      "Epoch [4/5], Step [1680/1685], Loss: 0.0031\n",
      "Epoch [5/5], Step [10/1685], Loss: 0.0069\n",
      "Epoch [5/5], Step [20/1685], Loss: 0.0019\n",
      "Epoch [5/5], Step [30/1685], Loss: 0.1280\n",
      "Epoch [5/5], Step [40/1685], Loss: 0.1715\n",
      "Epoch [5/5], Step [50/1685], Loss: 0.0022\n",
      "Epoch [5/5], Step [60/1685], Loss: 0.0274\n",
      "Epoch [5/5], Step [70/1685], Loss: 0.5613\n",
      "Epoch [5/5], Step [80/1685], Loss: 0.0042\n",
      "Epoch [5/5], Step [90/1685], Loss: 0.0509\n",
      "Epoch [5/5], Step [100/1685], Loss: 0.0197\n",
      "Epoch [5/5], Step [110/1685], Loss: 0.6445\n",
      "Epoch [5/5], Step [120/1685], Loss: 0.0084\n",
      "Epoch [5/5], Step [130/1685], Loss: 0.3435\n",
      "Epoch [5/5], Step [140/1685], Loss: 0.0064\n",
      "Epoch [5/5], Step [150/1685], Loss: 0.0767\n",
      "Epoch [5/5], Step [160/1685], Loss: 0.2398\n",
      "Epoch [5/5], Step [170/1685], Loss: 0.2864\n",
      "Epoch [5/5], Step [180/1685], Loss: 0.2854\n",
      "Epoch [5/5], Step [190/1685], Loss: 0.1041\n",
      "Epoch [5/5], Step [200/1685], Loss: 0.0936\n",
      "Epoch [5/5], Step [210/1685], Loss: 0.0266\n",
      "Epoch [5/5], Step [220/1685], Loss: 0.0729\n",
      "Epoch [5/5], Step [230/1685], Loss: 0.0205\n",
      "Epoch [5/5], Step [240/1685], Loss: 0.0423\n",
      "Epoch [5/5], Step [250/1685], Loss: 0.0032\n",
      "Epoch [5/5], Step [260/1685], Loss: 0.0387\n",
      "Epoch [5/5], Step [270/1685], Loss: 0.0275\n",
      "Epoch [5/5], Step [280/1685], Loss: 0.1709\n",
      "Epoch [5/5], Step [290/1685], Loss: 0.0115\n",
      "Epoch [5/5], Step [300/1685], Loss: 0.3109\n",
      "Epoch [5/5], Step [310/1685], Loss: 0.0133\n",
      "Epoch [5/5], Step [320/1685], Loss: 0.0124\n",
      "Epoch [5/5], Step [330/1685], Loss: 0.3003\n",
      "Epoch [5/5], Step [340/1685], Loss: 0.0135\n",
      "Epoch [5/5], Step [350/1685], Loss: 0.0041\n",
      "Epoch [5/5], Step [360/1685], Loss: 0.0068\n",
      "Epoch [5/5], Step [370/1685], Loss: 0.1365\n",
      "Epoch [5/5], Step [380/1685], Loss: 0.0054\n",
      "Epoch [5/5], Step [390/1685], Loss: 0.0225\n",
      "Epoch [5/5], Step [400/1685], Loss: 0.0378\n",
      "Epoch [5/5], Step [410/1685], Loss: 0.0646\n",
      "Epoch [5/5], Step [420/1685], Loss: 0.1154\n",
      "Epoch [5/5], Step [430/1685], Loss: 0.0204\n",
      "Epoch [5/5], Step [440/1685], Loss: 0.0061\n",
      "Epoch [5/5], Step [450/1685], Loss: 0.0051\n",
      "Epoch [5/5], Step [460/1685], Loss: 0.0851\n",
      "Epoch [5/5], Step [470/1685], Loss: 0.0138\n",
      "Epoch [5/5], Step [480/1685], Loss: 0.0120\n",
      "Epoch [5/5], Step [490/1685], Loss: 0.2718\n",
      "Epoch [5/5], Step [500/1685], Loss: 0.0748\n",
      "Epoch [5/5], Step [510/1685], Loss: 0.1859\n",
      "Epoch [5/5], Step [520/1685], Loss: 0.0028\n",
      "Epoch [5/5], Step [530/1685], Loss: 0.0049\n",
      "Epoch [5/5], Step [540/1685], Loss: 0.0289\n",
      "Epoch [5/5], Step [550/1685], Loss: 0.0461\n",
      "Epoch [5/5], Step [560/1685], Loss: 0.1902\n",
      "Epoch [5/5], Step [570/1685], Loss: 0.0105\n",
      "Epoch [5/5], Step [580/1685], Loss: 0.2274\n",
      "Epoch [5/5], Step [590/1685], Loss: 0.3372\n",
      "Epoch [5/5], Step [600/1685], Loss: 0.0070\n",
      "Epoch [5/5], Step [610/1685], Loss: 0.3082\n",
      "Epoch [5/5], Step [620/1685], Loss: 0.0353\n",
      "Epoch [5/5], Step [630/1685], Loss: 0.0281\n",
      "Epoch [5/5], Step [640/1685], Loss: 0.0279\n",
      "Epoch [5/5], Step [650/1685], Loss: 0.4147\n",
      "Epoch [5/5], Step [660/1685], Loss: 0.2991\n",
      "Epoch [5/5], Step [670/1685], Loss: 0.0002\n",
      "Epoch [5/5], Step [680/1685], Loss: 0.0646\n",
      "Epoch [5/5], Step [690/1685], Loss: 0.0012\n",
      "Epoch [5/5], Step [700/1685], Loss: 0.0492\n",
      "Epoch [5/5], Step [710/1685], Loss: 0.0164\n",
      "Epoch [5/5], Step [720/1685], Loss: 0.0024\n",
      "Epoch [5/5], Step [730/1685], Loss: 0.1043\n",
      "Epoch [5/5], Step [740/1685], Loss: 0.0644\n",
      "Epoch [5/5], Step [750/1685], Loss: 0.0455\n",
      "Epoch [5/5], Step [760/1685], Loss: 0.1530\n",
      "Epoch [5/5], Step [770/1685], Loss: 0.0711\n",
      "Epoch [5/5], Step [780/1685], Loss: 0.0748\n",
      "Epoch [5/5], Step [790/1685], Loss: 0.0022\n",
      "Epoch [5/5], Step [800/1685], Loss: 0.0225\n",
      "Epoch [5/5], Step [810/1685], Loss: 0.0909\n",
      "Epoch [5/5], Step [820/1685], Loss: 0.0368\n",
      "Epoch [5/5], Step [830/1685], Loss: 0.0336\n",
      "Epoch [5/5], Step [840/1685], Loss: 0.2913\n",
      "Epoch [5/5], Step [850/1685], Loss: 0.0035\n",
      "Epoch [5/5], Step [860/1685], Loss: 0.2820\n",
      "Epoch [5/5], Step [870/1685], Loss: 0.1200\n",
      "Epoch [5/5], Step [880/1685], Loss: 0.0103\n",
      "Epoch [5/5], Step [890/1685], Loss: 0.0040\n",
      "Epoch [5/5], Step [900/1685], Loss: 0.1348\n",
      "Epoch [5/5], Step [910/1685], Loss: 0.0800\n",
      "Epoch [5/5], Step [920/1685], Loss: 0.0078\n",
      "Epoch [5/5], Step [930/1685], Loss: 0.0837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Step [940/1685], Loss: 0.0031\n",
      "Epoch [5/5], Step [950/1685], Loss: 0.3535\n",
      "Epoch [5/5], Step [960/1685], Loss: 0.0018\n",
      "Epoch [5/5], Step [970/1685], Loss: 0.0016\n",
      "Epoch [5/5], Step [980/1685], Loss: 0.0043\n",
      "Epoch [5/5], Step [990/1685], Loss: 0.0581\n",
      "Epoch [5/5], Step [1000/1685], Loss: 0.0726\n",
      "Epoch [5/5], Step [1010/1685], Loss: 0.0016\n",
      "Epoch [5/5], Step [1020/1685], Loss: 0.0069\n",
      "Epoch [5/5], Step [1030/1685], Loss: 0.0140\n",
      "Epoch [5/5], Step [1040/1685], Loss: 0.0099\n",
      "Epoch [5/5], Step [1050/1685], Loss: 0.0108\n",
      "Epoch [5/5], Step [1060/1685], Loss: 0.0095\n",
      "Epoch [5/5], Step [1070/1685], Loss: 0.0106\n",
      "Epoch [5/5], Step [1080/1685], Loss: 0.0017\n",
      "Epoch [5/5], Step [1090/1685], Loss: 0.0101\n",
      "Epoch [5/5], Step [1100/1685], Loss: 0.0067\n",
      "Epoch [5/5], Step [1110/1685], Loss: 0.0579\n",
      "Epoch [5/5], Step [1120/1685], Loss: 0.0157\n",
      "Epoch [5/5], Step [1130/1685], Loss: 0.0089\n",
      "Epoch [5/5], Step [1140/1685], Loss: 0.0797\n",
      "Epoch [5/5], Step [1150/1685], Loss: 0.2478\n",
      "Epoch [5/5], Step [1160/1685], Loss: 0.1540\n",
      "Epoch [5/5], Step [1170/1685], Loss: 0.1214\n",
      "Epoch [5/5], Step [1180/1685], Loss: 0.0237\n",
      "Epoch [5/5], Step [1190/1685], Loss: 0.1191\n",
      "Epoch [5/5], Step [1200/1685], Loss: 0.0242\n",
      "Epoch [5/5], Step [1210/1685], Loss: 0.0110\n",
      "Epoch [5/5], Step [1220/1685], Loss: 0.0241\n",
      "Epoch [5/5], Step [1230/1685], Loss: 0.0250\n",
      "Epoch [5/5], Step [1240/1685], Loss: 0.2612\n",
      "Epoch [5/5], Step [1250/1685], Loss: 0.0087\n",
      "Epoch [5/5], Step [1260/1685], Loss: 0.0875\n",
      "Epoch [5/5], Step [1270/1685], Loss: 0.0120\n",
      "Epoch [5/5], Step [1280/1685], Loss: 0.1913\n",
      "Epoch [5/5], Step [1290/1685], Loss: 0.0312\n",
      "Epoch [5/5], Step [1300/1685], Loss: 0.2504\n",
      "Epoch [5/5], Step [1310/1685], Loss: 0.1090\n",
      "Epoch [5/5], Step [1320/1685], Loss: 0.0153\n",
      "Epoch [5/5], Step [1330/1685], Loss: 0.0433\n",
      "Epoch [5/5], Step [1340/1685], Loss: 0.0082\n",
      "Epoch [5/5], Step [1350/1685], Loss: 0.2732\n",
      "Epoch [5/5], Step [1360/1685], Loss: 0.0026\n",
      "Epoch [5/5], Step [1370/1685], Loss: 0.1641\n",
      "Epoch [5/5], Step [1380/1685], Loss: 0.2630\n",
      "Epoch [5/5], Step [1390/1685], Loss: 0.1025\n",
      "Epoch [5/5], Step [1400/1685], Loss: 0.0573\n",
      "Epoch [5/5], Step [1410/1685], Loss: 0.0470\n",
      "Epoch [5/5], Step [1420/1685], Loss: 0.0478\n",
      "Epoch [5/5], Step [1430/1685], Loss: 0.0106\n",
      "Epoch [5/5], Step [1440/1685], Loss: 0.0967\n",
      "Epoch [5/5], Step [1450/1685], Loss: 0.0094\n",
      "Epoch [5/5], Step [1460/1685], Loss: 0.0568\n",
      "Epoch [5/5], Step [1470/1685], Loss: 0.0061\n",
      "Epoch [5/5], Step [1480/1685], Loss: 0.0098\n",
      "Epoch [5/5], Step [1490/1685], Loss: 0.0070\n",
      "Epoch [5/5], Step [1500/1685], Loss: 0.7626\n",
      "Epoch [5/5], Step [1510/1685], Loss: 0.5234\n",
      "Epoch [5/5], Step [1520/1685], Loss: 0.0424\n",
      "Epoch [5/5], Step [1530/1685], Loss: 0.1021\n",
      "Epoch [5/5], Step [1540/1685], Loss: 0.0013\n",
      "Epoch [5/5], Step [1550/1685], Loss: 0.0264\n",
      "Epoch [5/5], Step [1560/1685], Loss: 0.0720\n",
      "Epoch [5/5], Step [1570/1685], Loss: 0.0216\n",
      "Epoch [5/5], Step [1580/1685], Loss: 0.1918\n",
      "Epoch [5/5], Step [1590/1685], Loss: 0.0315\n",
      "Epoch [5/5], Step [1600/1685], Loss: 0.0653\n",
      "Epoch [5/5], Step [1610/1685], Loss: 0.0334\n",
      "Epoch [5/5], Step [1620/1685], Loss: 0.0533\n",
      "Epoch [5/5], Step [1630/1685], Loss: 0.0242\n",
      "Epoch [5/5], Step [1640/1685], Loss: 0.2509\n",
      "Epoch [5/5], Step [1650/1685], Loss: 0.5962\n",
      "Epoch [5/5], Step [1660/1685], Loss: 0.4459\n",
      "Epoch [5/5], Step [1670/1685], Loss: 0.1797\n",
      "Epoch [5/5], Step [1680/1685], Loss: 0.0722\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59ec0b6e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.9 s, sys: 532 ms, total: 38.4 s\n",
      "Wall time: 37.8 s\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "%time train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 1870 test images: 92 %\n",
      "CPU times: user 476 ms, sys: 4 ms, total: 480 ms\n",
      "Wall time: 480 ms\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "%time test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
